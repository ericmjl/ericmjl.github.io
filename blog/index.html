<!doctype html>

<head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="utf-8">
    <!-- Bootstrap v4beta Imports -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">

    <style media="screen">
        body {
            padding-top: 70px;
            padding-bottom: 70px;
        }
    </style>

    <!-- Syntax Highlighter. Use both pygments and hl.js. -->
    <link rel="stylesheet" href="../static/pygments.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <!-- Google Analytics -->
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-12498603-2', 'auto');
        ga('send', 'pageview');
    </script>

    <!-- ClustrMaps Tracking -->
    <!-- <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=nhKoDpoTjWz4pC6CwI-fSy4hPoJ1uXwTLCfMCT3OK_8"></script> -->

    <!-- FontAwesome embed -->
    <script src="https://use.fontawesome.com/cb9dbe8e41.js"></script>
</head>

<title>Blog - Eric J. Ma's Personal Site</title>
<body class="body">
    <nav class="navbar navbar-expand-sm navbar-light fixed-top bg-light">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#local-links" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="local-links">
            <ul class="navbar-nav mr-auto">
                
                <li class="nav-item">
                    <a class="nav-link" href="/"><i class="fa fa-home" aria-hidden="true"></i> Home</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/resume"><i class="fa fa-file-text" aria-hidden="true"></i> Resume</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/blog"><i class="fa fa-rss" aria-hidden="true"></i> Blog</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/open-source"><i class="fa fa-code" aria-hidden="true"></i> Open Source</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/projects"><i class="fa fa-briefcase" aria-hidden="true"></i> Projects</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/talks"><i class="fa fa-microphone" aria-hidden="true"></i> Talks</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/teaching"><i class="fa fa-university" aria-hidden="true"></i> Teaching</a>
                </li>
                
            </ul>
        </div>
    </nav>
    <div class="container">
        
  
    
  <!-- <div class="blog-post"> -->
  <link rel="stylesheet" href="../static/pygments.css">
  <div>
  
    <h1><a href="../blog/2017/10/31/always-check-your-data/">Always Check Your Data</a></h1>
  
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2017-10-31
  </p>
  <p>True story, just happened today. I was trying to fit a Poisson likelihood to estimate event cycle times (in discreet weeks). For certain columns, everything went perfectly fine. Yet for other columns, I was getting negative infinityâ€™s likelihoods, and was banging my head over this problem for over an hour and a half.</p>
<p>As things turned out, those columns that gave me negative infinity likelihood initializations were doing so because of negative values in the data. Try fitting a Poisson likelihood, which only has positive support, on that!</p>
<p><img src="https://i.imgflip.com/1yl6ki.jpg" alt=""></p>
<p>This lost hour and a half was a good lesson in data checking/testing: <strong>always be sure to sanity check basic stats associated with the data - bounds (min/max), central tendency (mean/median/mode) and spread (variance, quartile range) - always check!</strong></p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../blog/2017/10/31/always-check-your-data/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- <div class="blog-post"> -->
  <link rel="stylesheet" href="../static/pygments.css">
  <div>
  
    <h1><a href="../blog/2017/10/27/random-forests-a-good-default-model/">Random Forests: A Good Default Model?</a></h1>
  
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2017-10-27
  </p>
  <p>I've been giving this some thought, and wanted to go out on a limb to put forth this idea:</p>
<p><strong>I think Random Forests (RF) are a good "baseline" model to try, after establishing a "random" baseline case.</strong></p>
<p>(Clarification: I'm using RF as a shorthand for "forest-based ML algorithms", including XGBoost etc.)</p>
<p>Before I go on, let me first provide some setup.</p>
<p>Let's say we have a two-class classification problem. Assume everything is balanced. One "dumb baseline"" case is a coin flip. The other "dumb baseline" is predicting everything to be one class. Once we have these established, we can go to a "baseline" machine learning model.</p>
<p>Usually, people might say, "go do logistic regression (LR)" as your first baseline model for classification problems. It sure is a principled choice! Logistic regression is geared towards classification problems, makes only linear assumptions about the data, and identifies directional effects as well. From a practical perspective, it's also very fast to train.</p>
<p>But I've found myself more and more being oriented towards using RFs as my baseline model instead of logistic regression. Here are my reasons:</p>
<ol>
<li>Practically speaking, any modern computer can train a RF model with ~1000+ trees in not much more time than it would need for an LR model.</li>
<li>By using RFs, we do not make linearity assumptions about the data.</li>
<li>Additionally, we don't have to scale the data (one less thing to do).</li>
<li>RFs will automatically learn non-linear interaction terms in the data, which is not possible without further feature engineering in LR.</li>
<li>As such, the out-of-the-box performance using large RFs with default settings is often very good, making for a much more intellectually interesting challenge in trying to beat that classifier.</li>
<li>With <code>scikit-learn</code>, it's a one-liner change to swap out LR for RF. The API is what matters, and as such, drop-in replacements are easily implemented!</li>
</ol>
<p>Just to be clear, I'm not advocating for throwing away logistic regression altogether. There are moments where interpretability is needed, and is more easily done by using LR. In those cases, LR can be the "baseline model", or even just back-filled in after training the baseline RF model for comparison.</p>
<p>Random Forests were the darling of the machine learning world before neural networks came along, and even now, remain the tool-of-choice for colleagues in the cheminformatics world. Given how easy they are to use now, why not just start with them?</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../blog/2017/10/27/random-forests-a-good-default-model/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- <div class="blog-post"> -->
  <link rel="stylesheet" href="../static/pygments.css">
  <div>
  
    <h1><a href="../blog/2017/10/11/pypy-impressive/">PyPy: Impressive!</a></h1>
  
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2017-10-11
  </p>
  <p>A few years on after trying out PyPy for the first time and wrestling with it, I still find it to be pretty awesome.</p>
<p>Now that PyPy officially supports <code>numpy</code>, I'm going to profile a few simple statistical simulation tasks:</p>
<ul>
<li>Computing the mean of a number of random number draws.</li>
<li>Simulating many coin flips</li>
</ul>
<p>I'll profile each of the tasks four ways:</p>
<ul>
<li>Pure Python implementation running from the CPython and PyPy interpreters</li>
<li><code>numpy</code> implementation running from the CPython and PyPy interpreters.</li>
</ul>
<p>So, how do PyPy and CPython fare? Let's show the results up front first.</p>
<p><a href="../blog/2017/10/11/pypy-impressive/profile.png"><img src="../blog/2017/10/11/pypy-impressive/profile-sm.png" alt="Profiling results."></a></p>
<p>Click on the image to view a higher resolution chart. The raw recorded measurements can be found <a href="https://docs.google.com/spreadsheets/d/1QB1hF7Z8SGYjvll8sYCjVYEYAgzL4pjqGt1dbO6B2Co/edit?usp=sharing">on Google Sheets</a>.</p>
<p>Here's a description of what's happening:</p>
<ul>
<li>(top-left): PyPy is approx. 10X faster than CPython at computing the mean of 10 million random numbers.</li>
<li>(top-right): When both are running <code>numpy</code>, the speed is identical.</li>
<li>(bottom-left): When simulating coin flips, PyPy with a custom <code>binomial()</code> function is about 3X faster than CPython.</li>
<li>(bottom-right): When using <code>numpy</code> instead, there is a bottleneck, and PyPy fails badly compared to CPython.</li>
</ul>
<p>It's pretty clear that when PyPy is dealing with "pure" data (i.e. not having to pass data between Python and C), PyPy runs very, very fast, and, at least in the scenarios tested here, it performs faster than the CPython interpreter. This is consistent with my previous observations, and probably explains why PyPy is very good for code that is very repetitive; the JIT tracer really speeds things up.</p>
<p>That last plot (bottom-right) is a big curiosity. Using the code below, I measured the random number generation is actually just as fast as it should be using CPython, but that PyPy failed badly when I was passing in a <code>numpy</code> array to the <code>Counter()</code> object (from the standard library). I'm not sure what is happening behind-the-scenes, but I have reached out to the PyPy developers to ask what's going on, and will update this post at a later date.</p>
<p><strong>UPDATE:</strong> I heard back from the PyPy devs <a href="https://bitbucket.org/pypy/pypy/issues/2680/slow-speed-going-from-numpy-data-structure">on BitBucket</a>, and this is indeed explainable by data transfer between the C-to-PyPy interface. It's probably parallel to the latency that arises from transferring data between the CPU and GPU, or between compute nodes.</p>
<p>So, what does this mean? It means that for pure Python code, PyPy can be a very powerful way to accelerate your code. One example I can imagine is agent-based simulations using Python objects. Another example that comes to mind is running a web server that only ever deals with strings, floats and JSONs (in contrast to matrix-heavy scientific computing).</p>
<p>Now, for those who are curious, here's the source code for the <strong>pure Python implementation of the mean of random numbers</strong>.</p>
<div class="hll"><pre><span></span><span class="c1"># Mean of 10 million random number draws.</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">random</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="n">n</span> <span class="o">=</span> <span class="mf">1E7</span>
<span class="n">rnds</span> <span class="o">=</span> <span class="mi">0</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">n</span><span class="p">)):</span>
    <span class="n">rnds</span> <span class="o">+=</span> <span class="n">random</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="n">rnds</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s2">&quot;{} seconds&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
</pre></div>
<p>And here's the source code for the <strong><code>numpy</code> implementation of the mean of random numbers</strong>.</p>
<div class="hll"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1E7</span><span class="p">)))</span>
<span class="k">print</span><span class="p">(</span><span class="n">mean</span><span class="p">)</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;{} seconds&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
</pre></div>
<p>Next, here's the source code for <strong>coin flips in pure Python</strong>:</p>
<div class="hll"><pre><span></span><span class="c1"># Simulate 10 million biased coin flips with p = 0.3</span>
<span class="kn">from</span> <span class="nn">random</span> <span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">bernoulli</span><span class="p">(</span><span class="n">p</span><span class="p">):</span>
    <span class="n">rnd</span> <span class="o">=</span> <span class="n">random</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">rnd</span> <span class="o">&lt;</span> <span class="n">p</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">True</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">False</span>


<span class="n">p</span> <span class="o">=</span> <span class="mf">0.3</span>
<span class="n">result</span> <span class="o">=</span> <span class="p">[</span><span class="n">bernoulli</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="mf">1E7</span><span class="p">))]</span>
<span class="k">print</span><span class="p">(</span><span class="n">Counter</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;{} seconds&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
</pre></div>
<p>And finally, source code for <strong>coin flips using <code>numpy</code></strong>:</p>
<div class="hll"><pre><span></span><span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">binomial</span>
<span class="kn">from</span> <span class="nn">time</span> <span class="kn">import</span> <span class="n">time</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>

<span class="n">start</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="n">coinflips</span> <span class="o">=</span> <span class="n">binomial</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">int</span><span class="p">(</span><span class="mf">1E7</span><span class="p">))</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>
<span class="k">print</span><span class="p">(</span><span class="s1">&#39;Time for numpy coinflips: {} seconds&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>

<span class="k">print</span><span class="p">(</span><span class="n">Counter</span><span class="p">(</span><span class="n">coinflips</span><span class="p">))</span>
<span class="n">end</span> <span class="o">=</span> <span class="n">time</span><span class="p">()</span>

<span class="k">print</span><span class="p">(</span><span class="s1">&#39;{} seconds&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">end</span> <span class="o">-</span> <span class="n">start</span><span class="p">))</span>
</pre></div>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../blog/2017/10/11/pypy-impressive/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- <div class="blog-post"> -->
  <link rel="stylesheet" href="../static/pygments.css">
  <div>
  
    <h1><a href="../blog/2017/10/10/pydata-nyc-2017/">PyData NYC 2017</a></h1>
  
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2017-10-10
  </p>
  <p>I'm seriously looking forward to PyData NYC this year -- there's a great lineup of talks that I'm particularly looking forward to hearing! The theme for my set of must-see talks this year is "Bayesian machine learning" - there's much for me to learn!</p>
<p>The first is by my fellow Boston Bayesian <strong><a href="https://colindcarroll.com/">Colin Caroll</a></strong> with his talk titled <a href="https://pydata.org/nyc2017/schedule/presentation/12/">Two views on regression with PyMC3 and scikit-learn</a>. Colin is a mathematician at heart, even though he does software engineering for living now, and I can't wait to hear about regularization strategies!</p>
<p>The second is by <strong><a href="https://pydata.org/nyc2017/speaker/profile/6/">Nicole Carlson</a></strong>, with her talk titled <a href="https://pydata.org/nyc2017/schedule/presentation/24/">Turning PyMC3 into scikit-learn</a>. Nicole's talk is of interest to me because I've implemented models in PyMC3 before, and now would like to know how to make them reusable!</p>
<p>The third talk is by <strong><a href="https://pydata.org/nyc2017/speaker/profile/118/">Chaya Stern</a></strong>, with her talk titled <a href="https://pydata.org/nyc2017/schedule/presentation/53/">Bayesian inference in computational chemistry</a>. Super relevant to my work at Novartis!</p>
<p>The fourth is by my fellow Boston Pythonista <strong><a href="https://pydata.org/nyc2017/speaker/profile/34/">Joe Jevnik</a></strong>, who will be speaking on the first day about his journey into deep learning on some really cool time-series data. He works at Quantopian, BUT the spoiler here is that his talk is NOT about financial data! (I've heard his talk outline already.)</p>
<p>The fifth is a tutorial by <strong><a href="https://pydata.org/nyc2017/speaker/profile/29/">Jacob Schrieber</a></strong>, with his talk titled <a href="https://pydata.org/nyc2017/schedule/presentation/30/">pomegranate: fast and flexible probabilistic modeling in python</a>. <code>pomegranate</code>'s API models after the <code>scikit-learn</code>'s API; with the API being the user-facing interface, and <code>scikit-learn</code> being the <em>de facto</em> go-to library for machine learning, I'd be interested to see how much more <code>pomegranate</code> adds to the ecosystem, particularly w.r.t. Bayesian models.</p>
<p>There are a swathe of other good talks that I'm expecting to be able to catch online later on. <strong><a href="https://matthewrocklin.com/">Matt Rocklin</a></strong>, who is the lead developer of Dask, has done a ton of work on speeding Python up through parallelism. His talk will be on <a href="https://pydata.org/nyc2017/schedule/presentation/22/">the use of Cython & Dask to speed up GeoPandas</a>.</p>
<p>Also, <strong><a href="https://pydata.org/nyc2017/speaker/profile/80/">Thomas Caswell</a></strong>, one of the <a href="http://matplotlib.org/"><code>matplotlib</code></a> lead devs who helped guide my first foray into open source contributions, is giving a tutorial on <a href="https://pydata.org/nyc2017/schedule/presentation/3/">developing interactive figures in matplotlib</a>. Highly recommended if you're into the visualization world!</p>
<p>Finally, the always-interesting, always entertaining <strong><a href="https://pydata.org/nyc2017/schedule/presentation/25/">en zyme</a></strong> will be speaking on an <a href="https://pydata.org/nyc2017/schedule/presentation/25/">interesting topic</a>.</p>
<p>Looking forward to being at the conference, and meeting old and new friends there!</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../blog/2017/10/10/pydata-nyc-2017/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- <div class="blog-post"> -->
  <link rel="stylesheet" href="../static/pygments.css">
  <div>
  
    <h1><a href="../blog/2017/10/10/recursive-programming-and-dags/">Recursive Programming and DAGs</a></h1>
  
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2017-10-10
  </p>
  <p>Over the past few days, I've found myself using recursive programming to implement a "model specification" system with inheritance for deep learning. The goal here is to enable reproducible computational experiments for particular deep learning hyperparameter sets. Reproducibility is something I learned from the Software/Data Carpentry initiative, thus I wanted to ensure that my own work was reproducible, even if it's not (because of corporate reasons) open-able, because it's the right thing to do.</p>
<p>So, how do these "model spec" files work? I call them "experiment profiles", and they specify a bunch of things: <strong>model architecture</strong>, <strong>training parameters</strong>, and <strong>data tasks</strong>. These experiment profiles are stored in YAML files on disk. A profile essentially looks like the following (dummy examples provided, naturally):</p>
<div class="hll"><pre><span></span><span class="c1"># Name: default.yaml</span>
<span class="l l-Scalar l-Scalar-Plain">parent</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">null</span>
<span class="l l-Scalar l-Scalar-Plain">data_tasks</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">tasks</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="nv">task1</span><span class="p p-Indicator">,</span> <span class="nv">task2</span><span class="p p-Indicator">,</span> <span class="nv">task3</span><span class="p p-Indicator">]</span>
<span class="l l-Scalar l-Scalar-Plain">model_architecture</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">hidden_layers</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="nv">20</span><span class="p p-Indicator">,</span> <span class="nv">20</span><span class="p p-Indicator">,</span> <span class="nv">20</span><span class="p p-Indicator">]</span>
    <span class="l l-Scalar l-Scalar-Plain">hidden_dropouts</span><span class="p p-Indicator">:</span> <span class="p p-Indicator">[</span><span class="nv">0.1</span><span class="p p-Indicator">,</span> <span class="nv">0.2</span><span class="p p-Indicator">,</span> <span class="nv">0.3</span><span class="p p-Indicator">]</span>
<span class="l l-Scalar l-Scalar-Plain">training_parameters</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">optimizer</span><span class="p p-Indicator">:</span> <span class="s">&quot;sgd&quot;</span>
    <span class="l l-Scalar l-Scalar-Plain">optimizer_options</span><span class="p p-Indicator">:</span>
        <span class="l l-Scalar l-Scalar-Plain">n_epochs</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">20</span>
</pre></div>
<p>In this YAML file, the key-value pairs essentially match the API of the tooling I've built on top of Keras' API to make myself more productive. (From the example, it should be clear that we're dealing with only feed-forward neural networks and nothing else more complicated.) The key here (pun unintended) is that I have a <code>parent</code> key-value pair that specifies another experiment profile that I can inherit from.</p>
<p>Let's call the above example <code>default.yaml</code>. Let's say I want to run another computational experiment that uses the <code>adam</code> optimizer instead of plain vanilla <code>sgd</code>. Instead of re-specifying the entire YAML file, by implementing an inheritance scheme, I can re-specify only the optimizer and optimizer_options.</p>
<div class="hll"><pre><span></span><span class="c1"># Name: adam.yaml</span>
<span class="l l-Scalar l-Scalar-Plain">parent</span><span class="p p-Indicator">:</span> <span class="s">&quot;default.yaml&quot;</span>
<span class="l l-Scalar l-Scalar-Plain">training_parameters</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">optimizer</span><span class="p p-Indicator">:</span> <span class="s">&quot;adam&quot;</span>
</pre></div>
<p>Finally, let's say I find out that 20 epochs (inherited from <code>default.yaml</code>) is too much for Adam - after all, Adam is one of the most efficient gradient descent algorithms out there - and I want to change it to 3 epochs instead. I can do the following:</p>
<div class="hll"><pre><span></span><span class="c1"># Name: adam-3.yaml</span>
<span class="l l-Scalar l-Scalar-Plain">parent</span><span class="p p-Indicator">:</span> <span class="s">&quot;adam.yaml&quot;</span>
<span class="l l-Scalar l-Scalar-Plain">training_parameters</span><span class="p p-Indicator">:</span>
    <span class="l l-Scalar l-Scalar-Plain">optimizer_options</span><span class="p p-Indicator">:</span>
        <span class="l l-Scalar l-Scalar-Plain">n_epochs</span><span class="p p-Indicator">:</span> <span class="l l-Scalar l-Scalar-Plain">3</span>
</pre></div>
<p>Okay, so specifying YAML files with inheritance is all good, but how do I ensure that I get the entire parameter set out correctly, without writing verbose code? This is where the power of recursive programming comes in. Using recursion, I can solve this problem with <strong>a single function that calls itself on one condition, and returns a result on another condition</strong>. That's a recursive function in its essence.</p>
<p>The core of this problem is traversing the inheritance path, from <code>adam-3.yaml</code> to <code>adam.yaml</code> to <code>default.yaml</code>. Once I have the inheritance path specified, loading the YAML files as a dictionary becomes the easy part.</p>
<p>How would this look like in code? Let's take a look at an implementation.</p>
<div class="hll"><pre><span></span><span class="kn">import</span> <span class="nn">yaml</span> 

<span class="k">def</span> <span class="nf">inheritance_path</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">,</span> <span class="n">path</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    :param str yaml_file: The path to the yaml file of interest.</span>
<span class="sd">    :param list path: A list specifying the existing inheritance path. First</span>
<span class="sd">        entry is the file of interest, and parents are recursively appended to</span>
<span class="sd">        the end of the list.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">,</span> <span class="s1">&#39;r+&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">p</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">p</span><span class="p">[</span><span class="s1">&#39;parent&#39;</span><span class="p">]</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">path</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">path</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="s1">&#39;parent&#39;</span><span class="p">])</span>
            <span class="k">return</span> <span class="n">inheritance_path</span><span class="p">(</span><span class="n">p</span><span class="p">[</span><span class="s1">&#39;parent&#39;</span><span class="p">],</span> <span class="n">path</span><span class="p">)</span>
</pre></div>
<p>The most important part of the function is in the <code>if</code>/<code>else</code> block. If I have reached the "root" of the inheritance path, (that is, I have hit <code>default.yaml</code> which has no parent), then I return the <code>path</code> traversed. Otherwise, I return into the <code>inheritance_path</code> function call again, but with an updated <code>path</code> list, and a different <code>yaml_file</code> to read. It's a bit like doing a <code>while</code> loop, but in my opinion, a bit more elegant aesthetically.</p>
<p>Once I've gotten the path list, I can finally load the parameters using a single function that calls on <code>inheritance_path</code>.</p>
<div class="hll"><pre><span></span><span class="k">def</span> <span class="nf">load_params</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">):</span>
    <span class="n">path</span> <span class="o">=</span> <span class="n">inheritance_path</span><span class="p">(</span><span class="n">yaml_file</span><span class="p">,</span> <span class="p">[</span><span class="n">yaml_file</span><span class="p">])</span>
    <span class="n">p</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">data_tasks</span><span class="o">=</span><span class="nb">dict</span><span class="p">(),</span> 
             <span class="n">model_architecture</span><span class="o">=</span><span class="nb">dict</span><span class="p">(),</span> 
             <span class="n">training_parameters</span><span class="o">=</span><span class="nb">dict</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">fn</span> <span class="ow">in</span> <span class="n">path</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>  <span class="c1"># go in reverse!</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fn</span><span class="p">,</span> <span class="s1">&#39;r+&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">yaml</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">p</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
                    <span class="n">p</span><span class="p">[</span><span class="n">k</span><span class="p">]</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">p</span>
</pre></div>
<p>This is the equivalent of traversing a Directed Acyclic Graph (DAG), or in some special cases, a tree data structure, but in a way where we don't have to know the entire tree structure ahead of time. The goal is to reach the root from any node:</p>
<pre><code>root
    |- A
        |- B
        |- C
            |- D
            |- E
    |- F
        |- G
        |- H
        |- I 
            |- J
</code></pre>
<p>Also, because we only have one pointer in each YAML file to its parent, we have effectively created a "Linked List" that we can use to trace a path back to the "root" node, along the way collecting the information that we need together. By using this method of traversal, we only need to know the neighbors, and at some point (however long it takes), we will reach the root.</p>
<pre><code>D -&gt; C -&gt; A -&gt; root
E -&gt; C -&gt; A -&gt; root
J -&gt; I -&gt; F -&gt; root
</code></pre>
<p>If you were wondering why linked lists, trees and other data structures might be useful as a data scientist, I hope this illustrates on productive example!</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../blog/2017/10/10/recursive-programming-and-dags/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  

  
  <div class="pagination">
    
      <span class="disabled">&laquo; Previous</span>
    
    | 1 |
    
      <a href="../blog/page/2/">Next &raquo;</a>
    
  </div>


    </div>

    <nav class="navbar navbar-expand-sm navbar-light fixed-bottom bg-light">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#external-links" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="external-links">
            <ul class="navbar-nav mr-auto">
                
                <li class="nav-item">
                    <a class="nav-link" href="https://www.linkedin.com/in/ericmjl"><i class="fa fa-linkedin" aria-hidden="true"></i> LinkedIn</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://twitter.com/ericmjl"><i class="fa fa-twitter" aria-hidden="true"></i> Twitter</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://github.com/ericmjl"><i class="fa fa-github" aria-hidden="true"></i> GitHub</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://stackoverflow.com/users/1274908/ericmjl"><i class="fa fa-stack-overflow" aria-hidden="true"></i> Stack Overflow</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://shortwhale.com/ericmjl"><i class="fa fa-envelope-o" aria-hidden="true"></i> Contact Me</a>
                </li>
                
            </ul>
        </div>
    </nav>
    <!-- Boostrap JS imports -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>

</body>
