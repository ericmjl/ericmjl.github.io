<!doctype html>

<head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="utf-8">
    <style media="screen">
        body {
            padding-top: 70px;
            padding-bottom: 70px;
        }

        /* Dark mode styles */
        body.dark-mode {
            background-color: #1a1a1a;
            color: #ffffff;
        }

        body.dark-mode .terminal {
            background-color: #1a1a1a;
            color: #ffffff;
        }

        body.dark-mode a {
            color: #66b3ff;
        }

        body.dark-mode .terminal-menu {
            background-color: #1a1a1a;
        }

        body.dark-mode .terminal-menu li a {
            color: #ffffff;
        }

        body.dark-mode .terminal-menu li a:hover {
            background-color: #333333;
        }

        /* Theme toggle button styles */
        .theme-toggle {
            position: absolute;
            top: 20px;
            right: 20px;
            z-index: 1000;
            padding: 8px 16px;
            border-radius: 4px;
            cursor: pointer;
            background-color: #f0f0f0;
            border: 1px solid #ccc;
        }

        body.dark-mode .theme-toggle {
            background-color: #333;
            color: #fff;
            border-color: #666;
        }

        .container {
            position: relative;
        }
    </style>
    
<!-- Syntax Highlighter. Use pygments. -->
<link rel="stylesheet" href="../../../../../static/pygments.css">


    

<meta property="og:title" content="Journal Club: Differentiable Search of Evolutionary Trees">
<meta property='og:url' content='http://ericmjl.github.io/blog//blog/journal-club-differentiable-search-of-evolutionary-trees' />



    <!-- Google Analytics -->
    <script>
        (function (i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function () {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-12498603-2', 'auto');
        ga('send', 'pageview');
    </script>

    <link rel="stylesheet" href="https://unpkg.com/terminal.css@0.7.2/dist/terminal.min.css" />
    <link rel="stylesheet" href="/static/css/custom.css" />

    <style>
        .blog-card-container {
            display: flex;
        }

        .blog-card-left {
            flex: 1;
        }

        .blog-card-right {
            flex: 3;
        }

        /* Add rounded corners to banner images */
        .banner-image {
            border-radius: 8px;
            max-width: 98%;
            height: auto;
            display: block;
            margin-left: auto;
            margin-right: auto;
        }

    </style>
    <!-- Mathjax -->
    <!-- <script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML">
        </script>

    <script type="text/x-mathjax-config">
        MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
    </script> -->

    <script>
        MathJax = {
            tex: {
                inlineMath: [['$', '$'], ['\\(', '\\)']]
            },
            svg: {
                fontCache: 'global'
            }
        };
    </script>
    <script type="text/javascript" id="MathJax-script" async
        src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js">
        </script>

    <!-- Mermaid.js -->
    <script type="module">
        import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
        mermaid.initialize({
            startOnLoad: true,
            theme: 'neutral',
            securityLevel: 'loose'
        });
    </script>
    <style>
        .mermaid {
            background-color: white;
            padding: 1em;
            margin: 1em 0;
            border-radius: 4px;
        }
    </style>

</head>

<title>Journal Club: Differentiable Search of Evolutionary Trees - Eric J. Ma's Personal Site</title>

<body>
    <div class="container">
        <button class="theme-toggle" onclick="toggleTheme()">🌙</button>
        <h1 class="logo">
            <a href="/">
                Eric J Ma's Website
            </a>
        </h1>
        <!-- Top Navigation (local links) -->

        <div class="terminal-nav">
            <nav class="terminal-menu" id="local-links">
                <ul>
                    
                    <li class="menu-item">
                        <!-- Set blinking cursor correctly on navigation -->
                        
                        
                        
                        
                        
                        <a href="/" rel="">Home</a>
                        
                    </li>
                    
                    <li class="menu-item">
                        <!-- Set blinking cursor correctly on navigation -->
                        
                        
                        
                        
                        
                        <a class="terminal-prompt" href="/blog" rel="">Blog</a>
                        
                    </li>
                    
                    <li class="menu-item">
                        <!-- Set blinking cursor correctly on navigation -->
                        
                        
                        
                        
                        
                        <a href="/books" rel="">Books</a>
                        
                    </li>
                    
                    <li class="menu-item">
                        <!-- Set blinking cursor correctly on navigation -->
                        
                        
                        
                        
                        
                        <a href="/open-source" rel="">Open Source</a>
                        
                    </li>
                    
                    <li class="menu-item">
                        <!-- Set blinking cursor correctly on navigation -->
                        
                        
                        
                        
                        
                        <a href="/projects" rel="">Projects</a>
                        
                    </li>
                    
                    <li class="menu-item">
                        <!-- Set blinking cursor correctly on navigation -->
                        
                        
                        
                        
                        
                        <a href="/talks" rel="">Talks</a>
                        
                    </li>
                    
                    <li class="menu-item">
                        <!-- Set blinking cursor correctly on navigation -->
                        
                        
                        
                        
                        
                        <a href="/teaching" rel="">Teaching</a>
                        
                    </li>
                    
                    <li class="menu-item">
                        <!-- Set blinking cursor correctly on navigation -->
                        
                        
                        
                        
                        
                        <a href="/user-manual" rel="">User Manual</a>
                        
                    </li>
                    
                    <li class="menu-item">
                        <!-- Set blinking cursor correctly on navigation -->
                        
                        
                        
                        
                        
                        <a href="/bio" rel="">Bio</a>
                        
                    </li>
                    
                </ul>
            </nav>
        </div>

        <!-- Body -->
        <div id="body">
            


<div class="terminal-card">
  <header id="post_title" name="post_title">
<!-- Set title style -->
<span name="title" id="title">Journal Club: Differentiable Search of Evolutionary Trees</span>
</header>
  <div class="card-body">
    
<!-- Append author -->
<small>
  <p>
    written by
    
    <a class="author" href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on
    <span id="pub_date" name="pub_date">2023-08-07</span>

    
    | tags:
    <!-- Append tags after author -->
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/machine learning/">
        machine learning
      </a>
    </span>
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/phylogenetics/">
        phylogenetics
      </a>
    </span>
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/protein engineering/">
        protein engineering
      </a>
    </span>
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/protein sequences/">
        protein sequences
      </a>
    </span>
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/ancestral sequences/">
        ancestral sequences
      </a>
    </span>
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/bioinformatics/">
        bioinformatics
      </a>
    </span>
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/computational biology/">
        computational biology
      </a>
    </span>
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/deep learning/">
        deep learning
      </a>
    </span>
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/differentiable computing/">
        differentiable computing
      </a>
    </span>
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/gradient-based optimization/">
        gradient-based optimization
      </a>
    </span>
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/phylogenetic trees/">
        phylogenetic trees
      </a>
    </span>
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/sequence representation/">
        sequence representation
      </a>
    </span>
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/tree adjacency/">
        tree adjacency
      </a>
    </span>
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/vae/">
        vae
      </a>
    </span>
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/protein design/">
        protein design
      </a>
    </span>
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/jax/">
        jax
      </a>
    </span>
    <span class="boxed" id="tags" name="tags">
      <a class="tags" href="../../../../tag/python/">
        python
      </a>
    </span>
    
  </p>
  
</small>

    <hr>

    
    
    <img src="logo.webp" class="banner-image" >
    

    <!-- NOTE: I am keeping this here just for preview purposes.
     We must rely on the webp logo for the blog post.
     Pre-commit hooks will ensure that the png logo is converted to webp.-->
    

    
    <div class="blog-summary">
      <i><p>I've just explored a fascinating paper on differentiable search of evolutionary trees.
It's a creative blend of math and biology,
using mathematical structures to solve a biological problem.
The authors have developed a way to infer both phylogenetic trees and ancestral protein sequences in a continuous, differentiable manner.
This opens up exciting new avenues for protein engineering and design.
Plus, the paper's figures are top-notch! 🧬🌳📊</p>
</i>
    </div>
    

    <span id="post_body" name="post_body">
      <div class="hll"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">jax</span><span class="w"> </span><span class="kn">import</span> <span class="n">random</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">seqlike</span><span class="w"> </span><span class="kn">import</span> <span class="n">aaSeqLike</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">jax.numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jax</span><span class="w"> </span><span class="kn">import</span> <span class="n">vmap</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jax.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">softmax</span>
</pre></div>
<p>Once in a while,
I find a research paper that is unique and creative --
one that is not just a matter of brute force scaling
but uses mathematical links creatively to solve a biological problem.
"Differentiable Search of Evolutionary Trees" is one such paper,
and I'm very happy for the authors Ramith Hettiarachchi, Avi Swartz, and Sergey Ovchinnikov
that it was accepted to ICML 2023!
(It was specifically accepted to the Sampling and Optimization in Discrete Space (SODS)
and Differentiable Almost Everything (DiffAE) workshops.)</p>
<p>I thought it would be worth sharing the methodology,
with a specific focus on how the authors take a non-differentiable problem
and turn it into a differentiable problem
through interconversion between mathematical data structures.</p>
<p>It's challenging to find a phylogenetic tree
that explains the existence of a collection of sequences --
the problem is NP-complete,
and the space of possible trees is combinatorially large.
Moreover, the <em>discrete</em> nature of phylogenetic trees
makes tree generation inherently non-differentiable.
Yet, we should be able to make phylogenetic tree search differentiable.
So how do we do that?</p>
<p>The answer to that question is the premise of this paper.
The key idea here is to recognize that:</p>
<ol>
<li>protein (and their corresponding nucleotide) sequences,
whose generation can be represented by a tree,
can be represented by a probability matrix (a.k.a. position-specific probability matrix).</li>
<li>trees are graphs,</li>
<li>graphs have a discrete matrix representation,</li>
<li>discrete matrices can be relaxed to continuous matrices,</li>
<li>continuous matrices can be <em>optimized</em> using gradients,</li>
</ol>
<p>Let's explore these ideas a bit more.</p>
<h2 id="sequences-have-a-probabilistic-representation">Sequences have a probabilistic representation</h2><p>To start,
we should know one definition of a phylogenetic tree:
Given $N$ <em>observed</em> sequences (also known as "leaves" in the tree),
there will be $N-1$ <em>ancestral</em> sequences that are assumed to exist.
This comes from the definition of a phylogenetic tree as a <em>bifurcating</em> tree,
which implies that each ancestral sequence gives rise to two children sequences,
which can either be ancestral themselves or the observed sequences.</p>
<p>Now, the <em>observed</em> sequences can be represented as a one-hot encoded matrix.
Using SeqLike to create the object,
we can convert it to a one-hot encoding easily:</p>
<div class="hll"><pre><span></span><span class="n">aaSeqLike</span><span class="p">(</span><span class="s2">&quot;MATHEMA&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to_onehot</span><span class="p">()</span>
</pre></div>
<div class="hll"><pre><span></span>    array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0],
           [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0],
           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,
            0],
           [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0],
           [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0],
           [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0],
           [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
            0]])
</pre></div>
<p>Visualized, it will look like this:</p>
<div class="hll"><pre><span></span><span class="n">alphabet</span> <span class="o">=</span> <span class="n">aaSeqLike</span><span class="p">(</span><span class="s2">&quot;MATHEMA&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">alphabet</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">aaSeqLike</span><span class="p">(</span><span class="s2">&quot;MATHEMA&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to_onehot</span><span class="p">())</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">alphabet</span><span class="p">);</span>
</pre></div>
<p><img src="output_11_0.webp" alt="png"></p>
<p>By default,
SeqLike will construct the matrix such that positions are rows and letters are columns.
However,
that matrix can also be transposed if desired.</p>
<p>One-hot encodings are a discrete representation;
however,
we can relax it to be a continuous representation by ensuring that each position (column)
is a probability vector that sums to 1.0.
Doing so gives us a position-specific weight matrix (PSWM)
(or the position-specific probability matrix).
This is the first trick the authors used to convert a discrete problem into a continuous problem.</p>
<p>In contrast to the observed sequences,
the $N-1$ ancestral sequences are not precisely known,
so we use a PSWM to represent them.
PSWMs can be visualized like this:</p>
<div class="hll"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">jax</span><span class="w"> </span><span class="kn">import</span> <span class="n">random</span><span class="p">,</span> <span class="n">nn</span>

<span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">22</span><span class="p">)</span>
<span class="n">k1</span><span class="p">,</span> <span class="n">k2</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">key</span><span class="p">)</span>

<span class="n">logits</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k1</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="s2">&quot;MATHEMA&quot;</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">alphabet</span><span class="p">)))</span>
<span class="n">probs</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">ax</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">probs</span><span class="p">)</span>
<span class="n">ax</span><span class="o">.</span><span class="n">set_xticklabels</span><span class="p">(</span><span class="n">alphabet</span><span class="p">);</span>
</pre></div>
<p><img src="output_14_0.webp" alt="png"></p>
<p>Phylogenies, however, don't operate with just one observed sequence;
there usually are a handful of sequences.
For illustrative purposes, we will generate and use three observed sequences throughout this post.</p>
<div class="hll"><pre><span></span><span class="n">observed_sequences</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span>
    <span class="p">(</span>
        <span class="n">aaSeqLike</span><span class="p">(</span><span class="s2">&quot;MATHEMA&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to_onehot</span><span class="p">(),</span>
        <span class="n">aaSeqLike</span><span class="p">(</span><span class="s2">&quot;MATHIMA&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to_onehot</span><span class="p">(),</span>
        <span class="n">aaSeqLike</span><span class="p">(</span><span class="s2">&quot;MITHIMA&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">to_onehot</span><span class="p">(),</span>
    <span class="p">)</span>
<span class="p">)</span>
</pre></div>
<h2 id="trees-are-graphs-and-graphs-have-discrete-matrix-representations-and-those-representations-can-be-relaxed-as-well">Trees are graphs, and graphs have discrete matrix representations, and those representations can be relaxed as well.</h2><p>A phylogenetic tree is a directed acyclic graph (DAG).
Every graph can be represented as a matrix,
such that nodes are ordered identically on the rows and columns.
Since we are talking about directed graphs,
we can treat the rows as "source" nodes and the columns as "target" nodes.
In the case of a DAG, we only need to consider the upper triangle;
due to the <em>acyclic</em> property of a DAG,
we never need edges from the target nodes back to the source nodes.
As such, for a tree (which is a DAG),
we're able to always find a topological sort of the nodes,
and hence only need the upper right triangle.</p>
<p>Here's an example.
This tree, which has 5 nodes (3 observed, A-C and 2 ancestral, D &amp; E),
looks like this in tree form:</p>
<div class="hll"><pre><span></span>    E
   / \
  D   B
 / \
A   C
</pre></div>
<p>That same tree, in matrix form,
(with rows being children and columns being parents to maintain conventions):</p>
<div class="hll"><pre><span></span>  A B C D E
A x i i 1 0
B x x i 0 1
C x x x 1 0
D x x x x 1
E x x x x x
</pre></div>
<p>Here, a value of</p>
<ul>
<li><code>1</code> indicates that the row is a child of the column,</li>
<li><code>0</code> indicates the absence of that relationship,</li>
<li><code>x</code> fills the lower left triangle because those are not applicable to trees, and</li>
<li><code>i</code> fills the <em>invalid</em> relations between observed sequences.</li>
</ul>
<p>If you're being astute and thinking ahead a few steps,
you'll quickly come to the realization that the <code>1</code>s and <code>0</code>s above
are the positions of the <em>parameterizable</em> parts of the tree model.
Those entries in the matrix are the ones that we need to infer
in order to find the best tree for a given collection of observed sequences.</p>
<p>Here are other observations that we can make:</p>
<ul>
<li>Every row must sum to 1.
This is because every child node can only be a child node to one parent node.</li>
<li>Every column must sum to 2.
This is because every parent node must have two children.</li>
</ul>
<p>Doing so gives us the following tensors,
as illustrated in the paper
(with my own annotations added for clarity):</p>
<p><img src="4217e1e3-63f9-4fce-ae2b-2224ebb7d98f.webp" alt="image.webp"></p>
<p>In the figure above,
Figure 3 from the paper,
the observed sequences are the input leaf nodes on the left.
From there, we construct the one-hot representation of the sequence,
which gives us an interpretable embedding of the sequences,
and we also add $N-1$ ancestral nodes into the matrix,
parameterizing them to be optimizable.
We've touched on these before.
What's most important for this section is the 3rd item in the figure: the adjacency matrix.
The characteristics described above are clearly visible here.</p>
<p>So how do we relax the restrictions of this matrix to be continuous and hence differentiable?</p>
<p>The fundamental trick is identical to that of sequence representations:
instead of a row of <code>0</code>s and <code>1</code>s, we allow floats,
subject to the restriction that <em>they must sum to 1.0</em>.
Doing so satisfies the property that each row must sum to 1
and affords us the interpretation that each entry in a row is the probability of an edge showing up.
This is enforceable using the softmax function applied to each row.
The resulting matrix would look like this:</p>
<div class="hll"><pre><span></span>  A   B   C   D   E
A x   i   i   0.9 0.1
B x   x   i   0.2 0.8
C x   x   x   0.8 0.2
D x   x   x   x   1.0
E x   x   x   x   x
</pre></div>
<p>In JAX, here's how we construct such a matrix.
Because of the nature of gradient-based optimization,
we often need to operate in infinite support space,
which means initializing a square array that looks like this:</p>
<div class="hll"><pre><span></span><span class="n">N</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">observed_sequences</span><span class="p">)</span>
<span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">22</span><span class="p">)</span>
<span class="n">params</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="n">shape</span><span class="p">)</span>
<span class="n">params</span>
</pre></div>
<div class="hll"><pre><span></span>    Array([[ 0.7082089 , -0.49484855, -1.5541736 ,  1.8314509 ,  0.15443915],
           [ 0.46382412,  1.0662364 , -1.237933  , -0.7060505 , -0.3103692 ],
           [ 1.4750988 , -1.3629559 ,  0.3568244 , -0.1817721 , -0.0103011 ],
           [-0.9509461 , -0.53857976,  0.19372661, -1.9654351 ,  0.8330064 ],
           [ 1.0176847 ,  0.24268225, -1.521127  ,  1.3994535 , -1.7868598 ]],      dtype=float32)
</pre></div>
<p>Doing so allows us to adjust the values by gradient descent
without worrying about hitting invalid values.
We use differentiable mathematical transforms, such as the softmax function,
to help us convert parameters from infinite support space back to finite support.</p>
<p>We'll also need an adjacency matrix mask that masks all invalid parameters.
This is constructed using a function that looks like this:</p>
<div class="hll"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">tree_adjacency_mask</span><span class="p">(</span><span class="n">N</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">ninfs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">tril</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">shape</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">NINF</span><span class="p">))</span>

    <span class="n">first_N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="p">),</span> <span class="n">fill_value</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">NINF</span><span class="p">)</span>
    <span class="n">next_N</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">full</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">fill_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span>
    <span class="n">adj_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">hstack</span><span class="p">([</span><span class="n">first_N</span><span class="p">,</span> <span class="n">next_N</span><span class="p">])</span> <span class="o">+</span> <span class="n">ninfs</span>
    <span class="n">adj_mask</span> <span class="o">=</span> <span class="n">adj_mask</span><span class="o">.</span><span class="n">at</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">adj_mask</span>


<span class="n">mask</span> <span class="o">=</span> <span class="n">tree_adjacency_mask</span><span class="p">(</span><span class="n">N</span><span class="p">)</span>
<span class="n">mask</span>
</pre></div>
<div class="hll"><pre><span></span>    Array([[-inf, -inf, -inf,   0.,   0.],
           [-inf, -inf, -inf,   0.,   0.],
           [-inf, -inf, -inf,   0.,   0.],
           [-inf, -inf, -inf, -inf,   0.],
           [-inf, -inf, -inf, -inf,   1.]], dtype=float32, weak_type=True)
</pre></div>
<p>If we apply the mask to our params and then apply the softmax function row-wise:</p>
<div class="hll"><pre><span></span><span class="n">branch_probabilities</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">softmax</span><span class="p">)(</span><span class="n">params</span> <span class="o">+</span> <span class="n">mask</span><span class="p">)</span>
<span class="n">branch_probabilities</span>
</pre></div>
<div class="hll"><pre><span></span>    Array([[0.        , 0.        , 0.        , 0.84250844, 0.15749158],
           [0.        , 0.        , 0.        , 0.4023504 , 0.5976496 ],
           [0.        , 0.        , 0.        , 0.45723698, 0.54276305],
           [0.        , 0.        , 0.        , 0.        , 1.        ],
           [0.        , 0.        , 0.        , 0.        , 1.        ]],      dtype=float32)
</pre></div>
<p>Thereby giving us the matrix that satisfies the first criteria --
DAG upper triangle with probabilities of edges existing between ancestral nodes
or between them and leaf nodes.
Visualized, it'll look like this:</p>
<div class="hll"><pre><span></span><span class="n">sns</span><span class="o">.</span><span class="n">heatmap</span><span class="p">(</span><span class="n">branch_probabilities</span><span class="p">)</span>
</pre></div>
<p><img src="output_38_1.webp" alt="png"></p>
<p>But if each row must sum to 1.0,
how do we guarantee that each column must sum to 2.0?</p>
<p>Finding two functions,
such that one is applied row-wise and the other column-wise,
such that two conditions are satisfied,
is difficult to find analytically.
Is there an alternative approach?</p>
<p>Yes, there is,
and that's where we go to the loss function.</p>
<h2 id="loss-function-term-1-bifurcating-tree-loss">Loss Function Term 1: Bifurcating Tree Loss</h2><p>But what if we relaxed the problem
and said that each column must <em>approximately</em> sum to 2.0?
If we do so,
we can turn the problem into an optimization problem.
That's where Equation 3 in the paper helps.</p>
<div>
$$L_b=\sum_{j=1}^{N-1} \text{abs}((\sum_{i=1}^{2N-2} A_{ij}) - 2)$$
</div><p>Implemented in JAX,
it'll look something like this:</p>
<div class="hll"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">tree_branch_loss</span><span class="p">(</span><span class="n">branch_probabilities</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">):</span>
    <span class="n">N</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="n">branch_probabilities</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span><span class="p">)</span>
    <span class="c1"># Create a NumPy array mask. This effectively stops gradients from flowing.</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">N</span><span class="o">-</span><span class="mi">1</span><span class="p">)])</span>
    <span class="n">col_wise_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">branch_probabilities</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">col_wise_loss</span> <span class="o">*</span> <span class="n">mask</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">loss</span>
</pre></div>
<p>This is a loss term that effectively acts like a regularizer.
It pulls the sum of the column values closer and closer to the target value of 2.
With this loss term,
each parameterized entry (non-<code>i</code> and non-<code>x</code> above) in the learned adjacency matrix
has two forces acting on it,
one being the "sum to 2.0" force acting column-wise,
and the other being the <em>maximum parsimony</em> force acting row-wise.</p>
<p>So what is this <em>maximum parsimony</em> force?
Where is that loss term coming from?</p>
<p>Remember that the premise of constructing a maximum parsimony phylogenetic tree
is to find the tree that explains the existence of the observed sequences
while minimizing the number of mutations induced.
This means that we also infer the ancestral sequences while constructing the tree.
That is where the parsimony loss function comes in.</p>
<p>The parsimony loss term is constructed by taking the inferred ancestral sequence
and diffing it from its parent.
How do we do that?</p>
<h2 id="detour-constructing-and-using-the-sequence-matrix">Detour: Constructing and using the sequence matrix</h2><p>To do that,
we need to first see how the <em>node sequence representation</em> matrix is constructed.</p>
<p>Recall that with $N$ observed sequences,
we will have $N-1$ ancestral nodes.
The observed sequences are precisely known
and hence take on a one-hot representation.
On the other hand,
the ancestral sequences are not precisely known and need to be inferred.
As such,
they need to be represented using a <em>position-specific probability matrix</em>,
more commonly known as a <em>position-specific weight matrix</em> (PSWM).
Let's see how this gets constructed.</p>
<p>To begin,
let's start with the observed sequences:</p>
<div class="hll"><pre><span></span><span class="n">observed_sequences</span>
</pre></div>
<div class="hll"><pre><span></span>    Array([[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0],
...
            [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
             0, 0]]], dtype=int32)
</pre></div>
<p>With its shape being:</p>
<div class="hll"><pre><span></span><span class="n">observed_sequences</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
<div class="hll"><pre><span></span>    (3, 7, 23)
</pre></div>
<p>Then, we initialize the ancestral sequences as a random tensor,
and apply the softmax transform to convert it to a PSWM.</p>
<div class="hll"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">jax</span><span class="w"> </span><span class="kn">import</span> <span class="n">random</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">jax.nn</span><span class="w"> </span><span class="kn">import</span> <span class="n">softmax</span>

<span class="n">k3</span><span class="p">,</span> <span class="n">k4</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">k2</span><span class="p">)</span>

<span class="n">ancestral_sequences_logit</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">k3</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">observed_sequences</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="s2">&quot;MATHEMA&quot;</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">alphabet</span><span class="p">)))</span>
<span class="n">ancestral_sequence_probs</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">ancestral_sequences_logit</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">ancestral_sequence_probs</span>
</pre></div>
<div class="hll"><pre><span></span>    Array([[[0.05529984, 0.09550589, 0.05429629, 0.05502843, 0.05728496,
             0.07386164, 0.14299779, 0.0227186 , 0.02109059, 0.01895124,
             0.03934288, 0.0723261 , 0.02412836, 0.030231  , 0.00949756,
             0.00631911, 0.00572626, 0.13230364, 0.02543964, 0.03110287,
             0.00736431, 0.00669383, 0.01248925],
...
            [0.00170834, 0.00756833, 0.0193548 , 0.00484694, 0.02180319,
             0.03584868, 0.03202888, 0.0341184 , 0.04679569, 0.08443339,
             0.0085059 , 0.02014623, 0.00671417, 0.02273462, 0.05624866,
             0.0476648 , 0.01502691, 0.0055133 , 0.07164659, 0.2207129 ,
             0.03011633, 0.02939364, 0.17706925]]], dtype=float32)
</pre></div>
<p>We can then concatenate (<code>vstack</code>) the two together:</p>
<div class="hll"><pre><span></span><span class="n">node_representation_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">vstack</span><span class="p">([</span><span class="n">observed_sequences</span><span class="p">,</span> <span class="n">ancestral_sequence_probs</span><span class="p">])</span>
<span class="n">node_representation_matrix</span>
</pre></div>
<div class="hll"><pre><span></span>    Array([[[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
             0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
             0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
             1.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
             0.00000000e+00, 0.00000000e+00, 0.00000000e+00, 0.00000000e+00,
             0.00000000e+00, 0.00000000e+00, 0.00000000e+00],
...
            [1.70834339e-03, 7.56833283e-03, 1.93547998e-02, 4.84694401e-03,
             2.18031872e-02, 3.58486772e-02, 3.20288837e-02, 3.41183990e-02,
             4.67956886e-02, 8.44333917e-02, 8.50589760e-03, 2.01462265e-02,
             6.71416940e-03, 2.27346178e-02, 5.62486649e-02, 4.76647951e-02,
             1.50269121e-02, 5.51329739e-03, 7.16465861e-02, 2.20712900e-01,
             3.01163271e-02, 2.93936413e-02, 1.77069247e-01]]], dtype=float32)
</pre></div>
<p>And just to check its shape:</p>
<div class="hll"><pre><span></span><span class="n">node_representation_matrix</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
<div class="hll"><pre><span></span>    (5, 7, 23)
</pre></div>
<p>It's what we expect:
5 (3 observed + 2 ancestral) sequences, 7 positions, 23 alphabet characters.</p>
<p>As such, we get the node representation matrix from Figure 6A.
(Figure 6 is reproduced below, with my own hand-written annotations on top.)</p>
<p><img src="figure-6a.webp" alt="image.webp"></p>
<h2 id="loss-function-term-2-parsimony-loss">Loss Function Term 2: Parsimony Loss</h2><p>With that, we can go back to seeing how the parsimony loss is calculated.</p>
<p>For every amino acid letter $k$,
we can isolate the <code>(sequence x position)</code> matrix (named $S_{pk}$),
in which each entry is the probability of observing that letter $k$
at that position $p$.
(Figure 6A)
For example,
from the <code>node_representation_matrix</code> above,
the character <code>M</code>'s position would look like this:</p>
<div class="hll"><pre><span></span><span class="n">S_pk</span> <span class="o">=</span> <span class="n">node_representation_matrix</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">alphabet</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;M&quot;</span><span class="p">)]</span>
<span class="n">S_pk</span>
</pre></div>
<div class="hll"><pre><span></span>    Array([[1.        , 0.        , 0.        , 0.        , 0.        ,
            1.        , 0.        ],
           [1.        , 0.        , 0.        , 0.        , 0.        ,
            1.        , 0.        ],
           [1.        , 0.        , 0.        , 0.        , 0.        ,
            1.        , 0.        ],
           [0.02412836, 0.02081719, 0.02660132, 0.04159116, 0.03831984,
            0.02087471, 0.09509952],
           [0.03010988, 0.01445912, 0.03894024, 0.1212726 , 0.2428476 ,
            0.0225838 , 0.00671417]], dtype=float32)
</pre></div>
<p>As a reminder, our sequences were MATHEMA, MATHIMA, and MITHIMA,
so the index 0 and 5 positions (columns) in the matrix should be 1.0
(which is what we observe).
When reconstructing the ancestral sequences,
we would optimize their matrix values for parsimony.</p>
<p>Now, by taking the dot product of the learned adjacency $A$ with $S_{pk}$,
we effectively get a <em>lookup table</em> $L$ of shape <code>(sequence x position)</code>.</p>
<div class="hll"><pre><span></span><span class="n">adjacency_matrix</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">softmax</span><span class="p">)(</span><span class="n">params</span><span class="p">)</span>
<span class="n">S_pk</span> <span class="o">=</span> <span class="n">node_representation_matrix</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">alphabet</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="s2">&quot;M&quot;</span><span class="p">)]</span>
<span class="n">lookup_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">adjacency_matrix</span><span class="p">,</span> <span class="n">S_pk</span><span class="p">)</span>
<span class="n">lookup_table</span>
</pre></div>
<div class="hll"><pre><span></span>    Array([[0.29598033, 0.01430949, 0.02061273, 0.039096  , 0.05093228,
            0.2931449 , 0.05862169],
           [0.8015775 , 0.00347282, 0.00693362, 0.01820618, 0.03276558,
            0.80039245, 0.00862753],
           [0.7749123 , 0.00401895, 0.00770602, 0.01963376, 0.03455837,
            0.7736227 , 0.01090632],
           [0.48796257, 0.00782338, 0.02017675, 0.06158708, 0.1219654 ,
            0.48412016, 0.0062212 ],
           [0.5144516 , 0.01023462, 0.01348249, 0.02227255, 0.02311018,
            0.512748  , 0.04558302]], dtype=float32)
</pre></div>
<p>The unique property of that lookup table
is that it is <em>probabilistic</em> in the relaxed case.
Let's think about why -
by looking at how each entry in that table is calculated.
(Figure 6B)</p>
<p><img src="figure-6b.webp" alt="image.webp"></p>
<p>We first calculate the probability of an edge
and multiply it by the probability of observing the letter $k$ at a position,
thus giving the joint probability of the two terms.
Then we sum up the terms
to give us the <em>total</em> probability of jointly observing that letter and that edge.</p>
<p>We get the total difference between the observed sequence and its ancestor
by taking the difference between $S_{pk}$ and $L$.
(Figure 6C)</p>
<p><img src="figure-6c.webp" alt="6c"></p>
<div class="hll"><pre><span></span><span class="c1"># NOTE: taking the absolute value of the difference is necessary;</span>
<span class="c1"># we need the differences, which are errors, to add up without canceling out each other.</span>
<span class="c1"># I have verified this point with the original author, Ramith.</span>
<span class="n">diff</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">S_pk</span> <span class="o">-</span> <span class="n">lookup_table</span><span class="p">)</span>
<span class="n">diff</span>
</pre></div>
<div class="hll"><pre><span></span>    Array([[0.70401967, 0.01430949, 0.02061273, 0.039096  , 0.05093228,
            0.70685506, 0.05862169],
           [0.19842249, 0.00347282, 0.00693362, 0.01820618, 0.03276558,
            0.19960755, 0.00862753],
           [0.2250877 , 0.00401895, 0.00770602, 0.01963376, 0.03455837,
            0.22637731, 0.01090632],
           [0.46383423, 0.01299381, 0.00642458, 0.01999593, 0.08364556,
            0.46324545, 0.08887832],
           [0.48434174, 0.0042245 , 0.02545775, 0.09900005, 0.21973743,
            0.4901642 , 0.03886886]], dtype=float32)
</pre></div>
<p>Summing this up, then, is the cost function for parsimony:</p>
<div class="hll"><pre><span></span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">diff</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>
</pre></div>
<div class="hll"><pre><span></span>    Array(5.0915833, dtype=float32)
</pre></div>
<p><em>Note:</em> Having asked Ramith to check the accuracy of my blog post,
he reminded me that division by two is necessary
to ensure that we don't double-count differences.
This is due to the diff being accounted for
in both the corresponding amino acid's slice of the matrix
and another time in the sum of the rest of the matrix.
Mathematically, this is more precise, though for the purposes of optimization,
division by two induces no changes in the partial derivative except for its magnitude.</p>
<p>And now, we do this for every single letter:</p>
<div class="hll"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">functools</span><span class="w"> </span><span class="kn">import</span> <span class="n">partial</span>
<span class="n">lookups</span> <span class="o">=</span> <span class="n">vmap</span><span class="p">(</span><span class="n">partial</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">,</span> <span class="n">adjacency_matrix</span><span class="p">),</span> <span class="n">in_axes</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">out_axes</span><span class="o">=</span><span class="mi">2</span><span class="p">)(</span><span class="n">node_representation_matrix</span><span class="p">)</span>
<span class="n">parsimony_loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">node_representation_matrix</span> <span class="o">-</span> <span class="n">lookups</span><span class="p">)</span>
<span class="n">parsimony_loss</span>
</pre></div>
<div class="hll"><pre><span></span>    Array(-6.4074993e-07, dtype=float32)
</pre></div>
<p>The smaller we can make the difference,
the better the ability of the joint distribution over the tree and inferred sequences
to explain the observed sequences.
And since $S_{pk}$ represents the sequence,
the loss function affects the probabilistic portion of the sequence tensor above (the bottom rows)
and the parameterized part of the adjacency matrix!</p>
<p>Thus, by combining the two losses together --
the bifurcating tree loss and the tree parsimony loss,
we can jointly infer ancestral sequences and phylogenetic trees <em>in continuous space</em>.</p>
<h2 id="the-novelty-and-new-directions">The novelty and new directions</h2><p>I'll note that jointly inferring trees and ancestral sequences is not new.
This has been done in discrete space,
with many decades of algorithm development work.
<strong>The novelty in this paper, though,
is the ability to jointly infer trees and ancestral sequences
in a continuous and differentiable fashion,
which now opens up many avenues of research.</strong></p>
<p>The first and most apparent extension to this paper
is to incorporate evolutionary rate models.
This is where one simultaneously infers the tree
alongside the rate at which amino acids in each position mutate between one another.
The rate parameter set is usually expressed as a tensor of $\theta$s
of shape <code>(sequence length, alphabet size, alphabet size)</code>,
its shape implying one transition matrix learned per position
or a position-specific substitution matrix (PSSM).
Since these rates are continuous parameters,
and since we usually impose a continuous likelihood
(e.g. a gamma or exponential distribution) for these rates,
then one can, in principle, include the joint likelihood of the parameters
as part of the optimization objective and hence,
jointly infer the tree, site-specific evolutionary rates,
and ancestral sequences together.</p>
<p>Since the entire optimization problem is done in continuous space
with a likelihood-based loss,
we can swap out gradient-based optimization with MCMC sampling instead
and, in turn, obtain <em>posterior distributions</em> on trees, rates, and ancestral sequences.
Depending on the complexity of the loss landscape,
we can get away with using Langevin dynamics or Hamiltonian Monte Carlo samplers,
both of which take advantage of gradients for sampling.</p>
<p>With a posterior distribution on ancestral sequences,
we can sample out new sequences for protein engineering.
There is emerging literature that suggests that
<a href="https://www.nature.com/articles/s41598-020-72418-4">ancestral sequence reconstruction</a> (ASR)
may be <a href="https://www.jbc.org/article/S0021-9258(22)00878-X/fulltext">advantageous for protein engineering</a>.
Once we have a posterior on ancestral sequences,
we should be able to sample out exact ancestral sequences
by sampling from the posterior distribution on letters at each sequence.</p>
<p>The next level of ideas brimming in my head includes probing the connection
between sequences sampled from a VAE
and sequences sampled from a posterior distribution on ancestral sequence reconstruction.
Having worked with VAEs and protein sequences before,
it's likely (but not proven, at least in my head)
that VAEs are learning an implicit distribution over ancestral sequences.
In contrast, a posterior ASR distribution would be an explicit distribution over ancestral sequences.
I would love to see the differences between the two methods regarding sampled sequences.</p>
<p>The final thing I can think of is figuring out how to integrate the learned phylogeny
into protein engineering and design.
We can use differentiable phylogenies as part of a library design strategy
that optimizes for functional activity of an enzyme.
That said, how exactly isn't 100% clear to me at this point,
which is why it's a direction for the future.</p>
<h2 id="final-thoughts">Final thoughts</h2><p>Once in a while,
an excellent paper comes along
that uses mathematical tricks in a new way,
bridges two ideas,
and opens up new avenues of research.
Even more remarkable is that the lead author of this paper did it while an undergraduate!
Kudos, Ramith -- very well done!</p>
<p>I also want to comment on the figures:
this paper's figures are the best!
It's clear to me that the authors strove for <em>clarity</em> in their writing.
They used diagrams to illustrate the linear algebra operations involved,
and those diagrams used <em>minimally complicated examples</em>.
Compared to the many machine learning papers I've read over the years,
this one sets the bar for clear communication.
Commendable work!</p>
<p>Finally, the topic and choice of methods:
this was another example of sitting down and thinking hard
about the generative model that backs a biological data structure
rather than throwing raw modelling power,
and hence billions of parameters, at the problem.
I enjoy this type of research!
Amongst the work surrounding differentiable computing,
this level of model building is the most mechanistic and creative.</p>

    </span>

    
    
    
    
    

    <hr>

    <i>Cite this blog post:</i>
    <div class="hll" style="position: relative;">
    <button class="copy-button" onclick="copyCitation()" title="Copy citation">
      <span class="copy-icon">📋</span>
    </button>
    <pre>
<span id="citation-text"><span><span style="color: darkblue; font-weight: bold">@article</span>{
    <span style="color: black; font-weight: bold">ericmjl-2023-journal-club-differentiable-search-of-evolutionary-trees</span>,
    <span style="color: green; font-weight:bold">author</span> = <span style="color: maroon">{Eric J. Ma}</span>,
    <span style="color: green; font-weight:bold">title</span> = <span style="color: maroon">{Journal Club: Differentiable Search of Evolutionary Trees}</span>,
    <span style="color: green; font-weight:bold">year</span> = <span style="color: maroon">{2023}</span>,
    <span style="color: green; font-weight:bold">month</span> = <span style="color: maroon">{08}</span>,
    <span style="color: green; font-weight:bold">day</span> = <span style="color: maroon">{07}</span>,
    <span style="color: green; font-weight:bold">howpublished</span> = <span style="color: maroon">{\url{https://ericmjl.github.io}}</span>,
    <span style="color: green; font-weight:bold">journal</span> = <span style="color: maroon">{Eric J. Ma's Blog}</span>,
    <span style="color: green; font-weight:bold">url</span> = <span style="color: maroon">{https://ericmjl.github.io/blog/2023/8/7/journal-club-differentiable-search-of-evolutionary-trees}</span>,
}
  </span></pre>
    </div>

    <script>
    function copyCitation() {
      const citationElement = document.getElementById('citation-text');
      const text = citationElement.textContent;

      // Create a temporary textarea element
      const textarea = document.createElement('textarea');
      textarea.value = text;
      document.body.appendChild(textarea);

      // Select and copy the text
      textarea.select();
      document.execCommand('copy');

      // Remove the temporary textarea
      document.body.removeChild(textarea);

      // Visual feedback
      const button = document.querySelector('.copy-button');
      const originalText = button.innerHTML;
      button.innerHTML = '<span class="copy-icon">✓</span>';
      button.style.backgroundColor = '#4CAF50';

      // Reset button after 2 seconds
      setTimeout(() => {
        button.innerHTML = originalText;
        button.style.backgroundColor = '';
      }, 2000);
    }
    </script>

    <style>
    .copy-button {
      position: absolute;
      top: 8px;
      right: 8px;
      background-color: transparent;
      color: #666;
      border: none;
      padding: 4px 8px;
      border-radius: 4px;
      cursor: pointer;
      font-size: 14px;
      transition: all 0.3s ease;
      z-index: 1;
    }

    .copy-button:hover {
      background-color: rgba(0, 0, 0, 0.1);
      color: #333;
    }

    .copy-icon {
      font-size: 14px;
    }
    </style>
    <hr>
    <p>
      <i>I send out a newsletter with tips and tools
        for data scientists. Come check it out at
        <a href="https://dspn.substack.com">Substack</a>.</i>
    </p>
    <p>
      <i><span>If you would like to sponsor the coffee that goes into making my posts,
        please consider </span>
        <a href="https://github.com/sponsors/ericmjl">GitHub Sponsors</a>!</i>
    </p>
    <p>
      <i><span>Finally, I do free 30-minute GenAI strategy calls for teams
        that are looking to leverage GenAI for maximum impact. Consider </span>
        <a href="https://calendly.com/ericmjl/llm-chat">booking a call on Calendly</a>
        if you're interested!</i>
      </i>
    </p>
  </div>
  <div class="giscus" id="giscus-container"></div>
  <script>
    // Determine theme from localStorage or fallback to light
    var theme = localStorage.getItem('theme') === 'dark' ? 'dark' : 'light';
    var giscusScript = document.createElement('script');
    giscusScript.src = 'https://giscus.app/client.js';
    giscusScript.setAttribute('data-repo', 'ericmjl/website');
    giscusScript.setAttribute('data-repo-id', 'MDEwOlJlcG9zaXRvcnk2MDIzMzAxNg==');
    giscusScript.setAttribute('data-category', 'Comments');
    giscusScript.setAttribute('data-category-id', 'DIC_kwDOA5cVOM4Crqx4');
    giscusScript.setAttribute('data-mapping', 'pathname');
    giscusScript.setAttribute('data-strict', '1');
    giscusScript.setAttribute('data-reactions-enabled', '1');
    giscusScript.setAttribute('data-emit-metadata', '0');
    giscusScript.setAttribute('data-input-position', 'top');
    giscusScript.setAttribute('data-theme', theme);
    giscusScript.setAttribute('data-lang', 'en');
    giscusScript.crossOrigin = 'anonymous';
    giscusScript.async = true;
    document.getElementById('giscus-container').appendChild(giscusScript);
  </script>
</div>



        </div>

        <!-- Bottom Navigation (external links) -->
        <div class="terminal-nav">
            <nav class="terminal-menu" id="local-links">
                <ul>
                    
                    <li class="nav-item">
                        <a class="nav-link" href="https://ericmjl.github.io/resume" rel="">
                            Resume</a>
                    </li>
                    
                    <li class="nav-item">
                        <a class="nav-link" href="https://www.linkedin.com/in/ericmjl" rel="">
                            LinkedIn</a>
                    </li>
                    
                    <li class="nav-item">
                        <a class="nav-link" href="http://github.com/ericmjl" rel="">
                            GitHub</a>
                    </li>
                    
                    <li class="nav-item">
                        <a class="nav-link" href="https://ericmjl--shortmail-run-app.modal.run/send/cce87ae9c1d7" rel="">
                            Contact Me</a>
                    </li>
                    
                    <li class="nav-item">
                        <a class="nav-link" href="https://ericmjl.github.io/blog.xml" rel="">
                            Blog RSS</a>
                    </li>
                    
                </ul>
            </nav>
        </div>

    </div>

    <script>
        // Theme toggle functionality
        function setGiscusTheme(theme) {
            const iframe = document.querySelector('iframe.giscus-frame');
            if (!iframe) return;
            iframe.contentWindow.postMessage(
                {
                    giscus: {
                        setConfig: {
                            theme: theme
                        }
                    }
                },
                'https://giscus.app'
            );
        }

        function toggleTheme() {
            const body = document.body;
            const themeToggle = document.querySelector('.theme-toggle');

            if (body.classList.contains('dark-mode')) {
                body.classList.remove('dark-mode');
                themeToggle.textContent = '🌙';
                localStorage.setItem('theme', 'light');
                setGiscusTheme('light');
            } else {
                body.classList.add('dark-mode');
                themeToggle.textContent = '☀️';
                localStorage.setItem('theme', 'dark');
                setGiscusTheme('dark');
            }
        }

        // Check for saved theme preference
        document.addEventListener('DOMContentLoaded', () => {
            const savedTheme = localStorage.getItem('theme');
            const themeToggle = document.querySelector('.theme-toggle');

            if (savedTheme === 'dark') {
                document.body.classList.add('dark-mode');
                themeToggle.textContent = '☀️';
                setTimeout(() => setGiscusTheme('dark'), 500);
            } else {
                setTimeout(() => setGiscusTheme('light'), 500);
            }
        });
    </script>
</body>
