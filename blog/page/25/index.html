<!doctype html>

<head>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta charset="utf-8">
    <!-- Bootstrap v4beta Imports -->
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/css/bootstrap.min.css" integrity="sha384-/Y6pD6FV/Vv2HJnA6t+vslU6fwYXjCFtcEpHbNJ0lyAFsXTsjBbfaDjzALeQsN6M" crossorigin="anonymous">

    <style media="screen">
        body {
            padding-top: 70px;
            padding-bottom: 70px;
        }
    </style>

    <!-- Syntax Highlighter. Use both pygments and hl.js. -->
    <link rel="stylesheet" href="../../../static/pygments.css">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/default.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

    <!-- Google Analytics -->
    <script>
        (function(i, s, o, g, r, a, m) {
            i['GoogleAnalyticsObject'] = r;
            i[r] = i[r] || function() {
                (i[r].q = i[r].q || []).push(arguments)
            }, i[r].l = 1 * new Date();
            a = s.createElement(o),
                m = s.getElementsByTagName(o)[0];
            a.async = 1;
            a.src = g;
            m.parentNode.insertBefore(a, m)
        })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

        ga('create', 'UA-12498603-2', 'auto');
        ga('send', 'pageview');
    </script>

    <!-- ClustrMaps Tracking -->
    <!-- <script type="text/javascript" id="clstr_globe" src="//cdn.clustrmaps.com/globe.js?d=nhKoDpoTjWz4pC6CwI-fSy4hPoJ1uXwTLCfMCT3OK_8"></script> -->

    <!-- FontAwesome embed -->
    <script src="https://use.fontawesome.com/cb9dbe8e41.js"></script>
</head>

<title>Blog - Eric J. Ma's Personal Site</title>
<body class="body">
    <nav class="navbar navbar-expand-sm navbar-light fixed-top bg-light">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#local-links" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="local-links">
            <ul class="navbar-nav mr-auto">
                
                <li class="nav-item">
                    <a class="nav-link" href="/"><i class="fa fa-home" aria-hidden="true"></i> Home</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/resume"><i class="fa fa-file-text" aria-hidden="true"></i> Resume</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/blog"><i class="fa fa-rss" aria-hidden="true"></i> Blog</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/open-source"><i class="fa fa-code" aria-hidden="true"></i> Open Source</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/projects"><i class="fa fa-briefcase" aria-hidden="true"></i> Projects</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/talks"><i class="fa fa-microphone" aria-hidden="true"></i> Talks</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="/teaching"><i class="fa fa-university" aria-hidden="true"></i> Teaching</a>
                </li>
                
            </ul>
        </div>
    </nav>
    <div class="container">
        
  
    
  <!-- <div class="blog-post"> -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>
  
    <h1><a href="../../../blog/2015/11/28/profiling-pypy-vs-python-for-agent-based-simulation/">Profiling PyPy vs. Python for Agent-Based Simulation</a></h1>
  
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2015-11-28
  </p>
  <h2 id="outline">Outline</h2><ol>
<li>Introduction:<ol>
<li>Motivation</li>
<li>Model description</li>
<li>Link to code</li>
</ol>
</li>
<li>Environment Setup</li>
<li>Performance<ol>
<li>Python vs. PyPy on one parameter set.</li>
<li>Vary number of hosts, record time.</li>
</ol>
</li>
</ol>
<h2 id="introduction">Introduction</h2><p>As part of my PhD dissertation, I wanted to investigate the role of host ecology on the generation of reassortant viruses. Knowing myself to be a fairly algebra-blind person, I decided that an agent-based model (ABM) was going to be much more manageable than writing ODEs. (Actually, the real reason is that I"m modelling discrete states, rather than continuous states, but yes, I will admit that I do take longer than your average programmer with algebra.)</p>
<h3 id="model-description">Model Description</h3><p>Starting with our intuition of host-pathogen interactions, I implemented a custom ABM using Python classes - "Hosts" and "Viruses".</p>
<h4 id="viruses">Viruses</h4><p>"Viruses" had two segments, representing a segmented virus (like the Influenza or Lassa virus), each with a color (red or blue), and can infect Hosts (which are likewise red or blue). Viruses that are of a particular color prefer to infect hosts of the same color, but can still infect hosts of of a different colour, just at a lower probability. If two viruses are present in the same host, then there can be, at some small probability, the opportunity for gene sharing to occur.</p>
<p>One of the virus' segments determines host immunity; if the virus encounters a host which has immunity against its color, then the probability of infection drastically decreases, and it is likely that the virus will eventually be cleared.</p>
<h4 id="hosts">Hosts</h4><p>"Hosts" are where viruses replicate. Hosts gain immunity to one of the segment's colors, after a set number of days of infection. When a host gains immunity to a particular virus color, it can much more successfully fend off a new infection with that same color. Hosts also interact with one another. They may have a strong preference for a host of the same color, a.k.a. homophily.</p>
<h3 id="code">Code</h3><p>My code for the simulations can be found on <a href="http://github.com/ericmjl/reassortment-simulator">this Github repository</a>. The details of the simulation are still a work in progress, as these ideas are still early stage. My point on this blog post here will be to try to compare PyPy against CPython on performance. However, I do welcome further comments on the modelling, if you've taken the time to read through my code.</p>
<p>Code for the statistical draws can be found on <a href="http://github.com/ericmjl/pypy_stats">this other Github repository</a>.</p>
<p><h2 id="environment-setup">Environment Setup</h2>
My CPython environment is managed by <code>conda</code>. (Highly recommended! Download <a href="https://www.continuum.io/downloads">here</a>. Make sure to get Python 3!)</p>
<p>I installed <code>pypy</code> and <code>pypy3</code> under my home directory on Ubuntu Linux, and ensured that my bash shell <code>$PATH</code> variable also pointed to <code>~/pypy[3]/bin</code>.</p>
<h2 id="performance">Performance</h2><p>Let's take a look at the performance of the CPython vs. PyPy using pure-Python code.</p>
<h3 id="default-parameters">Default parameters</h3><p>I first started with 1000 agents in the simulation, with the simulation running for 150 time steps.</p>
<p>Under these circumstances, on an old Asus U30J with 8GB RAM and an SSD hard disk, Core i3 2.27GHz, executing the simulation with PyPy required only 13.4 seconds, while executing with CPython required 110.5 seconds. 10x speedup.</p>
<h3 id="varying-number-of-hosts-in-the-model">Varying number of hosts in the model</h3><p>I wanted to measure the time complexity of the simulation as a function of the number of hosts. Therefore, I varied the number of hosts from 100 to 1600, in steps of 300.</p>
<p>Partial (mostly because of laziness) results are tabulated below. (Yes, this degree of laziness would never fly in grad school.)</p>
<table><colgroup> <col /> <col /> <col /> <col /> <col /> <col /> <col /> </colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Agents</th>
<th style="text-align: left;">PyPy Trial 1</th>
<th style="text-align: left;">PyPy Trial 2</th>
<th style="text-align: left;">PyPy Trial 3</th>
<th style="text-align: left;">CPython Trial 1</th>
<th style="text-align: left;">CPython Trial 2</th>
<th style="text-align: left;">CPython Trial 3</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1000</td>
<td style="text-align: left;">13.4</td>
<td style="text-align: left;">12.8</td>
<td style="text-align: left;">12.9</td>
<td style="text-align: left;">110.5</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">700</td>
<td style="text-align: left;">8.63</td>
<td style="text-align: left;">9.02</td>
<td style="text-align: left;">8.65</td>
<td style="text-align: left;">53.7</td>
<td style="text-align: left;"></td>
<td style="text-align: left;"></td>
</tr>
<tr class="odd">
<td style="text-align: left;">400</td>
<td style="text-align: left;">4.35</td>
<td style="text-align: left;">4.33</td>
<td style="text-align: left;">4.66</td>
<td style="text-align: left;">18.2</td>
<td style="text-align: left;">18.2</td>
<td style="text-align: left;"></td>
</tr>
<tr class="even">
<td style="text-align: left;">100</td>
<td style="text-align: left;">1.03</td>
<td style="text-align: left;">1.00</td>
<td style="text-align: left;">1.17</td>
<td style="text-align: left;">1.47</td>
<td style="text-align: left;">1.48</td>
<td style="text-align: left;">1.45</td>
</tr>
</tbody></table><p>As we can see, PyPy wins when the number of iterations is large.</p>
<h3 id="statistical-draws">Statistical Draws</h3><p>I use statistical Bernoulli trials (biased coin flips) extensively in the simulation. Yet, one thing that is conspicuously unavialable to PyPy users (in an easily installable format) is the scientific Python stack. Most of that boils down to <code>numpy</code>. Rather than fiddle with trying to get <code>numpy</code>, <code>scipy</code> and other packages installed, I re-implemented my own <code>bernoulli</code> function.
    ```python
    from random import random</p>
<pre><code>class bernoulli(object):
    """
    docstring for bernoulli
    """
    def __init__(self, p):
        super(bernoulli, self).__init__()
        self.p = p

    def rvs(self, num_draws):
        draws = []
        for i in range(num_draws):
            draws.append(int(random() &amp;gt; self.p))

        return draws
</code></pre>
<p>This is <em>almost</em> a drop-in replacement for <code>scipy.stats.bernoulli</code>. (The API isn't exactly the same.) I wanted to know whether the calling <code>bernoulli</code> function I wrote performed better than calling on the <code>scipy.stats</code> function. I therefore setup a series of small tests to determine at what scale of function calls it makes more sense to use PyPy vs. CPython.</p>
<p>I then wrote a simple block of code that times the Bernoulli draws. For the PyPy version:
    ```python
    from stats.bernoulli import bernoulli
    from time import time</p>
<pre><code>start = time()
bern_draws = bernoulli(0.5).rvs(10000)
mean = sum(bern_draws) / len(bern_draws)
end = time()

print(end - start)&lt;/code&gt;&lt;/pre&gt;
And for the CPython/scipy version:
&lt;pre&gt;&lt;code&gt;from scipy.stats import bernoulli
from time import time

start = time()
bern_draws = bernoulli(0/5).rvs(10000)
mean = sum(bern_draws) / len(bern_draws)   
end = time()

print(end - start)
</code></pre>
<table><colgroup> <col /> <col /> <col /> <col /> <col /> <col /> <col /> </colgroup>
<thead>
<tr class="header">
<th style="text-align: center;">Bernoulli Draws</th>
<th style="text-align: center;">PyPy + Custom (1)</th>
<th style="text-align: center;">PyPy + Custom (2)</th>
<th style="text-align: center;">PyPy + Custom (3)</th>
<th style="text-align: center;">CPython + SciPy (1)</th>
<th style="text-align: center;">CPython + SciPy (2)</th>
<th style="text-align: center;">CPython + SciPy (3)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">1000000</td>
<td style="text-align: center;">0.271</td>
<td style="text-align: center;">0.241</td>
<td style="text-align: center;">0.206</td>
<td style="text-align: center;">0.486</td>
<td style="text-align: center;">0.513</td>
<td style="text-align: center;">0.481</td>
</tr>
<tr class="even">
<td style="text-align: center;">100000</td>
<td style="text-align: center;">0.0437</td>
<td style="text-align: center;">0.0421</td>
<td style="text-align: center;">0.0473</td>
<td style="text-align: center;">0.0534</td>
<td style="text-align: center;">0.0794</td>
<td style="text-align: center;">0.0493</td>
</tr>
<tr class="odd">
<td style="text-align: center;">10000</td>
<td style="text-align: center;">0.0311</td>
<td style="text-align: center;">0.0331</td>
<td style="text-align: center;">0.0345</td>
<td style="text-align: center;">0.00393</td>
<td style="text-align: center;">0.00410</td>
<td style="text-align: center;">0.00387</td>
</tr>
</tbody></table><p>As we can see, <code>scipy</code> is quite optimized, and outperforms at lower number of statistical draws. Things only become better for PyPy as the number of draws increases.</p>
<h3 id="summary">Summary</h3><p>Some things that I've learned from this exercise:</p>
<ol>
<li>For pure-Python code, PyPy can serve as a drop-in replacement for CPython.</li>
<li>Because of the JIT compiler, PyPy is blazing fast when doing iterations!</li>
<li><code>numpy</code> is not, right now, easily <code>pip</code>-installable. Because of this, the rest of the Scientific Python stack is also not <code>pip</code>-installable in a PyPy environment. (I will admit to still being a learner here - I wouldn't be able to articulate why <code>numpy</code> doesn't work with PyPy out-of-the-box. Experts chime in please?)</li>
</ol>
<p>Some things I hope will happen:</p>
<ol>
<li>Let's port the scientific Python stack code to make it PyPy compatible! (Yeah, wishful thinking...)</li>
<li>Alternatively, let's hope the <code>numba</code> project allows JIT compilation when using Python objects instead.</li>
</ol>
<p>As is the usual case for me, starting a new project idea gave me the impetus to try out a new thing, as I wouldn't have to risk breaking workflows that have worked for existing projects. If you find yourself in the same spot, I would encourage you to try out PyPy, especially for pure-Python code (i.e. no external libraries are used).</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2015/11/28/profiling-pypy-vs-python-for-agent-based-simulation/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- <div class="blog-post"> -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>
  
    <h1><a href="../../../blog/2015/9/28/predicting-hiv-drug-resistance-phenotype-from-genotype/">Predicting HIV Drug Resistance Phenotype from Genotype</a></h1>
  
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2015-09-28
  </p>
  <p><strong>Note to Reader:</strong> I’d highly suggest reading this blog post on the left half of your screen, and have the <a href="https://github.com/ericmjl/hiv-resistance-prediction/blob/master/Predict%20HIV%20Genotype%20from%20Phenotype%20-%20Custom%20Funcs.ipynb">Jupyter notebook</a> on the right half of your screen. Makes things a bit easier to follow.</p>
<p>I recently have been writing a proposal to conduct some experiments to predict viral RNA polymerase activity, in some standardized unit, from protein genotype. The main application of this would be to be able to conduct quantitative surveillance in a precise fashion. For example, with HIV, as treatment progresses, the virus accumulates mutations that confer resistance. For a physician treating a patient infected with HIV, would it be possible to determine, from sequence data alone, what would be the degree of predicted viral resistance for that patient’s virus?</p>
<p>Knowing fully that this problem has been tackled many times in the past from multiple angles, I wanted to know how easily I could set up an ML workflow to go from sequence to predicted drug resistance. The goal of this blog post is to document how easy it was for me to get up and running, using Python packages really well-written Python packages.</p>
<h2 id="raw-code">Raw Code</h2><p>All of my code can be found on my Github <a href="https://github.com/ericmjl/hiv-resistance-prediction">repository</a>. You can also jump directly to the <a href="https://github.com/ericmjl/hiv-resistance-prediction/blob/master/Predict%20HIV%20Genotype%20from%20Phenotype%20-%20Custom%20Funcs.ipynb">Jupyter notebook</a> that I reference code from in this article.</p>
<h2 id="data-source-and-preprocessing">Data Source and Preprocessing</h2><p>I sourced the data from the <a href="http://hivdb.stanford.edu">Stanford HIV Drug Resistance Database</a>. Specifically, I downloaded the <a href="http://hivdb.stanford.edu/pages/genopheno.dataset.html">high quality, filtered set</a> of Genotype-to-Phenotype mappings, for protease inhibitors, nucleoside reverse transcriptase inhibitors, and non-nucleoside reverse transcriptase inhibitors. I wrote a few custom functions to preprocess the data, including the following steps:</p>
<ol>
<li>Replacing all "<code>-</code>" characters with the consensus sequences. I am guessing that they use the "<code>-</code>" character in place to help highlight where the mutations are; much more human readable.</li>
<li>Removing sequences that had more than one mutation present. Mostly a function of being lazy than anything else.</li>
<li>Removing sequences with ambiguous amino acids. These are a bit harder to deal with down the road. From biological background knowledge, it’s unlikely that excluding them would be detrmimental.</li>
<li>Dropping all conserved amino acid positions. They add nothing to the analysis.</li>
<li>Binarizing the columns. This transforms the letters of the amino acid into a first-pass feature set, in which the binarized columns indicate whether or not an amino acid is present at a given position or not.</li>
</ol>
<p>These are found in my <a href="https://github.com/ericmjl/hiv-resistance-prediction/blob/master/custom_funcs.py">custom_funcs.py</a> module, which I imported into the Jupyter notebooks. Having futzed around for about a day copying/pasting blocks of code, I refactored the code into separate functions for readability, so that only the “business logic” is shown in the notebook.</p>
<h2 id="train/test-split">Train/Test Split</h2><p>It is a standard practice to split the dataset into a training and test set. K-fold cross-validation is quite easy to do using <code>scikit-learn</code>. Given an <code>X</code> and a <code>Y</code> matrix, to split it into <code>X_train</code>, <code>X_test</code>, <code>Y_train</code>, and <code>Y_test</code>, simply do the function call:</p>
<p><code>X_train, X_test, Y_train, Y_test = train_test_split(X_binarized, Y)</code></p>
<h2 id="model-training">Model Training</h2><p>To train models, I used the <a href="http://scikit-learn.org/stable/">scikit-learn</a> package to help. It’s useful to note that <code>scikit-learn</code> has a consistent API - every regressor model has a <code>MODEL.fit()</code> and a <code>MODEL.predict()</code> function. This ‘modular’ style allowed me to wrap the series of function calls into single-line functions, and thus quickly try out a variety of models to see what out-of-box predictive power would be. Using the Random Forest Regressor as an example, I wrapped up the training and plotting phases:</p>
<p><code># Model Training</code>
<code>kwargs = {'n_jobs':-1, 'n_estimators':1000}</code>
<code>rfr, rfr_preds, rfr_mse, rfr_r2 = cf.train_model(*tts_data, model=RandomForestRegressor, modelargs=kwargs)</code></p>
<p><code># Plotting</code>
<code>cf.scatterplot_results(rfr_preds, Y_test, rfr_mse, rfr_r2, DRUG, 'Rand. Forest', figsize=std)</code></p>
<p>Here, <code>cf</code> simply refers to the <code>custom_funcs.py</code> module I wrote to refactor out the repetitive boilerplate code needed.</p>
<p>The ensemble learners also include feature importances, i.e. an identification of the columns in the data that best predict the outcome of interest. I wrapped the feature importances code to make it easy to plot:</p>
<p><code>cf.barplot_feature_importances(rfr, DRUG, 'Rand. Forest', figsize=std)</code></p>
<p>In particular, I used the ensemble learners, which are known to be pretty powerful for learning tasks. And for comparison, I pitted them against a number of linear models as well. As you can see in the notebooks, the ensemble learners outperformed the linear models, at least for a binarized amino acid feature set. This makes intuitive sense - protein sequence to function is non-linear, and highly contextual.</p>
<h2 id="neural-networks">Neural Networks!</h2><p>One of the things I wanted to highlight here was how <a href="http://danielnouri.org">Daniel Nouri’s</a> <a href="https://github.com/dnouri/nolearn">nolearn</a> package made it easy for me to start experimenting with neural networks. By no means am I a deep learning expert - I consider myself too algebra-blind (but by no means code-blind!) to learn the math behind it all. However, I know that my learning style of diving into the deep end and doing a lot of hands-on trials would help me get a fairly good intuitive grasp of how to do it. So after futzing around on a GPU cluster for a few days trying to get it configured right, I got <code>theano</code>, <code>lasagne</code> and <code>nolearn</code> up and running. (Note: A GPU makes light(er) work of training artificial neural nets. CPUs take around 5-10x more time. Highly recommended to use a GPU with neural nets. Shout-out to my PhD thesis committee member, Mark Bathe, for giving me access to his lab's GPU machine!)</p>
<p><code>nolearn</code>’s API is, by design, really close to the <code>scikit-learn</code> API. I find this to be a great thing - since neural networks are basically ML models, we get the familiar <code>neural_net.fit()</code> and <code>neural_net.predict()</code> function calls. (The importance of great APIs! Thank you, @dnouri!) The API also makes specifying a neural network architecture quite easy. For example, in the simple feed-forward network architecture that I show in the Jupyter notebook, network layers are specified as a list of parameters, and each layer’s properties can be specified by using named parameters that sync up with the specified names. The comments in my Jupyter notebook in some of the layers show my (rather simple) efforts in experimenting with different architectures. To note, the original structure of the feed-forward network is directly lifted from <a href="http://danielnouri.org/notes/2014/12/17/using-convolutional-neural-nets-to-detect-facial-keypoints-tutorial/">Daniel Nouri's tutorial</a>, so big thanks to him for publishing it!</p>
<p>FWIW, I also learned from other experienced Pythonistas (shout out to Rick Landau!) for teaching me <em>not</em> to use binary features on neural networks, something I happily disregarded for this first pass. But I’ve nonetheless still remembered that lesson! And in future iterations, probably I will try to change to non-binary features.</p>
<h2 id="summary-amp;-future-work">Summary &amp; Future Work</h2><p>So to summarize, the <code>scikit-learn</code> API makes it pretty easy to get my hands dirty doing machine learning. By keeping a <code>scikit-learn</code>-like API, <code>nolearn</code> also does a great job of keeping neural nets accessible.</p>
<p>What else do I think can be done? Well, there’s one idea I’ve been thinking about, to improve the regression score beyond what experimental error in the dataset may limit us to. Think convolutional networks (convnets) and their data input requirements. Basically, most image recognition convnets need a 2D image. But what if I used a convnet that can take in a 3D image instead? Calculating a numerical value, such as the electrostatic charge at every grid point in a static, modelled 3D protein structure, may be much more informative than simply using binarized amino acid columns. A recent paper that I read in <a href="http://www.biomedcentral.com/1471-2164/15/S5/S1">BMC genomics</a> (say "yay" for open access!) uses a computationally efficient shortcut representation to get structural features encoded as well, with great success; I would definitely be game for implementing their version as a first pass as well.</p>
<p>Apart from that, it's insufficient to run only on a single train/test split. There will always be unavoidable biases in the trained model. Training <code>k</code>-fold splits <code>k</code> times is a common practice I've heard. In papers I've read, usually k-fold splits are trained k times, and the standard deviation reported. I can imagine also doing it n times, to get a better feel for the generalization error (assuming the data are representative of the population).</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2015/9/28/predicting-hiv-drug-resistance-phenotype-from-genotype/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- <div class="blog-post"> -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>
  
    <h1><a href="../../../blog/2015/9/3/in-which-i-trained-a-neural-network/">In Which I Trained A Neural Network :)</a></h1>
  
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2015-09-03
  </p>
  <p>I have decided to link to my <a href="https://github.com/ericmjl/nnet-HA/blob/master/Prototype%20Neural%20Network%20for%20predicting%20HA%20host%20tropism.ipynb">Jupyter notebook</a> &amp; <a href="https://github.com/ericmjl/nnet-HA/tree/master">github repository</a> instead of re-writing the whole post here. I hope you enjoy it! :)</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2015/9/3/in-which-i-trained-a-neural-network/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- <div class="blog-post"> -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>
  
    <h1><a href="../../../blog/2015/8/18/software-engineering-skills-for-data-analytics/">Software Engineering Skills for Data Analytics</a></h1>
  
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2015-08-18
  </p>
  <p>When you think about software engineering skills, you probably don't think about the analytics types, or data scientist (DS) teams. This is a reasonable thought. Data scientists aren't in the business of building software, they're in the business of using software to analyze data. That said, I think it's still important for a data scientist (or analytics person, for that matter), to know some basic software engineering skills. Here's the why, followed by the what.</p>
<p>Why should an analytics person who uses code to perform analysis care about good software engineering practices?</p>
<ol>
<li>It will help you write better, reusable analysis code.</li>
<li>It will help you when you come back to your analysis code later on.</li>
<li>It will help you integrate with software teams that you may have to work with.</li>
<li>It will help you share the tools that you end up developing.</li>
</ol>
<p>What basic skills should one at least be knowledgeable about, if not able to implement?</p>
<ol>
<li><strong>Defining specifications:</strong> being able to create specific and precise descriptions of what's needed, using standardized language.</li>
<li><strong>Refactoring:</strong> being able to pull out chunks of code that are repeated, to turn them into function calls.</li>
<li><strong>Writing unit/data tests:</strong> being able to write tests that ensure the integrity of a function or a block of data.</li>
<li><strong>Packaging and distribution:</strong> being able to share the code, with other people.</li>
<li><strong>Version control:</strong> being able to keep track of every meaningful change made to the  code or data.</li>
<li><strong>Documentation:</strong> being able to explain to someone other than yourself what the software tool or analysis code is all about.</li>
<li><strong>Continual automated testing &amp; integration:</strong> having a continuous integration system automatically run the necessary software and data tests, and report code testing coverage.</li>
</ol>
<hr>
<p>Here's where I see them being implemented.</p>
<p><strong>Building your own tools</strong></p>
<p>Sometimes, the tools that you need to get your work done aren't already written, or they're scattered about. You'll need to write your own tools to get your job done. The history of some Python packages, like <code>numpy</code>, <code>pandas</code> or <code>seaborn</code> were born out of this necessity. Here, the ability to use good software engineering practices as described above will make a huge difference when trying to orient newcomers to the tool and help new contributors jump in easily. And even if you don't end up sharing the code with others, if you still end up reusing your code, all of the above practices will help with codebase maintenance.</p>
<p><strong>Developing your analysis pipeline</strong></p>
<p>Your analysis pipeline is the sequence of steps that are needed to get from raw data to interpretable data to insights. Along the way, you may encounter blocks of code that get copy/pasted elsewhere. Or, you might find yourself writing a series of functions that always take the same inputs - necessitating your own OOP-based tool. Knowing good software engineering practices will help you keep your code clean, readable, and reusable across your analysis pipeline. It'll also help others interpret and verify your data analysis steps. Documentation, in this realm, is particularly important.</p>
<p>Data tests, are something software engineers don't do, but you might wish to. Data tests that run on continuous integration platforms serve as an automatically-enforced contract between you, your data provider, and your data delivery. If anything changes, continuous integration should catch it early on, and not later. An example of a data test is ensuring that the number of columns/rows stays the same, or that a hash of the original data file hasn't changed (i.e. data integrity).</p>
<p><strong>Integrating with Data Products</strong></p>
<p>Data scientists in industry don't work in a vacuum: there is usually a software product waiting at the end. Those software products are (hopefully) maintained with good software engineering practices. Getting good at the practice of good software engineering will help lubricate communication and workflows with the software engineers who may be responsible for integrating the new algorithm or analysis pipeline. Even in the academic or government world, where the end product may not be a monolithic system but a series of, perhaps, web dashboards, knowing good software engineering practices will ease the transition from analytics to product.</p>
<hr>
<p>Of course, any good thing done too much will become a bad thing. Don't over-specify straight from the start, start with something clear enough. Don't over-refactor if it isn't necessary. Don't worry about finding every test case, just test enough so the most common bugs are caught. Or use something like Hypothesis to do property-based testing. (The maintainer threw in the towel recently, citing financial reasons, I think, but as it stands it's already pretty darn good.) Don't package every last tool you develop, only share what's necessary. And over-indulging on documentation can take away from good coding. Write enough for yourself to review your code, and let others help you refine it if it's actually needed. Tracking every little minor change is also probably overdoing it.</p>
<p>Happy coding!</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2015/8/18/software-engineering-skills-for-data-analytics/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  
    
  <!-- <div class="blog-post"> -->
  <link rel="stylesheet" href="../../../static/pygments.css">
  <div>
  
    <h1><a href="../../../blog/2015/7/13/on-the-humbleness-of-conference-attendees/">On the &#39;humbleness&#39; of conference attendees</a></h1>
  
  <p class="meta">
    written by
    
      <a href="https://twitter.com/ericmjl">Eric J. Ma</a>
    
    on 2015-07-13
  </p>
  <p>Conferences are made up of people, just like any other group of human beings grouping together.</p>
<p>What makes one differ from another really boils down to the people.</p>
<p>I read a tweet recently that described the SciPy 2015 conference as having really 'humble' attendees. This was exactly my feeling! It was great to see such a community of developers and scientists who, knowing that while they may be domain experts there is still much to learn, choose to carry themselves in a really humble way.</p>
<p>I think this was one of the reasons why I really enjoyed SciPy. It was devoid of the ego that plagues other field-specific conferences. Yet another reason to go again!</p>

  </div>

    <p><i>Did you enjoy this blog post? <a href="../../../blog/2015/7/13/on-the-humbleness-of-conference-attendees/#disqus">Let's discuss more</a>!</i></p>
    <hr class="fancy">
  

  
  <div class="pagination">
    
      <a href="../../../blog/page/24/">&laquo; Previous</a>
    
    | 25 |
    
      <a href="../../../blog/page/26/">Next &raquo;</a>
    
  </div>


    </div>

    <nav class="navbar navbar-expand-sm navbar-light fixed-bottom bg-light">
        <button class="navbar-toggler navbar-toggler-right" type="button" data-toggle="collapse" data-target="#external-links" aria-controls="navbarSupportedContent" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse" id="external-links">
            <ul class="navbar-nav mr-auto">
                
                <li class="nav-item">
                    <a class="nav-link" href="https://www.linkedin.com/in/ericmjl"><i class="fa fa-linkedin" aria-hidden="true"></i> LinkedIn</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://twitter.com/ericmjl"><i class="fa fa-twitter" aria-hidden="true"></i> Twitter</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://github.com/ericmjl"><i class="fa fa-github" aria-hidden="true"></i> GitHub</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://stackoverflow.com/users/1274908/ericmjl"><i class="fa fa-stack-overflow" aria-hidden="true"></i> Stack Overflow</a>
                </li>
                
                <li class="nav-item">
                    <a class="nav-link" href="http://shortwhale.com/ericmjl"><i class="fa fa-envelope-o" aria-hidden="true"></i> Contact Me</a>
                </li>
                
            </ul>
        </div>
    </nav>
    <!-- Boostrap JS imports -->
    <script src="https://code.jquery.com/jquery-3.2.1.slim.min.js" integrity="sha384-KJ3o2DKtIkvYIK3UENzmM7KCkRr/rE9/Qpg6aAZGJwFDMVNA/GpGFF93hXpG5KkN" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.11.0/umd/popper.min.js" integrity="sha384-b/U6ypiBEHpOf/4+1nzFpr53nxSS+GLCkfwBdFNTxtclqqenISfwAzpKaMNFNmj4" crossorigin="anonymous"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0-beta/js/bootstrap.min.js" integrity="sha384-h0AbiXch4ZDo7tp9hKZ4TsHbi047NrKGLO3SEJAg45jXxnGIfYzk4Si90RDIqNm1" crossorigin="anonymous"></script>

</body>
