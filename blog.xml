<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Eric Ma's Blog</title><link href="https://ericmjl.github.io/blog/" rel="alternate"/><link href="https://ericmjl.github.io/blog.xml" rel="self"/><id>urn:uuid:a7611166-dd1f-3792-b62b-0c03a4283350</id><updated>2025-10-28T00:00:00Z</updated><author><name/></author><entry><title>Use coding agents to write Marimo notebooks</title><link href="https://ericmjl.github.io/blog/2025/10/28/use-coding-agents-to-write-marimo-notebooks/" rel="alternate"/><updated>2025-10-28T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:820f6ef9-cdc4-384d-a08e-890efd1130b9</id><content type="html">&lt;p&gt;If you're like me, you might find coding with AI assistants somewhat addictive. And if you're like me, you might also like to write code in Marimo notebooks, the modern alternative to Jupyter that offers better reproducibility and cleaner Python development.&lt;/p&gt;
&lt;p&gt;Turns out there's a way to put these two together for automated Python development and data science workflows, creating a powerful combination for rapid prototyping and iterative coding.&lt;/p&gt;
&lt;h2 id="marimo-s-watch-flag"&gt;Marimo's &lt;code&gt;--watch&lt;/code&gt; Flag&lt;/h2&gt;&lt;p&gt;A few months ago, at SciPy 2025, my friend &lt;a href="https://trevorma.nz/"&gt;Trevor Manz&lt;/a&gt; showed me a cool neat trick for writing Marimo notebooks. Apart from launching a Marimo notebook in &lt;a href="https://docs.marimo.io/guides/package_management/inlining_dependencies/"&gt;sandbox mode&lt;/a&gt;, you add a &lt;code&gt;--watch&lt;/code&gt; flag:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;uvx&lt;span class="w"&gt; &lt;/span&gt;marimo&lt;span class="w"&gt; &lt;/span&gt;edit&lt;span class="w"&gt; &lt;/span&gt;--sandbox&lt;span class="w"&gt; &lt;/span&gt;my_notebook.py&lt;span class="w"&gt; &lt;/span&gt;--watch
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When edits are made to the source file &lt;code&gt;notebook.py&lt;/code&gt;, they will now be reflected in the browser as well. This was my reaction:&lt;/p&gt;
&lt;p&gt;&lt;img src="minion-what.webp" alt="minion-what.webp"&gt;&lt;/p&gt;
&lt;p&gt;If you ever meet Trevor in person, he can confirm that reaction of mine.&lt;/p&gt;
&lt;h2 id="ensure-code-quality-with-marimo-check"&gt;Ensure code quality with &lt;code&gt;marimo check&lt;/code&gt;&lt;/h2&gt;&lt;p&gt;So now, AI coding assistants can write your Marimo notebooks for you... but it's not always going to be correct first time, right? After all, the latest features of Marimo are not going to be part of the large language model training sets.&lt;/p&gt;
&lt;p&gt;Turns out, Marimo also ships with a &lt;code&gt;check&lt;/code&gt; command that you can ask coding agents to call on:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;uvx&lt;span class="w"&gt; &lt;/span&gt;marimo&lt;span class="w"&gt; &lt;/span&gt;check&lt;span class="w"&gt; &lt;/span&gt;my_notebook.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And that will print to stdout any issues that Marimo finds that break its execution model, such as variables that are repeated variables or invalid cells.&lt;/p&gt;
&lt;p&gt;You can instruct coding agents to always run &lt;code&gt;marimo check&lt;/code&gt; by adding the following prompt (or analogous) into &lt;code&gt;AGENTS.md&lt;/code&gt;:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;When editing Marimo notebooks, always run &lt;span class="sb"&gt;`uvx marimo check`&lt;/span&gt; on the file and fix all issues that you find.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This will virtually guarantee correctly-written, AI-generated notebooks. All that's left for us as users is to check the correctness of the analysis that was done.&lt;/p&gt;
&lt;h2 id="real-world-use"&gt;Real-world use&lt;/h2&gt;&lt;p&gt;Now, AI coding assistants (like Cursor, GitHub Copilot, or Claude Code) can write and edit large chunks of Marimo notebook cells for you, check what they wrote, and fix any syntactic issues that show up. And by checking that the cells are syntactically valid. Now you can speed-run those routine and yet highly mundane data manipulation code-writing activities while making yourself an espresso drink. This aligns perfectly with my philosophy on &lt;a href="../../../../2019/3/20/how-i-work/"&gt;optimizing for productivity in data science workflows&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;I've used this mode to speed-run first versions of &lt;a href="../../../4/3/bayesian-superiority-estimation-with-r2d2-priors-a-practical-guide-for-protein-screening/"&gt;probabilistic models in PyMC&lt;/a&gt;, create explainer notebooks for hard concepts, make notebooks that process data, and many, many more things that you'd usually be able to do within a coding notebook system. The key thing that makes this work is feedback given (via the command line) that the coding agent can use for self-correction.&lt;/p&gt;
&lt;h2 id="advanced-functionality-using-mcp-and-built-in-ai-features"&gt;Advanced functionality using MCP and built-in AI features&lt;/h2&gt;&lt;p&gt;It doesn't stop there, though. There's a new &lt;code&gt;--mcp&lt;/code&gt; flag that makes a notebook an MCP server that coding agents can connect to; read more about it &lt;a href="https://opensourcedev.substack.com/p/beyond-chatbots-how-i-turned-python"&gt;here&lt;/a&gt;. Marimo also has built-in AI editing capabilities itself as well. Check out the functionality &lt;a href="https://docs.marimo.io/guides/editor_features/ai_completion/#custom-copilots"&gt;here&lt;/a&gt;, as well as Vincent Warmerdam's short video on &lt;a href="https://www.youtube.com/shorts/CnHOGE46x3o"&gt;using coding agents from &lt;em&gt;within&lt;/em&gt; Marimo&lt;/a&gt;. He's got my vote for best facial/eyebrow expressions from a coding YouTuber!&lt;/p&gt;
</content></entry><entry><title>Exploring Skills vs MCP Servers</title><link href="https://ericmjl.github.io/blog/2025/10/20/exploring-skills-vs-mcp-servers/" rel="alternate"/><updated>2025-10-20T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:cb890a27-6c6a-351a-b47a-c3db04a3f25d</id><content type="html">&lt;p&gt;I spent time digging through Anthropic's skills repository. These are my first impressions, organized for clarity and future reference.&lt;/p&gt;
&lt;h2 id="what-the-anthropic-skills-repository-offers"&gt;What the Anthropic Skills repository offers&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Creative &amp;amp; design workflows&lt;/strong&gt;: &lt;code&gt;algorithmic-art&lt;/code&gt; (generative art with p5.js), &lt;code&gt;canvas-design&lt;/code&gt; (beautiful PNG/PDF outputs guided by design philosophies), &lt;code&gt;theme-factory&lt;/code&gt; (pre-set or on-the-fly themes), and &lt;code&gt;slack-gif-creator&lt;/code&gt; (animated GIFs tuned for Slack). These are turnkey “taste plus tooling” bundles that let the model produce high-quality visuals with consistent aesthetics.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Document skills for real formats&lt;/strong&gt;: &lt;code&gt;document-skills/&lt;/code&gt; cover &lt;code&gt;pptx&lt;/code&gt;, &lt;code&gt;docx&lt;/code&gt;, &lt;code&gt;pdf&lt;/code&gt;, and &lt;code&gt;xlsx&lt;/code&gt; with serious capabilities: layout/templates, tracked changes and comments, text/table extraction, merges/splits, charting, formulas, and formatting preservation. This feels like a pragmatic spec+runtime for working with binary formats—lean instructions up front, heavy lifting when needed.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Development &amp;amp; technical utilities&lt;/strong&gt;: &lt;code&gt;artifacts-builder&lt;/code&gt; (compose complex Claude HTML artifacts using React/Tailwind/shadcn), &lt;code&gt;webapp-testing&lt;/code&gt; (Playwright-driven UI testing), and &lt;code&gt;mcp-builder&lt;/code&gt; (guidance for creating high-quality MCP servers). These reduce boilerplate for the “build and test” loop.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Enterprise &amp;amp; communication&lt;/strong&gt;: &lt;code&gt;brand-guidelines&lt;/code&gt; (apply Anthropic’s official brand colors and typography) and &lt;code&gt;internal-comms&lt;/code&gt; (status reports, newsletters, FAQs). These encode editorial and brand guardrails so outputs stay on-message.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Meta skills and templates&lt;/strong&gt;: &lt;code&gt;skill-creator&lt;/code&gt; and &lt;code&gt;template-skill&lt;/code&gt; show how to structure your own skills: a folder per skill with a &lt;code&gt;SKILL.md&lt;/code&gt; (YAML front matter for &lt;code&gt;name&lt;/code&gt; and &lt;code&gt;description&lt;/code&gt;, plus instructions/examples/guidelines), optional scripts, and assets. This is the pattern to replicate.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you want the source for these examples, it’s viewable in the repo. Start here: &lt;code&gt;https://github.com/anthropics/skills&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="how-skills-are-loaded-and-used"&gt;How skills are loaded and used&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Minimal prompt footprint&lt;/strong&gt;: A skill's short description is passed up front. The larger &lt;code&gt;skill.md&lt;/code&gt; is only read when the model decides it needs more detail.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;On-demand details&lt;/strong&gt;: The model can iterate (ReAct loop) to fetch instructions and then execute scripts or read additional files.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This access pattern keeps the initial token budget small and defers detail until it’s actually needed.&lt;/p&gt;
&lt;h2 id="contrast-with-mcp-servers"&gt;Contrast with MCP servers&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;MCP call shape&lt;/strong&gt;: Tool names and descriptions are typically sent on every call. That keeps tools globally discoverable but increases token overhead.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Skills call shape&lt;/strong&gt;: A tiny descriptor up front; details fetched lazily. Lower baseline token cost.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Distribution model&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;MCP: Centrally hostable (e.g. web server) or vendable (e.g., a Python package). Easy to version, release, and update for many users at once.&lt;/li&gt;
&lt;li&gt;Skills: Feel local-first. You can drag-and-drop into a Claude workspace. Easy to customize, but harder to standardize and propagate updates across a team.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Given current industry patterns, MCP servers are the widely accepted way to expose functionality to LLMs across tools and vendors. Skills are Anthropic-specific at the moment.&lt;/p&gt;
&lt;h2 id="token-efficiency-and-why-its-emphasized"&gt;Token efficiency (and why it’s emphasized)&lt;/h2&gt;&lt;p&gt;Anthropic’s materials lean into token efficiency. The cost of LLM calls adds up, and repeatedly sending long tool descriptions can be expensive. Skills reduce baseline tokens: spend a handful of tokens to register intent, read detail only when needed, then execute. That’s the economic story.&lt;/p&gt;
&lt;h2 id="practical-trade-offs"&gt;Practical trade-offs&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Standardization vs customization&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;MCP servers: Strong for shared, versioned, and centrally updated capabilities.&lt;/li&gt;
&lt;li&gt;Skills: Great for rapid, local customization without infrastructure.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Discovery vs cost&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;MCP: High discoverability; the model always sees the tools. Higher token floor.&lt;/li&gt;
&lt;li&gt;Skills: Low token floor; details fetched when needed. Requires the model to choose to read more.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="open-questions-im-tracking"&gt;Open questions I’m tracking&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;How will teams distribute and update skills at scale without a central registry or packaging story?&lt;/li&gt;
&lt;li&gt;Will skills gain cross-vendor support, or remain Anthropic-only?&lt;/li&gt;
&lt;li&gt;What’s the best practice to map a complex skill into smaller, composable units without losing clarity?&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="early-take"&gt;Early take&lt;/h2&gt;&lt;p&gt;IMO, skills are a clear attempt to lower token costs and streamline task-specific workflows with minimal upfront context. MCP servers remain the well-understood, cross-ecosystem pattern for exposing capabilities. If your goal is a shareable, versioned interface for many users, MCP is still the safer default. If you need quick, local customization inside Claude with a lean prompt footprint, skills are compelling. But this field has been evolving at breawkneck speed anyways, so expect changes.&lt;/p&gt;
</content></entry><entry><title>How to expose any documentation to any LLM agent</title><link href="https://ericmjl.github.io/blog/2025/10/19/how-to-expose-any-documentation-to-any-llm-agent/" rel="alternate"/><updated>2025-10-19T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:70188213-fcc1-328b-8c1f-f1a5fa7f43e1</id><content type="html">&lt;p&gt;Like cars that lose value as soon as they roll off the lot, LLMs become outdated as soon as their training sets are fixed. Software documentation evolves constantly—new features, API changes, bug fixes, and best practices emerge daily. Yet AI agents are stuck with whatever knowledge was captured in their training data, creating a fundamental mismatch between what they know and what developers actually need in real-time.&lt;/p&gt;
&lt;p&gt;Building LlamaBot taught me something unexpected: the hardest part of AI-assisted development isn't writing better prompts or designing cleaner abstractions. It's equipping AI agents with up-to-date information in a stable, standardized fashion.&lt;/p&gt;
&lt;p&gt;Most developers know the frustration of context-switching between code and documentation. You're deep in a coding session, need to check how a specific function works, and suddenly you're hunting through static documentation files. AI agents face this same problem, but with an added layer of complexity—they need structured, queryable access to documentation that can be searched semantically.&lt;/p&gt;
&lt;p&gt;I discovered that web searches by coding agents were less reliable than manually adding context, but manual approaches don't scale. The solution emerged through the Model Context Protocol (MCP), a standard that enables LLMs to interact with external tools and data sources. In LlamaBot v0.13.10, I introduced a documentation MCP server that automatically equips AI agents with current information. This enables AI agents to access organizational knowledge, process documentation, and domain expertise in structured ways.&lt;/p&gt;
&lt;h2 id="the-obsolescence-problem-in-ai-assisted-development"&gt;The obsolescence problem in AI-assisted development&lt;/h2&gt;&lt;p&gt;The core issue more than mere documentation access, it's about obsolescence. LLMs are trained on data that becomes outdated the moment it's fixed in their training sets. Meanwhile, software documentation evolves constantly. New features are added, APIs change, bugs are fixed, and best practices emerge. Yet AI agents remain frozen in time, working with knowledge that may be months or years out of date.&lt;/p&gt;
&lt;p&gt;Consider a typical data science workflow: you're building an AI pipeline and need to understand how LlamaBot's StructuredBot handles data validation. The AI agent might reference documentation from six months ago, missing critical updates or new features that could solve your problem more elegantly. This creates a fundamental mismatch between what the agent knows and what's actually available.&lt;/p&gt;
&lt;p&gt;The deeper problem is that AI agents need structured, queryable access to documentation that can be searched semantically and updated automatically. They need to understand not just what functions exist, but how they relate to each other, what patterns they follow, and how they fit into broader workflows. Static documentation simply cannot provide this level of contextual understanding, particularly in data science environments where teams maintain scattered knowledge across wikis, Slack threads, and onboarding documents.&lt;/p&gt;
&lt;h2 id="building-a-semantic-documentation-layer"&gt;Building a semantic documentation layer&lt;/h2&gt;&lt;p&gt;LlamaBot's MCP server demonstrates how to give AI agents structured access to its documentation by creating a dynamic, queryable knowledge base that agents can search semantically. The &lt;a href="https://github.com/ericmjl/llamabot/blob/main/llamabot/mcp_server.py"&gt;implementation&lt;/a&gt; centers around a single tool:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@mcp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tool&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;docs_search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;limit&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Search through LlamaBot documentation and source code.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;docstore&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;retrieve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_results&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;limit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;query&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;results&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This interface sits in front of a data pipeline that builds a vector database for the documentation. The server fetches the latest documentation from GitHub, extracts Python module docstrings from source code, and constructs a LanceDB vector database optimized for semantic search. The database is built during CI/CD and packaged directly with the wheel distribution, giving users instant access without setup while staying current with each release.&lt;/p&gt;
&lt;p&gt;This approach works with any AI agent system through the MCP protocol, providing a standardized way to keep AI agents current with documentation.&lt;/p&gt;
&lt;h2 id="the-architecture-behind-semantic-documentation"&gt;The architecture behind semantic documentation&lt;/h2&gt;&lt;p&gt;The MCP server combines several technologies to create a robust documentation system. FastMCP handles the protocol implementation, enabling seamless communication between AI agents and the documentation database. LanceDB powers the semantic search capabilities, leveraging LlamaBot's existing &lt;code&gt;LanceDBDocStore&lt;/code&gt; class with hybrid search and reranking for optimal results.&lt;/p&gt;
&lt;p&gt;The system uses the checked-out documentation from the repository during the CI/CD build process, ensuring the packaged database contains current information. The build script first attempts to fetch docs from GitHub, but falls back to the local &lt;code&gt;docs/&lt;/code&gt; directory when available, making it work seamlessly in both CI/CD and development environments. The build process runs the &lt;code&gt;scripts/build_mcp_docs.py&lt;/code&gt; script during CI/CD, which creates the LanceDB database and copies it to &lt;code&gt;llamabot/data/mcp_docs/&lt;/code&gt; for packaging.&lt;/p&gt;
&lt;p&gt;I believe this architecture represents a fundamental shift in how we think about documentation for AI systems. Instead of treating documentation as static reference material, we're creating dynamic, queryable knowledge bases that AI agents can interact with directly.&lt;/p&gt;
&lt;h2 id="the-core-pattern-to-replicate"&gt;The core pattern to replicate&lt;/h2&gt;&lt;p&gt;The LlamaBot MCP server follows a straightforward pattern that any package or documentation source can replicate. Here's the essential blueprint:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Build a semantic database during CI/CD&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Extract documentation from your source (GitHub, local docs, API references)&lt;/li&gt;
&lt;li&gt;Parse and chunk the content appropriately for your domain&lt;/li&gt;
&lt;li&gt;Create a vector database (LanceDB, Chroma, or similar) with semantic search capabilities&lt;/li&gt;
&lt;li&gt;Package the database with your distribution&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;2. Create an MCP server with a search tool&lt;/strong&gt;&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@mcp&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tool&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;docs_search&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;limit&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Search through your documentation and source code.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;docstore&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;retrieve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_results&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;limit&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;query&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;results&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;results&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;3. Make it discoverable and configurable&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Provide a simple launch command (like &lt;code&gt;yourpackage mcp launch&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;Include clear setup instructions for MCP-compatible tools&lt;/li&gt;
&lt;li&gt;Ensure the database updates automatically with each release&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;4. Design for your specific knowledge domain&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Include not just API docs, but process documentation, examples, and institutional knowledge&lt;/li&gt;
&lt;li&gt;Structure the content for semantic search rather than keyword matching&lt;/li&gt;
&lt;li&gt;Consider what context your users need most when working with AI agents&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="seamless-integration-with-modern-development-tools"&gt;Seamless integration with modern development tools&lt;/h2&gt;&lt;p&gt;The MCP server works with any MCP-compatible coding environment, including Cursor, VSCode, and other modern development tools. Configuration requires a single command in your MCP settings:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;uvx&lt;span class="w"&gt; &lt;/span&gt;--with&lt;span class="w"&gt; &lt;/span&gt;llamabot&lt;span class="o"&gt;[&lt;/span&gt;all&lt;span class="o"&gt;]&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;llamabot&lt;span class="w"&gt; &lt;/span&gt;mcp&lt;span class="w"&gt; &lt;/span&gt;launch
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once configured, AI agents can query LlamaBot documentation using natural language queries. Ask "How do I use StructuredBot for data extraction?" and the agent receives structured results with content, relevance scores, and metadata. This contextual information enables agents to provide accurate, up-to-date assistance without manual documentation lookup.&lt;/p&gt;
&lt;p&gt;This reduces context-switching between code and documentation. AI agents can access relevant information and provide suggestions based on current code patterns and usage examples. This approach is particularly valuable for data science teams who need to maintain consistency across experiments while leveraging the latest library capabilities.&lt;/p&gt;
&lt;h2 id="comparing-documentation-approaches-for-ai-agents"&gt;Comparing documentation approaches for AI agents&lt;/h2&gt;&lt;p&gt;There are several ways to provide documentation to AI agents, each with distinct trade-offs. Understanding these approaches helps clarify why the MCP server approach represents a significant improvement.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Native tool documentation&lt;/strong&gt; (like Cursor's built-in docs capabilities) offers seamless integration and can fetch docs from online sources, but you're limited to how the tool fetches those docs. It may not be able to access certain systems gated behind access controls or include custom organizational knowledge and process documentation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Manual repository inclusion&lt;/strong&gt; works well for users familiar with IDEs, workspaces, and development concepts, but it requires familiarity with these practices that are unfamiliar to non-technical users or individual developers. It also doesn't scale beyond individual developers. The documentation becomes part of the context window, consuming valuable tokens and potentially overwhelming the agent with irrelevant information.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Copy-paste or file upload&lt;/strong&gt; (like Claude Projects) provides flexibility for non-technical users but creates maintenance overhead. You must manually update documentation when it changes, and there's no semantic search capability—agents can only work with what you explicitly provide.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Web search by agents&lt;/strong&gt; seems convenient but creates inefficiency in the development workflow. If the documentation is up-to-date, the LLM will find it eventually, but it requires multiple iterations of web searches to locate the right information. I discovered this firsthand when building LlamaBot—web searches by coding agents required more iterations than directly providing context, but manual approaches don't scale.&lt;/p&gt;
&lt;p&gt;The MCP server approach provides automatic updates, semantic search, system-agnostic compatibility, and organizational knowledge integration. It offers a standardized way to keep AI agents current with evolving documentation. The trade-off is initial setup complexity, but this is mitigated by the pre-built databases that ship with packages.&lt;/p&gt;
&lt;h2 id="beyond-software-documentation-surfacing-any-process-knowledge"&gt;Beyond software documentation - surfacing any process knowledge&lt;/h2&gt;&lt;p&gt;The MCP approach extends beyond software documentation. The LlamaBot server gives an example of how organizations can surface their process documentation, institutional knowledge, and domain expertise.&lt;/p&gt;
&lt;p&gt;I believe that data science teams could transform their workflow documentation—experimental protocols, data validation procedures, model evaluation criteria, or deployment checklists—from scattered wikis, Slack threads, and buried onboarding documents into structured, queryable knowledge bases that AI agents can access and reference during development.&lt;/p&gt;
&lt;p&gt;I can see this approach scaling beyond individual libraries to entire organizational knowledge. We have all imagined AI agents that can query your team's coding standards, understand your deployment procedures, or reference your data governance policies—all without leaving their development environment. Each organization would maintain their own specialized knowledge base, creating networks of interconnected AI-accessible process documentation. How would one implement this? A documentation MCP server may be a great way to start.&lt;/p&gt;
&lt;p&gt;This approach isn't just for software docs. Imagine surfacing your team's process knowledge, onboarding guides, or even those golden nuggets buried in Slack threads. The MCP server pattern can turn scattered, informal knowledge into a living, searchable resource for both humans and AI agents, especially if you treat your processes as versioned software to be exposed to AI agents!&lt;/p&gt;
&lt;p&gt;In my experience, the most valuable knowledge in organizations often exists in informal channels—Slack conversations, email threads, or tribal knowledge that never gets documented. I believe the MCP approach provides a framework for capturing and surfacing this knowledge in ways that AI agents can understand and reference.&lt;/p&gt;
&lt;h2 id="the-future-of-ai-assisted-development"&gt;The future of AI-assisted development&lt;/h2&gt;&lt;p&gt;Future iterations could include real-time updates that rebuild databases when documentation changes, cross-organizational knowledge graphs, and usage pattern analysis that learns from how teams implement processes.&lt;/p&gt;
&lt;p&gt;The goal is to make AI agents active participants in organizational processes, capable of understanding team workflows and providing context-aware recommendations.&lt;/p&gt;
&lt;p&gt;This vision requires rethinking how we structure and maintain organizational knowledge. Instead of writing documentation solely for human consumption, we need to design knowledge systems that serve both human team members and AI agents, creating a symbiotic relationship between human creativity and AI capability while preserving institutional knowledge in accessible, queryable formats.&lt;/p&gt;
&lt;h2 id="getting-started-with-semantic-documentation"&gt;Getting started with semantic documentation&lt;/h2&gt;&lt;p&gt;The MCP server is available in LlamaBot v0.13.10 and later. Getting started requires minimal setup:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Install LlamaBot with MCP support: &lt;code&gt;pip install llamabot[all]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Configure your coding tool to use the MCP server&lt;/li&gt;
&lt;li&gt;Begin coding with AI agents that understand LlamaBot's capabilities&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The documentation database ships pre-built with the package, eliminating setup friction. The server exposes a single &lt;code&gt;docs_search&lt;/code&gt; tool that agents can use to find relevant documentation and source code information, creating a seamless development experience.&lt;/p&gt;
&lt;p&gt;This approach makes documentation an integral part of the AI agent's toolkit, resulting in more capable assistants that can help developers work more effectively.&lt;/p&gt;
&lt;p&gt;The future of AI-assisted development involves better integration between AI agents and the tools they need. LlamaBot's MCP server demonstrates how this integration can work in practice.&lt;/p&gt;
</content></entry><entry><title>A practical comparison of DSPy and LlamaBot for structured LLM applications</title><link href="https://ericmjl.github.io/blog/2025/10/18/a-practical-comparison-of-dspy-and-llamabot-for-structured-llm-applications/" rel="alternate"/><updated>2025-10-18T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:0282121b-3e33-3ecb-9837-afd6b1121706</id><content type="html">&lt;p&gt;When Omar Khattabe presented &lt;a href="https://dspy.ai"&gt;DSPy 3.0&lt;/a&gt; at PyData Boston Cambridge last week, I finally had the chance to dig into a framework that's been generating significant buzz in the LLM development community. As someone who's built structured LLM applications with &lt;a href="https://ericmjl.github.io/llamabot/"&gt;LlamaBot&lt;/a&gt;, I was particularly curious about DSPy's core claim: that signatures represent the only abstraction you need for LLM-powered programs.&lt;/p&gt;
&lt;p&gt;The presentation focused on two key concepts: signatures as a new LLM abstraction and prompt optimization techniques. But what caught my attention was the practical similarity between DSPy's approach and what I've been doing with LlamaBot's StructuredBot. This led me to build a direct comparison using a real-world example from my personal expense tracking application.&lt;/p&gt;
&lt;h2 id="the-structured-llm-challenge"&gt;The structured LLM challenge&lt;/h2&gt;&lt;p&gt;Most developers working with LLMs face the same fundamental problem: how do you reliably extract structured data from unstructured inputs? Whether you're processing receipts, parsing documents, or analyzing text, you need consistent, typed outputs that integrate cleanly with your existing systems.&lt;/p&gt;
&lt;p&gt;Traditional approaches rely heavily on natural language prompts, which are fragile, hard to maintain, and difficult to optimize. DSPy proposes a different path through its signature abstraction, claiming this eliminates the need for verbose prompt engineering.&lt;/p&gt;
&lt;h2 id="a-real-world-comparison-receipt-processing"&gt;A real-world comparison: Receipt processing&lt;/h2&gt;&lt;p&gt;To test DSPy's claims, I built a practical comparison using an expense extraction system I developed for personal use. This application processes receipts in various formats (PNG, PDF, JPG, WEBP) and automatically extracts structured expense data into Notion — essentially a lightweight alternative to enterprise expense management systems.&lt;/p&gt;
&lt;p&gt;The challenge here is typical of structured LLM applications: converting unstructured visual and textual data into consistent, typed outputs that integrate with existing workflows. Let's see how both frameworks handle this task.&lt;/p&gt;
&lt;h3 id="llamabot-s-structuredbot-approach"&gt;LlamaBot's StructuredBot approach&lt;/h3&gt;&lt;p&gt;LlamaBot uses Pydantic models to define structured outputs, leveraging Python's type system for validation and documentation. The approach emphasizes explicit data modeling with detailed field descriptions:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;pydantic&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;enum&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Enum&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;typing&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Optional&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;pathlib&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;llamabot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;lmb&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nc"&gt;FlowType&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Enum&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;MONEY_OUT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Money Out&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;MONEY_IN&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Money In&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nc"&gt;TypeEnum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Enum&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;PAYMENT&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Payment&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;INVOICE&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Invoice&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nc"&gt;PaymentMethodEnum&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Enum&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;CASH&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Cash&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;BANK_TRANSFER&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Bank Transfer&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;CREDIT_CARD&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Credit Card&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;CHECK&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Check&amp;quot;&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nc"&gt;ExpenseData&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseModel&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;transaction_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Short, memorable description of the purchase. E.g.: &amp;#39;Anker Dock&amp;#39;, &amp;#39;Coffee at Triangle Bar&amp;#39;, &amp;#39;dbrand laptop skin&amp;#39;&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;date&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;transaction date&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;amount&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;transaction amount&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;category&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Business category, e.g. Office Supplies, Travel, Meals&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;TypeEnum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Either Payment or Invoice&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;flow&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;FlowType&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Either &amp;#39;Money Out&amp;#39; or &amp;#39;Money In&amp;#39;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;payment_method&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;PaymentMethodEnum&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;How the payment was made.&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;purpose&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Brief business purpose or description of the expense.&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;reference_number&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Optional&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Invoice/receipt number if visible&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;person&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Optional&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;Person responsible or who made the purchase if mentioned.&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Usage&lt;/span&gt;
&lt;span class="n"&gt;bot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StructuredBot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;system_prompt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;pydantic_model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ExpenseData&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;model_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ollama_chat/gemma3n:latest&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/path/to/receipt.png&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="dspy-s-signature-approach"&gt;DSPy's signature approach&lt;/h3&gt;&lt;p&gt;DSPy takes a different approach with its signature abstraction, which defines both inputs and outputs in a single class. The framework emphasizes simplicity and automatic prompt optimization:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;dspy&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nc"&gt;ExpenseExtraction&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dspy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Signature&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Extract expense information from receipt images.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class="n"&gt;receipt_image&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;dspy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dspy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;InputField&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;desc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Receipt image&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;transaction_name&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dspy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OutputField&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;desc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Short description of the purchase&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;date&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dspy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OutputField&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;desc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Transaction date (YYYY-MM-DD)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;amount&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dspy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OutputField&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;desc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Total transaction amount (number, no currency symbols)&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;category&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dspy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OutputField&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;desc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Business category (e.g., Office Supplies, Travel, Meals)&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="nb"&gt;type&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dspy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OutputField&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;desc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Transaction type, either &amp;#39;Payment&amp;#39; or &amp;#39;Invoice&amp;#39;&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;flow&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dspy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OutputField&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;desc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Cash flow direction, either &amp;#39;Money Out&amp;#39; or &amp;#39;Money In&amp;#39;&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;payment_method&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dspy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OutputField&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;desc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;How the payment was made (e.g., Cash, Bank Transfer, Credit Card, Check)&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;purpose&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dspy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OutputField&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;desc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Brief business purpose or description&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;reference_number&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dspy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OutputField&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;desc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Invoice/receipt number if present&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;person&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dspy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;OutputField&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;desc&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Person involved, if mentioned&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;default&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Usage&lt;/span&gt;
&lt;span class="n"&gt;lm&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dspy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LM&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ollama_chat/gemma3n:latest&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;dspy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;configure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lm&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;module&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;dspy&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Predict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;ExpenseExtraction&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;receipt_image&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;images&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="comparing-the-approaches"&gt;Comparing the approaches&lt;/h2&gt;&lt;p&gt;Both frameworks successfully extracted structured data from receipt images, but they take fundamentally different approaches to the problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;LlamaBot's StructuredBot&lt;/strong&gt; leverages Python's existing type system through Pydantic models. This approach provides several advantages: automatic validation, IDE support, and integration with existing Python data processing pipelines. The explicit type definitions make the data contract clear and enforceable.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DSPy's signatures&lt;/strong&gt; offer a more streamlined interface that combines input and output definitions in a single class. The framework's strength lies in its automatic prompt optimization capabilities, which can improve performance over time without manual intervention.&lt;/p&gt;
&lt;h2 id="key-differences-in-practice"&gt;Key differences in practice&lt;/h2&gt;&lt;p&gt;The most noticeable difference is verbosity. LlamaBot requires more explicit type definitions and imports, while DSPy's signature approach is more concise. However, this conciseness may come at the cost of some type safety and IDE support that Pydantic provides.&lt;/p&gt;
&lt;p&gt;Both frameworks use LiteLLM for model routing, making it easy to switch between different LLM providers. The model configuration syntax is identical, which suggests a common underlying architecture.&lt;/p&gt;
&lt;h2 id="the-schema-first-principle"&gt;The schema-first principle&lt;/h2&gt;&lt;p&gt;Regardless of which framework you choose, structured LLM applications require careful upfront schema design. The bulk of development time goes into defining your data model, not writing prompts. This schema-first approach is what makes these frameworks powerful—they force you to think clearly about your data requirements before implementation.&lt;/p&gt;
&lt;h2 id="looking-ahead-dspy-s-broader-vision"&gt;Looking ahead: DSPy's broader vision&lt;/h2&gt;&lt;p&gt;DSPy's claim that signatures are the only abstraction needed for LLM applications is ambitious but not entirely accurate. The framework includes additional abstractions like modules and optimizers that handle more complex scenarios. Signatures represent the core abstraction for simple input-output transformations, but building production LLM applications often requires more sophisticated orchestration.&lt;/p&gt;
&lt;p&gt;I'm planning to explore DSPy's more advanced features as I rebuild LlamaBot's agent abstractions. The goal is to understand how to construct autonomous LLM agent frameworks rather than individual agents—a challenge that requires thinking beyond simple input-output mappings.&lt;/p&gt;
&lt;p&gt;Being unfamiliar with DSPy's documentation initially, I found it challenging to follow, but thanks to fellow PyData Boston Cambridge organizer &lt;a href="https://www.linkedin.com/in/nnssa/"&gt;Nash Sabti&lt;/a&gt;'s guidance, I was able to make it happen and build this comparison.&lt;/p&gt;
&lt;p&gt;The structured LLM landscape is rapidly evolving, and frameworks like DSPy and LlamaBot are pushing the boundaries of what's possible. The key insight is that successful LLM applications require the same engineering discipline as traditional software: clear interfaces, robust error handling, and maintainable abstractions.&lt;/p&gt;
</content></entry><entry><title>How to Use Coding Agents Effectively</title><link href="https://ericmjl.github.io/blog/2025/10/14/how-to-use-coding-agents-effectively/" rel="alternate"/><updated>2025-10-14T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:11cedff3-fc21-31fb-b882-ef11b228c2e1</id><content type="html">&lt;p&gt;This past week, I went on a building spree, a part of my ongoing ultralearning practice, and built multiple projects using AI coding assistants. After many months of working with AI coding assistants on real projects, I've learned that effective agent usage requires more than just good prompts. You need systematic workflows, external memory systems, and a willingness to let the agent fail fast so you can discover architectural boundaries.&lt;/p&gt;
&lt;p&gt;These are the patterns that make coding agents productive.&lt;/p&gt;
&lt;h2 id="starting-out"&gt;Starting Out&lt;/h2&gt;&lt;p&gt;Effective agent usage starts with establishing a disciplined workflow that covers the complete development lifecycle. This isn't just about fancy prompts; we're talking about creating a repeatable process that works from start to finish.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The Complete Lifecycle&lt;/strong&gt;&lt;/p&gt;
&lt;pre class="mermaid"&gt;
flowchart TD
    A[Plan] --&gt; B[Write Tests]
    B --&gt; C[Implement Code]
    C --&gt; D[Run Tests]
    D --&gt; E{Tests Pass?}
    E --&gt;|No| F[Fix Issues]
    F --&gt; D
    E --&gt;|Yes| G[Document]
    G --&gt; H[Run Full Test Suite]
    H --&gt; I{All Tests Pass?}
    I --&gt;|No| F
    I --&gt;|Yes| J[Complete]

    style A fill:#e1f5fe
    style B fill:#f3e5f5
    style C fill:#e8f5e8
    style D fill:#fff3e0
    style G fill:#f1f8e9
    style J fill:#e8f5e8
&lt;/pre&gt;&lt;p&gt;Here's the systematic workflow that works best with coding agents:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Plan first, then execute. Break your work into planning and execution phases, just like you would if writing code yourself. Have the agent write planning documents it can follow. This separation matters because planning and execution often use different parts of the model, and sometimes "dumber" models execute plans better than expensive ones.&lt;/li&gt;
&lt;li&gt;Write tests before implementation. This is where TDD becomes crucial with agents. Write tests first, then implement, then test the code. When tests pass, document. This workflow becomes more important with AI assistants because they're working with small context windows compared to your entire codebase. You must have the AI write tests for everything it generates.&lt;/li&gt;
&lt;li&gt;Implement with clear feedback loops. The proper TDD flow is: tests are always written first, executed and failed (because the implementation is lacking), then implemented, and executed again—ideally succeeding on the first try. This is super important for highest effectiveness with coding agents. The AI needs the clear feedback loop of failing tests to understand what to implement.&lt;/li&gt;
&lt;li&gt;Document as you go. When tests pass, document the implementation. This creates a complete record of what was built and why.&lt;/li&gt;
&lt;li&gt;Loop back to tests until everything is fixed. This is the critical step that many people miss. Don't stop at the first passing test—run the full test suite, check edge cases, and iterate until all tests pass consistently. The agent should keep running tests and fixing issues until the entire system is stable.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Learn your tool's shortcuts and modes. In Cursor, for example, you can open a new agent window with Cmd+E, and use Shift+Tab to toggle to plan mode (yellow colored). These modes work different parts of the model—planning models are better at analyzing code and planning than executing, while execution models are cheaper and sometimes more reliable at following plans.&lt;/p&gt;
&lt;p&gt;In VS Code with GitHub Copilot, you can define custom modes. You can even get Agent Mode to write a Planning Mode for you as a way to bootstrap Plan Mode. This gives you specialized interfaces for different types of work.&lt;/p&gt;
&lt;p&gt;Most of us software builders like to do the building part, not the verification part. TDD with agents lets you delegate the tedious verification work while keeping the fun building part for humans, as long as you review the tests the agent writes. This is another place where agents excel at taking over work we'd rather not do ourselves.&lt;/p&gt;
&lt;p&gt;Without this discipline, you'll find yourself debugging issues that could have been caught earlier. The complete lifecycle ensures that every piece of code is tested, documented, and verified before moving on.&lt;/p&gt;
&lt;p&gt;Finally, break work into chunks you can maintain concentration for during review. This takes practice getting used to an LLM's outputs, but it's important for effectiveness. Start with smaller scopes and gradually increase as you get comfortable with the agent's output patterns. The goal is to find the sweet spot where you can maintain focus while the agent does meaningful work.&lt;/p&gt;
&lt;h2 id="building-momentum"&gt;Building Momentum&lt;/h2&gt;&lt;p&gt;When starting a new project, don't try to get everything right the first time. Instead, speed-run your project twice, perhaps even thrice, in quick iteration mode. Just accept and vibe-code your way to the point where it gets hard for the LLM to do what you're asking.&lt;/p&gt;
&lt;p&gt;On each speed-run, you'll likely find yourself cornered architecturally. Step back and diagnose what's going wrong. Then speed-run the process once more to see if you can corner yourself another way. On your third try, you'll have made enough mistakes to clarify the mental model of the problem.&lt;/p&gt;
&lt;p&gt;I recently built a dataset versioning package called Kirin this way. It took three iterations over about a week to get the architecture right. The first two attempts helped me understand the problem space; the third attempt succeeded because I had learned the boundaries. The UI was done twice, and only on the third try did I get it right—all within about a week. This really helps with the design process, similar to the principles in "Design of Everyday Things."&lt;/p&gt;
&lt;h2 id="systematic-improvement"&gt;Systematic Improvement&lt;/h2&gt;&lt;p&gt;Once you have a working system, agents work well for systematic improvement tasks. The key is to ask them to prioritize rather than trying to fix everything at once.&lt;/p&gt;
&lt;p&gt;Test coverage improvement: Instead of asking the agent to improve coverage on every line, ask it to prioritize based on highest impact for fewest changes. Get its ranking of issues, then pick the one you understand and can review. Sometimes you might find the 2nd or 3rd highest ranked issue to be the one you understand and can review, which is super important here. Then you pick, and build a plan around it before executing.&lt;/p&gt;
&lt;p&gt;Ask the agent to give you its ranking of issues with explanations. This helps you understand not just what to fix, but why it matters and what the trade-offs are.&lt;/p&gt;
&lt;p&gt;Refactoring: Look across a class of files (like HTML templates) and ask the agent to identify refactoring opportunities. Again, ask it to prioritize, pick one, and record the others as GitHub issues for later. Pick two more categories and record them as GitHub issues, and tackle them later.&lt;/p&gt;
&lt;p&gt;For example, ask the agent to look across HTML Jinja templates and identify places where common HTML elements can be reused. Use the same prioritization trick: ask it to rank opportunities, pick the one you understand, and build a plan around it.&lt;/p&gt;
&lt;p&gt;Documentation review: Have the agent examine all docs in your repo and identify where docs document something not present in code, where there are gaps, and where docs are inaccurate. Prioritize major categories, pick one to tackle, and leave the others as GitHub issues.&lt;/p&gt;
&lt;p&gt;Ask the agent to identify three specific problems: (a) where docs document something not present in code, (b) where there are gaps (things in code not documented), and (c) where docs are inaccurate relative to what's present in the code. Again, prioritize major categories, pick one to tackle, and leave the others as GitHub issues.&lt;/p&gt;
&lt;h2 id="advanced-patterns"&gt;Advanced Patterns&lt;/h2&gt;&lt;p&gt;Your repository's issue tracker becomes an organized external memory system. It's stateful, has conversation records, and is plain text in Markdown. Use it liberally.&lt;/p&gt;
&lt;p&gt;When you have plans you don't want to act on immediately, ask the agent to post them as GitHub issues using the &lt;code&gt;gh&lt;/code&gt; CLI. This prevents losing track of ideas and creates a backlog you can return to.&lt;/p&gt;
&lt;p&gt;Use this prompt:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;"ok, I would like you to put this up on github as an issue. use the gh cli to do that. check that i'm logged in as ericmjl and not on any other account."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This ensures the issue gets created in the right repository with the right account.&lt;/p&gt;
&lt;p&gt;For existing issues, ask the agent to evaluate whether they're still relevant and give its reasons. Codebases evolve, and you might be able to deprecate some issues. Take the agent's reasons and do a quick dive yourself to decide whether to tackle it or not. If you decide to proceed, launch a new agent and ask it to use the content of that GitHub issue as context.&lt;/p&gt;
&lt;p&gt;Create an &lt;code&gt;AGENTS.md&lt;/code&gt; file to document your architectural preferences and tool patterns. This teaches the coding agent your standards and helps it make better decisions.&lt;/p&gt;
&lt;p&gt;For example, when building Kirin, I started with HTMX+FastAPI but took three iterations to settle on "everything is an API endpoint, but CRUD endpoints must redirect to view endpoints." This also happened to be an architectural pattern that I settled on only after two iterations on the UI. Another pattern that I settled on was to build the Python API first, then reuse it behind web UI APIs, like building the backend API before the frontend. I settled on this pattern after discovering discrepancies between the UI's sluggish performance and the Python API's snappy performance.&lt;/p&gt;
&lt;p&gt;Document your favorite tools and patterns in &lt;code&gt;AGENTS.md&lt;/code&gt;. You can "teach" the agent to use the &lt;code&gt;gh&lt;/code&gt; CLI for GitHub operations rather than doing janky cURL commands. Use this file as a way to encode your development standards.&lt;/p&gt;
&lt;p&gt;For example, you can "teach" it to use the &lt;code&gt;gh&lt;/code&gt; CLI to get issues from GitHub by literally saying "use the &lt;code&gt;gh&lt;/code&gt; cli to get issue contents from this repo's issues" and it'll almost always reliably do so rather than doing janky cURL commands. This is an important part of building out your test harness and development workflow.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;MCP servers for specialized knowledge:&lt;/strong&gt; Plug in an MCP (Model Context Protocol) server that serves up documentation about core packages or specialized ways of working specific to your organization. This gives the agent access to your internal knowledge base, coding standards, and domain-specific patterns without cluttering the main context window. The agent can then reference this specialized knowledge when making architectural decisions or implementing features.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Custom shortcuts:&lt;/strong&gt; Slash commands are powerful shortcuts for giving textual context to coding agents. Create them freely, delete them freely, and merge them freely. Experiment to see what works with your habits.&lt;/p&gt;
&lt;p&gt;My favorites include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;/remember&lt;/code&gt; - Get the agent to remember important information in &lt;code&gt;AGENTS.md&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;/branch-and-stage&lt;/code&gt; - Create a new git branch and stage all changes after completing work&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here's the actual slash command for &lt;code&gt;/branch-and-stage&lt;/code&gt;:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;%% /branch-and-stage.md %%
Given everything we just did, or given what you see when you run git diff, give me a new git branch and git add to stage all the changes. You do not need to give me a commit message, I have a git commit message writer.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And for &lt;code&gt;/remember&lt;/code&gt;:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;%% /remember %%
Remember what you just learned (or what I am about to say) by writing it into AGENTS.md.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can phrase many of these tips as slash commands. The key is making repetitive tasks into simple text shortcuts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;No task is too small:&lt;/strong&gt; Agents work well for mundane tasks that humans find tedious. I have a slash command for markdown linting because I'm that nitpicky, but it proves the point: no task is too mundane for a coding agent, as long as it can access the output as text to verify it did the work correctly.&lt;/p&gt;
&lt;p&gt;This works so well because agents have gotten great at using command line tools, and command line outputs are exactly the kind of interface LLMs need: text. Every git command, every test run, every build process produces text that the agent can read, understand, and act upon.&lt;/p&gt;
&lt;p&gt;Use agents for CI/CD pipeline maintenance. If your CI/CD isn't conditional (running tests on PRs that only touch documentation), get the agent to make PR tests run only on relevant file changes. Make sure the changes are easily reviewable. This is an important part of building out your test harness.&lt;/p&gt;
&lt;p&gt;For example, if your CI/CD is not conditional and you run tests even on PRs that only touch documentation, get the agent to make your PR tests run only on changes to relevant files—source, config, etc., but not on docs.&lt;/p&gt;
&lt;p&gt;For large PRs, ask the agent to give you an overview of contents. Use your tool's "plan" mode to get a first-pass grasp of what's changed. This is especially useful when you have very large PRs to review—start with your tool's "plan" mode to help you get a first-pass grasp of the contents.&lt;/p&gt;
&lt;p&gt;The meta-workflow that works best is:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Plan (write as a .md file, save in plans/ directory)&lt;/li&gt;
&lt;li&gt;Execute (write the code)&lt;/li&gt;
&lt;li&gt;Write tests&lt;/li&gt;
&lt;li&gt;Run tests&lt;/li&gt;
&lt;li&gt;Re-execute as necessary until tests pass&lt;/li&gt;
&lt;li&gt;Audit - check the code against the plan&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;You don't need fancy prompts for this. Write out your high-level goals, have the tool write the plan, read the plan back to you, correct its assumptions, then proceed with steps 2-6.&lt;/p&gt;
&lt;p&gt;GIGO (Garbage In, Garbage Out) applies to AI coding just as much as everything else. If you're sloppy and undisciplined, you'll get predictably bad results.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;Effective agent usage isn't about finding the perfect prompt. It's about creating systematic workflows that use the agent's strengths while compensating for its weaknesses. It's about building external memory systems that persist across sessions. It's about teaching the agent your standards so it can make better decisions.&lt;/p&gt;
&lt;p&gt;The key is being willing to fail fast, learn from mistakes, and iterate quickly. The agent amplifies your development process, but only if you're disciplined about how you use it.&lt;/p&gt;
&lt;p&gt;Coding agents are becoming standard tools. The question isn't whether they'll replace developers, it's whether you'll learn to use them effectively. These patterns have changed how I approach development, and they can do the same for you.&lt;/p&gt;
</content></entry><entry><title>How to use multiple GitHub accounts on the same computer</title><link href="https://ericmjl.github.io/blog/2025/10/10/how-to-use-multiple-github-accounts-on-the-same-computer/" rel="alternate"/><updated>2025-10-10T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:ecca655b-1380-3661-949f-b51d525785a6</id><content type="html">&lt;h1 id="how-to-use-multiple-github-accounts-on-the-same-computer"&gt;How to use multiple GitHub accounts on the same computer&lt;/h1&gt;&lt;p&gt;I recently ran into a frustrating situation where I couldn't push to a repository even though I had the right permissions. The problem? I was trying to use two different GitHub accounts on the same computer, and Git was getting confused about which account to use.&lt;/p&gt;
&lt;p&gt;If you're in a similar situation - maybe you have a personal account and also contribute to a non-profit or open source project with a separate account - this guide will help you set everything up correctly.&lt;/p&gt;
&lt;h2 id="the-core-problem"&gt;The core problem&lt;/h2&gt;&lt;p&gt;Here's what was happening to me: I had switched my GitHub CLI to my other account using &lt;code&gt;gh auth switch&lt;/code&gt;, but when I tried to push, Git was still authenticating with my personal account's SSH key.&lt;/p&gt;
&lt;p&gt;The issue is that &lt;code&gt;gh auth switch&lt;/code&gt; only changes which account the GitHub CLI uses for API operations. It doesn't affect which SSH key Git uses for push/pull operations. Git and SSH operate independently from the &lt;code&gt;gh&lt;/code&gt; tool.&lt;/p&gt;
&lt;h2 id="what-you-ll-need"&gt;What you'll need&lt;/h2&gt;&lt;p&gt;Two GitHub accounts (I'll call them &lt;code&gt;personal-account&lt;/code&gt; and &lt;code&gt;volunteer-account&lt;/code&gt; in this guide), terminal access, admin permissions on your repositories, and about 10-15 minutes.&lt;/p&gt;
&lt;h2 id="step-1-create-separate-ssh-keys-for-each-account"&gt;Step 1: Create separate SSH keys for each account&lt;/h2&gt;&lt;p&gt;First, we need distinct SSH keys for each account. If you don't already have separate keys, create them:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Create a key for your volunteer account&lt;/span&gt;
ssh-keygen&lt;span class="w"&gt; &lt;/span&gt;-t&lt;span class="w"&gt; &lt;/span&gt;ed25519&lt;span class="w"&gt; &lt;/span&gt;-C&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;volunteer-email@example.com&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;~/.ssh/id_ed25519_volunteer

&lt;span class="c1"&gt;# Create a key for your personal account (if you don&amp;#39;t have one)&lt;/span&gt;
ssh-keygen&lt;span class="w"&gt; &lt;/span&gt;-t&lt;span class="w"&gt; &lt;/span&gt;ed25519&lt;span class="w"&gt; &lt;/span&gt;-C&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;personal-email@example.com&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;~/.ssh/id_ed25519_personal
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;When prompted for a passphrase, you can either set one or leave it empty (though a passphrase is more secure).&lt;/p&gt;
&lt;h2 id="step-2-add-the-ssh-keys-to-your-ssh-agent"&gt;Step 2: Add the SSH keys to your SSH agent&lt;/h2&gt;&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ssh-add&lt;span class="w"&gt; &lt;/span&gt;~/.ssh/id_ed25519_volunteer
ssh-add&lt;span class="w"&gt; &lt;/span&gt;~/.ssh/id_ed25519_personal
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You can verify both keys are loaded:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ssh-add&lt;span class="w"&gt; &lt;/span&gt;-l
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="step-3-add-the-public-keys-to-github"&gt;Step 3: Add the public keys to GitHub&lt;/h2&gt;&lt;p&gt;For each account, you need to add its corresponding public key:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Copy your volunteer account&amp;#39;s public key&lt;/span&gt;
cat&lt;span class="w"&gt; &lt;/span&gt;~/.ssh/id_ed25519_volunteer.pub
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Log into GitHub as your volunteer account&lt;/li&gt;
&lt;li&gt;Go to Settings → SSH and GPG keys → New SSH key&lt;/li&gt;
&lt;li&gt;Paste the public key there&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Repeat this process for your personal account with &lt;code&gt;id_ed25519_personal.pub&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id="step-4-configure-ssh-to-use-different-keys-for-different-hosts"&gt;Step 4: Configure SSH to use different keys for different "hosts"&lt;/h2&gt;&lt;p&gt;Edit or create &lt;code&gt;~/.ssh/config&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;# Default GitHub (personal account)
Host github.com
  HostName github.com
  User git
  AddKeysToAgent yes
  UseKeychain yes
  IdentityFile ~/.ssh/id_ed25519_personal

# GitHub for volunteer account
Host github.com-volunteer
  HostName github.com
  User git
  IdentityFile ~/.ssh/id_ed25519_volunteer
  IdentitiesOnly yes
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The &lt;code&gt;Host github.com-volunteer&lt;/code&gt; line creates a local alias that only exists in your SSH config. When Git tries to connect to &lt;code&gt;github.com-volunteer&lt;/code&gt;, SSH will actually connect to &lt;code&gt;github.com&lt;/code&gt; but use the specified SSH key.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;IdentitiesOnly yes&lt;/code&gt; line tells SSH to only use the key you specified and not try other keys from your SSH agent.&lt;/p&gt;
&lt;h2 id="step-5-update-your-repository-s-remote-url"&gt;Step 5: Update your repository's remote URL&lt;/h2&gt;&lt;p&gt;For any repository belonging to your volunteer account, you need to update the remote URL to use the SSH alias:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Navigate to your repo&lt;/span&gt;
&lt;span class="nb"&gt;cd&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;~/path/to/nonprofit-project

&lt;span class="c1"&gt;# Check current remote&lt;/span&gt;
git&lt;span class="w"&gt; &lt;/span&gt;remote&lt;span class="w"&gt; &lt;/span&gt;-v

&lt;span class="c1"&gt;# Update to use the volunteer account&amp;#39;s SSH config&lt;/span&gt;
git&lt;span class="w"&gt; &lt;/span&gt;remote&lt;span class="w"&gt; &lt;/span&gt;set-url&lt;span class="w"&gt; &lt;/span&gt;origin&lt;span class="w"&gt; &lt;/span&gt;git@github.com-volunteer:organization/nonprofit-project.git
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Notice the change: &lt;code&gt;git@github.com-volunteer:&lt;/code&gt; instead of &lt;code&gt;git@github.com:&lt;/code&gt;. This is necessary because the hostname in the URL is what triggers SSH to look up the configuration in your &lt;code&gt;~/.ssh/config&lt;/code&gt; file. When Git sees &lt;code&gt;github.com-volunteer&lt;/code&gt;, SSH matches it to the &lt;code&gt;Host github.com-volunteer&lt;/code&gt; entry and uses the correct key.&lt;/p&gt;
&lt;h2 id="step-6-test-the-connection"&gt;Step 6: Test the connection&lt;/h2&gt;&lt;p&gt;Before pushing, verify SSH is authenticating correctly:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;ssh&lt;span class="w"&gt; &lt;/span&gt;-T&lt;span class="w"&gt; &lt;/span&gt;git@github.com-volunteer
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should see:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Hi volunteer-account! You've successfully authenticated, but GitHub does not provide shell access.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If it says your personal account name instead, something's wrong with your SSH config.&lt;/p&gt;
&lt;p&gt;Now try pushing:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git&lt;span class="w"&gt; &lt;/span&gt;push
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="troubleshooting-common-issues"&gt;Troubleshooting common issues&lt;/h2&gt;&lt;h3 id="issue-1-ssh-still-authenticates-with-the-wrong-account"&gt;Issue 1: SSH still authenticates with the wrong account&lt;/h3&gt;&lt;p&gt;If &lt;code&gt;ssh -T git@github.com-volunteer&lt;/code&gt; shows your personal account name instead of your volunteer account, the problem is usually that SSH is trying multiple keys and GitHub is accepting the first one that works.&lt;/p&gt;
&lt;p&gt;Make sure you have &lt;code&gt;IdentitiesOnly yes&lt;/code&gt; in your &lt;code&gt;~/.ssh/config&lt;/code&gt; for the &lt;code&gt;github.com-volunteer&lt;/code&gt; host. This forces SSH to only use the specified key.&lt;/p&gt;
&lt;h3 id="issue-2-could-not-resolve-hostname-github.com-volunteer""&gt;Issue 2: "Could not resolve hostname github.com-volunteer"&lt;/h3&gt;&lt;p&gt;This usually means Git has a custom SSH command configured that's bypassing your SSH config file. Check:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git&lt;span class="w"&gt; &lt;/span&gt;config&lt;span class="w"&gt; &lt;/span&gt;--get&lt;span class="w"&gt; &lt;/span&gt;core.sshCommand
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If this returns something with &lt;code&gt;-F /dev/null&lt;/code&gt;, that's your problem. The &lt;code&gt;-F /dev/null&lt;/code&gt; flag tells SSH to ignore all config files.&lt;/p&gt;
&lt;p&gt;Remove it:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git&lt;span class="w"&gt; &lt;/span&gt;config&lt;span class="w"&gt; &lt;/span&gt;--unset&lt;span class="w"&gt; &lt;/span&gt;core.sshCommand
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="issue-3-config-changes-don-t-seem-to-apply"&gt;Issue 3: Config changes don't seem to apply&lt;/h3&gt;&lt;p&gt;If you have conditional Git configs (using &lt;code&gt;includeIf&lt;/code&gt; directives), they might be overriding your settings. Check:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git&lt;span class="w"&gt; &lt;/span&gt;config&lt;span class="w"&gt; &lt;/span&gt;--list&lt;span class="w"&gt; &lt;/span&gt;--show-origin&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;|&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;grep&lt;span class="w"&gt; &lt;/span&gt;sshCommand
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This shows you exactly which config file is setting the SSH command. You may need to edit that file directly.&lt;/p&gt;
&lt;p&gt;For example, I had a &lt;code&gt;~/.gitconfig-volunteer&lt;/code&gt; file that was automatically loaded for repos in certain directories, and it had a problematic &lt;code&gt;core.sshCommand&lt;/code&gt; setting that needed to be fixed.&lt;/p&gt;
&lt;h3 id="issue-4-repository-not-found-error"&gt;Issue 4: "Repository not found" error&lt;/h3&gt;&lt;p&gt;This means SSH is connecting and authenticating, but as the wrong account. Double-check:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Run &lt;code&gt;ssh -T git@github.com-volunteer&lt;/code&gt; and verify it shows the correct account name&lt;/li&gt;
&lt;li&gt;Verify the account has access to the repository on GitHub&lt;/li&gt;
&lt;li&gt;Check that your remote URL uses the correct alias: &lt;code&gt;git@github.com-volunteer:org/repo.git&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="optional-set-up-conditional-git-configs"&gt;Optional: Set up conditional Git configs&lt;/h2&gt;&lt;p&gt;If you keep repositories for your volunteer work in a specific directory (like &lt;code&gt;~/volunteer-projects/&lt;/code&gt;), you can automatically apply settings to all repos in that directory.&lt;/p&gt;
&lt;p&gt;Add this to your &lt;code&gt;~/.gitconfig&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[includeIf "gitdir:~/volunteer-projects/"]
    path = ~/.gitconfig-volunteer
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then create &lt;code&gt;~/.gitconfig-volunteer&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;[user]
    email = volunteer-email@example.com

[core]
    sshCommand = ssh
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This automatically sets your volunteer account's email for commits in that directory. The &lt;code&gt;sshCommand&lt;/code&gt; should be set to plain &lt;code&gt;ssh&lt;/code&gt; so it uses your &lt;code&gt;~/.ssh/config&lt;/code&gt; properly.&lt;/p&gt;
&lt;h2 id="how-this-all-works-together"&gt;How this all works together&lt;/h2&gt;&lt;p&gt;When you run &lt;code&gt;git push&lt;/code&gt;:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Git reads the remote URL: &lt;code&gt;git@github.com-volunteer:org/repo.git&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Git asks SSH to connect to &lt;code&gt;github.com-volunteer&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;SSH looks in &lt;code&gt;~/.ssh/config&lt;/code&gt; and finds the &lt;code&gt;Host github.com-volunteer&lt;/code&gt; entry&lt;/li&gt;
&lt;li&gt;SSH sees it should actually connect to &lt;code&gt;github.com&lt;/code&gt; but use the &lt;code&gt;id_ed25519_volunteer&lt;/code&gt; key&lt;/li&gt;
&lt;li&gt;SSH connects to GitHub with the correct key&lt;/li&gt;
&lt;li&gt;GitHub authenticates you as your volunteer account&lt;/li&gt;
&lt;li&gt;Push succeeds&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Each repository uses the correct account automatically based on its remote URL, so you never have to manually specify which key to use.&lt;/p&gt;
&lt;h2 id="wrapping-up"&gt;Wrapping up&lt;/h2&gt;&lt;p&gt;Managing multiple GitHub accounts on the same computer isn't intuitive, but once you understand that Git uses SSH keys (not &lt;code&gt;gh auth&lt;/code&gt; settings), the solution becomes clear. The SSH config host alias pattern is the standard way to handle this, and it works reliably once everything is configured correctly.&lt;/p&gt;
&lt;p&gt;The key points to remember:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;SSH keys are what matter for Git operations, not &lt;code&gt;gh auth&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Host aliases in &lt;code&gt;~/.ssh/config&lt;/code&gt; let you use different keys for different repos&lt;/li&gt;
&lt;li&gt;&lt;code&gt;IdentitiesOnly yes&lt;/code&gt; prevents SSH from trying multiple keys&lt;/li&gt;
&lt;li&gt;Your remote URL must use the alias (e.g., &lt;code&gt;git@github.com-volunteer:&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you run into issues, the troubleshooting section above covers the most common problems I encountered.&lt;/p&gt;
</content></entry><entry><title>How to teach your coding agent with AGENTS.md</title><link href="https://ericmjl.github.io/blog/2025/10/4/how-to-teach-your-coding-agent-with-agentsmd/" rel="alternate"/><updated>2025-10-04T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:af4c409b-5ba1-3670-8181-8f2a5e5d8a58</id><content type="html">&lt;p&gt;Let me start with the most valuable thing I learned this week: if there's anything you want your LLM coding agent to remember for future sessions, just tell it to "Please update AGENTS.md with..." and then specify what you want it to remember.&lt;/p&gt;
&lt;p&gt;That's it. That's the meta-tip that changes everything.&lt;/p&gt;
&lt;h2 id="what-is-agents-md-anyway"&gt;What is AGENTS.md anyway&lt;/h2&gt;&lt;p&gt;AGENTS.md is an emerging open standard that's been adopted by over 20,000 repositories on GitHub. Think of it as a README for your AI coding agents—a predictable location where you provide context, instructions, and preferences that your agent needs to work effectively on your project.&lt;/p&gt;
&lt;p&gt;You might think of this as similar to ChatGPT's memory feature, but there's a crucial difference: AGENTS.md is explicitly curated by you. You decide exactly what the agent remembers and how it applies that knowledge. I prefer this approach because it means I have control over what the agent knows, rather than the agent autonomously deciding what to remember about me and my preferences. It's transparent, version-controlled, and intentional.&lt;/p&gt;
&lt;p&gt;The format emerged from collaborative efforts across OpenAI, Google (Jules), Cursor, Factory, and other major players in the AI development space. It's just standard Markdown, which means it's accessible, portable, and fits naturally into any project structure.&lt;/p&gt;
&lt;p&gt;While your README.md is optimized for humans—covering project introductions, contribution guidelines, and quick starts—AGENTS.md serves as machine-readable instructions for your coding agents. Setup commands, testing workflows, coding style preferences, and project-specific conventions all live here.&lt;/p&gt;
&lt;h2 id="training-an-employee-not-programming-a-bot"&gt;Training an employee, not programming a bot&lt;/h2&gt;&lt;p&gt;I was inspired by &lt;a href="https://youtu.be/budTmdQfXYU?si=mRQVEbSDZOPRf-Xm"&gt;NetworkChuck's approach to building Terry&lt;/a&gt;, his N8n automation agent. The philosophy framing he uses is both brilliant and yet practical: you're not programming a bot, you're training an employee.&lt;/p&gt;
&lt;p&gt;In Terry's case, Chuck teaches the agent by continuously updating its system prompt with new instructions and context. The same principle applies perfectly to AGENTS.md in coding environments.&lt;/p&gt;
&lt;p&gt;Here's what makes this powerful: AGENTS.md gets sent with every LLM API call in Cursor, Claude Code, GitHub Copilot, and other modern coding tools. This means you can standardize on AGENTS.md, and as you progress through your project, you effectively teach the LLM what to do by instructing it to update this file with your preferences and learnings.&lt;/p&gt;
&lt;p&gt;The beauty is that these instructions persist across sessions. Your agent doesn't forget; it gets smarter as your project evolves.&lt;/p&gt;
&lt;h2 id="concrete-tip-1-enforce-markdown-standards-automatically"&gt;Concrete tip 1: Enforce markdown standards automatically&lt;/h2&gt;&lt;p&gt;One of my first uses for AGENTS.md was ensuring consistent markdown formatting. I asked my coding agent to update AGENTS.md with instructions to always run markdownlint on any markdown files it creates or edits.&lt;/p&gt;
&lt;p&gt;Here's what I added to the file:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gu"&gt;## Markdown standards&lt;/span&gt;

&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Always run markdownlint on any markdown files created or edited
&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Install using: &lt;span class="sb"&gt;`npx markdownlint-cli`&lt;/span&gt; or &lt;span class="sb"&gt;`pixie global install markdownlint-cli`&lt;/span&gt;
&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Fix all linting issues before completing the task
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The effect is immediate. Now, anytime my agent writes or edits a markdown file, it automatically runs markdownlint and fixes issues. I don't have to remember to ask for this. The agent just knows it's part of the workflow.&lt;/p&gt;
&lt;h2 id="concrete-tip-2-specify-your-testing-style"&gt;Concrete tip 2: Specify your testing style&lt;/h2&gt;&lt;p&gt;I prefer writing tests as &lt;code&gt;pytest&lt;/code&gt; style functions rather than unittest-style classes. Most LLMs default to the unittest approach because it's more prevalent in their training data.&lt;/p&gt;
&lt;p&gt;So I instructed my agent to add this to AGENTS.md:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gu"&gt;## Testing preferences&lt;/span&gt;

&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Write all Python tests as &lt;span class="sb"&gt;`pytest`&lt;/span&gt; style functions, not unittest classes
&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Use descriptive function names starting with &lt;span class="sb"&gt;`test_`&lt;/span&gt;
&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Prefer fixtures over setup/teardown methods
&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Use assert statements directly, not self.assertEqual
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now when I ask for tests, I consistently get &lt;code&gt;pytest&lt;/code&gt; style functions. The agent is steered toward my preferred approach without me having to specify it in every request.&lt;/p&gt;
&lt;h2 id="concrete-tip-3-stop-writing-throwaway-test-scripts"&gt;Concrete tip 3: Stop writing throwaway test scripts&lt;/h2&gt;&lt;p&gt;Here's a pattern I noticed with Cursor: when the agent wants to test something, it loves to write little throwaway scripts. You know the type—&lt;code&gt;test_random_thing.py&lt;/code&gt; or &lt;code&gt;quick_check.py&lt;/code&gt; that do some ad hoc verification and then just sit there cluttering your project.&lt;/p&gt;
&lt;p&gt;The problem is these scripts aren't real tests—yet they're also tests. They don't run with your test suite. They don't provide ongoing regression protection. They're just... there.&lt;/p&gt;
&lt;p&gt;I taught my agent to write proper tests instead:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gu"&gt;## Testing approach&lt;/span&gt;

&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Never create throwaway test scripts or ad hoc verification files
&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;If you need to test functionality, write a proper test in the test suite
&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;All tests go in the &lt;span class="sb"&gt;`tests/`&lt;/span&gt; directory following the project structure
&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Tests should be runnable with the rest of the suite (&lt;span class="sb"&gt;`pixi run pytest`&lt;/span&gt;)
&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Even for quick verification, write it as a real test that provides ongoing value
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now when the agent needs to verify something works, it writes an actual test that becomes part of the project. These tests continue to provide value by catching regressions, documenting expected behavior, and running in CI.&lt;/p&gt;
&lt;p&gt;The shift is subtle but powerful: instead of creating technical debt in the form of random scripts, you're building up a proper test suite.&lt;/p&gt;
&lt;h2 id="concrete-tip-4-teach-your-agent-about-new-tooling"&gt;Concrete tip 4: Teach your agent about new tooling&lt;/h2&gt;&lt;p&gt;I recently adopted Pixi as my main package manager. The problem? Most LLMs aren't familiar with Pixi commands yet. They kept trying to run &lt;code&gt;python&lt;/code&gt; directly when I only have Python available through Pixi.&lt;/p&gt;
&lt;p&gt;The solution was to teach the agent:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gu"&gt;## Package management&lt;/span&gt;

&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;This project uses Pixi for all package management
&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Never run commands directly (python, pytest, etc.)
&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Always prefix commands with &lt;span class="sb"&gt;`pixi run &amp;lt;command&amp;gt;`&lt;/span&gt;
&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Example: &lt;span class="sb"&gt;`pixi run python script.py`&lt;/span&gt; not &lt;span class="sb"&gt;`python script.py`&lt;/span&gt;
&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Example: &lt;span class="sb"&gt;`pixi run pytest`&lt;/span&gt; not &lt;span class="sb"&gt;`pytest`&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This works for any new tooling. If you've adopted &lt;code&gt;pixi&lt;/code&gt; or &lt;code&gt;uv&lt;/code&gt; or any other modern Python tools that aren't well-represented in LLM training data, you can explicitly teach your agent how to use them through AGENTS.md.&lt;/p&gt;
&lt;p&gt;The same principle applies to any domain-specific tools or workflows unique to your project. For example, if you're working with Marimo notebooks, which have a relatively strict syntax:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="gu"&gt;## Marimo notebook validation&lt;/span&gt;

&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;After creating or editing any Marimo notebook, always run validation
&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Command: &lt;span class="sb"&gt;`uvx marimo check &amp;lt;notebook&amp;gt;.py`&lt;/span&gt;
&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Fix any syntax errors reported before completing the task
&lt;span class="k"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;Marimo notebooks require strict syntax adherence for proper execution
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now your agent will automatically validate Marimo notebooks and get immediate feedback on syntax errors, ensuring notebooks are written correctly the first time.&lt;/p&gt;
&lt;h2 id="why-this-matters"&gt;Why this matters&lt;/h2&gt;&lt;p&gt;The traditional approach to working with coding agents involves repeating yourself constantly. "Remember to use this format." "Don't forget to run this command." "We prefer this style here."&lt;/p&gt;
&lt;p&gt;AGENTS.md flips this model. Instead of being a human repeating instructions to a forgetful assistant, you're building up institutional knowledge that persists. You're training your agent to work the way you work.&lt;/p&gt;
&lt;p&gt;As one developer observed, "it's all about simple human psychology: You get immediate feedback &amp;amp; results: You write it once, and your AI assistant immediately becomes more useful. The feedback loop is much longer with READMEs."&lt;/p&gt;
&lt;p&gt;This is the key insight. When you write a README, you're creating documentation for a future human reader who may or may not show up. When you write AGENTS.md, you get instant gratification—your next conversation with the agent immediately reflects what you just taught it. The AI won't judge you for weird conventions or hacky workarounds. It just learns and applies what you've documented.&lt;/p&gt;
&lt;p&gt;Each time you discover a preference, a gotcha, or a best practice for your project, you can capture it in AGENTS.md. The next time your agent encounters a similar situation, it already knows what to do.&lt;/p&gt;
&lt;p&gt;This is especially powerful in larger projects or monorepos. You can have AGENTS.md files in subdirectories, and agents will use the nearest file to the code being edited—similar to how .gitignore or ESLint configs work. This lets you provide context-specific instructions for different parts of your codebase.&lt;/p&gt;
&lt;h2 id="getting-started"&gt;Getting started&lt;/h2&gt;&lt;p&gt;If you already have agent instruction files like &lt;code&gt;.cursorrules&lt;/code&gt;, &lt;code&gt;CLAUDE.md&lt;/code&gt;, or &lt;code&gt;.github/copilot-instructions.md&lt;/code&gt;, you can simply rename them to AGENTS.md. Most modern coding agents now support this standard.&lt;/p&gt;
&lt;p&gt;Start simple. Create an AGENTS.md in your project root and add one or two critical preferences. Then, as you work with your agent, whenever you find yourself giving the same instruction twice, add it to AGENTS.md instead.&lt;/p&gt;
&lt;p&gt;The key insight is this: every time you teach your agent something, make it permanent by updating AGENTS.md. That's how you build an agent that truly understands your project.&lt;/p&gt;
</content></entry><entry><title>How data scientists can master life sciences and software skills for biotech using ultralearning</title><link href="https://ericmjl.github.io/blog/2025/10/1/how-data-scientists-can-master-life-sciences-and-software-skills-for-biotech-using-ultralearning/" rel="alternate"/><updated>2025-10-01T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:702334d0-8c72-3386-aa45-8bc48b858216</id><content type="html">&lt;p&gt;After 8 years working in biotech and 6 years of graduate training before that, I've observed something about the most effective data scientists in biotech: they aren't just T- or π-shaped -- posessing breadth in skill while being deep in 1 or 2 specialties. They're continuously learning new skills to bridge their knowledge gaps.&lt;/p&gt;
&lt;p&gt;There are two common knowledge gaps that I've observed. On one side, there's the vast world of life sciences: molecular biology, cell biology, genetics, immunology, neuroscience, analytical chemistry, organic chemistry, biochemistry. On the other, there's software development, the kind of skills that let you build reliable, maintainable tools that actually work in production.&lt;/p&gt;
&lt;p&gt;The challenge is that these two domains are fundamentally different in how you learn them. And here's the thing: you can't just "take courses" in those domains and call it done. The life sciences alone are too vast. You need a strategy for continuous, rapid learning in both domains over your entire career.&lt;/p&gt;
&lt;p&gt;When I interview data scientists for biotech roles, I assess five key areas: people skills, communication skills, scientific knowledge, coding skills, and modeling skills. The two domains I'm talking about here — life sciences and software development — map directly to scientific knowledge and coding skills. These aren't just nice-to-haves; they're essential for effectiveness in biotech data science.&lt;/p&gt;
&lt;p&gt;That's where "ultralearning" comes in. It's Scott Young's framework for aggressive, self-directed learning, and I know it works because I've lived it. I started as a bench scientist but taught myself computing, software development, and machine learning over the years. Now I want to show you how data scientists in biotech can do the same—whether you're learning domain knowledge or software skills.&lt;/p&gt;
&lt;p&gt;How do you strategically build depth in both life sciences and software over time? I'm going to walk through the 9 principles of ultralearning that Scott Young outlines and show you how they map to learning both domains for biotech data science. I've reordered them in a way that builds momentum, starting with what matters most.&lt;/p&gt;
&lt;h2 id="the-9-principles"&gt;The 9 principles&lt;/h2&gt;&lt;h3 id="principle-3-directness-learn-by-doing-the-real-thing"&gt;Principle 3: Directness - learn by doing the real thing&lt;/h3&gt;&lt;p&gt;Starting with principle 3, here's what directness means: you learn most viscerally in the actual context where you'll apply the skill. And I'm putting this first because it's where most people go wrong.&lt;/p&gt;
&lt;p&gt;Most people read textbooks and take courses. This isn't bad in and of itself, but if you assume that covering the material means you have learned it, you are wrong. Without a context to apply it, the knowledge doesn't stick. You need a real project where you actually use what you're learning.&lt;/p&gt;
&lt;p&gt;If you're already working in biotech, you have a huge advantage: you already have real projects with real stakes. These projects naturally focus your learning because you have a job to be done! This is why I put directness first: it leverages the learning environment you already have.&lt;/p&gt;
&lt;p&gt;For learning life sciences, this means treating your current project as your learning laboratory. You're analyzing RNA-seq data? Learn the biology behind the genes you're seeing, then immediately apply that knowledge to interpret your results and suggest follow-up experiments. You're working with metabolomics data? Learn the metabolic pathways, then use that understanding to identify which metabolites are actually biologically meaningful versus technical artifacts. You're building models for drug discovery? Learn the specific organic chemistry that you're working with, then apply it to explain why your model predicts certain compounds will work and others won't, and use that reasoning to guide your next round of experiments.&lt;/p&gt;
&lt;p&gt;Your current project is your learning laboratory. Treat the scientific knowledge gaps you encounter as targets for deep learning.&lt;/p&gt;
&lt;p&gt;And here's something I've noticed: when you write internal documentation or reports, that's actually retrieval practice for the science you're learning. More on retrieval later, but the point is your work gives you built-in learning opportunities if you use them intentionally.&lt;/p&gt;
&lt;p&gt;The same applies to learning software. Your pipeline is getting slow? Learn performance optimization and profiling, then immediately apply those techniques to identify bottlenecks and speed up your actual pipeline. Your code is getting hard to maintain? Learn design patterns, then refactor your existing codebase using those patterns to make it more modular and testable. You need to deploy something? Learn containerization and orchestration, then use those skills to get your tool running in production and accessible to your team.&lt;/p&gt;
&lt;p&gt;Your work projects provide the constraints and requirements that make software concepts meaningful. When you document your code or write design docs, you're forced to articulate the architectural decisions you're making—and that's when you really learn them.&lt;/p&gt;
&lt;h3 id="principle-4-drill-isolate-and-attack-weak-points"&gt;Principle 4: Drill - isolate and attack weak points&lt;/h3&gt;&lt;p&gt;Drilling is about identifying rate-limiting steps in your skills and practicing them specifically. Once you're doing direct practice through your projects, you'll notice where you keep getting stuck.&lt;/p&gt;
&lt;p&gt;Here, there is a meta-skill that I think is critical: self-awareness. You need to develop the ability to notice when you're missing context, without someone explicitly telling you. When you're reading a paper and realize you're lost, or when you're in a meeting and can't follow the reasoning, that's your signal. Learning to recognize these moments yourself is what makes drilling effective.&lt;/p&gt;
&lt;p&gt;For learning life sciences, drilling means identifying the specific skills that are blocking your work and practicing them repeatedly. Do you keep having to ask biologists what ChIP-seq tells you? Drill by practicing biological interpretation: take 20 different ChIP-seq results and explain what each peak means biologically—what transcription factor is binding, what genes are being regulated, and what biological process is affected. Are you on a project with immunologists but can't follow their reasoning? Drill by practicing experimental logic: read 15 immunology papers and predict what the next experiment should be based on the current results, then check your reasoning against what the authors actually did, or consult an immunologist colleague.&lt;/p&gt;
&lt;p&gt;Or perhaps consider the chemistry side of things. Are you analyzing mass spec data but you're shaky on ionization and fragmentation? Drill by working through 50 fragmentation problems: given a compound structure, predict the top 5 fragments you'd expect to see, then check your answers. Is your lack of organic chemistry preventing you from understanding drug modifications? Drill by practicing modification effects: take 30 different drug structures, make specific modifications (add methyl group, change functional group), and predict how each change would affect binding affinity, solubility, and metabolic stability.&lt;/p&gt;
&lt;p&gt;The key is identifying the real bottleneck in your knowledge and then designing a drill around that. If you have no good priors on how to do this, you should actually be asking your colleagues who have domain knowledge. They can help you pinpoint exactly what you're missing and suggest focused practice exercises.&lt;/p&gt;
&lt;p&gt;For learning software, the same principle applies. Do your pipelines keep breaking in production? Drill by practicing debugging: take your broken pipeline, run it locally with the same data that failed in production, and systematically test each step until you find the exact failure point. Are you blocked on deployment because of containerization issues? Drill this skill while debugging! Start with a minimal Dockerfile that just runs your script, then gradually add dependencies one by one until it works. Strip it back to bare minimum and rebuild it piece by piece, and your understanding will follow too.&lt;/p&gt;
&lt;p&gt;Code review keeps catching the same issues in your code? Drill on those patterns by refactoring your existing code to avoid them. Take one function at a time and rewrite it to follow the patterns your reviewers want. Do you avoid writing tests because you don't really understand testing frameworks? Start by adding a single test to your existing codebase, then gradually add more tests to the functions you use most.&lt;/p&gt;
&lt;p&gt;Identify the one software skill that's limiting your effectiveness right now, and drill it until it's not a bottleneck anymore. Maybe it's Git workflows that are slowing your team down, or packaging that's preventing tool distribution.&lt;/p&gt;
&lt;h3 id="principle-1-metalearning-map-the-territory-first"&gt;Principle 1: Metalearning - map the territory first&lt;/h3&gt;&lt;p&gt;Metalearning means researching how to learn something before diving in. I know it seems backwards to list this third when it's literally the first principle in Scott Young's framework, but here's why: directness and drilling are more immediately actionable. Once you're doing those, metalearning helps you be more strategic about what you're doing.&lt;/p&gt;
&lt;p&gt;For learning life sciences, this means understanding what you actually need to know for your work before diving into intensive learning. Is your team starting a new project in spatial transcriptomics? Before diving in, map out what you need: tissue biology, imaging concepts, the technology itself, analysis methods. Are you joining a drug discovery project? Identify the hierarchy of knowledge; do you need medicinal chemistry basics first, or can you start with binding assays and learn backward?&lt;/p&gt;
&lt;p&gt;Talk to the biologists or chemists you work with. Ask them what foundational concepts matter most for understanding their work. Look at the papers your team references most—what scientific knowledge do they assume? That's your map.&lt;/p&gt;
&lt;p&gt;Find the best resources for your specific need. Maybe it's that one review paper, or a specific textbook chapter, or that scientist down the hall who explains things well. Don't waste time learning areas that aren't relevant to your current work. Map what matters now.&lt;/p&gt;
&lt;p&gt;Additionally, be prepared to re-map! After working on a project for a while, you might find your initial map was wrong. That's totally OK! If you're making errors because you misunderstood what you needed to learn, that's a signal to step back and reassess. An incorrect map is worse than no map.&lt;/p&gt;
&lt;p&gt;For learning software, the same applies. Before diving into a new software skill, understand what good looks like in your specific context—biotech and scientific computing. Your team wants to adopt a new workflow system? Before learning it, map out what you need: workflow concepts, the specific tool's paradigms, container knowledge. Or can you start with examples?&lt;/p&gt;
&lt;p&gt;Look at mature tools in your space — &lt;code&gt;scikit-bio&lt;/code&gt;, &lt;code&gt;scanpy&lt;/code&gt;, or established pipelines like those from the ENCODE project. What patterns do they use? Are they functional-first or objects-first? And what are patterns in how they design their APIs? That's your north star. Talk to experienced engineers if you have access and ask what software skills actually matter for scientific tools.&lt;/p&gt;
&lt;p&gt;The key is understanding the learning path dependencies: do you need to understand Python packaging before you can learn about CI/CD? Or can you learn them together? Map out the shortest path to being effective, not the most comprehensive path to expertise. Focus on what will unblock your current work, not what would make you an expert in everything.&lt;/p&gt;
&lt;h3 id="principle-6-feedback-get-signal-on-your-progress"&gt;Principle 6: Feedback - get signal on your progress&lt;/h3&gt;&lt;p&gt;Feedback is getting useful information about what you're doing wrong and how to fix it. And in biotech, you have built-in feedback mechanisms if you use them intentionally.&lt;/p&gt;
&lt;p&gt;For learning life sciences, leverage the scientists you work with as your feedback mechanism. When you present results in team meetings and biologists or chemists correct your interpretation, that's high-value feedback. Pay attention!&lt;/p&gt;
&lt;p&gt;Join journal clubs if your company has them. When you misunderstand a paper, someone will point it out. If your company doesn't have journal clubs, look in your local community—Boston has several industry-focused options including &lt;a href="https://biotechtuesday.com/category/geographic-region-event/boston-ma/"&gt;BiotechTuesday&lt;/a&gt;, &lt;a href="https://massbio.microsoftcrmportals.com/event/?event=The_Science_of_Biotech_November"&gt;MassBio events&lt;/a&gt;, and &lt;a href="https://cambridgebiotechclub.org/"&gt;Cambridge Biotech Club&lt;/a&gt; networking events that often include research discussions.&lt;/p&gt;
&lt;p&gt;When you write up results or make slides, ask a scientist to review. Where they add clarifications shows your knowledge gaps. If your predictions about experimental outcomes are wrong, that's feedback about your biological understanding. I remember being challenged by a colleague while on a call with external collaborators, and that was the best feedback I had being wrong in a "public" setting! When you explain your interpretation to a biologist and they look confused, you've either misunderstood the science or can't articulate it yet.&lt;/p&gt;
&lt;p&gt;Your work products -- analyses, reports, presentations -- are opportunities to get feedback on your scientific understanding. Don't just present results. Explain your biological reasoning and see where it's challenged.&lt;/p&gt;
&lt;p&gt;For learning software, code review is your primary feedback mechanism. Take it seriously. The comments show you what you don't yet understand.&lt;/p&gt;
&lt;p&gt;Does your code actually work at scale with real-sized data? That's feedback on your software design. When someone else tries to use your tool and files issues, those edge cases reveal gaps in your software thinking. Pair programming with more experienced engineers shows you patterns you're not seeing.&lt;/p&gt;
&lt;p&gt;When onboarding a new team member to your code takes too long, that's feedback that your architecture or documentation needs work. Production failures are harsh but clear feedback: what software concepts do you need to learn to prevent them?&lt;/p&gt;
&lt;p&gt;Ask for architectural review before building something big. Feedback up front prevents expensive mistakes.&lt;/p&gt;
&lt;h3 id="principle-5-retrieval-test-yourself-actively"&gt;Principle 5: Retrieval - test yourself actively&lt;/h3&gt;&lt;p&gt;Retrieval is about actively recalling information, which strengthens learning more than passive review. And I want to emphasize something specific about this for life sciences learning.&lt;/p&gt;
&lt;p&gt;The vocabulary in life sciences is vast, and the meanings of everyday words often change in scientific contexts. Think about "competent" cells, "naive" T-cells, "promiscuous" enzymes, "housekeeping" genes. Good memory for vocabulary isn't just about rote memorization; it gives you the ability to name and label entities clearly, which is foundational even when your primary goal is understanding concepts.&lt;/p&gt;
&lt;p&gt;For learning life sciences, retrieval practice happens naturally in your work if you let it. When you're preparing a presentation for your team, try to explain the biological mechanism from memory first, then check your understanding. Before looking up that pathway or reaction mechanism again, try to draw it from memory. Where you get stuck shows what you haven't really learned.&lt;/p&gt;
&lt;p&gt;In meetings when discussing results, attempt to explain the biology without your notes. This reveals what you actually know versus what you've just read. When writing internal documentation about your project, explain the scientific concepts from memory, then verify. If you're presenting at journal club, practice explaining the paper's biology without constantly referring to slides.&lt;/p&gt;
&lt;p&gt;The act of trying to recall forces your brain to strengthen those neural pathways. Passive rereading doesn't do this. Your work already gives you retrieval opportunities—presentations, documentation, discussions with scientists. Use them.&lt;/p&gt;
&lt;p&gt;For learning software, the same principle applies. When you're about to look up how to do something in code, try to write it from memory first, then look it up if needed. Before copying a design pattern from Stack Overflow, try to implement it based on what you remember, then refine.&lt;/p&gt;
&lt;p&gt;In code review or design discussions, explain your architectural decisions from memory. If you can't, you don't really understand them yet. When documenting your code, write the explanation without constantly referencing the implementation. Try to debug issues by reasoning through the system before looking at logs—this builds your mental model.&lt;/p&gt;
&lt;p&gt;And here's something important: making it easy by constantly looking things up or always relying on AI to spoonfeed you answers is a surefire way to keep knowledge shallow. The struggle of trying to remember is what creates learning.&lt;/p&gt;
&lt;p&gt;This is especially true with AI-generated documentation. You can use AI to generate documentation, but the retrieval practice happens during review. When AI writes "this function calculates the binding affinity," question it: "Does it really? What's the actual algorithm? What are the inputs and outputs?" Challenge each line the AI wrote by trying to explain it from your own understanding. If you can't explain why a particular line is there or what it does, that's your signal to dig deeper into that concept.&lt;/p&gt;
&lt;h3 id="principle-2-focus-cultivate-deep-concentration"&gt;Principle 2: Focus - cultivate deep concentration&lt;/h3&gt;&lt;p&gt;Focus is about managing procrastination, distraction, and maintaining sustained attention. And this matters more than you might think for both domains.&lt;/p&gt;
&lt;p&gt;For learning life sciences, reading that dense Nature paper about a pathway relevant to your project requires deep, uninterrupted focus. You can't do it between Slack messages and meetings -- block time, and shut off communication channels. Understanding complex scientific concepts, such as metabolic regulation, signaling cascades, reaction mechanisms, requires holding multiple pieces in your head simultaneously. Context switching destroys this.&lt;/p&gt;
&lt;p&gt;Block time on your calendar specifically for deep scientific learning. Treat it like a critical meeting. The 15 minutes before standup isn't enough to understand that review paper you need to read. When you're learning a new biological domain for a project, protect longer blocks of focused time for it.&lt;/p&gt;
&lt;p&gt;Your brain needs sustained attention to build the mental models that make scientific knowledge useful, not just memorized.&lt;/p&gt;
&lt;p&gt;For learning software, the same applies. Understanding a complex codebase or debugging a tricky issue requires uninterrupted deep work. You can't do it effectively in fragments. Reading source code of mature projects—like &lt;code&gt;scikit-bio&lt;/code&gt;, &lt;code&gt;scanpy&lt;/code&gt;, or established pipelines—requires sustained attention to follow design decisions.&lt;/p&gt;
&lt;p&gt;Designing a new system architecture requires holding the entire design in your head. That's impossible with constant interruptions. Block time for focused software learning and development, not just cramming it into gaps. The cognitive load of building mental models for software systems is high. Protect that learning time.&lt;/p&gt;
&lt;p&gt;If you're learning a new framework or pattern for work, give it dedicated focus time, not scattered moments. It'll pay dividends many-fold over an entire technical career.&lt;/p&gt;
&lt;h3 id="principle-9-experimentation-explore-beyond-the-beaten-path"&gt;Principle 9: Experimentation - explore beyond the beaten path&lt;/h3&gt;&lt;p&gt;Experimentation is about trying new approaches, methods, and perspectives as you gain proficiency. This becomes more important as you build your foundation in both domains.&lt;/p&gt;
&lt;p&gt;For learning life sciences, as your foundational knowledge grows, start exploring adjacent domains that come up in your work. You're strong in genomics now? When an immunology opportunity comes up in a cross-functional meeting, that's your trigger to explore that domain.&lt;/p&gt;
&lt;p&gt;Try different ways of learning. Sometimes a textbook works, sometimes talking to the scientist at the next desk works better—especially if they're socratically coaching you. Sometimes it's working through a dataset. This reflects a key insight from ultralearning: there's no one-size-fits-all learning format. What works for learning genomics might not work for learning immunology, and what works for you might not work for your colleague. Experiment to find your optimal learning approach for each domain.&lt;/p&gt;
&lt;p&gt;Use insights from one domain to inform another. Cell signaling patterns you learned in neuroscience might help you understand what you're seeing in immunology data, since all cells have singaling pathways. As you work with both biologists and chemists, start connecting how chemical principles inform biological mechanisms. This cross-domain connection is a well-established way to improve retention and deepen understanding.&lt;/p&gt;
&lt;p&gt;Additionally, try experimenting with how you organize and retain scientific knowledge. What works for you personally? This exploration leads to developing your unique perspective, especially on how you synthesize biological and chemical knowledge differently than others.&lt;/p&gt;
&lt;p&gt;For learning software skills, as you gain proficiency, experiment with different approaches to the same problem at work. For example, have you been using conda to manage your Python environments? When you have time, try managing your environment with pixi instead to understand the tradeoffs.&lt;/p&gt;
&lt;p&gt;Try different testing strategies on real work projects to see what catches bugs most effectively for your use case. Experiment with different architectural patterns when building new tools—learn through direct comparison.&lt;/p&gt;
&lt;p&gt;As you grow, you'll develop engineering judgment: knowing when to use which approach, which rules to follow, which to break. This experimentation leads to finding your own effective patterns, not just copying what others do.&lt;/p&gt;
&lt;h3 id="principle-8-intuition-develop-deep-understanding"&gt;Principle 8: Intuition - develop deep understanding&lt;/h3&gt;&lt;p&gt;Intuition is about building mental models of how things actually work, not just memorizing. And this is where the real payoff comes in both domains.&lt;/p&gt;
&lt;p&gt;For learning life sciences, the drilling and retrieval practice we discussed earlier builds the mental models that become intuition. When you drill on fragmentation patterns and then retrieve that knowledge while analyzing mass spec data, you're doing more than mere memorizing: you're building understanding of how molecules break apart. When you practice explaining biological mechanisms from memory and get feedback, you develop the mechanistic reasoning that becomes intuition.&lt;/p&gt;
&lt;p&gt;This intuition lets you reason about new situations you haven't seen before. You can predict whether an experimental approach will work or identify when results don't make biological sense. The goal isn't encyclopedic knowledge, but rather to develop the ability to reason about biological and chemical systems!&lt;/p&gt;
&lt;p&gt;For learning software, the same principle applies. When you drill on design patterns by implementing them from memory, and follow it up by getting feedback through code review, you build understanding of what problems they solve and their tradeoffs. This develops the engineering judgment to know when to use which approach, which rules to follow. And once you know the rules, you know which ones can be broken :-).&lt;/p&gt;
&lt;p&gt;At the end of the day, intuition develops through the active practice of drilling and retrieval, and not through passive consumption of information. Keep that in mind!&lt;/p&gt;
&lt;h3 id="principle-7-retention-don-t-let-knowledge-leak-away"&gt;Principle 7: Retention - don't let knowledge leak away&lt;/h3&gt;&lt;p&gt;Retention is about understanding why we forget and using strategies to remember long-term. And this matters because you're constantly encountering new concepts in both domains.&lt;/p&gt;
&lt;p&gt;For learning life sciences, you're constantly encountering new biological and chemical concepts at work. Without retention strategies, you'll keep relearning the same things. When you write internal documentation or reports, you're creating reference material for future you and your team—but only if you structure it for easy retrieval and review. Create a personal knowledge base with clear tags and cross-references so you can quickly find concepts when they come up again. I use Obsidian for my personal work knowledge base; it is centered around projects, but I also curate and link facts in there.&lt;/p&gt;
&lt;p&gt;The knowledge you use regularly will naturally stick through repeated exposure. But concepts from past projects will fade without reinforcement. Identify which scientific knowledge you need for the long-term versus what you need just for this project. For long-term retention, create Anki decks for key terminology and mechanisms that keep appearing across different projects. With Obsidian, drilling with an spaced repetition is possible with the &lt;a href="https://www.stephenmwangi.com/obsidian-spaced-repetition/"&gt;Spaced Repetition&lt;/a&gt; plugin.&lt;/p&gt;
&lt;p&gt;Connect new concepts to existing knowledge from your work—this creates stronger memory traces. When you learn a new pathway, relate it to ones you already know. When you encounter a new protein, link it to similar ones you've worked with. Revisit foundational concepts periodically as they show up in different projects, and update your notes and records each time you work on a project.&lt;/p&gt;
&lt;p&gt;The scientific knowledge you use regularly in your work will stick. Everything else needs deliberate retention strategies.&lt;/p&gt;
&lt;p&gt;For learning software, patterns you don't use regularly will fade. Be deliberate about which ones you need to retain. Writing documentation about the systems you build serves as external memory you can reference later, but make it searchable and well-organized to facilitate retrieval later! Create code snippets and examples for patterns you want to remember, and archive them in your work knowledge vault, or share them with colleagues on shared documentation platforms like Confluence or Notion.&lt;/p&gt;
&lt;p&gt;The tools and patterns you use daily will stick naturally through repeated exposure. But specialized knowledge from past projects will fade without reinforcement. If you're not writing tests regularly in your work, you'll forget testing patterns. Find ways to practice what you need to retain: contribute to open source projects, build side projects, or create practice exercises.&lt;/p&gt;
&lt;p&gt;Connect new software concepts to existing knowledge from your work. When you learn a new framework, relate it to ones you already know. When you encounter a new design pattern, link it to similar patterns you've used. Contributing to the same codebase over time builds deep, lasting knowledge of its architecture through repeated exposure. When you learn something new for a project, consider whether it's one-time knowledge or something you'll need repeatedly. Prioritize retention for the latter.&lt;/p&gt;
&lt;h2 id="bringing-it-together"&gt;Bringing it together&lt;/h2&gt;&lt;p&gt;These principles reinforce each other when learning both life sciences and software. But here's the key insight I want to emphasize: if you feel pressure to ultralearn both domains simultaneously, the answer is an emphatic "no".&lt;/p&gt;
&lt;p&gt;Instead, you cycle between domains based on what's blocking you. When a scientific knowledge gap prevents you from making progress, whether it's immunology, organic chemistry, or protein biochemistry, you shift into intensive life sciences learning mode using these principles. When software limitations hold you back, you focus there.&lt;/p&gt;
&lt;p&gt;Over years, this creates deep expertise in both domains. Not through divided attention, but through strategic, focused learning periods in each.&lt;/p&gt;
&lt;p&gt;Here's the cycle I've seen work: work with real problems using directness, identify gaps in whichever domain is limiting you through drilling, focus intensively on that domain, get feedback from domain experts, test yourself through writing and building using retrieval, develop intuition, retain through continued practice, then identify the next limiting domain and cycle back.&lt;/p&gt;
&lt;p&gt;At a higher level, there's an interplay between the domains that makes this work. Scientific understanding informs what software to build and what analyses matter. Software skills enable you to answer scientific questions and build tools others can use. They feed each other.&lt;/p&gt;
&lt;p&gt;This approach beats traditional "take courses in both fields" for biotech data scientists because both domains are too vast to learn all at once. Ultralearning gives you a framework for continuous, targeted learning throughout your career. Remember, your goal is not to become a PhD scientist or a senior software engineer, but to build deep enough understanding in both to be effective at the intersection.&lt;/p&gt;
&lt;h2 id="conclusion-and-next-steps"&gt;Conclusion and next steps&lt;/h2&gt;&lt;p&gt;You don't need to apply all 9 principles to both domains at once. In fact, you shouldn't.&lt;/p&gt;
&lt;p&gt;Start with directness in whichever domain is currently limiting your effectiveness. If you can't interpret your results because you don't understand the biology or chemistry, focus there intensively for the next few months. If you can't scale your analyses or build reliable tools because of software gaps, focus there intensively.&lt;/p&gt;
&lt;p&gt;Add feedback loops from experts in that domain. Build from there using the other principles.&lt;/p&gt;
&lt;p&gt;Then, when you've made real progress, identify which domain is now the bottleneck and shift your intensive learning there.&lt;/p&gt;
&lt;p&gt;This is a career-long journey of alternating deep dives, not a sprint to learn everything at once. The most effective biotech data scientists I know are continuously learning in both domains, but wisely: one intensive focus at a time.&lt;/p&gt;
&lt;p&gt;Here's your actionable takeaway: Right now, which domain is most limiting your effectiveness? Pick one concrete gap in that domain. Spend the next month using ultralearning principles to address that specific gap. Only that one. Master it, then reassess.&lt;/p&gt;
&lt;p&gt;You can do it!&lt;/p&gt;
</content></entry><entry><title>The Data Science Bootstrap Notes: A major upgrade for 2025</title><link href="https://ericmjl.github.io/blog/2025/9/2/the-data-science-bootstrap-notes-a-major-upgrade-for-2025/" rel="alternate"/><updated>2025-09-02T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:808862ff-19aa-390b-bbf2-8e9b08358094</id><content type="html">&lt;p&gt;After 8 years since the first edition, I've completely overhauled &lt;em&gt;The Data Science Bootstrap Notes&lt;/em&gt; to reflect the dramatic changes in the Python data science ecosystem. What started as a collection of Obsidian notes has evolved into a comprehensive, modern guide that addresses the tools and practices that actually matter in 2025.&lt;/p&gt;
&lt;h2 id="from-obsidian-notes-to-a-proper-book"&gt;From Obsidian notes to a proper book&lt;/h2&gt;&lt;p&gt;The most visible change is the format itself. The original version existed as a navigable knowledge base in Obsidian, mimicking &lt;a href="https://notes.andymatuschak.org/About_these_notes"&gt;Andy Matuschak's online notes&lt;/a&gt;. While that format was intellectually interesting to make, I found that after years of experimentation, simple was indeed better than cool. The new version uses MkDocs to create a clean, linear book format that's easier to navigate and more accessible to readers.&lt;/p&gt;
&lt;p&gt;But the real transformation goes far deeper than just the presentation layer.&lt;/p&gt;
&lt;h2 id="the-tooling-revolution-conda-pip-pixi-uv"&gt;The tooling revolution: conda + pip → pixi + uv&lt;/h2&gt;&lt;p&gt;The biggest shift in my recommendations centers around environment management. In 2017, conda was the obvious choice for Python data science environments. Today, that's no longer the case.&lt;/p&gt;
&lt;h3 id="enter-pixi-the-environment-management-multi-tool"&gt;Enter pixi: the environment management multi-tool&lt;/h3&gt;&lt;p&gt;I've completely replaced conda with &lt;code&gt;pixi&lt;/code&gt;, a modern environment manager written in Rust that solves many of the fundamental problems that plagued the conda ecosystem. The key advantages are:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Automatic Lock Files&lt;/strong&gt;: Pixi automatically generates and maintains lock files (&lt;code&gt;pixi.lock&lt;/code&gt;) every time you modify your environment. This solves the critical "it works on my machine" problem that conda users faced when environments would drift over time.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Feature-Based Environments&lt;/strong&gt;: Instead of creating separate environments for each purpose, pixi lets you define reusable "features" that can be combined into different environments. You can have &lt;code&gt;tests&lt;/code&gt;, &lt;code&gt;docs&lt;/code&gt;, &lt;code&gt;notebook&lt;/code&gt;, and &lt;code&gt;cuda&lt;/code&gt; features that combine into purpose-built environments like &lt;code&gt;default&lt;/code&gt;, &lt;code&gt;docs&lt;/code&gt;, or &lt;code&gt;cuda&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Task Automation&lt;/strong&gt;: Pixi enables you to replace Makefiles with tasks defined in &lt;code&gt;pyproject.toml&lt;/code&gt;. Commands like &lt;code&gt;pixi run test&lt;/code&gt; or &lt;code&gt;pixi run docs&lt;/code&gt; standardize common operations across your team.&lt;/p&gt;
&lt;h3 id="uv-the-python-tool-manager"&gt;uv: the Python tool manager&lt;/h3&gt;&lt;p&gt;Complementing pixi is &lt;code&gt;uv&lt;/code&gt;, an extremely fast Python package installer and resolver written in Rust. UV handles global tool installation by automatically creating isolated environments for each tool, giving you the convenience of global tools without the mess of a global Python installation.&lt;/p&gt;
&lt;p&gt;This means you can run tools like &lt;code&gt;llamabot&lt;/code&gt; or my own &lt;code&gt;pyds-cli&lt;/code&gt; without worrying about dependency conflicts. The &lt;code&gt;uvx&lt;/code&gt; command even lets you run tools without installing them first.&lt;/p&gt;
&lt;h2 id="modern-project-scaffolding"&gt;Modern project scaffolding&lt;/h2&gt;&lt;p&gt;The new edition introduces &lt;code&gt;pyds-cli&lt;/code&gt;, my opinionated tooling for data scientists that scaffolds new projects using cookiecutter and pixi. Instead of manually setting up project structures, you can now run:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;pyds&lt;span class="w"&gt; &lt;/span&gt;project&lt;span class="w"&gt; &lt;/span&gt;init
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This creates a complete project structure with proper environment management, testing setup, documentation configuration, and CI/CD pipelines already configured.&lt;/p&gt;
&lt;h2 id="ai-integration-beyond-the-hype"&gt;AI integration beyond the hype&lt;/h2&gt;&lt;p&gt;Generative AI has fundamentally changed how I think about data science workflows. The new edition includes a comprehensive chapter on working with AI tools that goes beyond simple code generation to address:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The speed of thought&lt;/strong&gt;: AI tools help bridge the gap between how fast we can think and how fast we can type. There's fascinating research showing humans process information at $10^9$ bits/second but think at only 10 bits/second - AI helps bridge this massive gap.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The right kind of lazy&lt;/strong&gt;: I distinguish between being "Bill Gates lazy" (finding efficient ways to work) and being intellectually lazy (blindly trusting AI outputs). You must maintain intellectual responsibility.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Effective patterns&lt;/strong&gt;: I share specific strategies for structuring AI interactions, from starting with the big picture to rapid iteration and verification. This includes the "fat finger sketch" approach where you outline what you want before asking AI to fill in details.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Beyond code&lt;/strong&gt;: AI tools are particularly valuable for documentation acceleration, code review assistance, and learning new libraries or techniques.&lt;/p&gt;
&lt;p&gt;The key insight is that AI should amplify our capabilities, not replace our judgment. We need to develop a mindset that embraces these tools while maintaining intellectual rigor.&lt;/p&gt;
&lt;h2 id="ci/cd-and-automation"&gt;CI/CD and automation&lt;/h2&gt;&lt;p&gt;The new edition heavily emphasizes GitHub Actions for continuous integration and deployment. Instead of manual processes, you now have trigger-able bots that can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Run tests automatically on every commit&lt;/li&gt;
&lt;li&gt;Build and deploy documentation&lt;/li&gt;
&lt;li&gt;Validate code quality with pre-commit hooks&lt;/li&gt;
&lt;li&gt;Deploy applications to various environments&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This automation eliminates the drudgery that often accompanies data science projects and ensures consistency across team members. I've even applied this philosophy to the book itself; the entire publishing process is automated through GitHub Actions that build and deploy the website, while simultaneously updating the Leanpub version with every commit.&lt;/p&gt;
&lt;h2 id="philosophical-foundations"&gt;Philosophical foundations&lt;/h2&gt;&lt;p&gt;While the tools have changed dramatically, the core philosophies remain the same but are now more clearly articulated:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Know Your Compute Stack&lt;/strong&gt;: Deep understanding of your tools enables informed choices about what to automate&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Single Source of Truth&lt;/strong&gt;: Establish clear, unambiguous sources for data, code, and configuration&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Automate Relentlessly&lt;/strong&gt;: Invest in automation to eliminate repetitive tasks&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Categorize Everything&lt;/strong&gt;: Organize projects using logical categories that make maintenance easier&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These principles now have concrete implementations through modern tooling, making them more actionable than ever.&lt;/p&gt;
&lt;h2 id="what-s-been-removed"&gt;What's been removed&lt;/h2&gt;&lt;p&gt;Not everything made the cut. I've removed outdated advice about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Manual conda environment management (replaced with pixi automation)&lt;/li&gt;
&lt;li&gt;Complex conda-specific workflows (simplified with pixi features)&lt;/li&gt;
&lt;li&gt;Manual lock file generation (now automatic with pixi)&lt;/li&gt;
&lt;li&gt;Manual project scaffolding (now automated with pyds-cli)&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="the-path-forward"&gt;The path forward&lt;/h2&gt;&lt;p&gt;The new edition is designed to get you started quickly while building foundations that scale. It's not just a reference guide; it's a roadmap for establishing practices that grow with your ambitions. The tools and practices I recommend today are the ones I actually use in production, not just theoretical best practices.&lt;/p&gt;
&lt;p&gt;What excites me most about this upgrade is how it addresses the real pain points that data scientists face in 2025. Instead of wrestling with environment conflicts, you're now thinking about how to compose features into purpose-built environments. Instead of manually setting up projects, you're focusing on the actual analysis. Instead of fighting with dependency resolution, you're building reproducible workflows that work the same way for everyone on your team.&lt;/p&gt;
&lt;p&gt;The data science ecosystem has matured significantly since 2017, and this new edition reflects that maturity. It's about getting started the right way; establishing foundations that won't crumble as your projects grow in complexity and team size.&lt;/p&gt;
&lt;p&gt;You can read the book online at &lt;a href="https://ericmjl.github.io/data-science-bootstrap-notes/"&gt;the GitHub Pages site&lt;/a&gt;, and if you prefer a linear reading experience, there's also an &lt;a href="https://leanpub.com/dsbootstrap/"&gt;eBook version on LeanPub&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The future of data science is automated, reproducible, and collaborative. This new edition shows you how to get there.&lt;/p&gt;
</content></entry><entry><title>How to use AI to accelerate your career in 2025</title><link href="https://ericmjl.github.io/blog/2025/9/1/how-to-use-ai-to-accelerate-your-career-in-2025/" rel="alternate"/><updated>2025-09-01T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:0d018b08-65a0-3c3d-85a6-959936d7f3fb</id><content type="html">&lt;p&gt;Everyone knows LLMs can help with coding and drafting emails. But there are less obvious ways to hack your career with AI that can save you hours and make you more effective at work.&lt;/p&gt;
&lt;p&gt;Here are 10 strategies I've tested, with sample prompts you can steal:&lt;/p&gt;
&lt;h2 id="draft-presentations-that-actually-land"&gt;Draft presentations that actually land&lt;/h2&gt;&lt;p&gt;Most people start with slides. Start with your audience instead.&lt;/p&gt;
&lt;p&gt;First, research who you're presenting to. If you know specific attendees, have ChatGPT or Claude build dossiers from their public profiles - LinkedIn, company bios, recent interviews. Then ask the LLM what these people care about most.&lt;/p&gt;
&lt;p&gt;Next, have it craft your core message and angle based on those audience insights. Finally, get it to describe in words how each slide should look before you build anything - making sure to feed in both your audience research and your refined message. This approach works because you're designing for actual humans, not abstract concepts.&lt;/p&gt;
&lt;p&gt;Pro tip: Or just skip the manual work entirely and use Gamma.ai.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample prompts:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I'm presenting to &lt;code&gt;[specific people/roles]&lt;/code&gt;. Here are their LinkedIn profiles: &lt;code&gt;[paste]&lt;/code&gt;. What do they care about most professionally right now?&lt;/p&gt;
&lt;p&gt;My presentation topic is &lt;code&gt;[topic]&lt;/code&gt;. My audience cares about &lt;code&gt;[insights from above]&lt;/code&gt;. Help me craft a compelling angle that will resonate with them.&lt;/p&gt;
&lt;p&gt;Generate slide-by-slide instructions for a &lt;code&gt;[number]&lt;/code&gt;-slide presentation on &lt;code&gt;[topic]&lt;/code&gt;. My audience is &lt;code&gt;[audience description]&lt;/code&gt; and they care about &lt;code&gt;[audience insights from research]&lt;/code&gt;. My core message is &lt;code&gt;[refined message/angle]&lt;/code&gt;. For each slide, tell me: the title (which should be the main point of that slide), what elements to include, and how to lay them out. The title style should be &lt;code&gt;[describe your preferred title style - e.g., "a clear statement that makes the key point, not just a topic heading"]&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="research-your-negotiation-counterparts-like-a-detective"&gt;Research your negotiation counterparts like a detective&lt;/h2&gt;&lt;p&gt;Context is everything in negotiations. Feed your LLM everything you know about the other party - their backgrounds, the situation they're in, potential constraints they're facing.&lt;/p&gt;
&lt;p&gt;Describe your own circumstances, goals, and BATNA (Best Alternative to a Negotiated Agreement). Then iterate with the LLM on potential objections and counter-strategies.&lt;/p&gt;
&lt;p&gt;The more specific information you provide, the better it gets at uncovering blind spots you hadn't considered.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample prompts:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I'm negotiating &lt;code&gt;[situation]&lt;/code&gt; with &lt;code&gt;[specific people/roles]&lt;/code&gt;. Here's what I know about them: &lt;code&gt;[paste background]&lt;/code&gt;. Here are my goals: &lt;code&gt;[paste goals]&lt;/code&gt;. My BATNA is: &lt;code&gt;[paste alternative]&lt;/code&gt;. What objections might they raise?&lt;/p&gt;
&lt;p&gt;Given this context: &lt;code&gt;[paste situation details]&lt;/code&gt;, what leverage points might I have that I'm not seeing?&lt;/p&gt;
&lt;p&gt;If I propose &lt;code&gt;[specific ask]&lt;/code&gt;, how might they respond based on &lt;code&gt;[paste their constraints/motivations]&lt;/code&gt;? Help me prepare counter-responses.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="transform-content-between-formats-effortlessly"&gt;Transform content between formats effortlessly&lt;/h2&gt;&lt;p&gt;You wrote a technical document that needs to become a blog post. Or you have a blog post that needs to become a slide deck. Instead of starting from scratch, let the LLM remix your existing content into the right format.&lt;/p&gt;
&lt;p&gt;The key is being specific about your target audience. A technical document transformed for executives needs different language and emphasis than one transformed for peer engineers. Without clear audience context, the LLM can't make effective choices about tone, depth, and focus.&lt;/p&gt;
&lt;p&gt;This works for any content transformation - meeting notes to executive summaries, brainstorming sessions to project proposals, quarterly reviews to team updates. Just remember: same content, different audience, different approach.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample prompts:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Transform this technical document into a blog post for &lt;code&gt;[specific people/roles - describe their background, interests, and level of technical knowledge]&lt;/code&gt;: &lt;code&gt;[paste content]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Turn these meeting notes into an executive summary for &lt;code&gt;[specific people/roles - include their role, priorities, and what they care about]&lt;/code&gt;: &lt;code&gt;[paste notes]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Convert this brainstorming session into a structured project proposal for &lt;code&gt;[specific people/roles - describe their concerns and what convinces them]&lt;/code&gt;: &lt;code&gt;[paste ideas]&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="fill-out-administrative-forms-without-the-dread"&gt;Fill out administrative forms without the dread&lt;/h2&gt;&lt;p&gt;OKRs, performance reviews, expense reports - we all have forms that feel like bureaucratic hurdles. Here's the hack: don't write directly into the form.&lt;/p&gt;
&lt;p&gt;Instead, do a brain dump by talking through your accomplishments and goals. Transcribe this (voice memos work great), then paste the form questions plus your transcript into ChatGPT. Have it fill out the form for you, then copy-paste back.&lt;/p&gt;
&lt;p&gt;What used to take half a day now takes 30 minutes.&lt;/p&gt;
&lt;p&gt;Check out the Dia browser, which lets you insert LLM-generated text directly into web forms.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample prompts:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Here are my form questions: &lt;code&gt;[paste]&lt;/code&gt;. Here's my brain dump of accomplishments: &lt;code&gt;[paste transcript]&lt;/code&gt;. Fill out the form professionally.&lt;/p&gt;
&lt;p&gt;Help me write OKRs based on this verbal dump of my goals: &lt;code&gt;[paste transcript]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Turn this expense description into proper business justification: &lt;code&gt;[paste description]&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="ghost-write-in-your-own-voice"&gt;Ghost-write in your own voice&lt;/h2&gt;&lt;p&gt;Including this blog post, I start by verbally dumping my ideas into a Markdown file in Obsidian, usually via voice transcription. Then I have Claude ghostwrite using my tone - my verbal dump contains my natural writing patterns, plus I feed it samples of my previous blog posts.
The key is multiple editing rounds. I push hard on the LLM during edits, which is how I make the content truly mine. I don't publish anything until it's been through at least two rounds of refinement.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample prompts:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Here's my verbal brain dump: &lt;code&gt;[paste]&lt;/code&gt;. Here are samples of my writing style: &lt;code&gt;[paste examples]&lt;/code&gt;. Ghostwrite my brain dump as a blog post in my voice.&lt;/p&gt;
&lt;p&gt;This draft doesn't sound like me yet. Make it more &lt;code&gt;[specific style notes]&lt;/code&gt;. Here's what my natural voice sounds like: &lt;code&gt;[paste examples]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;Polish this draft but keep my conversational tone and specific phrases: &lt;code&gt;[paste draft]&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="prepare-manager-updates-that-actually-help-your-career"&gt;Prepare manager updates that actually help your career&lt;/h2&gt;&lt;p&gt;Use the same strategy as ghostwriting, but tailor it to your manager's level and scope. You want your manager to have the details they need to advocate for you effectively.&lt;/p&gt;
&lt;p&gt;Most managers don't have time to critique your use of AI - they just need to stay informed. Keep a running log in a shared space, ideally structured like "updates, problems, questions" organized by project.&lt;/p&gt;
&lt;p&gt;As both an employee and a manager, I can tell you: teammates who spoon-feed structured weekly updates are gold. It shapes your manager's memory of your contributions and honestly makes my job as a manager easier because it lets me focus on coaching and strategic support rather than hunting for status updates.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample prompts:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Turn this brain dump into a structured manager update: &lt;code&gt;[paste notes]&lt;/code&gt;. Format as Updates/Problems/Questions by project.&lt;/p&gt;
&lt;p&gt;My manager is &lt;code&gt;[specific people/roles - description of their role/priorities]&lt;/code&gt;. Here's what I accomplished this week: &lt;code&gt;[paste list]&lt;/code&gt;. Write an update that helps them advocate for me.&lt;/p&gt;
&lt;p&gt;Summarize my quarterly achievements in a way that highlights impact and aligns with &lt;code&gt;[paste company priorities]&lt;/code&gt;: &lt;code&gt;[paste accomplishments]&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="personalize-communications-with-important-people"&gt;Personalize communications with important people&lt;/h2&gt;&lt;p&gt;Use research mode to build context on VIPs you're meeting. Feed that research into your communication planning. This works because our digital footprints reveal what we care about, and LLMs are trained on massive examples of human interaction.&lt;/p&gt;
&lt;p&gt;The result: messages that land because they're tailored to what actually matters to the recipient.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample prompts:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I'm reaching out to &lt;code&gt;[specific people/roles]&lt;/code&gt; about &lt;code&gt;[topic]&lt;/code&gt;. Here's their background: &lt;code&gt;[paste research]&lt;/code&gt;. Help me craft a personalized message that will resonate.&lt;/p&gt;
&lt;p&gt;Based on this person's recent posts/interviews: &lt;code&gt;[paste]&lt;/code&gt;, what communication style and topics should I focus on?&lt;/p&gt;
&lt;p&gt;I need to follow up on &lt;code&gt;[situation]&lt;/code&gt; with &lt;code&gt;[specific people/roles who have these characteristics]&lt;/code&gt;. Write a message that acknowledges &lt;code&gt;[paste their priorities/constraints]&lt;/code&gt;.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="make-seamless-edits-to-any-document"&gt;Make seamless edits to any document&lt;/h2&gt;&lt;p&gt;When you have a long piece of writing (code, essays, reports), don't manually hunt for where to make changes. Just dictate your edits into an LLM chat.&lt;/p&gt;
&lt;p&gt;Add this to your system prompt: "When you make edits that I request, please make them seamless with the rest of the context."
This prevents the LLM from injecting walls of text and instead makes surgical, contextual changes.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample prompts:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;In this document: &lt;code&gt;[paste]&lt;/code&gt;, I need to add information about &lt;code&gt;[topic]&lt;/code&gt; in the section about &lt;code&gt;[section]&lt;/code&gt;. Make it seamless with the existing content.&lt;/p&gt;
&lt;p&gt;Change the tone of this section from &lt;code&gt;[current tone]&lt;/code&gt; to &lt;code&gt;[desired tone]&lt;/code&gt; without changing the key points: &lt;code&gt;[paste section]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;This paragraph needs to be more concise while keeping the main message: &lt;code&gt;[paste paragraph]&lt;/code&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="organize-your-accomplishments-by-competency"&gt;Organize your accomplishments by competency&lt;/h2&gt;&lt;p&gt;Dump all your achievements into a voice memo and have an LLM organize them by your company's competency framework. Most performance reviews follow standard patterns - leadership, technical skills, collaboration.&lt;/p&gt;
&lt;p&gt;LLMs excel at categorizing your work and suggesting which examples best demonstrate each competency. No more staring at blank forms trying to remember what you did six months ago.&lt;/p&gt;
&lt;p&gt;This is essentially automating the &lt;a href="https://www.youtube.com/shorts/gbkv8Asadh0"&gt;brag doc that Steve Huynh recommends&lt;/a&gt; - but with AI doing the heavy lifting of organization and categorization. I've written more about &lt;a href="https://ericmjl.github.io/blog/2024/2/29/your-first-90-days-at-work-what-should-you-do/"&gt;building your accomplishments record in your first 90 days&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample prompts:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Here are my accomplishments from this year: &lt;code&gt;[paste list]&lt;/code&gt;. Organize them by these competencies: &lt;code&gt;[paste framework]&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;I need examples that demonstrate leadership. Here's everything I've done: &lt;code&gt;[paste]&lt;/code&gt;. Which examples best show leadership impact?&lt;/p&gt;
&lt;p&gt;Help me identify gaps in my competency examples. Here's what I have: &lt;code&gt;[paste organized list]&lt;/code&gt;. What areas need stronger examples?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="practice-difficult-conversations-before-they-happen"&gt;Practice difficult conversations before they happen&lt;/h2&gt;&lt;p&gt;Feed in everything you know about the person you need to talk to - their communication style, recent stressors, past reactions, what motivates them. Have the LLM help you craft the right tone and timing, then practice by having it roleplay as them.&lt;/p&gt;
&lt;p&gt;Difficult conversations often fail not because of what you say, but how and when you say it. This prep work is like having a rehearsal before the real performance.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=yMOmmnjy3sE"&gt;Jeremy Utley demonstrates this technique&lt;/a&gt; of using AI to roleplay difficult conversations before they happen.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample prompts:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I need to have a difficult conversation with &lt;code&gt;[specific people/roles]&lt;/code&gt; about &lt;code&gt;[topic]&lt;/code&gt;. Here's their communication style: &lt;code&gt;[paste description]&lt;/code&gt;. Here's the situation: &lt;code&gt;[paste context]&lt;/code&gt;. Help me plan my approach.&lt;/p&gt;
&lt;p&gt;Roleplay as &lt;code&gt;[specific people/roles with these characteristics]&lt;/code&gt; while I practice this conversation about &lt;code&gt;[topic]&lt;/code&gt;. Push back as they would based on &lt;code&gt;[paste their known concerns]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;I want to ask for &lt;code&gt;[specific request]&lt;/code&gt;. This person typically responds to &lt;code&gt;[paste motivation style]&lt;/code&gt;. How should I frame this conversation?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="force-ai-to-challenge-your-assumptions"&gt;Force AI to challenge your assumptions&lt;/h2&gt;&lt;p&gt;Don't just use LLMs to confirm what you already think. Actively ask them to disagree with you and surface blind spots. This is especially powerful for strategic decisions, project planning, or career moves where you might be too close to see potential problems.&lt;/p&gt;
&lt;p&gt;The key is being explicit about wanting pushback. LLMs are trained to be helpful and agreeable, so you need to specifically request criticism and alternative perspectives.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sample prompts:&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I'm planning to &lt;code&gt;[paste decision/strategy]&lt;/code&gt;. Play devil's advocate - what are the strongest arguments against this approach?&lt;/p&gt;
&lt;p&gt;Challenge my assumptions about &lt;code&gt;[situation]&lt;/code&gt;. Here's how I see it: &lt;code&gt;[paste your perspective]&lt;/code&gt;. What am I missing or getting wrong?&lt;/p&gt;
&lt;p&gt;I think &lt;code&gt;[paste opinion/plan]&lt;/code&gt;. Give me three reasons why someone smart might disagree with me, and explain their reasoning.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="the-principles-that-make-this-work"&gt;The principles that make this work&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Know your audience first.&lt;/strong&gt; Whether you're transforming content, crafting presentations, or writing updates for your manager, everything starts with understanding who you're communicating with. LLMs can't make effective choices about tone, depth, and focus without clear audience context.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Research beats assumptions.&lt;/strong&gt; Don't guess what people want or how they'll react. Feed LLMs specific information about negotiation counterparts, presentation audiences, or conversation partners. The more context you provide, the better the output.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Speak, don't type.&lt;/strong&gt; Voice transcription captures your natural patterns and is faster than typing. Use it for brain dumps, accomplishment reviews, and initial drafts.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Automate the tedious, elevate the strategic.&lt;/strong&gt; Let LLMs handle forms, formatting, and content transformation so you can focus on relationships, creative problem-solving, and high-level strategy.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Practice before it matters.&lt;/strong&gt; Use LLMs to rehearse difficult conversations, anticipate objections in negotiations, and stress-test your thinking before real situations.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Seek challenge, not just confirmation.&lt;/strong&gt; Explicitly ask LLMs to disagree with you, surface blind spots, and present counterarguments. This prevents echo chambers and sharpens your thinking.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Document everything for future you.&lt;/strong&gt; Keep accomplishment records, conversation insights, and successful prompts organized. Your future self will thank you during performance reviews and job transitions.&lt;/p&gt;
&lt;p&gt;The real power isn't in the AI doing your thinking for you - it's in using AI to handle the mechanics so you can focus on the strategy, relationships, and creative problem-solving that actually advance your career.&lt;/p&gt;
&lt;h2 id="ai-is-a-mirror"&gt;AI is a mirror&lt;/h2&gt;&lt;p&gt;As Jeremy Utley puts it: "AI is a mirror." If we want our brains to rot, we can use AI to make our brains rot. Or we can use AI to sharpen how we're thinking, be more effective and efficient at how we're working.&lt;/p&gt;
&lt;p&gt;The choice is yours. Use these techniques to elevate your career, not replace your judgment.&lt;/p&gt;
</content></entry><entry><title>How to communicate with lab scientists (when you're the data person)</title><link href="https://ericmjl.github.io/blog/2025/8/24/how-to-communicate-with-lab-scientists-when-youre-the-data-person/" rel="alternate"/><updated>2025-08-24T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:49bf5cd1-1578-32ca-9e45-4a16a6ece092</id><content type="html">&lt;p&gt;Imagine this scenario: A data scientist explains a hierarchical Bayesian model for 45 minutes. Beautiful math. Elegant handling of batch effects. The lab scientists are polite but glazed over. Finally, someone interrupts: "Sorry, but should we move this compound forward or not?"&lt;/p&gt;
&lt;p&gt;The data scientist hadn't even calculated that probability.&lt;/p&gt;
&lt;p&gt;Sound familiar?&lt;/p&gt;
&lt;p&gt;If you're a statistician or data scientist in biotech, you've probably been there. You've spent hours on sophisticated analyses, crafted beautiful slides about your methods, and watched your audience's eyes glaze over while you explained mixed-effects models.&lt;/p&gt;
&lt;p&gt;Meanwhile, they just needed to know if they should spend $200K on the next experiment.&lt;/p&gt;
&lt;p&gt;Here's the thing: Lab scientists aren't struggling to understand your statistics because they're not smart enough. They're brilliant experts who've spent years mastering protein folding, cell signaling, or synthetic chemistry. They're just juggling their own complex problems and need you to translate your analysis into something they can act on.&lt;/p&gt;
&lt;p&gt;Today, I'm going to show you exactly how to do that.&lt;/p&gt;
&lt;h2 id="here-s-what-we-re-covering"&gt;Here's what we're covering:&lt;/h2&gt;&lt;ol&gt;
&lt;li&gt;Your communication budget is finite — spend it wisely&lt;/li&gt;
&lt;li&gt;Know what mode they're in before you open your laptop&lt;/li&gt;
&lt;li&gt;Use the three-layer translation model&lt;/li&gt;
&lt;li&gt;Decode what they're really asking&lt;/li&gt;
&lt;li&gt;Build trust through clarity, not complexity&lt;/li&gt;
&lt;li&gt;Master the decision-first meeting structure&lt;/li&gt;
&lt;li&gt;Ask yourself what they'll ask you&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="1-your-communication-budget-is-finite-spend-it-wisely"&gt;1. Your communication budget is finite — spend it wisely&lt;/h2&gt;&lt;p&gt;Every interaction has a finite "communication budget" — limited attention, time, and cognitive load. Most data scientists spend this budget like tourists with foreign currency, not realizing the exchange rate.&lt;/p&gt;
&lt;p&gt;Think about your last presentation. Where did you spend your time?&lt;/p&gt;
&lt;p&gt;🚫 &lt;strong&gt;The typical (failed) allocation:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;60% on methodology and statistical details&lt;/li&gt;
&lt;li&gt;30% on results (tables, coefficients, credible intervals)&lt;/li&gt;
&lt;li&gt;10% on "what this means" (usually rushed at the end)&lt;/li&gt;
&lt;li&gt;0% on "what you should do next"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I get it. We're trained to show our work. We think rigor equals value. We assume that if we explain our methods thoroughly enough, scientists will understand what to do.&lt;/p&gt;
&lt;p&gt;But here's what actually works:&lt;/p&gt;
&lt;p&gt;✅ &lt;strong&gt;The allocation that drives decisions:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;10% on methods (just enough for credibility)&lt;/li&gt;
&lt;li&gt;20% on results (simplified, visual, contextual)&lt;/li&gt;
&lt;li&gt;40% on implications for their specific decisions&lt;/li&gt;
&lt;li&gt;30% on uncertainty and what it means for their next steps&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;But context matters. A curious scientist with time might genuinely want 30% methods—they're building mental models for future decisions. Someone facing a go/no-go decision tomorrow? They need 70% decision implications, minimal methods.&lt;/p&gt;
&lt;p&gt;The key is adopting &lt;a href="https://en.wikipedia.org/wiki/BLUF_(communication"&gt;BLUF (Bottom-Line Up-Front)&lt;/a&gt;). Structure your presentation by working backwards from the decision to be made.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try this:&lt;/strong&gt; Start with the decision and recommendation, then work backwards to the evidence that supports it. Lead with "Based on our analysis, I recommend we proceed with Compound A because there's an 87% probability it meets our potency threshold."&lt;/p&gt;
&lt;p&gt;This tells them immediately what they need to know, then you can spend the remaining time explaining why.&lt;/p&gt;
&lt;p&gt;Here's what happens when you don't use BLUF: A data scientist spent an entire program review meeting walking through their elegant approach to handling missing data. Really sophisticated stuff. Multiple imputation with careful consideration of the missing-at-random assumption.&lt;/p&gt;
&lt;p&gt;Twenty minutes in, the program lead interrupted: "This is interesting, but we need to decide today whether to advance this molecule. Does it meet our potency threshold or not?"&lt;/p&gt;
&lt;p&gt;They hadn't even calculated that probability. They'd spent their entire communication budget on something that wasn't even the program lead's concern that day.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;With BLUF, they would have started:&lt;/strong&gt; "Based on our analysis, I recommend we advance this molecule. There's an 82% probability it meets our potency threshold, even accounting for the missing data. Here's how I handled the missing data to arrive at this conclusion..."&lt;/p&gt;
&lt;h2 id="2-know-what-mode-they-re-in-before-you-open-your-laptop"&gt;2. Know what mode they're in before you open your laptop&lt;/h2&gt;&lt;p&gt;Lab scientists operate in three distinct modes, and each requires a completely different communication approach.&lt;/p&gt;
&lt;h3 id="decision-mode-most-of-the-time"&gt;Decision Mode (Most of the time)&lt;/h3&gt;&lt;p&gt;They're under time pressure for go/no-go decisions. Maybe it's a pipeline review tomorrow. Maybe they need to order materials today. Maybe the synthesis team is literally waiting for their answer.&lt;/p&gt;
&lt;p&gt;Signs you'll hear:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"What's the bottom line?"&lt;/li&gt;
&lt;li&gt;"Should we proceed?"&lt;/li&gt;
&lt;li&gt;"Just tell me if it worked"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What they need: Probability of success and a clear recommendation. That's it.&lt;/p&gt;
&lt;h3 id="learning-mode-when-they-have-bandwidth"&gt;Learning Mode (When they have bandwidth)&lt;/h3&gt;&lt;p&gt;They're genuinely curious about your methods. Maybe they're trying to understand why this analysis differs from last time. Maybe they're building intuition for future experiments.&lt;/p&gt;
&lt;p&gt;Signs you'll hear:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"How does that work?"&lt;/li&gt;
&lt;li&gt;"Why did you choose that approach?"&lt;/li&gt;
&lt;li&gt;"Can you explain the intuition behind this?"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What they need: Mental models and intuition, not mathematical formulas.&lt;/p&gt;
&lt;h3 id="validation-mode-testing-if-they-can-trust-you"&gt;Validation Mode (Testing if they can trust you)&lt;/h3&gt;&lt;p&gt;They're not really interested in learning—they're assessing whether they can rely on your judgment for million-dollar decisions.&lt;/p&gt;
&lt;p&gt;Signs you'll hear:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"What assumptions did you make?"&lt;/li&gt;
&lt;li&gt;"How does this handle batch effects?"&lt;/li&gt;
&lt;li&gt;"What if the data is wrong?"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What they need: Confidence that you've been rigorous without the full mathematical proof.&lt;/p&gt;
&lt;p&gt;Here's the mistake most of us make:&lt;/p&gt;
&lt;p&gt;🚫 &lt;strong&gt;Wrong approach:&lt;/strong&gt; Launch into methods explanation regardless of mode&lt;/p&gt;
&lt;p&gt;✅ &lt;strong&gt;Right approach:&lt;/strong&gt; Start with the decision and recommendation, then adapt the explanation depth based on their mode&lt;/p&gt;
&lt;p&gt;Most data scientists default to teaching mode when scientists are in decision mode. That's like giving someone a recipe when they just asked if dinner's ready.&lt;/p&gt;
&lt;p&gt;Consider this scenario: A scientist approaches a data scientist with dose-response data. The data scientist starts explaining their Bayesian approach to EC50 estimation. Five minutes in, the scientist stops them: "I just need to know if this is more potent than our current lead."&lt;/p&gt;
&lt;p&gt;She was in Decision Mode. The data scientist was in Teaching Mode. Complete mismatch.&lt;/p&gt;
&lt;p&gt;The better approach is to be deliberate rather than reactive. Before any meeting, clarify the goals upfront. Ask what they're trying to decide. Talk to stakeholders beforehand to understand the context. Do the pre-work rather than trying to read body language in real-time.&lt;/p&gt;
&lt;h2 id="3-use-the-three-layer-translation-model"&gt;3. Use the three-layer translation model&lt;/h2&gt;&lt;p&gt;You think in distributions. They think in decisions. This gap is why brilliant analyses often fail to drive action.&lt;/p&gt;
&lt;p&gt;Here's the framework that works for bridging that gap:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Layer 1: Statistical Reality (Keep this in your head)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Full posterior distributions&lt;/li&gt;
&lt;li&gt;Model assumptions&lt;/li&gt;
&lt;li&gt;Fancy math&lt;/li&gt;
&lt;li&gt;All the technical details you love&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This layer is for you. It ensures your analysis is rigorous. But it stays in your head or the appendix.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Layer 2: Scientific Meaning (The bridge)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What the analysis means for their biological hypothesis&lt;/li&gt;
&lt;li&gt;How the statistics relate to their experimental design&lt;/li&gt;
&lt;li&gt;The full richness of uncertainty&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here's the key: Keep the full distribution at this layer. Don't collapse to point estimates yet. You're translating statistics to science, but you're not making decisions yet.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Layer 3: Decision Layer (What they actually need)&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NOW integrate over posteriors for specific probabilities&lt;/li&gt;
&lt;li&gt;"There's an 87% chance this compound beats your TPP threshold"&lt;/li&gt;
&lt;li&gt;"With 90% probability, this is your best compound"&lt;/li&gt;
&lt;li&gt;"You need 12 more samples to reach 95% confidence"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The magic is waiting until the last possible moment to collapse distributions into decision probabilities. Why? Because different decisions need different integrations of the same posterior.&lt;/p&gt;
&lt;p&gt;Let me show you what I mean:&lt;/p&gt;
&lt;p&gt;🚫 &lt;strong&gt;Wrong (Layer 1 bleeding into communication):&lt;/strong&gt;
"The posterior distribution for the treatment effect has a 95% credible interval of [0.15, 0.31] with a mean of 0.23."&lt;/p&gt;
&lt;p&gt;What does a lab scientist do with this? Nothing. It's statistical reality without translation.&lt;/p&gt;
&lt;p&gt;✅ &lt;strong&gt;Right (Layer 3, decision-focused):&lt;/strong&gt;
"There's a 92% probability your treatment exceeds the TPP threshold. If you need 95% confidence for the program milestone, run 20 more samples. If 90% is acceptable for an early read, you can proceed now."&lt;/p&gt;
&lt;p&gt;See the difference? One is statistical reporting. The other enables a decision.&lt;/p&gt;
&lt;p&gt;The same posterior distribution might need to answer multiple questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"What's the probability this exceeds our TPP?" (integrate above threshold)&lt;/li&gt;
&lt;li&gt;"What's the probability this is our best compound?" (compare posteriors)&lt;/li&gt;
&lt;li&gt;"How many samples until 95% confidence?" (project forward)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By keeping the full distribution until Layer 3, you can answer whatever decision question they actually have, not the one you assumed they had.&lt;/p&gt;
&lt;h2 id="4-decode-what-they-re-really-asking"&gt;4. Decode what they're really asking&lt;/h2&gt;&lt;p&gt;Scientists may ask statistics questions when they mean decision questions. Learning to translate is a superpower.&lt;/p&gt;
&lt;p&gt;Here's your decoder ring:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;"Is this significant?"&lt;/strong&gt; They're not asking about p-values. They're asking: "Should I continue this line of research?"&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;"What's the confidence?"&lt;/strong&gt; They don't want credible intervals. They're asking: "How wrong could this decision be?"&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;"Did it work?"&lt;/strong&gt; They don't care about effect sizes. They're asking: "Is the effect large enough to matter for my application?"&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;"Can you check the stats?"&lt;/strong&gt; They don't want a methods seminar. They're asking: "I need ammunition for my go/no-go meeting tomorrow."&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;"How robust is this?"&lt;/strong&gt; They're not necessarily interested in sensitivity analyses. They're asking: "Can I trust this decision?"&lt;/p&gt;
&lt;p&gt;Every lab scientist in biotech faces the same five decisions over and over:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Resource allocation:&lt;/strong&gt; Should I invest more time/money/FTEs in this direction?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Pipeline progression:&lt;/strong&gt; Is this ready for the next stage?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Experimental design:&lt;/strong&gt; Should I modify my approach or repeat?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Program decisions:&lt;/strong&gt; Continue, pivot, or kill?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Platform decisions:&lt;/strong&gt; Is this assay/method worth scaling up?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Your job isn't to answer their literal question. It's to figure out which of these five decisions they're really trying to make.&lt;/p&gt;
&lt;p&gt;🚫 &lt;strong&gt;Wrong:&lt;/strong&gt; Answer the literal statistics question they asked&lt;/p&gt;
&lt;p&gt;✅ &lt;strong&gt;Right:&lt;/strong&gt; Answer the decision they're trying to make&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Try this:&lt;/strong&gt; When first asked to partner on an analysis, ask: "What decision are you trying to make with this data?"&lt;/p&gt;
&lt;p&gt;Then frame everything around that decision.&lt;/p&gt;
&lt;p&gt;Here's a common scenario: A scientist asks the data team to "check if the groups are different." The data scientist could run their standard analysis and report "statistically significant difference detected." Technically correct. Completely useless.&lt;/p&gt;
&lt;p&gt;Instead, imagine asking: "What decision does this inform?"&lt;/p&gt;
&lt;p&gt;Turns out, they need to know if the new formulation is at least 20% better than the current one — otherwise, it wasn't worth the reformulation costs. The groups were statistically different, but only by 8%. The real answer was: "Don't reformulate."&lt;/p&gt;
&lt;p&gt;That's the difference between answering questions and enabling decisions.&lt;/p&gt;
&lt;h2 id="5-build-trust-through-clarity-not-complexity"&gt;5. Build trust through clarity, not complexity&lt;/h2&gt;&lt;p&gt;Here's the paradox: Most data scientists think trust comes from showing their work.&lt;/p&gt;
&lt;p&gt;This is more nuanced than you might think.&lt;/p&gt;
&lt;p&gt;Over-explaining methods actually reduces trust because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It signals insecurity about your results&lt;/li&gt;
&lt;li&gt;It wastes precious communication budget&lt;/li&gt;
&lt;li&gt;It feels like gatekeeping with jargon&lt;/li&gt;
&lt;li&gt;It suggests you don't understand what they need&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What actually builds trust:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Leading with clear probabilities for their go/no-go decisions&lt;/li&gt;
&lt;li&gt;Showing how probability changes with more data&lt;/li&gt;
&lt;li&gt;Being precise about uncertainty without hedging&lt;/li&gt;
&lt;li&gt;Speaking their language, not yours&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;🚫 &lt;strong&gt;Trust-killing:&lt;/strong&gt; "Well, it depends on your assumptions about the prior, and if we consider the hierarchical structure of the random effects, controlling for batch-to-batch variation, we can say that under certain conditions..."&lt;/p&gt;
&lt;p&gt;This sounds like you're not confident in your answer.&lt;/p&gt;
&lt;p&gt;✅ &lt;strong&gt;Trust-building:&lt;/strong&gt; "There's an 89% chance this works. If you need 95% confidence before scaling up, test 3 more concentrations. Here's why I'm confident in that number: I've accounted for batch effects, and even in the worst-case scenario, you're still above 82%."&lt;/p&gt;
&lt;p&gt;Clear. Actionable. Confident.&lt;/p&gt;
&lt;p&gt;The beauty of probabilistic thinking here: "We're 78% confident" is infinitely clearer than "statistically significant." You can directly answer: "What's the probability we're making the wrong decision?"&lt;/p&gt;
&lt;p&gt;That's a question every scientist understands.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The methods appendix approach:&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;When you do need to establish technical credibility, try this structure:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;One slide of method basics (just enough to show rigor)&lt;/li&gt;
&lt;li&gt;Key assumptions in plain English (Pro tip: AI tools can help translate technical assumptions into audience-appropriate language)&lt;/li&gt;
&lt;li&gt;Details available but not forced&lt;/li&gt;
&lt;li&gt;For the genuinely curious: "Happy to dive into the model after we nail down your decision"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Smart data scientists keep a technical appendix for every analysis. It has all the details they're proud of—the clever missing data handling, the hierarchical structure, the prior specifications.&lt;/p&gt;
&lt;p&gt;But they only show it when asked. And here's what happens: People trust them more because they respect everyone's time enough not to force it on them.&lt;/p&gt;
&lt;h2 id="6-master-the-decision-first-meeting-structure"&gt;6. Master the decision-first meeting structure&lt;/h2&gt;&lt;p&gt;Stop opening with methods. Stop it right now.&lt;/p&gt;
&lt;p&gt;Start with their decision.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The structure that works:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;State the decision context upfront&lt;/strong&gt;: "We're here to discuss [specific decision]. Here's what the data tells us."&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Give the probability and recommendation immediately&lt;/strong&gt;: "There's an 89% probability of success. I recommend proceeding."&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Show how probability changes with more data&lt;/strong&gt;: "With 10 more samples, we'd get to 95% confidence."&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Discuss what could change your assessment&lt;/strong&gt;: "This assumes your batch effects stay consistent."&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Offer details only if requested&lt;/strong&gt;: "Want me to walk through how I got there?"&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let me show you the difference:&lt;/p&gt;
&lt;p&gt;🚫 &lt;strong&gt;Wrong meeting flow:&lt;/strong&gt;
"Thanks for coming. So I started by examining the data structure, and I noticed some heteroscedasticity in the residuals, which suggested we might need a more complex variance structure. I tried several approaches, including a Box-Cox transformation, but ultimately settled on a hierarchical model because... [20 minutes later]... so in conclusion, it might work."&lt;/p&gt;
&lt;p&gt;By the time you get to the conclusion, they've stopped listening.&lt;/p&gt;
&lt;p&gt;✅ &lt;strong&gt;Right meeting flow:&lt;/strong&gt;
"We're here to discuss whether to advance Compound X to synthesis. Based on your assay data, there's an 89% probability this compound exceeds your 10nM potency requirement. I recommend proceeding to synthesis scale-up. If you need 95% confidence instead of 89%, I'd recommend testing 3 more concentrations first. Want me to walk through how I got there?"&lt;/p&gt;
&lt;p&gt;Notice how the second version:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Answers their question immediately&lt;/li&gt;
&lt;li&gt;Gives them options based on risk tolerance&lt;/li&gt;
&lt;li&gt;Respects their time&lt;/li&gt;
&lt;li&gt;Offers details rather than forcing them&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;The email version:&lt;/strong&gt;&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;Subject: Compound X: 89% probability of meeting TPP

Hi Sarah,

**Decision:** Compound X has an 89% probability of meeting your 10nM potency requirement. Recommend proceeding to synthesis.

**Key evidence:**
- Consistent effect across all three batches
- Dose-response curve shows clear relationship
- Even worst-case scenario keeps you above 15nM

**Next steps:** If you need &amp;gt;95% confidence, test 3 additional concentrations. Otherwise, proceed with synthesis.

Technical details in attached appendix if interested.

Best,
[Your name]
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's it. Decision first. Evidence second. Details optional.&lt;/p&gt;
&lt;h2 id="7-speak-their-language-literally"&gt;7. Speak their language (literally)&lt;/h2&gt;&lt;p&gt;Here's what most data scientists miss: You need to understand their domain as deeply as they do.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The measurement method matters more than your statistical method.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;A scientist tells you they're using ELISA to measure protein levels. You nod and proceed with your analysis. But did you ask:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What's the detection limit?&lt;/li&gt;
&lt;li&gt;How does the antibody specificity affect your readout?&lt;/li&gt;
&lt;li&gt;Are there known cross-reactivities that could confound your results?&lt;/li&gt;
&lt;li&gt;What's the coefficient of variation across replicates?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These aren't &lt;em&gt;merely&lt;/em&gt; statistical questions — they're also biological questions that determine whether your analysis is even valid.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Be deeply curious about their methods.&lt;/strong&gt; Ask about:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The specific assay they're using and its limitations&lt;/li&gt;
&lt;li&gt;How they handle sample preparation and storage&lt;/li&gt;
&lt;li&gt;What controls they're running and why&lt;/li&gt;
&lt;li&gt;The historical performance of this measurement in their hands&lt;/li&gt;
&lt;li&gt;What could go wrong and how they'd know&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Learn their terminology.&lt;/strong&gt; Don't just understand what they're measuring, but understand how they think about it. When they say "potency," do they mean EC50, IC50, or something else? When they talk about "efficacy," are they referring to maximal response, potency, or both?&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Quick domain mastery checklist:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What are the three most common failure modes for this assay?&lt;/li&gt;
&lt;li&gt;What does "good" look like in their world?&lt;/li&gt;
&lt;li&gt;What would make them suspicious of the data?&lt;/li&gt;
&lt;li&gt;How do they typically handle outliers or unexpected results?&lt;/li&gt;
&lt;li&gt;What's the gold standard measurement they're comparing against?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example: A data scientist was analyzing dose-response data from a cell-based assay. The scientist mentioned they were using a "luminescence readout." The data scientist asked about the detection range, learned it was $10^3$ to $10^6$ RLU, and immediately spotted that their highest concentration was saturating the detector. The analysis would have been meaningless without understanding that technical limitation.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;The payoff:&lt;/strong&gt; When you speak their language, you don't just communicate better, you also analyze better. You spot confounders they might miss. You suggest controls they haven't thought of. You become a true collaborator, not just a service provider.&lt;/p&gt;
&lt;h2 id="8-ask-yourself-what-they-ll-ask-you"&gt;8. Ask yourself what they'll ask you&lt;/h2&gt;&lt;p&gt;Every scientist has patterns. Learn them.&lt;/p&gt;
&lt;p&gt;Your PI always asks about sample size? Pre-calculate the probability of detecting meaningful effects.
Your biomarker lead obsesses over false positives? Lead with the posterior probability of true effects.
The chemistry team cares about synthesis feasibility? Include yield probabilities from your Bayesian model.&lt;/p&gt;
&lt;p&gt;This isn't mind-reading. It's paying attention.&lt;/p&gt;
&lt;p&gt;🚫 &lt;strong&gt;Reactive approach:&lt;/strong&gt;
Wait for their questions, scramble for answers, promise to "get back to you on that"&lt;/p&gt;
&lt;p&gt;✅ &lt;strong&gt;Proactive approach:&lt;/strong&gt;
"I know you usually want to know about batch effects, so I checked—they're negligible. Here's how I verified..."&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pattern matching checklist:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;What did they ask in the last three meetings?&lt;/li&gt;
&lt;li&gt;What decisions do they usually struggle with?&lt;/li&gt;
&lt;li&gt;What makes them nervous about moving forward?&lt;/li&gt;
&lt;li&gt;What would convince them this is real?&lt;/li&gt;
&lt;li&gt;What got them in trouble before?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Example: One program lead always asks: "What if we're wrong?" Every. Single. Time.&lt;/p&gt;
&lt;p&gt;The smartest data scientists now anticipate this and always include: "If we're wrong about this, here's what we'd see in the next experiment. Here's our bail-out plan. Here's the cost of being wrong versus the cost of being slow."&lt;/p&gt;
&lt;p&gt;She doesn't ask anymore. She trusts that they've thought it through.&lt;/p&gt;
&lt;p&gt;Another scientist always wants to know if we have enough evidence. So the prepared data scientist leads with: "There's an 85% probability that the treatment effect exceeds your minimum meaningful difference."&lt;/p&gt;
&lt;p&gt;Pre-answering questions isn't just efficient—it builds massive trust. It shows you understand their concerns and you're thinking ahead. Trust me, &lt;strong&gt;this is a career hack!&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="the-bottom-line"&gt;The bottom line&lt;/h2&gt;&lt;p&gt;Most data scientists in biotech spend 80% of their communication budget on methods that their collaborators—brilliant scientists juggling their own complex problems—don't have bandwidth to process.&lt;/p&gt;
&lt;p&gt;You're doing the equivalent of giving someone a recipe when they just asked if dinner's ready.&lt;/p&gt;
&lt;p&gt;The shift is simple but not easy: Stop defaulting to education mode. Start asking "What decision are you trying to make?" Then translate your sophisticated analysis into the probability they need to make that decision.&lt;/p&gt;
&lt;p&gt;This isn't about dumbing down your work. It's about translating between two expert domains—like a diplomat translating between heads of state. The lab scientists you work with have spent years mastering complex biological systems. They need translation, not education.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Your action items:&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;When first asked for analysis:&lt;/strong&gt; Start with "What decision are you trying to make with this data?" Don't begin any analysis until you know.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Review your last presentation:&lt;/strong&gt; Did you lead with the decision (BLUF) or bury it in methods? If methods came first, restructure.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Practice probability statements:&lt;/strong&gt; Instead of showing credible intervals, say "There's an X% probability that..." It's clearer and more actionable.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Learn their measurement methods:&lt;/strong&gt; Ask about detection limits, controls, and failure modes before analyzing their data.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Build a pattern map:&lt;/strong&gt; Write down what each of your regular collaborators usually asks. Answer it proactively next time.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Create a technical appendix:&lt;/strong&gt; Put all your beautiful methods somewhere. Just don't force people to sit through it.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The best data scientists are the ones whose collaborators make the best decisions.&lt;/p&gt;
&lt;p&gt;And that starts with spending your communication budget on what actually matters to the people you're trying to help.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;What patterns have you noticed in your collaborations? What questions do your scientists always ask? Let me know; I'd love to hear what's working (or not working) for you!&lt;/em&gt;&lt;/p&gt;
</content></entry><entry><title>Wicked Python trickery - dynamically patch a Python function's source code at runtime</title><link href="https://ericmjl.github.io/blog/2025/8/23/wicked-python-trickery-dynamically-patch-a-python-functions-source-code-at-runtime/" rel="alternate"/><updated>2025-08-23T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:3ced3d47-73a9-378e-aead-d8279180cc9f</id><content type="html">&lt;p&gt;So today, I learned a very dangerous and yet fascinating trick.&lt;/p&gt;
&lt;p&gt;It's possible to dynamically change a Python function's source code &lt;em&gt;at runtime&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;What this does is open a world of possibilities in building AI bots!&lt;/p&gt;
&lt;h2 id="how-this-actually-works"&gt;How this actually works&lt;/h2&gt;&lt;p&gt;Every function has a &lt;code&gt;.__code__&lt;/code&gt; attribute. For example, for this function:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;something&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;NotImplementedError&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;something.__code__&lt;/code&gt; looks like this:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;code&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt; &lt;span class="n"&gt;something&lt;/span&gt; &lt;span class="n"&gt;at&lt;/span&gt; &lt;span class="mh"&gt;0x149bdfc90&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/var/folders/36/vb250n_s0zncstw3sk74qfxr0000gn/T/marimo_80086/__marimo__cell_kJqw_.py&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;If I were to execute &lt;code&gt;something()&lt;/code&gt;, it would return a &lt;code&gt;NotImplementedError&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Now, let's say that, for some reason that I shall not speculate, I decided that I wanted &lt;code&gt;something()&lt;/code&gt; to instead do multiplication by 2. I can create new source code:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;new_code&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="s2"&gt;def something(x: int) -&amp;gt; int:&lt;/span&gt;
&lt;span class="s2"&gt;    return x * 2&lt;/span&gt;
&lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I can do the following three magical steps to swap it in.&lt;/p&gt;
&lt;p&gt;Firstly, compile the code into bytecode:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;compiled&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;compile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;new_code&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;lt;magic&amp;gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;exec&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The three arguments to &lt;code&gt;compile&lt;/code&gt; are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The code to compile (&lt;code&gt;new_code&lt;/code&gt;),&lt;/li&gt;
&lt;li&gt;The filename in which the code is compiled (&lt;code&gt;&amp;lt;magic&amp;gt;&lt;/code&gt;), and&lt;/li&gt;
&lt;li&gt;The mode in which compilation happens (in this case, &lt;code&gt;exec&lt;/code&gt; mode).&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;On the third point, the docstring of &lt;code&gt;compile&lt;/code&gt; explains what the three modes are:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The mode must be 'exec' to compile a module, 'single' to compile a single (interactive) statement, or 'eval' to compile an expression.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;code&gt;compiled&lt;/code&gt; object now is a "code object":&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;code&lt;/span&gt; &lt;span class="nb"&gt;object&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt;&lt;span class="n"&gt;module&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;at&lt;/span&gt; &lt;span class="mh"&gt;0x149bcbad0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;file&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;&amp;lt;magic&amp;gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;line&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="o"&gt;&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I can then execute the compiled code to make it imported into a particular namespace.&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;ns&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="n"&gt;exec&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;compiled&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;{},&lt;/span&gt; &lt;span class="n"&gt;ns&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here, the three arguments passed to &lt;code&gt;exec&lt;/code&gt; are:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The code we want to execute (&lt;code&gt;compiled&lt;/code&gt;), and in this case, by "executing" it after being compiled in &lt;code&gt;exec&lt;/code&gt; mode, we are really just simulating an &lt;code&gt;import&lt;/code&gt; into our namespace.&lt;/li&gt;
&lt;li&gt;The globals (&lt;code&gt;{}&lt;/code&gt;), which in this case are passed in as an empty dictionary. These are the global variables that are available to the function at runtime.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ns&lt;/code&gt; is the "namespace" in which we want the function to be present; namespaces in Python are just dictionary mappings from function/object name to the function/object itself.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Finally, I can replace my existing function with the compiled function inserted into the &lt;code&gt;ns&lt;/code&gt; namespace:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;something_new&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;ns&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;something&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;something_new&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;21&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;  &lt;span class="c1"&gt;# this will print 42 to stdout!&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;But really, the real lesson here is not that one can monkeypatch over an existing Python function's source code at runtime, but that you can actually &lt;strong&gt;compile the string of a Python function definition and give it access to a namespace's variables&lt;/strong&gt;, including that of the current global namespace.&lt;/p&gt;
&lt;h2 id="when-would-you-ever-want-to-do-this"&gt;When would you ever want to do this?&lt;/h2&gt;&lt;p&gt;At first glance, never really! This is a bit of hackery that lives on the fringes of Python-land, and is basically a party trick.&lt;/p&gt;
&lt;p&gt;But as it turns out, I &lt;em&gt;actually&lt;/em&gt; had a real motivation for wanting to do this.&lt;/p&gt;
&lt;p&gt;Within &lt;a href="https://github.com/ericmjl/llamabot"&gt;LlamaBot&lt;/a&gt;, I've always had &lt;code&gt;AgentBot&lt;/code&gt; as a first-pass implementation of what I think an LLM agent should look like, having studied LLM agent implementations in other libraries. However, I've never been fully satisfied with &lt;code&gt;AgentBot&lt;/code&gt;'s implementation. The core issue was that it mixed too many concerns together - function execution, function call determination, and user response generation all lived in the same loop.&lt;/p&gt;
&lt;p&gt;Here's what &lt;code&gt;AgentBot&lt;/code&gt; looked like at a high level:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nc"&gt;AgentBot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SimpleBot&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tools&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;messages&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_iterations&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;num_iterations&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;...&lt;/span&gt; &lt;span class="c1"&gt;# get response object, passing in messages&lt;/span&gt;
            &lt;span class="n"&gt;results&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
            &lt;span class="c1"&gt;# Execute tool calls if they are present&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tool_calls&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;tool_call&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tool_calls&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                    &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name_to_tools&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;tool_call&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;](&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loads&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tool_call&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;arguments&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

            &lt;span class="c1"&gt;# continue until LLM decides we&amp;#39;re done.&lt;/span&gt;
            &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="c1"&gt;# just respond to users.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;While this worked, it wasn't great at separating concerns. I had function execution mixed in with function call determination mixed in with responding to a user.&lt;/p&gt;
&lt;p&gt;The bigger limitation was with code execution tools. My original implementation isolated generated code in a Docker container sandbox, which was secure but meant the code couldn't access variables from my current Python runtime. This severely limited what kinds of useful tasks the bot could perform with my existing data and variables.&lt;/p&gt;
&lt;p&gt;I realized that if I could:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use an LLM to generate Python functions that referenced existing variables in my runtime,&lt;/li&gt;
&lt;li&gt;Compile those functions on-the-fly within the same Python environment, and&lt;/li&gt;
&lt;li&gt;Execute them with access to my current namespace,&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I could build something much more powerful. This led me to create &lt;code&gt;ToolBot&lt;/code&gt; within LlamaBot.&lt;/p&gt;
&lt;h2 id="toolbot-focuses-on-tool-selection-instead-of-execution"&gt;ToolBot focuses on tool selection instead of execution&lt;/h2&gt;&lt;p&gt;&lt;code&gt;ToolBot&lt;/code&gt; takes a different approach - it focuses purely on tool selection rather than execution. Here's the key structure:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nc"&gt;ToolBot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;SimpleBot&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;system_prompt&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tools&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Initialize with core tools like today_date and respond_to_user&lt;/span&gt;
        &lt;span class="n"&gt;all_tools&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;today_date&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;respond_to_user&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;tools&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;all_tools&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;extend&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tools&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tools&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;json_schema&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;all_tools&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;name_to_tool_map&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="n"&gt;f&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="vm"&gt;__name__&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;f&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;all_tools&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="fm"&gt;__call__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;message&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="c1"&gt;# Process message and return tool calls (but don&amp;#39;t execute them)&lt;/span&gt;
        &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;make_response&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;message_list&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;tool_calls&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;extract_tool_calls&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;tool_calls&lt;/span&gt;  &lt;span class="c1"&gt;# Just return the calls, don&amp;#39;t execute&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The key insight: &lt;code&gt;ToolBot&lt;/code&gt; just selects a tool to be executed, but does &lt;em&gt;not&lt;/em&gt; execute it. Instead, it returns the tools to be called to the external environment, giving you full control over execution.&lt;/p&gt;
&lt;h2 id="the-magic-happens-with-write-and-execute-code"&gt;The magic happens with write_and_execute_code&lt;/h2&gt;&lt;p&gt;One of the most powerful tools that can be chosen is &lt;code&gt;write_and_execute_code&lt;/code&gt;. Here's the core implementation:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;write_and_execute_code&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;globals_dict&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="nd"&gt;@tool&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;write_and_execute_code_wrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;placeholder_function&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;keyword_args&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;dict&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;Write and execute `placeholder_function` with the passed in `keyword_args`.&lt;/span&gt;

&lt;span class="s2"&gt;        Use this tool for any task that requires custom Python code generation and execution.&lt;/span&gt;
&lt;span class="s2"&gt;        This tool has access to ALL globals in the current runtime environment (variables, dataframes, functions, etc.).&lt;/span&gt;
&lt;span class="s2"&gt;        Perfect for: data analysis, calculations, transformations, visualizations, custom algorithms.&lt;/span&gt;

&lt;span class="s2"&gt;        ## Code Generation Guidelines:&lt;/span&gt;

&lt;span class="s2"&gt;        1. **Write self-contained Python functions** with ALL imports inside the function body&lt;/span&gt;
&lt;span class="s2"&gt;        2. **Place all imports at the beginning of the function**: import statements must be the first lines inside the function&lt;/span&gt;
&lt;span class="s2"&gt;        3. **Include all required libraries**: pandas, numpy, matplotlib, etc. - import everything the function needs&lt;/span&gt;
&lt;span class="s2"&gt;        4. **Leverage existing global variables**: Can reference variables that exist in the runtime&lt;/span&gt;
&lt;span class="s2"&gt;        5. **Include proper error handling** and docstrings&lt;/span&gt;
&lt;span class="s2"&gt;        6. **Provide keyword arguments** when the function requires parameters&lt;/span&gt;
&lt;span class="s2"&gt;        7. **Make functions reusable** - they will be stored globally for future use&lt;/span&gt;
&lt;span class="s2"&gt;        8. **ALWAYS RETURN A VALUE**: Every function must explicitly return something - never just print, display, or show results without returning them. Even for plotting functions, return the figure/axes object.&lt;/span&gt;

&lt;span class="s2"&gt;        ## Function Arguments Handling:&lt;/span&gt;

&lt;span class="s2"&gt;        **CRITICAL**: You MUST match the function signature with the keyword_args:&lt;/span&gt;
&lt;span class="s2"&gt;        - **If your function takes NO parameters** (e.g., `def analyze_data():`), then pass an **empty dictionary**: `&lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;&lt;span class="s2"&gt;`&lt;/span&gt;
&lt;span class="s2"&gt;        - **If your function takes parameters** (e.g., `def filter_data(min_age, department):`), then pass the required arguments as a dictionary: `{&amp;quot;min_age&amp;quot;: 30, &amp;quot;department&amp;quot;: &amp;quot;Engineering&amp;quot;}`&lt;/span&gt;
&lt;span class="s2"&gt;        - **Never pass keyword_args that don&amp;#39;t match the function signature** - this will cause execution errors&lt;/span&gt;

&lt;span class="s2"&gt;        ## Code Structure Example:&lt;/span&gt;

&lt;span class="s2"&gt;        ```python&lt;/span&gt;
&lt;span class="s2"&gt;        # Function with NO parameters - use empty dict &lt;/span&gt;&lt;span class="si"&gt;{}&lt;/span&gt;
&lt;span class="s2"&gt;        def analyze_departments():&lt;/span&gt;
&lt;span class="s2"&gt;            &amp;#39;&amp;#39;&amp;#39;Analyze department performance.&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="s2"&gt;            import pandas as pd&lt;/span&gt;
&lt;span class="s2"&gt;            import numpy as np&lt;/span&gt;
&lt;span class="s2"&gt;            result = fake_df.groupby(&amp;#39;department&amp;#39;)[&amp;#39;salary&amp;#39;].mean()&lt;/span&gt;
&lt;span class="s2"&gt;            return result&lt;/span&gt;
&lt;span class="s2"&gt;        # Function WITH parameters - pass matching keyword_args&lt;/span&gt;
&lt;span class="s2"&gt;        def filter_employees(min_age, department):&lt;/span&gt;
&lt;span class="s2"&gt;            &amp;#39;&amp;#39;&amp;#39;Filter employees by criteria.&amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;span class="s2"&gt;            import pandas as pd&lt;/span&gt;
&lt;span class="s2"&gt;            filtered = fake_df[(fake_df[&amp;#39;age&amp;#39;] &amp;gt;= min_age) &amp;amp; (fake_df[&amp;#39;department&amp;#39;] == department)]&lt;/span&gt;
&lt;span class="s2"&gt;            return filtered&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;pre&gt;&lt;code&gt;    ## Return Value Requirements:

    - **Data analysis functions**: Return the computed results (numbers, DataFrames, lists, dictionaries)
    - **Plotting functions**: Return the figure or axes object (e.g., `return fig` or `return plt.gca()`)
    - **Filter/transformation functions**: Return the processed data
    - **Calculation functions**: Return the calculated values
    - **Utility functions**: Return relevant output (status, processed data, etc.)
    - **Never return None implicitly** - always have an explicit return statement

    ## Code Access Capabilities:

    The generated code will have access to:
    - All global variables and dataframes in the current session
    - Any previously defined functions
    - The ability to import any standard Python libraries within the function
    - The ability to create new reusable functions that will be stored globally
    :param placeholder_function: The function to execute (complete Python function as string).
    :param keyword_args: The keyword arguments to pass to the function (dictionary matching function parameters).
    :return: The result of the function execution.
    """

    # Parse the code to extract the function name
    tree = ast.parse(placeholder_function)
    function_name = None
    for node in ast.walk(tree):
        if isinstance(node, ast.FunctionDef):
            function_name = node.name
            break
    # Compile and execute the function with access to globals
    ns = globals_dict
    compiled = compile(placeholder_function, "&amp;lt;llm&amp;gt;", "exec")
    exec(compiled, globals_dict, ns)
    return ns[function_name](**keyword_args)

return write_and_execute_code_wrapper
&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;
This extensive docstring gets passed as part of the JSON schema and effectively serves as instructions to the LLM on when and how to use this tool. I stripped out logging and error handling to simplify what's shown here, but the actual codebase has more robustness built in.

Notice how `ToolBot`, and more specifically `write_and_execute_code`, gains explicit access to the `globals()` dictionary when a user passes it in. This approach allows us to ensure that function execution takes place within the proper namespace. If `ToolBot` chooses `write_and_execute_code`, I can control exactly where and how it executes within my Python runtime environment - and this opens up a world of possibilities!

For example, inspired by [the Marimo blog](https://marimo.io/blog/marimo-chat), which wrote about generative UIs and tool calling:

&amp;gt; marimo’s chat interface supports Generative UI - the ability to stream rich, interactive UI components directly from LLM responses. This goes beyond traditional text and markdown outputs, allowing chatbots to return dynamic elements like tables, charts, and interactive visualizations.

I decided to build out a _generalized_ version of a tool that an LLM could choose to call on that would also have access to any variable present within the runtime environment... much like Marimo's AI chat has access to any variable within the environment with an `@variable_name`, now I just dump the full set of `globals()` into the LLM's context window, and that's what `write_and_execute_code` looked like.

Here's an example, imagine I have two dataframes that I want an LLM to manipulate. Without `write_and_execute_code`, I'd have to write bespoke tools for the dataframe, in which I access the `df` as a "global" variable, much like the following:

```python
@lmb.tool
def chart_data(x_encoding: str, y_encoding: str, color: str):
    """Generate an altair chart"""
    import altair as alt
    return (
        alt.Chart(df)
        .mark_circle()
        .encode(x=x_encoding, y=y_encoding, color=color)
        .properties(width=500)
    )
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So the writing on the wall is that I'd have to write one tool for every possible operation that I'd desire, but that's a big hassle. With this &lt;code&gt;globals()&lt;/code&gt;, &lt;code&gt;compile&lt;/code&gt;, and &lt;code&gt;exec&lt;/code&gt; trickery baked into &lt;code&gt;write_and_execute_code&lt;/code&gt;, I no longer have to specify bespoke tools for the environment that I'm in!&lt;/p&gt;
&lt;p&gt;Further more, inspired by the Marimo blog post, &lt;code&gt;ToolBot&lt;/code&gt; is designed to just do the tool picking, delegating the execution and return of the broader LLM-powered Python program back to the developer. In this way, I can give myself more flexibility when building entire "Agentic" programs, more so than if I were to use &lt;code&gt;AgentBot&lt;/code&gt; in its current form. It allowed me to build a more powerful version of a tool-calling agent using &lt;code&gt;ToolBot&lt;/code&gt; with generative UIs in a Marimo notebook. For this, it's easier to demo via a screencast instead of by me describing it in prose:&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/tk5wvb556f8?si=sXSRulZ2ooBplapr" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen&gt;&lt;/iframe&gt;&lt;p&gt;And if you're curious to try running it, you can run it with the following command:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;uvx&lt;span class="w"&gt; &lt;/span&gt;marimo&lt;span class="w"&gt; &lt;/span&gt;edit&lt;span class="w"&gt; &lt;/span&gt;--sandbox&lt;span class="w"&gt; &lt;/span&gt;https://raw.githubusercontent.com/ericmjl/website/refs/heads/main/content/blog/wicked-python-trickery-dynamically-patch-a-python-functions-source-code-at-runtime/agents.py
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="security-concerns-are-very-real-with-this-approach"&gt;Security concerns are very real with this approach&lt;/h2&gt;&lt;p&gt;Comparing this to what we had before with &lt;code&gt;write_and_execute_script&lt;/code&gt;, which performed execution in a sandboxed Docker container with limited read/write capabilities, &lt;code&gt;write_and_execute_code&lt;/code&gt; is &lt;em&gt;much, much less secure&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Obviously, I'm playing with fire here. A malicious LLM output could run code directly and do enormous damage to my machine and from my machine to the outside world. I have yet to implement code sanitization, but one big idea I have, which I just learned through discourse with GPT-4, is to use &lt;a href="https://github.com/zopefoundation/RestrictedPython"&gt;Restricted Python&lt;/a&gt;. I think that will be the next big upgrade after I let the current version of &lt;code&gt;write_and_execute_code&lt;/code&gt; sit for a while.&lt;/p&gt;
&lt;p&gt;As such, I don't suggest that the &lt;code&gt;write_and_execute_code&lt;/code&gt; pattern be used for anything really serious in its current form.&lt;/p&gt;
&lt;h2 id="what-i-learned-from-this-python-trickery"&gt;What I learned from this Python trickery&lt;/h2&gt;&lt;p&gt;This journey taught me several things. First, Python's runtime is far more malleable than I initially realized - the ability to compile strings into executable code and inject them into specific namespaces opens up incredible possibilities for dynamic programming.&lt;/p&gt;
&lt;p&gt;Second, building effective LLM agents isn't just about the AI - it's about thoughtful system design. Separating tool selection from execution (as &lt;code&gt;ToolBot&lt;/code&gt; does) creates much more flexible and controllable systems than monolithic agents.&lt;/p&gt;
&lt;p&gt;Finally, this wouldn't have been possible without &lt;a href="https://ericmjl.github.io/blog/2025/6/7/principles-for-using-ai-autodidactically/"&gt;autodidactic learning with LLMs&lt;/a&gt;. I'm becoming more and more convinced that LLMs are a great tool for learning, but one must learn how to use them for learning, and one must &lt;a href="https://ericmjl.github.io/blog/2025/7/13/earn-the-privilege-to-use-automation/"&gt;earn the automation&lt;/a&gt; as well.&lt;/p&gt;
</content></entry><entry><title>Data scientists aren't becoming obsolete in the LLM era</title><link href="https://ericmjl.github.io/blog/2025/8/15/data-scientists-arent-becoming-obsolete-in-the-llm-era/" rel="alternate"/><updated>2025-08-15T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:8aed9fe5-d6f7-3703-9c7f-efb092c73a5d</id><content type="html">&lt;p&gt;I keep hearing the same question: "Are data scientists becoming obsolete now that LLMs can code?"&lt;/p&gt;
&lt;p&gt;The anxiety is understandable. When you watch Claude or ChatGPT write Python scripts, build models, and even debug code, it's natural to wonder where that leaves us. But here's what I've found after spending months integrating LLMs into my own workflow: they're not replacing us. They're fundamentally reshaping what it means to be a data scientist.&lt;/p&gt;
&lt;p&gt;To ponder this question properly, I examine it from two angles.&lt;/p&gt;
&lt;h2 id="how-are-llms-enhancing-our-existing-work"&gt;How are LLMs enhancing our existing work?&lt;/h2&gt;&lt;p&gt;The first angle is using LLMs as tools for data scientists. This means finding ways to incorporate them into our day-to-day work as consumers of LLM-powered applications.&lt;/p&gt;
&lt;p&gt;I've experienced the productivity-enhancing benefits firsthand. GitHub Copilot and Cursor have dramatically accelerated my coding. Research agents like Elicit.org help me navigate literature in ways that would have taken hours before. I use transcription tools to type faster than I can touch type by hand, getting my thoughts out of my brain closer to the speed at which I'm actually thinking. I rely on AI for cleaning up messy thoughts and as a thinking tool to help me draw out what I'm really trying to articulate.&lt;/p&gt;
&lt;p&gt;Having lived with these tools for months now, I think being proficient with AI-assisted coding is table stakes.&lt;/p&gt;
&lt;p&gt;Just as spreadsheets changed what we expected from accountants, AI assistance is now a baseline expectation. But there's a crucial skill here that goes beyond just using the tools: knowing how to use AI to verify information and catch the inevitable errors these systems make.&lt;/p&gt;
&lt;p&gt;More importantly, this is just the beginning.&lt;/p&gt;
&lt;h2 id="how-are-we-building-custom-llm-solutions"&gt;How are we building custom LLM solutions?&lt;/h2&gt;&lt;p&gt;The second angle is more profound: data scientists becoming part of the team that builds custom LLM agent workflows to accelerate others' work.&lt;/p&gt;
&lt;p&gt;Here's what this looks like in practice: You get hands-dirty with business workflows. You co-create with business partners to build new tools and ways of working that remove boring work from their plates. You build technical prototypes that prove out value, then partner with engineers for custom app builds where appropriate.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;scientist&lt;/em&gt; skill becomes crucial here: experimentation. You're figuring out whether a thing is actually working by measuring performance of LLM-based workflows and tying it back to business value. This is fundamentally different from being an app developer, a machine learning engineer, or a business analyst doing reporting and dashboards.
Those aren't really data science roles. The scientist in data science lies in hypothesizing, defining metrics and estimates, then testing and measuring them.&lt;/p&gt;
&lt;h2 id="what-does-the-scientist-in-data-scientist-mean-in-the-llm-era"&gt;What does the 'scientist' in 'data scientist' mean in the LLM era?&lt;/h2&gt;&lt;p&gt;Taking Hamel Husain and Shreya Shankar's course on LLM evaluation crystallized this for me. I'm much more convinced that the role of a data scientist is to measure, evaluate, and design metrics. It's going back to the science.&lt;/p&gt;
&lt;p&gt;Think about the parallel here. In discovery science, data scientists work with laboratory scientists and statisticians to hypothesize about relationships between molecular structure and biological activity, then together define what estimate we need to measure the performance of biological or chemical systems. They build machine learning models to predict those estimands from sequence and structure, test the hypotheses, and measure whether they hold. The estimands matter because they connect to whether a drug works or a process is optimized.&lt;/p&gt;
&lt;p&gt;With LLM applications automating business processes, it's analogous but the stakes are operational performance. You hypothesize that a particular LLM workflow will improve efficiency or accuracy. You define evaluation metrics—the equivalent of the assays you measure in lab science. You design experiments to test whether your hypothesis about the LLM's impact is correct. You build automation around measurement to continuously validate whether your hypotheses about improved workflows are actually playing out.&lt;/p&gt;
&lt;p&gt;In both contexts, you hypothesize, define, test, and measure. &lt;strong&gt;That's what a scientist does!&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;In what I'd describe as a meta move, data scientists should absolutely be experimenting with LLMs to create LLM-based tooling for their own work. We're uniquely positioned to understand both the technical possibilities and the measurement challenges these systems present.&lt;/p&gt;
&lt;h2 id="why-this-matters-more-than-ever"&gt;Why this matters more than ever&lt;/h2&gt;&lt;p&gt;This role differs fundamentally from what people might think we should become. We're not primarily app developers (that should be for software developers), even if we might ship and app or two out of necessity. We're not machine learning engineers building complex production pipelines (though we should be able to ship components that get stitched together on platforms). We're not business analysts doing reporting and dashboards, even if we do build visualizations to help with communication.&lt;/p&gt;
&lt;p&gt;Rather, we're scientists who hypothesize, define metrics, design estimates, test our ideas, and measure whether things work.&lt;/p&gt;
&lt;p&gt;Instead of making data scientists obsolete, the LLM era is returning us to our scientific roots while giving us incredibly powerful tools to work with. We're becoming builders of measurement systems that work at the intersection of business value and statistical rigor.&lt;/p&gt;
&lt;p&gt;I'd strongly encourage you to try both angles: become proficient with LLM tools for your daily work, and start experimenting with building custom LLM workflows for your organization. The beauty of this approach is that you're amplifying your ability to hypothesize what might work, define what matters, and measure whether it's actually working.&lt;/p&gt;
</content></entry><entry><title>Stop guessing at priors: R2D2's automated approach to Bayesian modeling</title><link href="https://ericmjl.github.io/blog/2025/8/6/stop-guessing-at-priors-r2d2s-automated-approach-to-bayesian-modeling/" rel="alternate"/><updated>2025-08-06T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:1426f8db-c4c6-3abf-8ecd-7449e42852d4</id><content type="html">&lt;p&gt;When I first encountered the R2D2 (R²-induced Dirichlet Decomposition) framework (Zhang et al., 2020), I was struck by its intuitive approach to Bayesian regularization. Instead of placing priors on individual regression coefficients and hoping for the best, R2D2 lets you directly specify your beliefs about how much variance the model should explain. But what really fascinated me was how the framework elegantly extends from simple linear regression to complex multilevel models through a series of principled modifications.&lt;/p&gt;
&lt;p&gt;This post documents my journey understanding the progression from the basic R2D2 shrinkage prior to its sophisticated multilevel variant (R2D2M2), with stops along the way to explore generalized linear models. What emerged was a beautiful mathematical architecture where each extension builds naturally on the previous.&lt;/p&gt;
&lt;h2 id="the-foundation-r2d2-shrinkage-prior"&gt;The foundation: R2D2 shrinkage prior&lt;/h2&gt;&lt;p&gt;The journey begins with the elegant insight that motivated the original R2D2 framework: why not place a prior directly on the coefficient of determination (R²) rather than fumbling with individual coefficient priors? The challenge with individual coefficient priors isn't just knowing where to center them, but defining appropriate variance parameters - it's remarkably difficult to know a priori how much variability each coefficient should have.&lt;/p&gt;
&lt;h3 id="the-core-mathematical-insight"&gt;The core mathematical insight&lt;/h3&gt;&lt;p&gt;For any model, R² represents the proportion of output variance that can be explained:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;R² = explained variance / total variance = W / (W + σ²)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Rearranging this relationship shows us what W actually represents:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;R² = W / (W + σ²)
R²(W + σ²) = W
R²W + R²σ² = W
R²σ² = W - R²W = W(1 - R²)
W = σ² * (R² / (1 - R²))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This reveals that W is the &lt;strong&gt;total explained variance&lt;/strong&gt; (on the data scale), which equals the signal-to-noise ratio multiplied by the noise variance. Let's define the signal-to-noise ratio as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;τ² = R² / (1 - R²)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So that:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;W = σ² * τ²
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives us:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;τ² = 1&lt;/strong&gt;: Signal equals noise (R² = 0.5)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;τ² = 4&lt;/strong&gt;: Signal is 4 times stronger than noise (R² = 0.8)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;τ² = 0.25&lt;/strong&gt;: Noise is 4 times stronger than signal (R² = 0.2)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The R2D2 framework starts by placing a Beta prior on R²:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;r_squared&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Beta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;r_squared&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;tau_squared&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tau_squared&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r_squared&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;r_squared&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Zhang et al. show that when R² has a Beta(a,b) prior, the induced prior density for τ² = R²/(1-R²) follows a Beta Prime distribution BP(a,b), giving us intuitive control over model fit through the familiar Beta hyperparameters.&lt;/p&gt;
&lt;h3 id="allocating-explained-variance-the-dirichlet-decomposition"&gt;Allocating explained variance: the Dirichlet decomposition&lt;/h3&gt;&lt;p&gt;But here's where R2D2 gets clever. Instead of requiring the modeler to manually specify variance parameters for each predictor's prior, it uses a &lt;strong&gt;Dirichlet decomposition&lt;/strong&gt; to automatically allocate the total explained variance W across predictors:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# W is the total explained variance to allocate&lt;/span&gt;
&lt;span class="n"&gt;phi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dirichlet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;phi&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;a_pi&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;predictors&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;lambda_j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lambda_j&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;predictors&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This means &lt;code&gt;φⱼ × W = λⱼ&lt;/code&gt; answers the question: &lt;em&gt;"What fraction of the total explained variance does predictor j get?"&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: If τ² = 4 (signal is 4 times stronger than noise) and σ² = 2, then W = 8, and if φ = [0.5, 0.3, 0.2], then:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Predictor 1: λ₁ = 0.5 × 8 = 4 (gets 50% of explained variance)&lt;/li&gt;
&lt;li&gt;Predictor 2: λ₂ = 0.3 × 8 = 2.4 (gets 30% of explained variance)&lt;/li&gt;
&lt;li&gt;Predictor 3: λ₃ = 0.2 × 8 = 1.6 (gets 20% of explained variance)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As Zhang et al. describe, this creates adaptive behavior where "the heavy tail reduces the bias in estimation of large coefficients, while the high concentration around zero shrinks the irrelevant coefficients heavily to zero, thus reducing the noise" - factors that explain a lot of the output variance get allocated more of the total explained variance (larger λⱼ values), while factors that don't explain much output variance get allocated less explained variance (smaller λⱼ values).&lt;/p&gt;
&lt;h3 id="the-r2d2-model"&gt;The R2D2 model&lt;/h3&gt;&lt;p&gt;Bringing these pieces together - the R² prior, the Dirichlet variance allocation, and the coefficient distributions - we get the R2D2 model:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coords&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;coords&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# Noise level (data scale)&lt;/span&gt;
    &lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HalfNormal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sigma&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# R² prior (intuitive model fit control)&lt;/span&gt;
    &lt;span class="n"&gt;r_squared&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Beta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;r_squared&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Signal-to-noise ratio&lt;/span&gt;
    &lt;span class="n"&gt;tau_squared&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tau_squared&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r_squared&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;r_squared&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="c1"&gt;# Total explained variance&lt;/span&gt;
    &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;W&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;tau_squared&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Local variance allocation (competitive)&lt;/span&gt;
    &lt;span class="n"&gt;phi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dirichlet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;phi&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;full&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a_pi&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;predictors&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;lambda_j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lambda_j&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;predictors&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Coefficients with allocated variance&lt;/span&gt;
    &lt;span class="n"&gt;scale_j&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lambda_j&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# For Laplace priors&lt;/span&gt;
    &lt;span class="n"&gt;beta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Laplace&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;beta&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;scale_j&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;predictors&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Standard linear likelihood&lt;/span&gt;
    &lt;span class="n"&gt;mu&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;X&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;likelihood&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;y&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;observed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;obs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The beauty of this approach lies in the competitive nature of the Dirichlet allocation: all predictors compete for the total explained variance W. If one predictor becomes more important (higher φⱼ), others must become less important. This creates natural sparsity and prevents overfitting. The signal-to-noise ratio τ² provides intuitive control over model complexity, while W gives us the actual variance scale for coefficient priors.&lt;/p&gt;
&lt;h2 id="first-extension-r2d2-for-generalized-linear-models"&gt;First extension: R2D2 for generalized linear models&lt;/h2&gt;&lt;p&gt;The first major challenge came when extending R2D2 to non-Gaussian outcomes. Yanchenko et al. (2021) tackled this problem by developing clever approximation methods that preserve the intuitive R² interpretation. The beautiful relationship &lt;code&gt;R² = W/(W+σ²)&lt;/code&gt; that made everything work cleanly suddenly becomes complex when dealing with Poisson counts, binary outcomes, or other GLM families.&lt;/p&gt;
&lt;h3 id="the-challenge-no-more-simple-s2"&gt;The challenge: no more simple σ²&lt;/h3&gt;&lt;p&gt;In GLMs, the "noise" isn't a simple σ² anymore. Instead, we have:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Poisson&lt;/strong&gt;: Variance equals the mean (&lt;code&gt;σ²(η) = e^η&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Binomial&lt;/strong&gt;: Variance depends on probability (&lt;code&gt;σ²(η) = μ(η)[1-μ(η)]&lt;/code&gt;)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Gaussian&lt;/strong&gt;: Still simple (&lt;code&gt;σ²(η) = σ²&lt;/code&gt;)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This breaks our clean R² = W/(W+σ²) relationship because now both the signal and noise are functions of the linear predictor η.&lt;/p&gt;
&lt;h3 id="the-elegant-solution-linear-approximation"&gt;The elegant solution: linear approximation&lt;/h3&gt;&lt;p&gt;The GLM extension uses a brilliant linear approximation approach. As Yanchenko et al. describe, "applying a first-order Taylor series approximation of μ(η) and σ²(η) around β₀" allows them to handle the GLM complexity. We approximate the complex GLM relationship around the intercept β₀:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;R² ≈ W/(W + s²(β₀))
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Where &lt;code&gt;s²(β₀) = σ²(β₀)/[μ'(β₀)]²&lt;/code&gt; is the "effective noise" for each GLM family:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Gaussian&lt;/strong&gt;: &lt;code&gt;s²(β₀) = σ²&lt;/code&gt; (no change needed!)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Poisson&lt;/strong&gt;: &lt;code&gt;s²(β₀) = e^{-β₀}&lt;/code&gt; (depends on baseline rate)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Logistic&lt;/strong&gt;: &lt;code&gt;s²(β₀) = μ(β₀)(1-μ(β₀))/[μ'(β₀)]²&lt;/code&gt; (depends on baseline probability)&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="what-this-achieves"&gt;What this achieves&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;The genius&lt;/strong&gt;: We keep all the interpretability and mathematical structure of the linear R2D2 case, but just compute a smarter "noise" term that respects the GLM family's variance structure.&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coords&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;coords&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# Same intuitive R² prior!&lt;/span&gt;
    &lt;span class="n"&gt;r_squared&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Beta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;r_squared&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;b&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# GLM-specific &amp;quot;effective noise&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;family&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;poisson&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;s_sq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;s_sq&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;beta0&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="k"&gt;elif&lt;/span&gt; &lt;span class="n"&gt;family&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;binomial&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;exp_beta0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beta0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;mu_beta0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp_beta0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;exp_beta0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;mu_prime_beta0&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;exp_beta0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;pt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;power&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;exp_beta0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;s_sq&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;s_sq&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu_beta0&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;mu_beta0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;pt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;power&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mu_prime_beta0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="c1"&gt;# Same competitive allocation structure!&lt;/span&gt;
    &lt;span class="n"&gt;tau_squared&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tau_squared&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r_squared&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;r_squared&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;W&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;W&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tau_squared&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;s_sq&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;phi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dirichlet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;phi&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;full&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_components&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;xi0&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;components&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The elegance of this approach becomes clear when we step back and see what's happening conceptually. We're essentially asking "what would σ² be if this GLM were actually a linear model?" and using that as our effective noise term. This preserves all the intuitive benefits of R2D2 while handling GLM complexity. The signal-to-noise ratio τ² remains the same intuitive control parameter, while W adapts to the GLM's variance structure.&lt;/p&gt;
&lt;h2 id="the-great-leap-r2d2m2-for-multilevel-models"&gt;The great leap: R2D2M2 for multilevel models&lt;/h2&gt;&lt;p&gt;The most sophisticated extension addresses the challenge of multilevel models with multiple grouping factors - the kind of complex experimental designs common in laboratory research. Aguilar &amp;amp; Bürkner (2022) developed the R2D2M2 prior to handle this complexity while preserving the intuitive variance decomposition interpretation.&lt;/p&gt;
&lt;h3 id="the-multilevel-challenge"&gt;The multilevel challenge&lt;/h3&gt;&lt;p&gt;Consider a laboratory experiment with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Predictors&lt;/strong&gt;: Gene expression, Age, Treatment dose&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Grouping factors&lt;/strong&gt;: Mouse ID, MicroRNA ID, Stress condition&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Traditional approaches assign independent priors to each effect:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Traditional (problematic) approach&lt;/span&gt;
&lt;span class="n"&gt;beta_gene&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ_gene&lt;/span&gt;&lt;span class="err"&gt;²&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;beta_age&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ_age&lt;/span&gt;&lt;span class="err"&gt;²&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;mouse_effects&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ_mouse&lt;/span&gt;&lt;span class="err"&gt;²&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;microRNA_effects&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ_microRNA&lt;/span&gt;&lt;span class="err"&gt;²&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;stress_effects&lt;/span&gt; &lt;span class="o"&gt;~&lt;/span&gt; &lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;λ_stress&lt;/span&gt;&lt;span class="err"&gt;²&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;The problem&lt;/strong&gt;: As you add more predictors and grouping factors, the implied R² prior becomes increasingly concentrated near 1 (the maximum possible R² value). This happens because each additional effect adds its own independent variance contribution, causing the total expected explained variance to grow without bound, leading to overfitting-prone models that expect near-perfect fit a priori.&lt;/p&gt;
&lt;h3 id="the-r2d2m2-solution-type-level-variance-allocation"&gt;The R2D2M2 solution: type-level variance allocation&lt;/h3&gt;&lt;p&gt;The key insight from Aguilar &amp;amp; Bürkner is that R2D2M2 extends the Dirichlet decomposition to handle multiple &lt;strong&gt;types&lt;/strong&gt; of effects while preserving hierarchical pooling:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Component calculation for laboratory data&lt;/span&gt;
&lt;span class="n"&gt;n_components&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;n_predictors&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;n_grouping_factors&lt;/span&gt;
&lt;span class="n"&gt;n_components&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;  &lt;span class="c1"&gt;# gene_expr + age + dose + mouse + microRNA + stress&lt;/span&gt;

&lt;span class="n"&gt;component_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;population_gene_expr&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;    &lt;span class="c1"&gt;# Population-level effects&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;population_age&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;population_dose&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;mouse_intercepts&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;        &lt;span class="c1"&gt;# Group-specific intercept types&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;microRNA_intercepts&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;stress_intercepts&amp;#39;&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The key innovation here is subtle but powerful: instead of allocating variance to individual groups (Mouse 1, Mouse 2, etc.), we allocate variance to &lt;strong&gt;types&lt;/strong&gt; of effects. All mice share one variance prior, all microRNAs share another, etc.&lt;/p&gt;
&lt;h3 id="the-complete-r2d2m2-framework"&gt;The complete R2D2M2 framework&lt;/h3&gt;&lt;p&gt;Let's see how this all comes together in practice. The R2D2M2 model combines the R² prior, the extended Dirichlet allocation, and the hierarchical variance structure:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coords&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;coords&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="c1"&gt;# Same intuitive R² control&lt;/span&gt;
    &lt;span class="n"&gt;r_squared&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Beta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;r_squared&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;alpha_r2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;beta_r2&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;tau_squared&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tau_squared&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;r_squared&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;r_squared&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="c1"&gt;# Extended Dirichlet allocation across ALL effect types&lt;/span&gt;
    &lt;span class="n"&gt;phi&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dirichlet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;phi&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;full&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;concentration&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;components&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Population-level effects - each gets its own φ component&lt;/span&gt;
    &lt;span class="n"&gt;beta_gene&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;beta_gene&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_squared&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;tau_squared&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;beta_age&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;beta_age&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                        &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_squared&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;tau_squared&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
    &lt;span class="n"&gt;beta_dose&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;beta_dose&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                         &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_squared&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;tau_squared&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="c1"&gt;# Group-specific intercepts - each type gets its own φ component&lt;/span&gt;
    &lt;span class="n"&gt;mouse_intercepts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mouse_intercepts&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_squared&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;tau_squared&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mice&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;microRNA_intercepts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;microRNA_intercepts&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                   &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_squared&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;tau_squared&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                   &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;microRNAs&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;stress_intercepts&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;stress_intercepts&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                                 &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;pt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_squared&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;phi&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;5&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;tau_squared&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
                                 &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;stress_conditions&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Linear predictor combining all effects&lt;/span&gt;
    &lt;span class="n"&gt;eta&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;beta_gene&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;gene_expr&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;beta_age&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;age&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;beta_dose&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;dose&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
           &lt;span class="n"&gt;mouse_intercepts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mouse_ids&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
           &lt;span class="n"&gt;microRNA_intercepts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;microRNA_ids&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt;
           &lt;span class="n"&gt;stress_intercepts&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;stress_conditions&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now that we've seen the mathematical structure, let's understand what makes this approach so effective.&lt;/p&gt;
&lt;h3 id="why-this-works-so-well"&gt;Why this works so well&lt;/h3&gt;&lt;p&gt;&lt;strong&gt;Hierarchical pooling preserved&lt;/strong&gt;: Individual mice still borrow strength from each other because they share the same variance component. Mouse A and Mouse B both use &lt;code&gt;mouse_scale&lt;/code&gt;, but have different intercept values.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Automatic factor importance&lt;/strong&gt;: The φ allocation tells you which experimental factors matter most. If φ = [0.15, 0.25, 0.05, 0.35, 0.15, 0.05], then mouse differences account for 35% of total explained variance - more than any single predictor!&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Scalable complexity&lt;/strong&gt;: Works with any number of crossed or nested grouping factors without parameter explosion.&lt;/p&gt;
&lt;h2 id="the-unified-architecture"&gt;The unified architecture&lt;/h2&gt;&lt;p&gt;What strikes me most about this progression is how each extension elegantly handles new complexity:&lt;/p&gt;
&lt;p&gt;All three approaches maintain &lt;strong&gt;consistent R² control&lt;/strong&gt;, letting you directly specify beliefs about model fit through the same intuitive Beta prior on R². The competitive variance allocation through the Dirichlet mechanism creates healthy competition between effects across all approaches, preventing any single component from dominating. This leads to highly interpretable results - every approach produces φ components that directly tell you "what percentage of explained variance does each effect contribute?"&lt;/p&gt;
&lt;p&gt;The mathematical elegance is striking: each extension modifies just what needs to change. The GLM extension changes the noise term (σ² → s²(β₀)), while the M2 extension extends the allocation to multiple effect types. Finally, all approaches provide the same practical benefits - automatic shrinkage, sparsity induction, and protection against overfitting while maintaining computational tractability.&lt;/p&gt;
&lt;h2 id="when-to-use-what"&gt;When to use what&lt;/h2&gt;&lt;p&gt;Given these unified principles, how do you choose which approach fits your specific modeling scenario? Through this exploration, clear use cases emerged:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Approach&lt;/th&gt;
&lt;th&gt;When to Use&lt;/th&gt;
&lt;th&gt;Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;R2D2 Shrinkage&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Simple linear regression with multiple predictors, no grouping&lt;/td&gt;
&lt;td&gt;Gene expression ~ drug dose + age + weight&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;R2D2 GLM&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Non-Gaussian outcomes with simple structure&lt;/td&gt;
&lt;td&gt;Bacterial counts, binary outcomes, rate data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;R2D2M2&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;Complex laboratory designs with multiple grouping factors (&lt;strong&gt;the laboratory default&lt;/strong&gt;)&lt;/td&gt;
&lt;td&gt;Laboratory experiments with mouse ID + microRNA ID + stress condition&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;h2 id="looking-forward"&gt;Looking forward&lt;/h2&gt;&lt;p&gt;R2D2 solves a common frustration in Bayesian modeling: how do you set reasonable priors on dozens of coefficients without spending hours tweaking hyperparameters? Instead of guessing at individual coefficient priors, you specify one intuitive parameter - how much of the data variation you expect your model to explain - and R2D2 automatically figures out how to distribute that explanatory power across your predictors.&lt;/p&gt;
&lt;p&gt;For laboratory researchers especially, R2D2M2 delivers actionable scientific insight. When your model tells you that "mouse differences account for 35% of explained variance while stress conditions only account for 5%," you immediately know where to focus your experimental design efforts.&lt;/p&gt;
&lt;p&gt;This practical approach - starting with an intuitive question about model fit and letting the mathematics handle the details - shows how thoughtful statistical frameworks can make sophisticated modeling more accessible to working scientists. The PyMC library has implemented a modified form of R2D2M2 as the &lt;a href="https://www.pymc.io/projects/extras/en/stable/generated/pymc_extras.distributions.R2D2M2CP.html"&gt;&lt;code&gt;R2D2M2CP&lt;/code&gt; distribution&lt;/a&gt;, making these powerful priors readily available for practical use.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Zhang, Y. D., Naughton, B. P., Bondell, H. D., &amp;amp; Reich, B. J.&lt;/strong&gt; (2020). Bayesian Regression Using a Prior on the Model Fit: The R2-D2 Shrinkage Prior. &lt;em&gt;Journal of the American Statistical Association&lt;/em&gt;, 117(538), 862-874. &lt;a href="https://www.tandfonline.com/doi/full/10.1080/01621459.2020.1825449"&gt;https://www.tandfonline.com/doi/full/10.1080/01621459.2020.1825449&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Yanchenko, E., Bondell, H. D., &amp;amp; Reich, B. J.&lt;/strong&gt; (2021). The R2D2 Prior for Generalized Linear Mixed Models. &lt;em&gt;arXiv preprint arXiv:2111.10718&lt;/em&gt;. &lt;a href="https://arxiv.org/abs/2111.10718"&gt;https://arxiv.org/abs/2111.10718&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Aguilar, J. &amp;amp; Bürkner, P.&lt;/strong&gt; (2022). Intuitive Joint Priors for Bayesian Linear Multilevel Models: The R2D2M2 prior. &lt;em&gt;arXiv preprint arXiv:2208.07132&lt;/em&gt;. &lt;a href="https://arxiv.org/abs/2208.07132"&gt;https://arxiv.org/abs/2208.07132&lt;/a&gt;&lt;/p&gt;
</content></entry><entry><title>From nerd-sniped to shipped using AI as a thinking tool</title><link href="https://ericmjl.github.io/blog/2025/7/21/from-nerd-sniped-to-shipped-using-ai-as-a-thinking-tool/" rel="alternate"/><updated>2025-07-21T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:28012e93-0cdf-3534-aa24-409f9d057e10</id><content type="html">&lt;p&gt;What if I told you I shipped a complex feature rewrite in just two days using AI as a design partner?&lt;/p&gt;
&lt;p&gt;Before you roll your eyes at another "AI did everything for me" story, here's the catch: those two days were only possible because I spent months doing the hard work of earning that automation. Fresh off being thoroughly nerd-sniped by Joe Cheng (Posit PBC's CTO) at SciPy 2025, I found myself on a plane with a mission: finally implement robust graph-based memory for my Llamabot project.&lt;/p&gt;
&lt;p&gt;What happened next taught me everything about the difference between delegating thinking to AI versus using AI to amplify your thinking. The key insight? You have to earn your automation first.&lt;/p&gt;
&lt;h2 id="first-i-had-to-struggle-and-that-was-the-point"&gt;First, I had to struggle (and that was the point)&lt;/h2&gt;&lt;p&gt;The timeline is crucial to understanding why this approach worked. For four months, I'd been mulling over how graph-based memory for LLM applications could work. Then Joe read my Llamabot code (which at the time didn't have graph-based memory), we chatted, and I got completely nerd-sniped. Over the next few days, I decided I had to make graph memory happen, so I finally built a working prototype during my week in Seattle for work. (All on my personal laptop, keeping work and personal projects separate.)&lt;/p&gt;
&lt;p&gt;What made this experience transformative was having Joe look at my code. Here was someone I'd never met taking such a thorough look at my design choices - I was deeply impressed by how careful and thoughtful he was. That validation convinced me: it was time to do this right.&lt;/p&gt;
&lt;p&gt;But here's what mattered most: my prototype was fragile. Things were very intertwined with one another. Because everything was so coupled, I was naturally feeling the difficulty in making any changes. This hands-on struggle was teaching me exactly what needed to be separated and how to think about the architecture.&lt;/p&gt;
&lt;p&gt;This struggle wasn't wasted time - it was earning my automation.&lt;/p&gt;
&lt;h2 id="why-struggling-first-was-essential"&gt;Why struggling first was essential&lt;/h2&gt;&lt;p&gt;This connects directly to &lt;a href="https://ericmjl.github.io/blog/2025/7/13/earn-the-privilege-to-use-automation/"&gt;an earlier blog post I wrote&lt;/a&gt; about earning your automation. I wouldn't have been able to critique AI the way I did if I hadn't first developed taste through hands-on struggle.&lt;/p&gt;
&lt;p&gt;That initial prototype work - building something fragile but functional by hand - gave me the judgment needed to meaningfully critique AI's suggestions. Without that foundation, I would have been delegating critical thinking to AI instead of using it as a thinking partner.&lt;/p&gt;
&lt;p&gt;The prototype taught me what worked, what didn't, and most importantly, what the real problems were that needed solving. When AI later proposed architectural changes, I could evaluate them against my lived experience of the pain points.&lt;/p&gt;
&lt;p&gt;This preparation set the stage for what happened next on that plane ride home.&lt;/p&gt;
&lt;h2 id="then-i-unleashed-ai-as-a-design-partner"&gt;Then I unleashed AI as a design partner&lt;/h2&gt;&lt;p&gt;At SEA-TAC airport with three hours until boarding, I decided this was it. Time to compress all my implementation work into a focused sprint. But instead of jumping straight into coding, I started with what felt like a radical approach: a pure design phase.&lt;/p&gt;
&lt;p&gt;(Now, to be clear, it's not exactly radical - lots of people have said you should write requirements first. But most vibe coders don't actually follow this practice.)&lt;/p&gt;
&lt;p&gt;I asked AI to critique my existing prototype and propose a new architecture. What followed was intense iteration on a design document right there in the airport. I did try to continue on the plane, but JetBlue's spotty Wi-Fi made that unproductive. Most of the design thinking and iteration happened during those airport hours - no code written yet, just pure design thinking.&lt;/p&gt;
&lt;p&gt;AI proposed an interface with chat memory at the high level, with separate graph memory and list memory structures underneath. It included a visualization module (originally tailored for graphs) and a node selector module for intelligent node selection. The design doc grew to at least 400-500 lines of markdown.&lt;/p&gt;
&lt;p&gt;The beauty of this approach? I could look at the prospective code in markdown blocks and play through scenarios in my head. How would someone use this API? How would the internals work? By asking very specific "how" questions, I could probe deeper and make sure I truly understood and agreed with every design choice.&lt;/p&gt;
&lt;p&gt;The major breakthrough came when I scrutinized the design and asked: why do we have two chat memory implementations, one for linear memory and one for graph memory?&lt;/p&gt;
&lt;p&gt;The natural follow-up hit me: lists are just linear graphs, so why do I need two separate structures? I can just have one that defaults to a linear graph, and then use an LLM for intelligent node selection in the threaded case.&lt;/p&gt;
&lt;p&gt;So I generalized everything to use NetworkX graphs underneath, with intelligent node selection for threaded memory. This single insight simplified the entire architecture.&lt;/p&gt;
&lt;p&gt;This is exactly what I mean about earning your automation - I could inject my own opinions into the design because I understood the problem space. We were iterating on a design doc, not just generating code.&lt;/p&gt;
&lt;h2 id="the-real-power-ai-as-a-critical-thinking-amplifier"&gt;The real power: AI as a critical thinking amplifier&lt;/h2&gt;&lt;p&gt;Here's where things got really powerful. After creating that 400-500 line design document, I had way too much detail to synthesize mentally. Time to leverage one of AI's core strengths: knowledge retrieval and pattern matching.&lt;/p&gt;
&lt;p&gt;I commanded the AI: "Go look for any inconsistencies you can see within the doc. Pick out all inconsistencies and surface them for me."&lt;/p&gt;
&lt;p&gt;This is where the magic happened. AI surfaced seven or eight inconsistencies, some I agreed with, others I dismissed as inconsequential. But because I'd just reviewed everything, it was all fresh in my mind - I could make informed decisions about each point.&lt;/p&gt;
&lt;p&gt;Then I asked it to check one more time: "Double check for me. Do you see any more inconsistencies?"&lt;/p&gt;
&lt;p&gt;Now, I wasn't fully offloading this work to AI. I was still doing synthesis in my head, trying to catch things myself. In fact, I caught an inconsistency the AI missed - sometimes I was using &lt;code&gt;bot.memory&lt;/code&gt; and sometimes &lt;code&gt;bot.chat_memory&lt;/code&gt; between the documentation and API, while continually refining and reviewing the documentation.&lt;/p&gt;
&lt;p&gt;The key insight here is about inversion - one of the core skills of critical thinking. The usual lazy pattern is to just assume things are correct (what I call "vibe coding"). But with AI assistance, we should invert and ask, "What if it's not correct?"&lt;/p&gt;
&lt;p&gt;If it's not correct, the logical follow-up becomes: can I get AI to tell me where it's wrong? This combines inversion with one of AI's key strengths - knowledge retrieval. Yes, AI struggles with needle-in-haystack problems, but for big needles in smaller haystacks? It's incredibly powerful.&lt;/p&gt;
&lt;p&gt;The "needle" here is: where am I self-contradictory? Where am I discordant? Where is my design not self-coherent? All those assumptions I might have about text-based work can be checked using AI as a tool for critical thinking.&lt;/p&gt;
&lt;p&gt;If in doubt, always invert - and now we have a lightning-fast tool for helping us do exactly that.&lt;/p&gt;
&lt;p&gt;This principle became the foundation for everything that followed.&lt;/p&gt;
&lt;h2 id="putting-the-method-into-practice-tests-first-then-code"&gt;Putting the method into practice: tests first, then code&lt;/h2&gt;&lt;p&gt;With the design doc solid, it was time for the next phase. I told the AI: "Go write the tests. Write all the tests. Follow the directory structure. Make sure the test structure matches what you're proposing."&lt;/p&gt;
&lt;p&gt;I reviewed every single test - lots of code review. But here's what's cool about AI-generated tests: they don't tend to be complicated. They're usually on the simpler side. I don't see parameterized tests using property-based testing like Hypothesis. Instead, I see example-based tests.&lt;/p&gt;
&lt;p&gt;As a first pass, example-based tests are perfect - they're concrete, easy to grasp, and I can have confidence that if the test is testing what I think it should test, then it'll pass when the implementation is written.&lt;/p&gt;
&lt;p&gt;The test review process was lightning-fast because I was so grounded in what the code was supposed to do. The design doc grounded the tests, the tests would ground the implementation. Each layer validated the next. This is the "earn your automation" principle in action - I could review tests quickly because I understood what the code should do.&lt;/p&gt;
&lt;h2 id="when-things-break-and-why-that-s-exactly-what-you-want"&gt;When things break (and why that's exactly what you want)&lt;/h2&gt;&lt;p&gt;When I finally had AI generate the implementation code and ran the tests, a lot failed - and I was totally okay with that. The first pass had maybe 20+ failing tests, but I figured out an efficient way to iterate through them in batches.&lt;/p&gt;
&lt;p&gt;I literally copied and pasted &lt;code&gt;pytest&lt;/code&gt; output and got AI to categorize the failures by common patterns. AI is blazing fast at pattern recognition - what would take me ages to figure out was near instantaneous for AI.&lt;/p&gt;
&lt;p&gt;Categorizing the failures was key. If I could group them, I could knock out three, four, sometimes even seven failing tests with targeted code changes. Even better, sometimes the failures revealed misunderstandings - either mine about the code or the AI's about the design. This forced clarifying decisions that resolved the discordance between what the test expected versus what the code actually did.&lt;/p&gt;
&lt;p&gt;With this approach, I quickly narrowed those 20+ failing tests down to maybe three or four individual syntax errors. Finally, everything worked - all tests passed, discordances resolved, ready to ship.&lt;/p&gt;
&lt;p&gt;Remember that inversion principle I mentioned earlier? This is how it played out in practice. Instead of assuming the generated code was correct, I actively looked for where it was wrong and used AI to help categorize and fix the problems systematically.&lt;/p&gt;
&lt;h2 id="the-payoff-two-days-from-design-to-deployment"&gt;The payoff: two days from design to deployment&lt;/h2&gt;&lt;p&gt;The timeline tells the whole story. I flew on Sunday morning, starting this work while at the airport, and by Monday evening had the pull request done and up to my expectations. The entire implementation phase - from final design doc to merged pull request - took just two days.&lt;/p&gt;
&lt;p&gt;But this compressed timeline was only possible because of all the preparation: four months marinating on the idea, one week during the conference to write the prototype and let it simmer while in Seattle and Tacoma, then intense design iteration with AI assistance.&lt;/p&gt;
&lt;p&gt;This teaches us something crucial about AI-assisted development: AI doesn't replace thinking and preparation - it amplifies it. I had a crystal-clear goal of what needed shipping after all that prep work. Once I was done with the prototype phase and figuring out the actual problem, bam - two days to ship.&lt;/p&gt;
&lt;p&gt;That's incredible. But notice what made this possible: not AI magic, but AI amplifying months of preparation and struggle.&lt;/p&gt;
&lt;h2 id="what-i-actually-built-and-why-it-matters"&gt;What I actually built (and why it matters)&lt;/h2&gt;&lt;p&gt;As someone who has worked with graphs before, in my eyes, the result is beautiful. Conversations are now represented as graphs, and since I work exclusively in Marimo notebooks, I can run and view Mermaid diagrams right inline. With a Mermaid diagram in a Marimo notebook, it's incredibly powerful - I can actually jump around conversation threads using the graph as visual memory to continue probing the AI system in sophisticated ways.&lt;/p&gt;
&lt;p&gt;&lt;img src="graph-memory.webp" alt=""&gt;&lt;/p&gt;
&lt;p&gt;What I love about this implementation is that it's not just a technical achievement - it's become a practical thinking tool. The visual graph helps me navigate complex AI conversations and switch between threads mentally more easily.&lt;/p&gt;
&lt;p&gt;And I could only build this effectively because I'd earned the right to automate through that initial prototype struggle.&lt;/p&gt;
&lt;h2 id="how-this-approach-scales-the-power-of-ai-assisted-pair-coding"&gt;How this approach scales: the power of AI-assisted pair coding&lt;/h2&gt;&lt;p&gt;I have a hypothesis that this approach works even better with two people and an AI assistant - but not more than two, because you can't have too many cooks. At Moderna's Data Science and AI teams, we instituted pair coding early on so we could help each other and share knowledge. Yes, we get less done in the same time, but in the long run, we move much faster. This shared knowledge means I can quickly jump into someone else's codebase.&lt;/p&gt;
&lt;p&gt;Pair coding as a practice needs maintenance though - I noticed recently I was getting isolated into solo coding. But during my Seattle trip, I experienced pair coding with AI assistance alongside my colleague Dan Luu from the ML Platform Team. We were learning prompting tips from each other, and it was incredible - we had a chance to share practices for how to use AI to amplify ourselves.&lt;/p&gt;
&lt;p&gt;What used to be "here's how you write the function" became sharing how we're actually thinking. We've elevated the level at which we share knowledge. As Dan prompts the AI or I prompt the AI, we're learning how each other thinks in a way that's smooth, fluent, and not bogged down by syntax or implementation details. It operates at a higher plane than mere code.&lt;/p&gt;
&lt;p&gt;This is incredibly powerful because we're sharing practices for how to use AI to amplify ourselves, learning prompting techniques from each other in real time.&lt;/p&gt;
&lt;p&gt;What used to require teaching syntax and implementation details now becomes sharing thinking patterns and problem-solving approaches. We've elevated the conversation.&lt;/p&gt;
&lt;h2 id="the-pattern-that-changes-everything"&gt;The pattern that changes everything&lt;/h2&gt;&lt;p&gt;What made this approach work wasn't AI magic - it was a specific sequence that amplified months of preparation into two days of execution.&lt;/p&gt;
&lt;p&gt;First, I had to struggle. Building that fragile prototype by hand taught me what the real problems were. Without that lived experience, I couldn't have meaningfully critiqued AI's suggestions or made good design decisions. You can't skip this step.&lt;/p&gt;
&lt;p&gt;Then I could partner strategically with AI. Instead of using it as a code generator, I used it as a critical thinking amplifier. The inversion principle became key - actively asking "what's wrong here?" and leveraging AI's pattern recognition to find inconsistencies and categorize problems.&lt;/p&gt;
&lt;p&gt;Finally, I followed a systematic progression: design document first, then comprehensive tests, then implementation. When tests inevitably failed, I used AI to categorize failures and fix them in batches rather than one by one.&lt;/p&gt;
&lt;p&gt;The two days it took me to ship graph memory weren't about AI being magical. They were about using AI properly after doing the hard work of earning that automation. The months of struggle weren't wasted time - they were the essential foundation that made AI partnership effective.&lt;/p&gt;
&lt;p&gt;This is how you go from vibe coding to strategic automation. Not by delegating thinking to AI, but by using AI to amplify the thinking you've already earned the right to do.&lt;/p&gt;
</content></entry><entry><title>How to use xarray for unified laboratory data storage</title><link href="https://ericmjl.github.io/blog/2025/7/15/how-to-use-xarray-for-unified-laboratory-data-storage/" rel="alternate"/><updated>2025-07-15T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:ab8f5511-7289-3d99-b9e8-026ea9a58088</id><content type="html">&lt;p&gt;What if your laboratory and machine learning related data could be managed within a single data structure? From raw experimental measurements to computed features to model outputs, everything coordinate-aligned and ready for analysis.&lt;/p&gt;
&lt;p&gt;I've been thinking about this problem across different experimental contexts. We generate measurement data, then computed features, then model outputs, then train/test splits. Each piece typically lives in its own file, its own format, with its own indexing scheme. The cognitive overhead of keeping track of which sample corresponds to which row in which CSV is exhausting.&lt;/p&gt;
&lt;p&gt;Let me illustrate this with a microRNA expression study as a concrete example.&lt;/p&gt;
&lt;p&gt;Here's an approach that could solve this: &lt;strong&gt;store everything in a unified xarray Dataset where sample identifiers are the shared coordinate system&lt;/strong&gt;. Your experimental measurements, computed features, statistical estimates, and data splits all aligned by the same IDs. No more integer indices. No more file juggling. Just clean, coordinated data that scales to the cloud.&lt;/p&gt;
&lt;h2 id="what-s-wrong-with-traditional-laboratory-data-management"&gt;What's wrong with traditional laboratory data management?&lt;/h2&gt;&lt;p&gt;Picture this: you're three months into a microRNA expression study. You've got the following files:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;expression measurements in &lt;code&gt;expression_data.csv&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;ML features in &lt;code&gt;sequence_features.parquet&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;model outputs in &lt;code&gt;model_results.h5&lt;/code&gt;, and&lt;/li&gt;
&lt;li&gt;train/test splits scattered across &lt;code&gt;train_indices.npy&lt;/code&gt; and &lt;code&gt;test_indices.npy&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each file has its own indexing scheme - some use row numbers, others use identifiers, and you're constantly writing index-matching code just to keep everything aligned.&lt;/p&gt;
&lt;p&gt;The cognitive overhead is brutal. Which microRNA corresponds to row 47 in the features file? Did you remember to filter out the same samples from both your training data and your metadata? When you subset your data for analysis, do all your indices still match?&lt;/p&gt;
&lt;p&gt;I've lost count of how many times I've seen analysis pipelines break because someone forgot to apply the same filtering to all their data files. It's not just inefficient - it's error-prone and exhausting.&lt;/p&gt;
&lt;h2 id="how-does-xarray-solve-this"&gt;How does xarray solve this?&lt;/h2&gt;&lt;p&gt;Xarray changes the game by making &lt;strong&gt;coordinates the foundation of your data structure&lt;/strong&gt;. Instead of managing separate files with separate indexing schemes, you create one unified dataset where &lt;em&gt;every piece of data knows exactly which microRNA it belongs to&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The beauty lies in the coordinate system. Each data point is labeled with meaningful coordinates: not just row numbers, but actual experimental factors like microRNA ID, treatment condition, time point, and replicate. When you slice your data, everything stays aligned automatically.&lt;/p&gt;
&lt;p&gt;This is transformative! When everything shares the same coordinate system, you can slice across any dimension and everything stays connected. Want features for specific microRNAs? The model results for those same microRNAs come along automatically.&lt;/p&gt;
&lt;h2 id="what-does-unified-data-storage-look-like"&gt;What does unified data storage look like?&lt;/h2&gt;&lt;p&gt;Let me walk you through how this works in practice. We start with a coordinate system that captures the experimental design:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Coordinates:
* mirna           (150 microRNAs: hsa-miR-1, hsa-miR-2, ...)
* treatment       (3 conditions: control, hypoxia, inflammation)
* time_point      (5 timepoints: 2h, 6h, 12h, 24h, 48h)
* replicate       (3 replicates: rep_1, rep_2, rep_3)
* cell_line       (10 cell lines: cell_line_01, cell_line_02, ...)
* experiment_date (4 dates: experiment dates)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Then we progressively add data that aligns with these coordinates:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Stage 1: Expression measurements&lt;/span&gt;
&lt;span class="n"&gt;unified_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dataset&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;expression_level&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mirna&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;treatment&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;time_point&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;replicate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;cell_line&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;expression_data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;

&lt;span class="c1"&gt;# Stage 2: Bayesian estimation results&lt;/span&gt;
&lt;span class="n"&gt;unified_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unified_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;mirna_effects&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mirna&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;mirna_coefficients&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;mirna_effects_std&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mirna&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;mirna_coefficient_errors&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;treatment_effects&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;treatment&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;treatment_coefficients&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;treatment_effects_std&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;treatment&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;treatment_coefficient_errors&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;time_effects&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;time_point&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;time_coefficients&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;time_effects_std&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;time_point&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;time_coefficient_errors&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;replicate_effects&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;replicate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;replicate_coefficients&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;replicate_effects_std&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;replicate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;replicate_coefficient_errors&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;cell_line_effects&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;cell_line&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;cell_line_coefficients&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;cell_line_effects_std&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;cell_line&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;cell_line_coefficient_errors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;

&lt;span class="c1"&gt;# Stage 3: ML features&lt;/span&gt;
&lt;span class="n"&gt;unified_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unified_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;ml_features&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mirna&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;feature&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feature_matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign_coords&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feature&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;nt_A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;nt_T&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;nt_G&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;nt_C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;length&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;gc_content&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

&lt;span class="c1"&gt;# Stage 4: Train/test splits&lt;/span&gt;
&lt;span class="n"&gt;unified_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unified_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;train_mask&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mirna&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;split_type&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;train_masks&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;test_mask&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mirna&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;split_type&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;test_masks&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The magic happens when you realize that &lt;strong&gt;every piece of data is automatically aligned by the shared coordinate system&lt;/strong&gt;. Need to analyze expression patterns for microRNAs in your training set? It's just coordinate selection:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Get training mask for random 80/20 split&lt;/span&gt;
&lt;span class="n"&gt;train_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unified_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;split_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;random_80_20&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Get ML features for training microRNAs&lt;/span&gt;
&lt;span class="n"&gt;train_features&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unified_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ml_features&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Get expression data for the same microRNAs&lt;/span&gt;
&lt;span class="n"&gt;train_expression&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unified_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;expression_level&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Everything stays connected automatically. No manual bookkeeping required.&lt;/p&gt;
&lt;h2 id="how-do-we-build-this-step-by-step"&gt;How do we build this step by step?&lt;/h2&gt;&lt;p&gt;The approach is straightforward - &lt;strong&gt;progressive data accumulation&lt;/strong&gt;. You don't need to have everything figured out upfront. Start with your core experimental data, then add layers as your analysis develops.&lt;/p&gt;
&lt;h3 id="stage-1-laboratory-measurements"&gt;Stage 1: Laboratory measurements&lt;/h3&gt;&lt;p&gt;Your foundation is the experimental data with meaningful coordinates:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Expression data automatically aligned by coordinates&lt;/span&gt;
&lt;span class="n"&gt;expression_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataArray&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;measurements&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;coords&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;mirna&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;mirna_ids&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;treatment&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;control&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;hypoxia&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;inflammation&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;replicate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;rep_1&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;rep_2&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;rep_3&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;time_point&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;2h&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;6h&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;12h&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;24h&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;48h&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;cell_line&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;cell_lines&lt;/span&gt;
    &lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mirna&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;treatment&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;replicate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;time_point&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;cell_line&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;You should note here how the coordinates basically mirror the experimental design.&lt;/p&gt;
&lt;h3 id="stage-2-bayesian-estimation"&gt;Stage 2: Bayesian estimation&lt;/h3&gt;&lt;p&gt;Add effect estimates that align with your experimental coordinates:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Bayesian effects model results&lt;/span&gt;
&lt;span class="n"&gt;unified_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unified_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;mirna_effects&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mirna&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;mirna_coefficients&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;mirna_effects_std&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mirna&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;mirna_coefficient_errors&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;treatment_effects&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;treatment&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;treatment_coefficients&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;treatment_effects_std&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;treatment&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;treatment_coefficient_errors&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;time_effects&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;time_point&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;time_coefficients&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;time_effects_std&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;time_point&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;time_coefficient_errors&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;replicate_effects&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;replicate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;replicate_coefficients&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;replicate_effects_std&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;replicate&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;replicate_coefficient_errors&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;cell_line_effects&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;cell_line&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;cell_line_coefficients&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;cell_line_effects_std&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;cell_line&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;cell_line_coefficient_errors&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The beauty is that your Bayesian effects model estimates align perfectly with your experimental design coordinates. Each experimental factor gets its own effect estimate with uncertainty, organized by the same coordinate system as your raw data.&lt;/p&gt;
&lt;h3 id="stage-3-ml-features"&gt;Stage 3: ML features&lt;/h3&gt;&lt;p&gt;Features slot right into the same coordinate system:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# ML features aligned by microRNA ID&lt;/span&gt;
&lt;span class="n"&gt;unified_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unified_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;ml_features&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mirna&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;feature&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;feature_matrix&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign_coords&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;feature&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;nt_A&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;nt_T&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;nt_G&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;nt_C&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;length&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;gc_content&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="stage-4-train/test-splits"&gt;Stage 4: Train/test splits&lt;/h3&gt;&lt;p&gt;Even data splits become part of the unified structure:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Boolean masks aligned by microRNA coordinate&lt;/span&gt;
&lt;span class="n"&gt;unified_dataset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unified_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;assign&lt;/span&gt;&lt;span class="p"&gt;({&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;train_mask&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mirna&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;split_type&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;train_masks&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="s1"&gt;&amp;#39;test_mask&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;mirna&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;split_type&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;test_masks&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;})&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;Progressive build = reduced cognitive load&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;The beauty of this approach is that you can build it incrementally. Start with your core experimental data, then add statistical results, then ML features, then splits. Each stage builds on the previous coordinate system, so everything stays aligned automatically.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="what-are-the-practical-benefits"&gt;What are the practical benefits?&lt;/h2&gt;&lt;h3 id="no-more-index-juggling"&gt;No more index juggling&lt;/h3&gt;&lt;p&gt;Remember the nightmare of keeping track of which microRNA corresponds to which row in which file? That's gone. Every piece of data knows its own coordinates.&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Before: manual index matching across files&lt;/span&gt;
&lt;span class="n"&gt;expression_subset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;expression_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_indices&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;features_subset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;features_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;loc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mirna_ids&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_indices&lt;/span&gt;&lt;span class="p"&gt;]]&lt;/span&gt;
&lt;span class="n"&gt;model_results_subset&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;iloc&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;train_indices&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# After: coordinate-based selection&lt;/span&gt;
&lt;span class="n"&gt;train_data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;unified_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;unified_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;train_mask&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;split_type&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;random_80_20&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;drop&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="bulletproof-data-consistency"&gt;Bulletproof data consistency&lt;/h3&gt;&lt;p&gt;When you slice your data, everything stays aligned automatically. No more worrying about applying the same filtering to all your files.&lt;/p&gt;
&lt;h3 id="cloud-native-scaling"&gt;Cloud-native scaling&lt;/h3&gt;&lt;p&gt;Store everything in Zarr format and your unified dataset becomes cloud-native. Load it from S3, query specific slices, and everything scales seamlessly. (Note: Zarr has some limitations with certain data types like U8, but xarray supports multiple storage formats to work around these issues.)&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Save entire workflow to cloud&lt;/span&gt;
&lt;span class="n"&gt;unified_dataset&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;to_zarr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;s3://biodata/mirna_screen_2024.zarr&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Load and analyze anywhere&lt;/span&gt;
&lt;span class="n"&gt;experiment&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;xr&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;open_zarr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;s3://biodata/mirna_screen_2024.zarr&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="reproducible-analysis-pipelines"&gt;Reproducible analysis pipelines&lt;/h3&gt;&lt;p&gt;Your analysis becomes more reproducible because the data structure itself enforces consistency. Share the dataset and the analysis code just works.&lt;/p&gt;
&lt;h2 id="what-tools-make-this-possible"&gt;What tools make this possible?&lt;/h2&gt;&lt;p&gt;The tooling ecosystem has evolved dramatically in recent years. A few years ago, I would have told you to use parquet files with very unnatural tabular setups to get everything into tidy format. But &lt;strong&gt;xarray is changing the game&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Xarray&lt;/strong&gt; provides the coordinate system and multidimensional data structures that make this unified approach possible. It's like pandas for higher-dimensional data, but with meaningful coordinates instead of just integer indices.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Zarr&lt;/strong&gt; gives you cloud-native storage that preserves all your coordinate information and metadata. It supports chunking, compression, and parallel access - perfect for scaling your unified datasets.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt; The tools we've got are just getting better and better. I wouldn't have imagined that we'd be able to use xarray for this kind of unified laboratory data storage just a few years ago. The ecosystem is maturing rapidly, and these approaches are becoming more accessible every year.&lt;/p&gt;
&lt;h2 id="what-s-next"&gt;What's next?&lt;/h2&gt;&lt;p&gt;If you're working with multidimensional experimental data, I'd strongly encourage you to try this unified approach. Start small - take your next experiment and see if you can structure it as a single xarray Dataset instead of multiple files.&lt;/p&gt;
&lt;p&gt;The cognitive overhead reduction is immediate. No more wondering if your indices are aligned. No more writing index-matching code. Just clean, coordinated data that scales to the cloud.&lt;/p&gt;
&lt;p&gt;Time will distill the best practices in your context, but I've found this unified approach eliminates so much friction from the experimental data lifecycle. Give it a try and see how it feels in your workflow.&lt;/p&gt;
&lt;p&gt;I cooked up this synthetic example while attending Ian Hunt-Isaak's talk &lt;a href="https://cfp.scipy.org/scipy2025/talk/AARA39/"&gt;"Xarray across biology. Where are we and where are we going?"&lt;/a&gt; at SciPy 2025. His presentation on using xarray for biological data really crystallized how powerful this coordinate-based approach could be for the typical experimental workflow.&lt;/p&gt;
</content></entry><entry><title>Reflections on the SciPy 2025 Conference</title><link href="https://ericmjl.github.io/blog/2025/7/14/reflections-on-the-scipy-2025-conference/" rel="alternate"/><updated>2025-07-14T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:65ab7a38-7622-37a4-bce5-8af2c9cb1881</id><content type="html">&lt;p&gt;This year marks my 10th year of being involved with the Scientific Python Conference, and it has been an absolute blast! What started as curiosity about the intersection of science and software has grown into a decade of learning, teaching, and contributing to this incredible community.&lt;/p&gt;
&lt;h2 id="conference-activities-summary"&gt;Conference Activities Summary&lt;/h2&gt;&lt;p&gt;This year's SciPy was particularly active for me. I taught two tutorials: "Building with LLMs Made Simple" (a new one) and "Network Analysis Made Simple" (my longtime favorite). After the tutorials, I attended several inspiring talks, including an especially motivating presentation on XArray in biology that prompted me to create a Marimo notebook demonstrating XArray's applications in biological data analysis.&lt;/p&gt;
&lt;p&gt;One of my favorite conference activities this year was recording conversations with fellow attendees. In lieu of my Insta360 camera, I brought my DJI mic everywhere and captured numerous insightful discussions, creating an informal podcast collection of SciPy conversations. Finally, during the sprints, I felt more tapped out than usual but still managed to contribute to Llamabot development with others and work on the XArray biology materials I had envisioned.&lt;/p&gt;
&lt;h2 id="tutorials"&gt;Tutorials&lt;/h2&gt;&lt;h3 id="building-with-llms-made-simple"&gt;Building with LLMs Made Simple&lt;/h3&gt;&lt;p&gt;This was my first time teaching this tutorial, and I was thrilled to use Marimo notebooks throughout the entire session. The tutorial covered three main areas: simple LLM interactions, structured generation, and RAG (Retrieval-Augmented Generation). You can find the tutorial materials at: &lt;a href="https://github.com/ericmjl/building-with-llms-made-simple"&gt;https://github.com/ericmjl/building-with-llms-made-simple&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The structured generation section was particularly powerful. I emphasized that structured generation is fundamentally about automating form-filling using natural language. Having free text input and getting a filled-out Pydantic model output is incredibly valuable for productivity. One participant mentioned the concept of automating "the dangerous, the dull, and the dirty" - which perfectly captures how LLMs can handle routine tasks.&lt;/p&gt;
&lt;p&gt;For RAG, I clarified that RAG doesn't necessarily equal vector databases - it's about information retrieval through various means including keyword search. I demonstrated custom chunking strategies for standard operating procedures, showing how simple solutions (like appending source references) often work better than complex hierarchical structures.&lt;/p&gt;
&lt;p&gt;The tutorial concluded with brief demos on evaluation and agents. I shared my experience testing different models (Gemma, Llama 3, Llama 4) for docstring generation, emphasizing the importance of experimentation and model selection. For agents, I stressed starting with simpler structured generation approaches before building complex autonomous systems.&lt;/p&gt;
&lt;p&gt;Thanks to Modal's generous credit allocation from their DevRel Charles, I was able to deploy an Ollama endpoint in the cloud, making the tutorial accessible to all participants.&lt;/p&gt;
&lt;h3 id="network-analysis-made-simple"&gt;Network Analysis Made Simple&lt;/h3&gt;&lt;p&gt;This marked either my ninth or tenth time teaching this tutorial at SciPy - my longtime favorite. This year I made the significant transition from Jupyter to Marimo notebooks, which was an experiment that generally worked well despite some setup challenges. You can find the tutorial materials at: &lt;a href="https://github.com/ericmjl/Network-Analysis-Made-Simple"&gt;https://github.com/ericmjl/Network-Analysis-Made-Simple&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The tutorial faced some technical hurdles for installation with the Network Analysis Made Simple package being published on my own PyPI server, plus some participants weren't familiar with Marimo. Fortunately, Erik Welch from NVIDIA was present to help assist participants. By the end of the conference talk days, I was able to resolve the issue by changing the notebooks to draw from the Network Analysis Made Simple source directly instead of my own PyPI server, which solved most of the installation problems.&lt;/p&gt;
&lt;p&gt;What I loved most was the audience engagement. We didn't cover as much content as usual because participants asked so many thoughtful questions, especially during the visualization section. This interaction made the session incredibly valuable, as people were clearly learning and developing new ideas for their own work.&lt;/p&gt;
&lt;p&gt;The Marimo experiment succeeded in shifting the learning environment with minimal overhead. For future iterations, I'm considering eliminating the separate NAMS package and making the entire notebook self-contained with answers included at the bottom.&lt;/p&gt;
&lt;h3 id="overarching-thoughts-on-the-tutorials"&gt;Overarching thoughts on the tutorials&lt;/h3&gt;&lt;p&gt;Both tutorials were conducted entirely within Marimo notebooks, which convinced quite a few participants to switch over to Marimo. They saw the power of fully reactive notebooks and the ability to seamlessly share analysis from one person to another - something that's much more cumbersome with traditional Jupyter notebooks.&lt;/p&gt;
&lt;p&gt;Both tutorials will also be available on YouTube! There was a technical glitch with the Building with LLMs Made Simple tutorial recording, so I'm planning to re-record the full tutorial this coming Saturday - including content we didn't get to cover during the live session. This should actually result in a better, more complete recording for the YouTube release, which I'll also release to my own channel.&lt;/p&gt;
&lt;h2 id="talks-and-presentations"&gt;Talks and Presentations&lt;/h2&gt;&lt;p&gt;I attended several inspiring talks throughout the conference. Here are short summaries of the key presentations that caught my attention:&lt;/p&gt;
&lt;h3 id="xarray-in-biology-ian-hunt-isaak"&gt;XArray in Biology (Ian Hunt-Isaak)&lt;/h3&gt;&lt;p&gt;This talk was particularly inspiring and prompted me to create a Marimo notebook demonstrating XArray applications in biology. Ian, a biologist and microscopist from Earthmover (funded by the Chan Zuckerberg Initiative), presented a compelling case for XArray adoption in biological research.&lt;/p&gt;
&lt;p&gt;XArray excels at handling multi-dimensional biological data like time-series microscopy images, multi-channel fluorescent data, and complex experimental metadata. Its semantic indexing capabilities (e.g., &lt;code&gt;data.sel(time='30.5min', field_of_view=1, channel='GFP').max('z')&lt;/code&gt;) make biological data analysis much more intuitive.&lt;/p&gt;
&lt;p&gt;Despite its benefits, XArray has seen limited adoption in biology due to awareness barriers and lack of biology-specific examples. Recent improvements like DataTree for hierarchical data structures and flexible indices for complex coordinate systems address many biological data needs. The roadmap includes developing biology-specific documentation and building a user community within the next year.&lt;/p&gt;
&lt;h3 id="scipy-statistical-distributions-infrastructure-albert-steppi"&gt;SciPy Statistical Distributions Infrastructure (Albert Steppi)&lt;/h3&gt;&lt;p&gt;Albert, one of SciPy's maintainers, presented the complete rewrite of SciPy's statistical distributions framework, primarily designed by Matt Haberland. The new infrastructure addresses significant limitations of the old system, including memory leaks, inflexible documentation, and parameter processing overhead.&lt;/p&gt;
&lt;p&gt;Key improvements include a single consistent API where distributions are classes users instantiate, better performance, arithmetic operations on distributions (shifting, scaling, transformations), and simplified custom distribution creation. Future development will focus on distribution-specific fitting methods and support for alternative array backends like PyTorch and JAX.&lt;/p&gt;
&lt;h3 id="high-level-api-dispatching-for-community-scaling-erik-welch"&gt;High-Level API Dispatching for Community Scaling (Erik Welch)&lt;/h3&gt;&lt;p&gt;This presentation explored how dispatching enables scaling of open source communities while managing contributor burden. The speaker shared implementation experiences with NetworkX (3-year evolution from pure Python to supporting faster implementations) and Scikit-Image (1-year implementation dispatching to NVIDIA cuCIM).&lt;/p&gt;
&lt;p&gt;The talk emphasized community engagement importance, careful bandwidth management, and maintaining balance between users, library maintainers, and backend developers. While dispatching is "deceptively simple," it requires careful consideration of nuanced implementation choices.&lt;/p&gt;
&lt;h3 id="marimo-the-future-of-notebooks-akshay-agrawal"&gt;Marimo: The Future of Notebooks (Akshay Agrawal)&lt;/h3&gt;&lt;p&gt;I was thrilled to see Marimo's founder Akshay give a talk about the future of notebooks. His live demo showcasing all of Marimo's capabilities was as gutsy as my own Data-Driven Pharma talk (which was also done entirely in a Marimo notebook).&lt;/p&gt;
&lt;p&gt;The fundamental change Marimo has brought to my workflow has been amazing. Not having to specify a separate manifest file for dependencies like with Jupyter notebooks was one of the big selling points for me. We had dinner together with a large group and got to discuss Marimo's future development - it was awesome to meet him in person and share thoughts on where the platform is heading.&lt;/p&gt;
&lt;h2 id="recording-conversations-and-networking"&gt;Recording Conversations and Networking&lt;/h2&gt;&lt;p&gt;One of my favorite activities this year was bringing my DJI mic everywhere and recording conversations with fellow attendees. Over the years, I've realized how informative and valuable these SciPy conversations are, so I decided to capture them as informal podcast content.&lt;/p&gt;
&lt;p&gt;The first recording happened over breakfast with Hugo Bowne-Anderson. We were discussing everything while eating salmon frittata - we now call it "the frittata chat." Hugo loved the idea so much that he sent it to his editor, and it will appear on his podcast "Vanishing Gradients" soon.&lt;/p&gt;
&lt;p&gt;I continued this approach with Daniel Chen (my conference doppelganger - we get mistaken for each other at every conference) and Ryan Cooper. I also had an incredible hour-and-twenty-minute conversation with Zweli, covering topics from Bayes and graphs to apartheid and parenting. While I missed some talks due to these extended conversations, that's often the real purpose of conferences - engaging in dialogue we don't usually get to have.&lt;/p&gt;
&lt;p&gt;Whether I'll release these as formal podcast episodes depends partly on my energy levels and whether the participants agree, but the conversations themselves provided immense value and captured knowledge I didn't want to lose.&lt;/p&gt;
&lt;h2 id="nerd-sniping-and-code-reviews"&gt;Nerd Sniping and Code Reviews&lt;/h2&gt;&lt;p&gt;I got thoroughly nerd-sniped by Joe Cheng, CTO of Posit, who found Llamabot and conducted an impromptu code review. We first met at the NVIDIA event while I was recording a conversation with Daniel Chen about AI education and assessment.&lt;/p&gt;
&lt;p&gt;Joe had recently decided that generative AI was a productive area for Posit and found Llamabot during his research. Standing outside the Glass Museum for half an hour, he grilled me with questions about design choices I'd never had the chance to discuss with anyone before. The nerd sniping continued over ramen takeout in the hotel lobby from Thekoi (awesome restaurant by the way!), where he asked about corners of the codebase with the thoroughness of a technical interview that I've subjected multiple people to. Talk about karma!&lt;/p&gt;
&lt;p&gt;Joe also ended up nerd sniping himself during our discussions and built something with the OpenAI real-time API that he showed me on Thursday evening. It was incredibly fun - we were on his computer together, nerding out about tweaking the real-time API settings to fit a user experience that would work with my brain, where I take a bit more time to respond and don't necessarily like the rapid-fire conversation turns.&lt;/p&gt;
&lt;p&gt;This nerd sniping cascade had a knock-on effect: it led me to implement graph-based memory for Llamabot, which then revealed that the chat memory API really wasn't optimal and needed another rewrite. There's now a 0.13 release of Llamabot planned in my head that will need to happen soon - all thanks to Joe's infectious curiosity and builder mentality!&lt;/p&gt;
&lt;h2 id="sprints"&gt;Sprints&lt;/h2&gt;&lt;p&gt;The sprints provided a chance to contribute to open source projects, though I felt more tapped out than usual this year. Despite the fatigue, I managed to make meaningful contributions to three key areas.&lt;/p&gt;
&lt;h3 id="llamabot-development"&gt;Llamabot Development&lt;/h3&gt;&lt;p&gt;Joe Cheng's nerd sniping during the conference led me to spend time during the sprints implementing graph-based memory for Llamabot. The challenge was representing conversation turns as pairs of human and AI messages while inferring the most probable message that a human is responding to when creating new branches in the conversation.&lt;/p&gt;
&lt;p&gt;I successfully implemented this graph memory system, which required determining how to connect new human messages to existing assistant messages in the conversation graph. This feature allows for more sophisticated conversation tracking and branching compared to traditional linear chat histories. You can see the implementation in this pull request: &lt;a href="https://github.com/ericmjl/llamabot/pull/226"&gt;https://github.com/ericmjl/llamabot/pull/226&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="xarray-biology-contributions"&gt;XArray Biology Contributions&lt;/h3&gt;&lt;p&gt;Inspired by Ian's talk on XArray in biology, I worked on creating Marimo notebook examples demonstrating how XArray can be effectively used for biological data analysis. This contribution aims to bridge the gap between XArray's powerful capabilities and the biology community's needs for multi-dimensional data handling.&lt;/p&gt;
&lt;p&gt;The goal was to provide concrete examples that biologists could use as starting points for their own projects, helping to increase XArray adoption in biological research by making its benefits more tangible and accessible. You can find the completed notebook at: &lt;a href="https://gist.github.com/ericmjl/e5b267782f9cbd27f712153deab426e1"&gt;https://gist.github.com/ericmjl/e5b267782f9cbd27f712153deab426e1&lt;/a&gt;&lt;/p&gt;
&lt;h3 id="teen-track-talk"&gt;Teen Track Talk&lt;/h3&gt;&lt;p&gt;Inessa Pawson asked if I would be willing to give a talk to the teens attending the conference. I shared stories about building your own tools and recounted experiences from my career journey. I told them how I got in through the back door and walked out through the front door of grad school, emphasizing how much you can learn along the way.&lt;/p&gt;
&lt;p&gt;Using the same approach from my Data-Driven Pharma talk, I showed them how I can build my own tools without relying on PowerPoint - by showing them live that I built my own slide deck generator. I shared how I picked up programming and made 70+ pull requests with the Matplotlib team, which was an incredible learning experience, and how the learning experience helped me later professionally at Novartis and Moderna, where being able to build tools for myself helped me be the change I wanted to see in the world. The goal was to inspire them to see that they too can build their own tools and, perhaps, be the change they wanted to see.&lt;/p&gt;
&lt;h2 id="conference-tidbits"&gt;Conference Tidbits&lt;/h2&gt;&lt;p&gt;A few smaller moments that captured the spirit of SciPy and the power of modern notebook sharing: I helped Hugo with a quick analysis during the conference and was able to simply airdrop him a Marimo notebook with the complete analysis. The fact that I could share a fully self-contained, executable analysis so seamlessly really demonstrated how far we've come in making scientific computing more collaborative and accessible.&lt;/p&gt;
&lt;p&gt;Another remarkable tidbit: I went to Chili Thai for the sixth and seventh time in two years, which is pretty remarkable considering that I've only been at the conference for a total of 14 days. Chili Thai really earns high ratings from me - the duck curry and the panang curry are amongst the best I've ever had.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;Attending the SciPy conference for about a decade now has been an immense resource for my career growth. Beyond being a participant, I've also been involved as an organizer, serving on the financial aid committee for almost a decade. It's my little way of giving back to a community that has given me so much, and I'm always looking for ways to contribute even more.&lt;/p&gt;
&lt;p&gt;What makes SciPy special is its incredible community of people who are curious, nerdy, and remarkably ego-free. There's a genuine spirit of learning and teaching - many are educators at heart, eager to share knowledge and help others grow. This creates an environment where meaningful connections and learning happen naturally.&lt;/p&gt;
&lt;p&gt;I'd really recommend more people attend SciPy if their company finances allow for it. The value you get from the tutorials, talks, networking, and collaborative spirit is immense. However, I do know from helping organize the conference that this year we ran at a deficit, which isn't financially sustainable. I hope we can find more sponsors for next year to keep this amazing event accessible.&lt;/p&gt;
&lt;p&gt;If possible, I'd love to help sponsor the conference, especially the Financial Aid program. Being able to bring new people to the conference - particularly community contributors who have demonstrated need - would be amazing. I was a beneficiary of financial aid myself early in my career, and it made all the difference in my ability to participate and grow within this community.&lt;/p&gt;
&lt;p&gt;The SciPy conference continues to be a cornerstone of my professional development and a source of inspiration for pushing the boundaries of what's possible with scientific computing!&lt;/p&gt;
</content></entry><entry><title>Earn the privilege to use automation</title><link href="https://ericmjl.github.io/blog/2025/7/13/earn-the-privilege-to-use-automation/" rel="alternate"/><updated>2025-07-13T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:ab61e0b2-3ffb-313f-b904-63d2b3293835</id><content type="html">&lt;p&gt;AI in education was supposed to be transformative.&lt;/p&gt;
&lt;p&gt;We imagined students with AI tutors available 24/7, personalized learning at scale, and democratized access to high-quality education. The promise was intoxicating: every student could have their own Socrates, guiding them through complex concepts with infinite patience.&lt;/p&gt;
&lt;p&gt;Then reality hit.&lt;/p&gt;
&lt;h2 id="when-ai-integration-fails-spectacularly"&gt;When AI integration fails spectacularly&lt;/h2&gt;&lt;p&gt;Lorena Barba, a respected engineering professor at George Washington University, shared her experience at SciPy 2025 of deciding to fully embrace AI in her computational engineering course. She built a custom AI tool with her technical partners, complete with document upload capabilities, retrieval augmented generation, and safety moderation features. She gave her students what seemed like the perfect educational AI assistant.&lt;/p&gt;
&lt;p&gt;The results were devastating.&lt;/p&gt;
&lt;p&gt;Her course evaluations plummeted from 4.8/5 to 2.3/5. Students stopped attending class. They stopped doing homework with any rigor. Some copied entire assignment questions, including instructions like "your code here," and expected complete answers they could submit without understanding.&lt;/p&gt;
&lt;p&gt;The most damning feedback? Students told her: "I would have learned better if AI were not present."&lt;/p&gt;
&lt;p&gt;What went wrong? Lorena had given her students unbridled access to AI without ensuring they had the foundational skills to use it effectively. Students developed what she called an "illusion of competence"—they overestimated their knowledge because AI made everything feel easy. They missed the deep processing necessary for long-term memory formation.&lt;/p&gt;
&lt;p&gt;After 20 years of successful teaching, Lorena experienced what she called a "frustrating, humbling failure." She's now considering returning to oral examinations to preserve assessment authenticity.&lt;/p&gt;
&lt;h2 id="the-assessment-validity-crisis"&gt;The assessment validity crisis&lt;/h2&gt;&lt;p&gt;Lorena's experience reveals a fundamental problem: AI has broken traditional assessment methods. If students can get AI to do their work, how do we evaluate their actual understanding? How do we conduct meaningful assessments in both educational and workplace settings?&lt;/p&gt;
&lt;p&gt;This question hits close to home for me. As a team lead, I constantly assess whether candidates are ready for the job and whether my teammates are performing at expected levels. If I'm only looking at work outputs—the final code, the completed analysis, the polished presentation—that's an inadequate assessment method. AI has made it trivially easy to produce impressive-looking outputs while learning nothing.&lt;/p&gt;
&lt;p&gt;I need to understand &lt;em&gt;how&lt;/em&gt; people think through problems, not just whether they can deliver results. This challenge sparked intense conversations with educators at SciPy 2025. Daniel Chen (University of British Columbia) and Ryan Cooper (University of Connecticut) each brought unique perspectives on adapting our assessment methods to this new reality.&lt;/p&gt;
&lt;h2 id="assessing-the-process-not-just-the-product"&gt;Assessing the process, not just the product&lt;/h2&gt;&lt;p&gt;Daniel Chen had to fundamentally shift his approach. He moved up Bloom's taxonomy for assessment, focusing on questioning and synthesis rather than factual regurgitation. His key insight: &lt;strong&gt;when students ask questions, it reveals their level of understanding&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;Insightful questions indicate pursuit of mastery. Surface-level "how do I get this done" questions reveal a lack of deep engagement.&lt;/p&gt;
&lt;p&gt;Daniel proposed assessing students through their AI chat transcripts. Instead of only evaluating final products, we could examine both process and outcome. This approach reveals &lt;em&gt;how&lt;/em&gt; students think through problems, potentially restoring validity to our assessments.&lt;/p&gt;
&lt;p&gt;Ryan Cooper had already started implementing this idea, collecting chat transcripts to understand student thinking patterns. He also experimented with having students generate their own exam questions—leveraging the fact that creation sits at the highest level of Bloom's taxonomy.&lt;/p&gt;
&lt;p&gt;Ryan gave students access to a curated AI system conditioned with course context, generating on-the-fly assessment questions. While innovative, he encountered challenges with rubric-based grading when AI suggested grades without clear criteria.&lt;/p&gt;
&lt;h2 id="why-this-matters-in-the-workplace"&gt;Why this matters in the workplace&lt;/h2&gt;&lt;p&gt;These educational assessment challenges directly mirror my daily reality as a team lead. AI assistance allows me to work solo and move incredibly fast—I love that turbocharged feeling. But this speed creates a dangerous blind spot that affects both my personal development and my team's growth.&lt;/p&gt;
&lt;p&gt;Here's my dilemma: if I don't slow down to demonstrate my thinking process, we lose opportunities to train junior team members. More concerning, if I can't see how my team members approach problems—only their final outputs—I can't effectively assess their capabilities or guide their development.&lt;/p&gt;
&lt;p&gt;When team members use GitHub Copilot or similar tools, I need visibility into their thought processes, not just their code. Are they asking insightful questions? Do they understand the trade-offs they're making? Can they spot when the AI suggests something problematic? Without access to their reasoning process, I'm essentially conducting performance reviews based on AI-assisted outputs rather than human capability.&lt;/p&gt;
&lt;p&gt;This visibility gap threatens knowledge transfer and continuity. We risk training a generation of practitioners who can orchestrate AI to produce impressive results but lack the foundational understanding to innovate when the tools fail or evolve.&lt;/p&gt;
&lt;h2 id="earning-the-privilege-of-automation"&gt;Earning the privilege of automation&lt;/h2&gt;&lt;p&gt;The solution to this assessment crisis—both educational and professional—isn't to ban AI tools or ignore their impact. Instead, we need a fundamental shift in how we think about automation access.&lt;/p&gt;
&lt;p&gt;Here's the central insight that crystallized from these conversations: &lt;strong&gt;people must earn the privilege to use automation&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;The use of large language models for coding is automated code drafting. If you lack the skills to evaluate and verify correctness, you shouldn't use LLMs for anything important. This isn't about restricting access, but rather, it's about ensuring people develop foundational competencies first, then demonstrate those competencies before gaining access to powerful automation.&lt;/p&gt;
&lt;p&gt;The principle is straightforward: &lt;strong&gt;demonstrate you can verify AI output before using AI for critical work.&lt;/strong&gt; This means:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Understanding underlying concepts well enough to spot errors&lt;/li&gt;
&lt;li&gt;Having skills to validate AI-generated solutions&lt;/li&gt;
&lt;li&gt;Developing judgment to recognize when something doesn't make sense&lt;/li&gt;
&lt;li&gt;Building fortitude to dig deeper when results seem questionable&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I'm fine with "vibe coding" in unfamiliar languages for throwaway explorations—that's valuable for learning. But for work that matters, the ability to verify correctness is non-negotiable.&lt;/p&gt;
&lt;h2 id="the-path-forward"&gt;The path forward&lt;/h2&gt;&lt;p&gt;Lorena's lessons teach us that unrestricted AI access without foundational skills leads to degraded learning outcomes. We need systematic approaches to ensure people earn their automation privileges. These include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Moving assessments up Bloom's taxonomy&lt;/strong&gt; to focus on higher-order thinking skills that AI can't easily replicate&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evaluating process alongside product&lt;/strong&gt; through chat transcript analysis and collaborative work&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Encouraging creation and synthesis&lt;/strong&gt; rather than regurgitation—have students generate exam questions, not just answer them&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Implementing pair programming and mentoring&lt;/strong&gt; that reveals thinking patterns and preserves knowledge transfer&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Maintaining human elements&lt;/strong&gt; in learning and development to counteract AI's tendency to create isolated workers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The future belongs to those who can effectively collaborate with AI while maintaining the critical thinking skills to guide and verify that collaboration. But they must demonstrate mastery of fundamentals before earning that privilege.&lt;/p&gt;
&lt;p&gt;We're not trying to halt progress or ban useful tools. We're ensuring that powerful automation serves human capability rather than replacing it.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;With thanks to Lorena Barba (George Washington University), Daniel Chen (University of British Columbia), Ryan Cooper (University of Connecticut), and Emily Dorne (Driven Data) for sharing their experiences and insights. Their perspectives as professional educators and industry practitioners navigating AI's impact on learning, assessment, and hiring shaped these reflections.&lt;/em&gt;&lt;/p&gt;
</content></entry><entry><title>The job your docs need to do</title><link href="https://ericmjl.github.io/blog/2025/7/7/the-job-your-docs-need-to-do/" rel="alternate"/><updated>2025-07-07T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:3253a2a7-3c77-3b72-b320-3aa34f3c80d0</id><content type="html">&lt;h1 id="what-is-the-job-that-your-docs-need-to-do"&gt;What is the job that your docs need to do?&lt;/h1&gt;&lt;p&gt;Two threads have been running through my mind recently, and I keep finding connections between them that I can't shake. The first is Diataxis - a structured framework for documentation that divides all docs into four distinct types: tutorials, how-to guides, reference, and explanation. The second is Clayton Christensen's jobs theory, which asks a deceptively simple question: what is the job that your customer needs to get done?&lt;/p&gt;
&lt;p&gt;Side note: I've been heavily inspired by Clayton Christensen's books recently, and have audiobooked my way through Innovator's Dilemma/Solution/DNA, as well as Competing Against Luck. All good books, 100% recommended if you're interested in understanding how innovation actually works.&lt;/p&gt;
&lt;p&gt;Here's the key insight: your documentation isn't competing with other documentation. It's competing with every other way someone could accomplish their job.&lt;/p&gt;
&lt;h2 id="the-competition-you-didn-t-know-you-had"&gt;The competition you didn't know you had&lt;/h2&gt;&lt;p&gt;When someone opens your internal documentation, they're looking for more than information. They're trying to accomplish something specific, and they're evaluating whether your docs are the right tool for that job.&lt;/p&gt;
&lt;p&gt;For internal company documentation—whether it's for internally built software, processes, or systems—the competition is different but equally real. Your how-to guide competes with asking a colleague, digging through Slack history, or reverse-engineering from existing code. Your reference docs compete with reading the source code directly, checking configuration files, or experimenting in a staging environment.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;&lt;strong&gt;Diataxis Doc Type&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Jobs to be Done (JTBD)&lt;/strong&gt;&lt;/th&gt;
&lt;th&gt;&lt;strong&gt;Alternative Product Categories That Could Be Hired&lt;/strong&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;How-to Guides&lt;/td&gt;
&lt;td&gt;"Show me how to achieve a specific outcome."&lt;/td&gt;
&lt;td&gt;Asking a colleague, Slack/Teams search, reverse-engineering from existing code, trial and error in staging&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Reference&lt;/td&gt;
&lt;td&gt;"Give me exact technical information I can look up quickly."&lt;/td&gt;
&lt;td&gt;Reading source code, checking config files, database schemas, API endpoint testing, environment variables&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Explanation&lt;/td&gt;
&lt;td&gt;"Help me understand how/why it works."&lt;/td&gt;
&lt;td&gt;Architecture diagrams, code comments, git history, team knowledge sharing sessions, design documents&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Tutorials&lt;/td&gt;
&lt;td&gt;"Help me learn by doing, in a safe structured way."&lt;/td&gt;
&lt;td&gt;Pair programming, shadowing a colleague, sandbox environments, local development setup walkthroughs&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Once you see this competition, it reframes how you think about documentation structure.&lt;/p&gt;
&lt;h2 id="the-opportunity-hiding-in-plain-sight"&gt;The opportunity hiding in plain sight&lt;/h2&gt;&lt;p&gt;Here's what's fascinating about internal documentation: the competition is actually pretty terrible. Think about it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Asking a colleague interrupts their work and creates context switching for both of you&lt;/li&gt;
&lt;li&gt;Digging through Slack history is time-consuming and often incomplete&lt;/li&gt;
&lt;li&gt;Reverse-engineering from existing code is slow and error-prone&lt;/li&gt;
&lt;li&gt;Trial and error in staging environments wastes time and resources&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This means that even moderately good internal documentation has a much lower bar to clear than external documentation. Your internal how-to guide doesn't need to compete with polished YouTube tutorials - it just needs to be better than interrupting Sarah from accounting or spending 30 minutes searching through #engineering-general.&lt;/p&gt;
&lt;p&gt;This is actually a huge opportunity. While external documentation faces fierce competition from Stack Overflow's crowdsourced answers and professionally produced tutorials, internal documentation often competes with... nothing systematic at all.&lt;/p&gt;
&lt;p&gt;The result? Even basic improvements to internal documentation can have outsized impact on team productivity. When your reference docs are faster than reading source code, people will use them. When your how-to guides are clearer than tribal knowledge, they become the default choice.&lt;/p&gt;
&lt;h2 id="understanding-the-competition"&gt;Understanding the competition&lt;/h2&gt;&lt;p&gt;Most documentation is written from the perspective of the product being documented. It's organized around features, capabilities, and technical architecture. But when you flip the perspective to focus on jobs-to-be-done, you can structure information more effectively around what readers actually need to accomplish.&lt;/p&gt;
&lt;h2 id="a-job-focused-approach-in-practice"&gt;A job-focused approach in practice&lt;/h2&gt;&lt;p&gt;Let me show you what this looks like in practice. Say you're writing a how-to guide for deploying your company's internal microservice. The traditional approach focuses on what information to include. The job-focused approach starts with the specific outcome: "Help me get this service deployed so I can test my feature and merge my PR."&lt;/p&gt;
&lt;p&gt;That job-focused lens shifts how you structure the guide:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;You lead with the most common use case and a working example first, then dive into edge cases&lt;/li&gt;
&lt;li&gt;You include troubleshooting steps for the most common failure modes&lt;/li&gt;
&lt;li&gt;You assume they're in a hurry and want to get back to their main project&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Every decision gets filtered through the lens of "does this help the reader accomplish their job better?"&lt;/p&gt;
&lt;p&gt;The result? Documentation that people actually use because it's genuinely better at helping them accomplish their jobs.&lt;/p&gt;
&lt;h2 id="how-to-apply-this-framework"&gt;How to apply this framework&lt;/h2&gt;&lt;p&gt;Start by identifying the specific job your reader is trying to accomplish. Not the general topic area, but the specific outcome they need to achieve. Then ask yourself:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What alternatives could they hire instead of your documentation?&lt;/li&gt;
&lt;li&gt;What unique value can your documentation provide that those alternatives can't?&lt;/li&gt;
&lt;li&gt;How can you structure the information to make their job easier to accomplish?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;For that third question especially, consider using AI to help you think through different structural approaches. You can prompt an AI with the specific job your reader needs to accomplish and ask it to suggest multiple ways to organize the information, then choose the approach that best serves that job.&lt;/p&gt;
&lt;p&gt;Take reference documentation as an example. The job goes beyond providing comprehensive information about all internal API endpoints or configuration options. The real job is "give me exact technical information I can look up quickly." This means your reference docs need to be faster and more precise than someone could get from reading your source code, checking configuration files, or asking in Slack.&lt;/p&gt;
&lt;p&gt;If someone can figure out what they need faster by just reading the source code or asking a colleague, your reference docs aren't doing their job.&lt;/p&gt;
&lt;h2 id="how-this-applies-to-ai-assisted-documentation"&gt;How this applies to AI-assisted documentation&lt;/h2&gt;&lt;p&gt;Here's where this gets really interesting. When documentation is designed around jobs-to-be-done, it creates a positive feedback loop that extends beyond the direct reader.&lt;/p&gt;
&lt;p&gt;Well-structured, job-focused documentation helps humans and also helps AI systems understand context and provide better assistance to future users. When your how-to guide is crystal clear about the specific outcome it helps achieve, an AI can better understand when to recommend that guide to someone with a similar job.&lt;/p&gt;
&lt;p&gt;The result is that good documentation becomes a force multiplier. Beyond helping the direct reader, it helps AI systems help other readers accomplish similar jobs faster and more accurately.&lt;/p&gt;
&lt;p&gt;This creates a flywheel effect: better documentation helps more people accomplish their jobs, which generates more usage data and feedback, which leads to even better documentation that helps both humans and AI serve users more effectively.&lt;/p&gt;
&lt;h2 id="applying-this-perspective"&gt;Applying this perspective&lt;/h2&gt;&lt;p&gt;The next time you write internal documentation, start with what job your reader is trying to accomplish rather than what you want to explain.&lt;/p&gt;
&lt;p&gt;Ask yourself: if someone could accomplish this job faster or more reliably using a different approach, why would they choose your documentation instead? For internal docs, this question often has a surprising answer: because the alternatives are genuinely worse.&lt;/p&gt;
&lt;p&gt;This is liberating. Your internal documentation doesn't need to be perfect - it just needs to be better than the current chaos of tribal knowledge and ad-hoc problem-solving.&lt;/p&gt;
&lt;p&gt;When you can answer that question clearly, you'll write documentation that people find genuinely useful. And when people use your documentation successfully, they become more successful with your product.&lt;/p&gt;
&lt;p&gt;This matters because documentation is ultimately about scaling our collective knowledge and decision-making capacity. But that scaling only happens when people actually use the documentation. And people only use documentation when it helps them accomplish specific jobs they need to get done.&lt;/p&gt;
&lt;p&gt;For internal documentation, this scaling opportunity is especially significant. Every time someone uses your docs instead of interrupting a colleague, you're not just solving one person's problem - you're preserving focus and momentum across your entire team.&lt;/p&gt;
&lt;p&gt;That's the value of thinking about internal documentation as a product designed around jobs-to-be-done. It creates a better experience for everyone who interacts with your work, and unlike external documentation, you don't need to beat world-class competition to succeed.&lt;/p&gt;
</content></entry><entry><title>One hour and eight minutes: Building a receipt scanner with the weirdest tech stack imaginable</title><link href="https://ericmjl.github.io/blog/2025/7/1/one-hour-and-eight-minutes-building-a-receipt-scanner-with-the-weirdest-tech-stack-imaginable/" rel="alternate"/><updated>2025-07-01T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:a195e873-9317-3fef-b7e0-a0c3c634425f</id><content type="html">&lt;p&gt;After bouncing between Cursor and GitHub Copilot for the past couple of years, I kept hearing about Claude Code. People's experiences were really piquing my curiosity, so I decided to give it a shot. What happened next completely changed how I think about rapid prototyping.&lt;/p&gt;
&lt;p&gt;I built a fully functional receipt scanning and expense tracking app in exactly one hour and eight minutes. But here's the kicker—I used a technology stack so unconventional that most developers would probably laugh at me. And it worked beautifully.&lt;/p&gt;
&lt;p&gt;Let me tell you what I learned about the immersive power of terminal-based development and why weird tech combinations might be the secret to lightning-fast tool building.&lt;/p&gt;
&lt;h2 id="the-problem-i-wanted-to-solve"&gt;The problem I wanted to solve&lt;/h2&gt;&lt;p&gt;At work, I noticed SAP Concur can automatically extract fields from uploaded receipts. I thought, "What if I could replicate that at home?" I wanted to track my expenses without paying for QuickBooks, using Notion as my database instead.&lt;/p&gt;
&lt;p&gt;Most developers would reach for the standard stack: React frontend, PostgreSQL backend, maybe throw in some Express.js. That's the sensible approach.&lt;/p&gt;
&lt;p&gt;But I'm not building production software for thousands of users. I'm a data scientist experimenting with tools for myself. So I decided to get weird with it.&lt;/p&gt;
&lt;h2 id="the-stack-that-shouldn-t-work-but-does"&gt;The stack that shouldn't work but does&lt;/h2&gt;&lt;p&gt;Here's what Claude Code helped me build with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;FastAPI&lt;/strong&gt; for the backend (this part makes sense)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;HTMX&lt;/strong&gt; for the frontend instead of React (getting unusual)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Vanilla HTML/CSS&lt;/strong&gt; with minimal JavaScript (now we're talking)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;LlamaBot&lt;/strong&gt; for AI interactions (I made it, so I know it works)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Notion&lt;/strong&gt; as the database (yes, you read that right)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If I were to describe this stack to a seasoned developer, they'd probably be surprised, then laugh out loud, and then go "what?" But when I described it to Claude Code and specified that I wanted everything in a single &lt;code&gt;app.py&lt;/code&gt; file that I could run with &lt;code&gt;uv run app.py&lt;/code&gt;, Claude Code got creative.&lt;/p&gt;
&lt;p&gt;It generated a beautiful single-file application with PEP 723 metadata at the top. The code was clean and well-structured. It took a few iterations of AI-generated code writing followed by testing, but it was always generally headed in the right direction. And this is the result:&lt;/p&gt;
&lt;p&gt;&lt;img src="screenshot.webp" alt=""&gt;&lt;/p&gt;
&lt;h2 id="the-development-experience-that-changed-everything"&gt;The development experience that changed everything&lt;/h2&gt;&lt;p&gt;Here's what blew my mind about using Claude Code: the immersive experience.&lt;/p&gt;
&lt;p&gt;I spent the entire development session in just two terminal tabs. One tab running Claude Code, another tab with my &lt;code&gt;uvicorn&lt;/code&gt; server running with auto-reload. That's it. No switching between file explorers, no hunting through directory structures, no context switching between different applications.&lt;/p&gt;
&lt;p&gt;I was in what I can only describe as "vibe-ish coding" mode—not quite the &lt;a href="https://simonwillison.net/2025/Mar/19/vibe-coding/"&gt;vibe coding that Simon Willison describes&lt;/a&gt;, but close. I'd type a request to Claude Code, see the changes instantly in my browser, then iterate. The feedback loop was immediate and distraction-free.&lt;/p&gt;
&lt;p&gt;This terminal-focused workflow kept me in the zone in a way that traditional IDEs never have. Without all the little icons, bells, and whistles that can distract you in an IDE, I could maintain focus on the actual problem I was solving instead of fighting with tools.&lt;/p&gt;
&lt;h2 id="what-got-built-in-68-minutes"&gt;What got built in 68 minutes&lt;/h2&gt;&lt;p&gt;By the time my terminal session ended, I had a fully functional application that could:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Upload single or multiple receipt images&lt;/li&gt;
&lt;li&gt;Extract expense data using LlamaBot's AI capabilities&lt;/li&gt;
&lt;li&gt;Allow manual editing of fields that the AI got wrong (inside Notion)&lt;/li&gt;
&lt;li&gt;Handle enumerated types for expense categories&lt;/li&gt;
&lt;li&gt;Automatically populate a Notion database with extracted data&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The AI integration was seamless. I provided my OpenAI API key, and through LlamaBot I was able to hit the OpenAI API while Claude Code handled all the integration complexity. When I needed to add file upload functionality, I pasted some Notion API documentation as context, and Claude Code implemented it correctly.&lt;/p&gt;
&lt;p&gt;The end result? I can now drag and drop receipts into a web interface, hit submit, and watch the data appear automatically in my Notion expense tracker. Exactly what I wanted.&lt;/p&gt;
&lt;h2 id="pushing-language-models-to-their-limits"&gt;Pushing language models to their limits&lt;/h2&gt;&lt;p&gt;Here's the thing that really fascinated me about this experiment: I deliberately chose this weird tech stack to test Claude Code's boundaries.&lt;/p&gt;
&lt;p&gt;Think about it—if I had gone with React, Node.js, and PostgreSQL, that would be easy for any language model. Those patterns show up constantly in training data.&lt;/p&gt;
&lt;p&gt;But I wanted to push to the edges. What happens when you combine technologies that people don't usually think about together? HTMX with FastAPI? Notion as a database backend? A single-file Python app doing receipt processing with AI?&lt;/p&gt;
&lt;p&gt;This is uncharted territory for most language models. There aren't thousands of tutorials showing how to integrate LlamaBot with HTMX forms, or how to structure FastAPI routes that return HTML fragments for dynamic updates.&lt;/p&gt;
&lt;p&gt;Yet Claude Code handled it beautifully. It figured out how to make these disparate pieces work together, even when the combination got weird.&lt;/p&gt;
&lt;h2 id="why-this-matters-for-tool-building"&gt;Why this matters for tool building&lt;/h2&gt;&lt;p&gt;This experience reinforced something I've been thinking about lately: the best time to build custom tools is right now, and the barrier to entry has never been lower.&lt;/p&gt;
&lt;p&gt;I wrote about this recently in my post on building your own tools with AI coding assistants. If you need a tool, just build it. Don't wait for the perfect stack or the right framework. Pick technologies that let you move fast and iterate quickly.&lt;/p&gt;
&lt;p&gt;The ability to combine unusual technologies successfully opens up new possibilities. Instead of being constrained by conventional wisdom about what technologies "should" work together, you can experiment with combinations that solve your specific problem elegantly.&lt;/p&gt;
&lt;h2 id="the-immersive-development-advantage"&gt;The immersive development advantage&lt;/h2&gt;&lt;p&gt;The most valuable lesson from this experiment wasn't about technology—it was about workflow.&lt;/p&gt;
&lt;p&gt;Claude Code's terminal-based approach created an immersive development environment that kept me focused. No file system distractions, no IDE complexity, just pure problem-solving in a clean interface.&lt;/p&gt;
&lt;p&gt;This suggests that tool choice matters more than we often acknowledge. The best coding assistant isn't necessarily the one with the most features—it's the one that keeps you in flow state while you build.&lt;/p&gt;
&lt;h2 id="what-s-next"&gt;What's next&lt;/h2&gt;&lt;p&gt;I'm already planning my next experiment with Claude Code. Maybe a document processing pipeline using Docling, Anthropic's API, and Airtable. Or a personal CRM built with FastAPI, HTMX, and Google Sheets as the backend.&lt;/p&gt;
&lt;p&gt;The point isn't to build production-ready applications with these stacks. It's to explore what becomes possible when you remove the friction from experimentation.&lt;/p&gt;
&lt;p&gt;In an hour and eight minutes, I went from idea to working application. That's the kind of development velocity that changes what you're willing to attempt.&lt;/p&gt;
&lt;p&gt;Sometimes the weirdest combinations turn out to be exactly what you need.&lt;/p&gt;
</content></entry><entry><title>Build your own tools!</title><link href="https://ericmjl.github.io/blog/2025/6/27/build-your-own-tools/" rel="alternate"/><updated>2025-06-27T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:bdee977f-1cea-30bd-8d27-2c5c3cf1032a</id><content type="html">&lt;p&gt;On 25 June 2025, I delivered a talk at Data-Driven Pharma, an event organized by &lt;a href="https://www.linkedin.com/in/dricaptain/"&gt;Ilya Captain&lt;/a&gt; and the namesake Data-Driven Pharma organization. In the run-up to the talk, I had been reflecting on two points:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I hate making slides, and&lt;/li&gt;
&lt;li&gt;I really love building tools.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;To that end, I decided... well, I'm not going to bother with making slides. And I'll build a tool that makes slides for me instead. Hence [DeckBot], which currently lives in a marimo notebook, was born. I started off by telling the crowd how much I hated making slides:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;In an age of LLMs and plain .txt, I understand why I have such a disdain for powerpoint: you can't easily automate their creation, there's too much that can be hidden behind a bullet point, and it's just an all-round ineffective media for &lt;em&gt;lasting&lt;/em&gt; crystal clear communication. By contrast, Markdown slides are better.&lt;/p&gt;
&lt;p&gt;-- Original post link &lt;a href="https://www.linkedin.com/feed/update/urn:li:activity:7335296923488194561?trk=public_post_embed_social-actions-reactions"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And how even Andrej Karpathy laments the absence of an LLM-enabled tool for building slides:&lt;/p&gt;
&lt;p&gt;&lt;blockquote class="twitter-tweet"&gt;&lt;p lang="en" dir="ltr"&gt;Making slides manually feels especially painful now that you know Cursor for slides should exist but doesn’t.&lt;/p&gt;&amp;mdash; Andrej Karpathy (@karpathy) &lt;a href="https://twitter.com/karpathy/status/1931042840966222046?ref_src=twsrc%5Etfw"&gt;June 6, 2025&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;Also, my informal poll of the audience revealed that approximately 2/3 of the crowd also hated making slides. Not surprising!&lt;/p&gt;
&lt;p&gt;So I decided to take that as a nerdsnipe and actually make DeckBot. After showing the audience (live!) how I can make rando slides for completely nondescript topics, such as, "Why eating well is so important" or "pros and cons of buying a thing", I then proceeded with the real exciting challenge of this talk: to get an LLM to generate my entire slide deck for the actual topic I wanted to talk about, from which I would present. And that topic was, well, "Build your own tools!". I then proceeded to copy/paste in the first draft of this blog post into the notebook, and 1 minute later, I had my slides, from which I presented live.&lt;/p&gt;
&lt;p&gt;Below is a writeup of what I actually presented, including a written description of some of the interactions.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;My main message to everybody today is this: If you're a data scientist, computational biologist, or software developer, you should learn how to build your own tools. Building your own tools is a liberating endeavor. It injects joy back into your day-to-day work. People were made to be creative creators. Build your own tools.&lt;/p&gt;
&lt;h2 id="a-flashback-from-my-grad-school-days"&gt;A flashback from my grad school days&lt;/h2&gt;&lt;p&gt;&lt;img src="https://ericmjl.github.io/nxviz/images/circos.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;Do you know what this diagram is? The audience came in clutch, many people knew what this was -- it's a Circos plot. Some may have seen it with arcs rather than dots around the edges, but the concept remains the same: prioritize ordering nodes and then draw in the edges.&lt;/p&gt;
&lt;p&gt;I wanted to learn how to make a graph visualization like this. But the only tool I saw out there was written in a different language (Perl), had no Python bindings, and was way too complicated for me—a beginner programmer in 2014—to learn. So I decided to leverage two other tools that I knew at the time, Python and matplotlib, to make my own Python package, both to learn software development and to understand the principles of rational network visualization.&lt;/p&gt;
&lt;p&gt;The precursor to nxviz, &lt;code&gt;circosplot&lt;/code&gt;, was born in 2015. One year later, I knew enough to make all sorts of network visualizations!&lt;/p&gt;
&lt;p&gt;Like this, the matrix plot:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ericmjl.github.io/nxviz/examples/matrix/output_4_0.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;Or this, a geo plot:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ericmjl.github.io/nxviz/examples/geo/output_6_1.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;Or this, an arc plot:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ericmjl.github.io/nxviz/examples/arc_node_labels/output_2_1.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;Or this, another circos plot:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ericmjl.github.io/nxviz/examples/circos_node_labels/output_3_0.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;Or this beautiful thing, a hive plot:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ericmjl.github.io/nxviz/api/high-level-api/output_12_0.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;What's the unifying thread behind all of those plots? As it turns out, the thing I learned while building my own graph visualization tool was that &lt;strong&gt;rational and beautiful graph visualization starts with knowing how to order &lt;em&gt;nodes&lt;/em&gt; in a graph, and then drawing in the edges&lt;/strong&gt;. I would have never learned that had I not attempted to reinvent the wheel (or, perhaps, Circos plots)! Additionally, being able to build my own Python package was superbly empowering, especially as a graduate student! I could build my own tools, archive them in the public domain, and never have to solve the same problem again. This echoed Simon Willison's approach to software development:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I realized that one of the best things about open source software is that you can solve a problem once and then you can slap an open source license on that solution and you will &lt;em&gt;never&lt;/em&gt; have to solve that problem ever again, no matter who's employing you in the future.&lt;/p&gt;
&lt;p&gt;It's a sneaky way of solving a problem permanently.&lt;/p&gt;
&lt;p&gt;-- Original post link by Simon Willison &lt;a href="https://simonwillison.net/2025/Jan/24/selfish-open-source/"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;If I didn't know how to build my own tools, I'd have been stuck, and I'd never have learned anything new.&lt;/p&gt;
&lt;h2 id="fast-forward-to-2018-at-novartis"&gt;Fast-forward to 2018 at Novartis&lt;/h2&gt;&lt;p&gt;My colleague Brant Peterson showed me the R package &lt;code&gt;janitor&lt;/code&gt;, and I thought, "Why can't Pythonistas have nice things?"&lt;/p&gt;
&lt;p&gt;Then, I remembered Gandhi's admonition&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;"Be the change you wish to see in the world."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And so, &lt;code&gt;pyjanitor&lt;/code&gt; was born.&lt;/p&gt;
&lt;p&gt;Your dataframe manipulation and processing code can now be more expressive than native pandas:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;from_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;company_sales&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;remove_columns&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Company1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;dropna&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;subset&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Company2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Company3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rename_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Company2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Amazon&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;rename_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Company3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Facebook&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_column&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Google&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;450.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;550.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;800.0&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;By being the change I wanted to see, Pythonistas now have one more nice thing available to them.&lt;/p&gt;
&lt;p&gt;And of course, I just &lt;em&gt;had&lt;/em&gt; to inject this in: that was all in 2018.&lt;/p&gt;
&lt;p&gt;It's now 2025. Use polars. :)&lt;/p&gt;
&lt;h2 id="building-resilience-at-moderna"&gt;Building resilience at Moderna&lt;/h2&gt;&lt;p&gt;Fast-forward to 2021. I joined Moderna, attracted by the forward-thinking Digital leadership and their suite of high-power home-grown tools. It was a dog-fooding culture back then—one I've fought hard to keep alive within the Digital organization.&lt;/p&gt;
&lt;p&gt;Since I was only data scientist #6 at Moderna and was hired into a relatively senior role (Principal Data Scientist), I saw the chance to set standards for Moderna data scientists.&lt;/p&gt;
&lt;p&gt;Together with my wonderful colleague &lt;a href="https://www.linkedin.com/in/adriannaloback/"&gt;Adrianna Loback&lt;/a&gt; and our manager &lt;a href="https://www.linkedin.com/in/giessel/"&gt;Andrew Giessel&lt;/a&gt;, we hammered out what Data Scientists would ship: dockerized CLI tools run in the cloud, and Python packages, and designed our entire project initialization workflow around deploying those two things. As time progressed, the tooling evolved, and &lt;a href="https://www.linkedin.com/in/dandluu/"&gt;Dan Luu&lt;/a&gt; helped us be a caretaker of the tooling as well, continually improving it and modernizing it.&lt;/p&gt;
&lt;p&gt;By standardizing on what we ship and then standardizing on the toolchain, we implemented a design pattern that made it easy for us to help one another. I can jump into a colleague's codebase dealing with Clinical Development and be helpful in a modestly short amount of time, even when I mostly work on Research projects.&lt;/p&gt;
&lt;p&gt;And here's a side effect: we designed a portable way of working that works best when you give a Moderna data scientist access to a raw Linux machine. As Andrew Giessel once mentioned to me:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Eventually, tools that abstract away the Linux operating system will fail to satisfy users as they grow up and master Linux. They'll want to jump out of a container and just run raw Linux. Anything that tries to abstract away the filesystem, shell scripts, and more eventually runs into edge cases, so why not just give people access to a raw Linux machine with tools pre-installed?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;As it turns out, this evening's other presenter &lt;a href="https://www.linkedin.com/in/%F0%9F%8E%AF-ming-tommy-tang-40650014/"&gt;Tommy Tang&lt;/a&gt; is also a big fan of the shell:&lt;/p&gt;
&lt;iframe src="https://www.linkedin.com/embed/feed/update/urn:li:share:7341052068264128513?collapsed=0" height="265" width="504" frameborder="0" allowfullscreen="" title="Embedded post"&gt;&lt;/iframe&gt;&lt;p&gt;So now I'm a big fan of giving people access to a raw Linux box, outside of a sandboxed container. Being able to build and run a container is a fundamental skill nowadays—so much so that as a community of data scientists, we've effectively said "no" to vendor tooling that forces us to do our day-to-day work within a Docker container.&lt;/p&gt;
&lt;p&gt;And here's the most awesome part: we did this in an "internally open source" fashion. &lt;em&gt;Anyone&lt;/em&gt; with a complaint about the tooling can propose a fix to our tools. Even better, we'll walk you through making the fix "the right way," so you gain the superpower of software development along the way!&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/3ZTGwcHQfLY?si=_FLzvFyCp88ZlzGm" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen&gt;&lt;/iframe&gt;&lt;p&gt;At least on this dimension, we are never beholden to someone else's (or a vendor's) roadmap! We are now &lt;em&gt;resilient&lt;/em&gt;—just like Dustin from Smarter Every Day described when he made this video about trying to make things "in America."&lt;/p&gt;
&lt;p&gt;I'll end this section with a huge lesson I've learned during my time working here:&lt;/p&gt;
&lt;iframe src="https://www.linkedin.com/embed/feed/update/urn:li:share:7337223460651220992" height="349" width="504" frameborder="0" allowfullscreen="" title="Embedded post"&gt;&lt;/iframe&gt;&lt;h2 id="building-teaches-you-the-domain"&gt;Building teaches you the domain&lt;/h2&gt;&lt;p&gt;Do you remember these beautiful graph diagrams from earlier?&lt;/p&gt;
&lt;p&gt;&lt;img src="https://ericmjl.github.io/nxviz/examples/arc_node_labels/output_2_1.png" alt=""&gt;&lt;/p&gt;
&lt;p&gt;Building is a great way to learn new things. Building nxviz helped me learn the principles of graph visualization. Building LlamaBot helped me learn about making LLM applications.&lt;/p&gt;
&lt;p&gt;In 2023, I created LlamaBot because I was confused about how to interact with and build LLMs, particularly RAG applications. I decided to turn to my favorite learning tool: building software. This was clarifying—I was forced to encode my understanding into code, and if the code did unexpected things, I knew my understanding was wrong. After all:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Computers are the best students there are. If you teach the computer something wrong, it'll give you back wrong answers. If you design things wrongly, this student will make life hard for you. So you learn to get good at verification.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;I've rewritten LlamaBot at least 4 times, each time updating the codebase with the best of my knowledge. Each time round, my understanding improved, and the abstractions changed along with them, and the ergonomics of using LlamaBot got better, more natural. Throughout the changes, some things that have stayed constant:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The "Bot" analogy, which predates the term "agents," turns out to be a natural way to express Agents.&lt;/li&gt;
&lt;li&gt;The docstore abstraction simplifies storage and retrieval for pure text applications.&lt;/li&gt;
&lt;li&gt;My distaste for writing commit messages and release notes—hence the automated writers for both remain deeply ingrained as dog-fooded tools.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Some things that have evolved:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;QueryBot used to do entire RAG workflows all-in-one—from PDF-to-text conversion to embedding to retrieval. I've since learned it's much better to break those out into separate steps.&lt;/li&gt;
&lt;li&gt;ChatBot used to have a built-in ChatUI. I dropped it because it was too opinionated and unwieldy. Marimo has really good chat UI primitives that should be used instead.&lt;/li&gt;
&lt;li&gt;Inspiration from the &lt;code&gt;ell&lt;/code&gt; library: &lt;code&gt;lmb.user("some prompt")&lt;/code&gt; or &lt;code&gt;lmb.system("some prompt")&lt;/code&gt; for convenient creation of system and user prompts.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In the process of building and designing software, we have to learn the domain so well that we become linguistics experts in that domain. Vocabulary, terms, and their relationships become natural extensions of what we already know. If our code maps to the domain properly, our abstractions become so natural they're self-documenting. If our code maps poorly onto a solid understanding of the problem space, it'll end up being a tangled mess that warrants a rewrite. There's nothing wrong with that! Embrace the need to rewrite—with AI assistance nowadays, the activation energy barriers to building your own tools is dramatically reduced.&lt;/p&gt;
&lt;h2 id="internal-tooling-requires-organizational-buy-in"&gt;Internal tooling requires organizational buy-in&lt;/h2&gt;&lt;p&gt;I then made my next point: you want to make sure you have organizational buy-in to any tool building efforts. It's super telling if your line management doesn't agree with you. On the other hand, it's super awesome if someone is going to be hired explicitly for tooling, like at Quora below:&lt;/p&gt;
&lt;p&gt;&lt;blockquote class="twitter-tweet"&gt;&lt;p lang="en" dir="ltr"&gt;We are opening up a new role at Quora: a single engineer who will use AI to automate manual work across the company and increase employee productivity. I will work closely with this person. &lt;a href="https://t.co/iKurWS6W7v"&gt;pic.twitter.com/iKurWS6W7v&lt;/a&gt;&lt;/p&gt;&amp;mdash; Adam D&amp;#39;Angelo (@adamdangelo) &lt;a href="https://twitter.com/adamdangelo/status/1936504553916309617?ref_src=twsrc%5Etfw"&gt;June 21, 2025&lt;/a&gt;&lt;/blockquote&gt; &lt;script async src="https://platform.twitter.com/widgets.js" charset="utf-8"&gt;&lt;/script&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src="https://pbs.twimg.com/media/Gt_VT5nakAANTdj?format=png&amp;amp;name=large" alt=""&gt;&lt;/p&gt;
&lt;p&gt;Reading this tweet triggered a thought in my mind: sustaining internal tool builds help with organizational buy-in. Does your organization empower you to build the tools you need to get your work done? I was lucky to have full leadership buy-in through Andrew Giessel and Dave Johnson, and my current manager Wade keeps roadblocks away from innovating on how we work. I also try to encourage this across teams I have influence with, even without direct managerial responsibilities.&lt;/p&gt;
&lt;p&gt;But as I also mentioned earlier, even though sustaining an internal tool build can be boosted with organizational buy-in, &lt;em&gt;culture needs no permission&lt;/em&gt;. We always have agency. We always have the free will to make things happen. We always can go forth and build. Build the smallest thing that gets roadblocks out of your way and move on. Throwaway builds are OK! No permission required.&lt;/p&gt;
&lt;h2 id="expert-practitioners-agree-build-your-own-tools"&gt;Expert practitioners agree: build your own tools&lt;/h2&gt;&lt;p&gt;If my arguments don't convince you, perhaps Hamel Husain, one of the leading AI eval practitioners, will:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;&lt;strong&gt;Build a custom annotation tool.&lt;/strong&gt; This is the single most impactful investment you can make for your AI evaluation workflow. With AI-assisted development tools like Cursor or Lovable, you can build a tailored interface in hours. I often find that teams with custom annotation tools iterate ~10x faster.&lt;/p&gt;
&lt;p&gt;Custom tools excel because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;They show all your context from multiple systems in one place&lt;/li&gt;
&lt;li&gt;They can render your data in a product-specific way (images, widgets, markdown, buttons, etc.)&lt;/li&gt;
&lt;li&gt;They're designed for your specific workflow (custom filters, sorting, progress bars, etc.)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Off-the-shelf tools may be justified when you need to coordinate dozens of distributed annotators with enterprise access controls. Even then, many teams find the configuration overhead and limitations aren't worth it.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;He makes a great point: "With AI-assisted development tools like Cursor or Lovable, you can build a tailored interface in hours."&lt;/p&gt;
&lt;p&gt;The barrier to entry for building your own tools nowadays is so much lower than before. Much of the grunt work can be automated away using templating and LLM assistance. If you want to build, now is the time to build.&lt;/p&gt;
&lt;h2 id="software-development-scales-everything"&gt;Software development scales everything&lt;/h2&gt;&lt;p&gt;I love the work I do partly because it is in the service of the discovery of medicines, and partly because I have an outlet for expressing creativity through the tools I make for myself and others. Through nearly 10 years of making tools, I've crystallized this lesson in scaling:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Software scales our labor.&lt;/li&gt;
&lt;li&gt;Documentation scales our brains.&lt;/li&gt;
&lt;li&gt;Tests scale others' trust in our code.&lt;/li&gt;
&lt;li&gt;Design scales our agility.&lt;/li&gt;
&lt;li&gt;Agents scale our processes.&lt;/li&gt;
&lt;li&gt;Open source scales opportunity for impact.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If you can build software tools for yourself, you can scale yourself. If you teach others to use those same tools, you can scale their labor. You can scale your brain by documenting those tools well. If you test those tools thoroughly, you can scale trust in the codebase, enabling others to contribute with confidence. If you design the software well—and more importantly, design the business process that software supports well—you can become nimble and agile without the trappings of Big Fake Agile. If you use agents, and more generally automation as part of the custom tooling, you can scale those same processes even further. If you make your tooling open source (whether internally or externally), you scale the opportunity for others to contribute.&lt;/p&gt;
&lt;p&gt;Culture needs no permission (another great lesson that I learned from Andrew Giessel), and if you need to unblock yourself, build your own tools. There is no magic sauce in the choice of tools that we use and make. &lt;strong&gt;The magic sauce is in the people who choose to show up and build.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;And so, my fellow builders, let's build. Not because your company wants it of you, but because patients are waiting. Patients have no patience. We joined this line of work because we want to have the greatest impact on patients with our medicines. Computational types like should never be the bottleneck to shipping medicines. Building tools for ourselves empowers us to keep ourselves unstuck, remove the viscous traps that slow you down, and keep medicines moving.&lt;/p&gt;
&lt;p&gt;I'll now leave you with a final quote, from Michael Jackson's song, Heal the World:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;There are people dying, if you care enough for the living, make a better place for you and for me.&lt;/p&gt;
&lt;p&gt;— Heal the World (Michael Jackson)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And so to my fellow techies in bio, it's time to build. Thank you.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="reactions"&gt;Reactions&lt;/h2&gt;&lt;p&gt;After Tommy's talk, we had another round of networking, which was awesome. I heard some great perspectives. &lt;a href="https://www.linkedin.com/in/kucukural/"&gt;Alper Kucukural&lt;/a&gt;, who is both an industry and academia person, mentioned how his students needed to hear the message that they can be empowered to build their own tools, no permission required. Too many get stuck. Students -- learn how to build!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/maciejpacula/"&gt;Maciej Pacula&lt;/a&gt; also posted his reaction on LinkedIn:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;I had a great time at the &lt;a href="https://www.linkedin.com/company/datadrivenpharma/"&gt;DataDrivenPharma&lt;/a&gt; event at Moderna yesterday. Thanks &lt;a href="https://www.linkedin.com/in/ACoAAAd30LUB7VdWv0AHDEMM72Cm0cyIZgSNc_4"&gt;&lt;/a&gt;&lt;a href="https://www.linkedin.com/in/dricaptain/"&gt;Ilya Captain, PhD&lt;/a&gt; for organizing, and hope you bring more such events to the East Coast!&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/ericmjl/"&gt;Eric Ma&lt;/a&gt;'s talk about building your own tools and using them as a force multiplier not just for yourself but for others resonated deeply.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/%F0%9F%8E%AF-ming-tommy-tang-40650014/"&gt;🎯 Ming "Tommy" Tang&lt;/a&gt;'s talk about "good enough" reproducibility made the excellent point that sometimes you just need to talk to the lab scientists (what a concept!) and collaborate on common standards.&lt;/p&gt;
&lt;p&gt;Appreciated the shout out for &lt;a href="https://www.linkedin.com/company/gofigr/"&gt;GoFigr.io&lt;/a&gt;, &lt;a href="https://www.linkedin.com/in/%F0%9F%8E%AF-ming-tommy-tang-40650014/"&gt;🎯 Ming "Tommy" Tang&lt;/a&gt; :-)&lt;/p&gt;
&lt;p&gt;Thanks &lt;a href="https://www.linkedin.com/in/ted-natoli-compbio/"&gt;Ted Natoli&lt;/a&gt;, &lt;a href="https://www.linkedin.com/in/colles/"&gt;Colles Price M.S., Ph.D.&lt;/a&gt;, &lt;a href="https://www.linkedin.com/in/wesserg/"&gt;Sergiusz Wesolowski&lt;/a&gt;, &lt;a href="https://www.linkedin.com/in/ilyashl/"&gt;Ilya Shlyakhter&lt;/a&gt;, &lt;a href="https://www.linkedin.com/in/vasant-marur/"&gt;Vasant Marur&lt;/a&gt;, &lt;a href="https://www.linkedin.com/in/kucukural/"&gt;Alper Kucukural, PhD&lt;/a&gt;, &lt;a href="https://www.linkedin.com/in/jamesjcrowley/"&gt;James J. Crowley&lt;/a&gt;, &lt;a href="https://www.linkedin.com/in/gunjan-singh-thakur-b8251620/"&gt;Gunjan Singh Thakur&lt;/a&gt; for the company and conversation.&lt;/p&gt;
&lt;p&gt;-- Original post link &lt;a href="https://www.linkedin.com/feed/update/urn:li:activity:7344004044933226496/"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/ericmerle/"&gt;Eric Merle&lt;/a&gt;'s reaction is below:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A lot is possible when we build the right tools...&lt;/p&gt;
&lt;p&gt;Yesterday's DataDrivenPharma event at &lt;a href="https://www.linkedin.com/company/modernatx/"&gt;Moderna&lt;/a&gt; completely energized my thinking about exactly that and I'll tell you specifically why.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://www.linkedin.com/in/ericmjl/"&gt;Eric Ma&lt;/a&gt; from Moderna shared something that hit home: "Data scientists should never become bottlenecks in getting medicines to patients who need them." His approach to building custom tools that scale impact, from automated slide generation to standardized project workflows, showed exactly how thoughtful tooling can accelerate discovery.
&lt;a href="https://www.linkedin.com/in/%F0%9F%8E%AF-ming-tommy-tang-40650014/"&gt;🎯 Ming "Tommy" Tang&lt;/a&gt; from &lt;a href="https://www.linkedin.com/company/astrazeneca/"&gt;AstraZeneca&lt;/a&gt; complemented this perfectly with his presentation on reproducible bioinformatics practices. His insights on proper file naming conventions (how many of us are guilty of having final1, final2, final3 files?), consistent folder structures, and creating reproducible workflows provided the foundation that makes scaling actually possible. You can't build lasting tools without these fundamentals in place.&lt;/p&gt;
&lt;p&gt;Both emphasized that that it's not just writing code, but also about building infrastructure. Eric's philosophy around scaling through software combined with Tommy's disciplined approach to reproducibility showed how the right practices can create tools that continue delivering value long after the original builder moves on.&lt;/p&gt;
&lt;p&gt;The potential to create AI tools that don't just automate routine tasks but fundamentally change how we approach patient care and drug development feels limitless. Both presentations reinforced that we're now building the infrastructure that could accelerate how quickly life-saving treatments reach patients. The timing feels perfect. We have AI capabilities that can scale impact in ways that weren't possible even two years ago.&lt;/p&gt;
&lt;p&gt;Thanks to &lt;a href="https://www.linkedin.com/in/dricaptain/"&gt;Ilya Captain, PhD&lt;/a&gt; at &lt;a href="https://www.linkedin.com/company/datadrivenpharma/"&gt;DataDrivenPharma&lt;/a&gt; for organizing this excellent event and to &lt;a href="https://www.linkedin.com/in/louise-liu-phd-mba-b195b3343/"&gt;Louise Liu, PhD, MBA&lt;/a&gt; from &lt;a href="https://www.linkedin.com/company/hill-research/"&gt;Hill Research&lt;/a&gt; for the introduction to Tommy and recommending I attend.&lt;/p&gt;
&lt;p&gt;What tools are you building to scale your impact? Curious to hear what others are working on in this space.&lt;/p&gt;
&lt;p&gt;PS: Happy to have been able to chat with Eric and Tommy&lt;/p&gt;
&lt;p&gt;-- Original post link &lt;a href="https://www.linkedin.com/posts/ericmerle_digitalhealth-ai-datascience-activity-7344158374910996480-ViiO/?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAAKTdlUBKWeDvuvNDNpOBmAV1OszCr-W__c"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;And &lt;a href="https://www.linkedin.com/in/originalpatrick/"&gt;Patrick Hofmann&lt;/a&gt;:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;A couple of great talks by &lt;a href="https://www.linkedin.com/in/ericmjl/"&gt;Eric Ma&lt;/a&gt; and &lt;a href="https://www.linkedin.com/in/%F0%9F%8E%AF-ming-tommy-tang-40650014/"&gt;🎯 Ming "Tommy" Tang&lt;/a&gt; at &lt;a href="https://www.linkedin.com/in/dricaptain/"&gt;Ilya Captain, PhD&lt;/a&gt;’s Data Driven Pharma event last night. Eric made a strong case for data scientists building their own tools. I’m no programmer, but I have dabbled in woodworking and it reminded me of all the jigs I’ve built for various projects.&lt;/p&gt;
&lt;p&gt;There are many facets to the ‘buy vs build’ question and here’s one I think often gets overlooked: If an off the shelf solution is available, will it do precisely what you want? Or will you need to conform your project to it? The answer isn’t always clear cut but it’s worth considering when choosing how to allocate your time and resources.&lt;/p&gt;
&lt;p&gt;-- Original post link &lt;a href="https://www.linkedin.com/posts/originalpatrick_a-couple-of-great-talks-by-eric-ma-and-activity-7343995611756511234-uO_5?utm_source=share&amp;amp;utm_medium=member_desktop&amp;amp;rcm=ACoAAAKTdlUBKWeDvuvNDNpOBmAV1OszCr-W__c"&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Afterwards, in our discussion, Patrick had a great point about the parallel between custom tools and woodworking jigs: you can either make your own jigs or buy them, but if you buy them, you now have to conform your woodworking to the jig, and not the other way around. Little compromises compound against the quality of the final deliverable!&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="but-where-is-deckbot"&gt;But where is deckbot?&lt;/h2&gt;&lt;p&gt;Ok, I bet you're just like me, you hate making slides, and you want to see DeckBot. You can find it linked &lt;a href="slides-maker.py"&gt;here&lt;/a&gt; as a marimo notebook! To run it, you'll need an OpenAI API key mapped to the &lt;code&gt;OPENAI_API_KEY&lt;/code&gt; environment variable. Download the notebook and run this:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;OPENAI_API_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sk-your-api-key-here&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;uvx&lt;span class="w"&gt; &lt;/span&gt;marimo&lt;span class="w"&gt; &lt;/span&gt;edit&lt;span class="w"&gt; &lt;/span&gt;--sandbox&lt;span class="w"&gt; &lt;/span&gt;/your/path/to/slides_maker.py
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="and-what-were-the-slides-you-actually-presented"&gt;And what were the slides you actually presented?&lt;/h2&gt;&lt;p&gt;I archived them for posterity &lt;a href="index.md"&gt;here&lt;/a&gt;. Enjoy!&lt;/p&gt;
</content></entry><entry><title>Rethinking LLM interfaces, from chatbots to contextual applications</title><link href="https://ericmjl.github.io/blog/2025/6/14/rethinking-llm-interfaces-from-chatbots-to-contextual-applications/" rel="alternate"/><updated>2025-06-14T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:a76ca6c0-c657-334a-ad8e-545e0667a5a8</id><content type="html">&lt;p&gt;Chat interfaces were a great starting point for interacting with large language models, but they're not the endgame. &lt;strong&gt;My thesis is that we should build LLM applications as contextual tools embedded in structured workflows, not as open-ended chat interfaces.&lt;/strong&gt; This insight came from three converging threads that fundamentally changed how I think about building LLM-powered applications.&lt;/p&gt;
&lt;p&gt;The first thread came from a conversation with my colleague &lt;a href="https://www.linkedin.com/in/michelle-faits/"&gt;Michelle Faits&lt;/a&gt;, who articulated that apps powered by generative AI really need to end up looking less like chat interfaces and more like TurboTax -- where there's a well-defined process that needs to happen, and instead of users filling out forms manually, we ask an AI to help with the form-filling process.&lt;/p&gt;
&lt;p&gt;The second thread was a YouTube video titled "&lt;a href="https://youtu.be/mRqBjKFyfLc?si=9sRDPg-hH5iBLiFf"&gt;AI UX Design: ChatGPT interfaces are already obsolete&lt;/a&gt;" by Alan Pike from Vancouver. In it, he talks about shifting from chatbot to context-native interfaces, a change that's both subtle and dramatic. It's subtle because there's little visible change, but dramatic because the way you interact with the interface changes fundamentally. You're no longer stuck with the drudge work of filling out yet another form, but are instead presented with an AI-powered interface capable of understanding what your next action is likely to be and anticipating it just in time.&lt;/p&gt;
&lt;p&gt;The third thread is Clayton Christensen's "jobs to be done" theory. What I've been noticing is that there are too many ChatGPT copycat clones, and those chat clones don't really help me accomplish the job that I'm trying to do. It takes a different type of interface to make that happen.&lt;/p&gt;
&lt;h2 id="these-threads-converge-on-a-simple-truth"&gt;These threads converge on a simple truth&lt;/h2&gt;&lt;p&gt;What connects TurboTax's structured approach, Pike's context-native interfaces, and jobs-to-be-done theory is this: &lt;strong&gt;the most effective LLM applications will embed AI capabilities directly into well-defined workflows rather than forcing users to articulate their needs through chat.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This means moving from "tell the AI what you want" to "let the AI assist you as you work through a process you already understand."&lt;/p&gt;
&lt;h2 id="the-turbotax-moment"&gt;The TurboTax moment&lt;/h2&gt;&lt;p&gt;Michelle's insight about TurboTax really stuck with me. TurboTax works because it represents a well-defined business process with pretty routine steps that we need to walk through, but some of the steps do require judgment calls. Do you fill out this section or not? And what do you fill in? You need to determine that from context, so there's a little bit of agency for LLM bots inside there. But for the most part, it's just form filling.&lt;/p&gt;
&lt;p&gt;This is a powerful analogy for LLM apps, one that gets to the heart of any app build. The question becomes: how do you go about building a user interface that works like this? When we build chat interfaces, we put a lot of onus on the LLM to make smart decisions on behalf of us. But what if chat wasn't the primary way of interacting? What if we had well-defined business workflows supported by custom apps that just require us to fill out forms in a delightful way?&lt;/p&gt;
&lt;h2 id="the-obsolescence-of-chat-interfaces"&gt;The obsolescence of chat interfaces&lt;/h2&gt;&lt;p&gt;Alan Pike's perspective really crystallized something I'd been feeling. In his talk, he showed how we're moving from text-based interfaces that are powerful but confounding to 90% of people, toward context-native interfaces that inject AI capabilities right where you need them.&lt;/p&gt;
&lt;p&gt;Think about it: we've already started seeing hints of tools pushing chat to the side. ChatGPT has Canvas mode now, where if you ask it to co-author a document, it sticks the chat up in the corner and lets you focus on the work you're doing. But this is still just the beginning.&lt;/p&gt;
&lt;p&gt;Pike showed examples of right-click contextual actions, natural language search that understands intent rather than requiring exact phrases, and date pickers where you can just say "next Thursday at 11" instead of clicking through calendar grids. These represent a fundamental shift in how we think about human-computer interaction.&lt;/p&gt;
&lt;p&gt;I thought the talk was quite good, and I'm embedding it below to share.&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/mRqBjKFyfLc?si=9sRDPg-hH5iBLiFf" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen&gt;&lt;/iframe&gt;&lt;h2 id="jobs-to-be-done-theory-meets-llm-apps"&gt;Jobs to be done theory meets LLM apps&lt;/h2&gt;&lt;p&gt;Clayton Christensen's jobs-to-be-done framework is perfect for thinking about LLM applications. When I look at most LLM interfaces, we've become hooked on chat -- but they don't necessarily always help me accomplish the specific job I'm trying to do. Generic chat interfaces put the burden on me to figure out how to express my needs and on the LLM to figure out what I actually want. What if we could do better?&lt;/p&gt;
&lt;p&gt;I think what we're going to see is an evolution of LLM-powered apps from being text and chat driven to being deeply embedded within applications, making it possible to flow through business processes in a way that's much smoother and more delightful than what was possible before. It's not really about agentic capabilities, which are nice, but the winners will be the interfaces that inject LLMs in just the right places -- in the boring work!&lt;/p&gt;
&lt;h2 id="building-deckbot-demonstrates-this-approach"&gt;Building DeckBot demonstrates this approach&lt;/h2&gt;&lt;p&gt;Let me show you what this looks like in practice. I built a Markdown slide deck generator called DeckBot, deliberately avoiding chat as the primary interface because it was too freeform and unreliable.&lt;/p&gt;
&lt;p&gt;Instead of starting with a UI, I began with the data model: defining a &lt;code&gt;Slide&lt;/code&gt; as a Pydantic model with title, content, and type. I tested individual slide generation in a Marimo notebook until each component worked reliably. Then I put them together into a &lt;code&gt;SlideDeck&lt;/code&gt; Pydantic model. This allowed me to compose a &lt;code&gt;SlideDeck&lt;/code&gt; from individually-generated &lt;code&gt;Slides&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The next breakthrough came when I realized I could inject LLM capabilities directly into the data objects themselves. Instead of an agent orchestrating external tools, my data models gained natural language-powered methods:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nc"&gt;SlideDeck&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseModel&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;slides&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;Slide&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;talk_title&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;


    &lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;edit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;change&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Edit the slide at a given index using natural language.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
        &lt;span class="n"&gt;current_slide&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;slides&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;render&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="n"&gt;new_slide&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;slidemaker&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;slidemaker_edit&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;current_slide&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;change&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;slides&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;index&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;new_slide&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This represents a fundamental shift: instead of putting all intelligence in a central agent, I distributed it into the data models themselves. Each Pydantic model knows how to manipulate itself based on natural language instructions.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;DeckBot sits at step 5 of the maturity ladder I'll describe below&lt;/strong&gt;; it provides LLM-augmented interfaces that understand context and assist with specific tasks, but within a structured framework.&lt;/p&gt;
&lt;h2 id="the-future-of-llm-applications"&gt;The future of LLM applications&lt;/h2&gt;&lt;p&gt;I believe we're going to see LLM applications become more like TurboTax and less like open-ended chat interfaces. These will be applications built around well-defined business processes that users can flow through smoothly, with AI providing assistance at just the right moments.&lt;/p&gt;
&lt;p&gt;There's still a place for agents, but we need to recognize that adoption follows a ladder of maturity:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Unstructured work relying on human intuition&lt;/li&gt;
&lt;li&gt;Documented SOPs and manual processes&lt;/li&gt;
&lt;li&gt;Digital UIs guiding humans through structured processes&lt;/li&gt;
&lt;li&gt;Rule-based automation for predictable parts of workflows&lt;/li&gt;
&lt;li&gt;LLM-augmented interfaces providing contextual assistance&lt;/li&gt;
&lt;li&gt;Semi-autonomous LLM components handling defined subtasks&lt;/li&gt;
&lt;li&gt;Full agent orchestration with human oversight&lt;/li&gt;
&lt;li&gt;Truly autonomous agent systems managing entire business processes&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Customer support agents have emerged as one of the first places for LLM agents, and I suspect it's because customer support as a business process has more or less been well-standardized. The fact that we can "agentify" it stems from decades of process refinement. Other business domains need to undergo similar transformation before they're ready for full agent automation.&lt;/p&gt;
&lt;p&gt;At Moderna, we've embraced generative AI heavily, relying on ChatGPT and custom GPTs. But I know this cannot be the only way we interact with LLMs. There are ways to surgically inject LLMs into workflows so users can accomplish what they're trying to do in a structured fashion, but in a delightfully smooth and flowing way.&lt;/p&gt;
&lt;p&gt;The big lesson I learned building DeckBot is understanding where and when to inject LLMs very surgically into custom LLM applications. It's not about replacing human decision-making with AI decision-making; it's about augmenting human workflows with AI capabilities at precisely the right moments.&lt;/p&gt;
&lt;h2 id="key-principles-for-contextual-llm-applications"&gt;Key principles for contextual LLM applications&lt;/h2&gt;&lt;p&gt;Drawing from the TurboTax insight, Pike's context-native approach, and jobs-to-be-done theory, here are the essential principles I've learned:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Start with the data model, not the interface.&lt;/strong&gt; Get clear on what you're actually trying to accomplish and model that as structured data first. Design APIs around those data models that work through clean function calls before adding any LLM capabilities.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Inject LLMs surgically into workflows.&lt;/strong&gt; Identify the specific points where natural language understanding or generation adds value, rather than building everything around chat or agents.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Test with structured examples first.&lt;/strong&gt; Use notebooks to validate that your core functions work properly before thinking about user interfaces.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Build for the job-to-be-done.&lt;/strong&gt; Don't chase the latest agentic capabilities just because they're exciting. Focus on making specific workflows easier and more delightful.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="the-path-forward"&gt;The path forward&lt;/h2&gt;&lt;p&gt;Chat was the beginning of our journey with LLMs, but it is most certainly not the destination. The three threads I described, namely, Michelle's TurboTax-esque structured approach, Pike's context-native interfaces, and Christensen's jobs-to-be-done framework, all point toward the same future: &lt;strong&gt;LLM applications that flow smoothly through business processes, where AI assistance appears exactly when and where it's needed.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;This isn't about replacing human decision-making with AI decision-making. It's about augmenting human workflows with AI capabilities at precisely the right moments, without forcing users to translate their intentions into chat prompts or rely on agents to make all decisions for them.&lt;/p&gt;
&lt;p&gt;We're at the beginning of an incredible generation of software and products, and it's an exciting time to build not just the software but the processes around them too! The question we have now is this: how quickly we can move beyond chat alone to build contextual applications that truly help people accomplish their goals?&lt;/p&gt;
</content></entry><entry><title>Principles for using AI autodidactically</title><link href="https://ericmjl.github.io/blog/2025/6/7/principles-for-using-ai-autodidactically/" rel="alternate"/><updated>2025-06-07T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:0983e738-2271-3166-ad98-defee09d63c1</id><content type="html">&lt;h2 id="we-need-to-move-beyond-passive-consumption"&gt;We need to move beyond passive consumption&lt;/h2&gt;&lt;p&gt;Imagine having a personal tutor who's absorbed millions of books, papers, and discussions across every field of human knowledge. That's essentially what Large Language Models (LLMs) offer us today. As David Duvenaud aptly describes them, LLMs are a "&lt;a href="https://x.com/DavidDuvenaud/status/1895139380198584794"&gt;galaxy brain&lt;/a&gt;" of knowledge waiting to be tapped.&lt;/p&gt;
&lt;p&gt;But &lt;em&gt;having access&lt;/em&gt; to information isn't the same as &lt;em&gt;learning from it&lt;/em&gt;. The difference lies in how we engage with these AI tools - passively consuming their outputs versus actively using them to expand our understanding. Through my interviews with researchers and digital professionals, I've discovered patterns in how the most effective learners use AI autodidactically - teaching themselves with AI as their assistant, not their replacement.&lt;/p&gt;
&lt;h2 id="lessons-from-autodidactic-ai-users-at-work"&gt;Lessons from autodidactic AI users at work&lt;/h2&gt;&lt;p&gt;I have conducted many interviews at work about how folks in Moderna's Research and Digital organizations use AI. While the discussions are insanely specific to work and sometimes touch on IP that I cannot reveal, there are principles and patterns in what I observe the best folks do when using AI in their day-to-day work to learn new stuff.&lt;/p&gt;
&lt;h3 id="generate-a-personalized-syllabus-for-learning"&gt;Generate a personalized syllabus for learning&lt;/h3&gt;&lt;p&gt;They recognize that any kind of learning involves effort and hard work, and that the pain of the process is a non-negotiable to make anything stick. So instead of using AI to do stuff for them, they start by using AI to provide a tailored syllabus that allows them to progressively move up the knowledge ladder with increasing effort.&lt;/p&gt;
&lt;p&gt;This is what I would call "scaffolding a personalized syllabus". Their prompts here often include a bit of their current role, their prior training, their own objectives for learning, and what they know from prior experience about how they learn best. On the basis of the syllabus, iterate and follow up.&lt;/p&gt;
&lt;h3 id="apply-one-s-ability-to-think-critically-to-llm-outputs"&gt;Apply one's ability to think critically to LLM outputs&lt;/h3&gt;&lt;p&gt;They recognize that questions are a great way to learn, so they will continuously question and LLM to draw out answers. The act of generating a question as a human is part of the effort needed.&lt;/p&gt;
&lt;p&gt;They apply the skill of critical thinking to the answers generated by an LLM, asking questions such as, "if this is true..." or "is this coherent with...".  They do not blindly accept the output of an LLM!&lt;/p&gt;
&lt;p&gt;Apart from self-coherence with what they know, they verify by cross-checking reputable sources on the internet -- scholarly literature, expert writing, etc.&lt;/p&gt;
&lt;p&gt;At a meta-level, if they find an angle that demands explanation, knowing that sometimes an LLM can be blinded by conversation history, they will explicitly prompt an LLM on contrary points, using prompts that start with, "but I remember that..." or "this sounds suspicious, could it be that..."&lt;/p&gt;
&lt;p&gt;Also, in the absence of another human, they use LLMs to provide initial critique about what they have produced (e.g. in writing form). They use LLMs in the same way jazz musicians riff off one another.&lt;/p&gt;
&lt;h2 id="what-s-the-core-trick"&gt;What's the core trick?&lt;/h2&gt;&lt;p&gt;At its core, the main "trick" to using an LLM autodidactically is to avoid delegating critical thinking to the LLM and instead applying the full force of one's agency. We need to leverage the galaxy brain of knowledge from its training set (and, where applicable, internet search capabilities) and apply individual effort by critically thinking through LLM outputs. Essentially, every skill we were taught to hone in literature class in high school, debate club in junior college, science philosophy class in undergrad, and scientific journal clubs during graduate training!&lt;/p&gt;
&lt;p&gt;AI has brought the philosophical points of human agency into sharp relief. Like any tool, LLMs can be used to increase your agency or diminish it. It's a double-edged sword. Use it for the former!&lt;/p&gt;
</content></entry><entry><title>The invisible polish of automatic model routing</title><link href="https://ericmjl.github.io/blog/2025/5/25/the-invisible-polish-of-automatic-model-routing/" rel="alternate"/><updated>2025-05-25T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:12869f09-d6d3-311d-92c1-16fdf5ec643c</id><content type="html">&lt;p&gt;I've been using Cursor's latest updates, and while the surface-level improvements are nice—better edge rounding, refined colors, thoughtful layering—there's one change that's got me genuinely excited: automatic model routing.&lt;/p&gt;
&lt;p&gt;No more model picker. No more stopping mid-thought to decide between OpenAI's models, Claude, or whatever other model might be appropriate for my current task. Cursor just figures it out and routes my request to the right model automatically.&lt;/p&gt;
&lt;h2 id="why-model-pickers-are-ui-bugs"&gt;Why model pickers are UI bugs&lt;/h2&gt;&lt;p&gt;I remember reading somewhere (probably on Twitter, let's be honest) that model pickers are fundamentally a UI bug. The argument was simple: users shouldn't need to understand the technical differences between models to get their work done. They should just describe what they want, and the system should handle the rest.&lt;/p&gt;
&lt;p&gt;At the time, I nodded along but didn't fully appreciate how right this was until I experienced Cursor's implementation. Before this change, I was making micro-decisions about model selection multiple times per day. Should I use the faster model for this simple refactoring? Do I need the more capable model for this complex architectural question? Each decision was small, maybe taking 2-3 seconds, but they added up.&lt;/p&gt;
&lt;h2 id="the-cognitive-tax-of-micro-decisions"&gt;The cognitive tax of micro-decisions&lt;/h2&gt;&lt;p&gt;These tiny decisions represent what I think of as cognitive tax: small mental overhead that accumulates throughout the day. Each model selection forced a brief context switch: I had to step out of my coding flow, evaluate the complexity of my request, weigh speed versus capability, and make a choice.&lt;/p&gt;
&lt;p&gt;The individual cost was negligible. The cumulative cost was not. By the end of the day, I'd made dozens of these micro-decisions, each one pulling a small amount of mental energy away from the actual problem I was trying to solve.&lt;/p&gt;
&lt;p&gt;Cursor's automatic routing eliminates this entirely. I describe what I want, hit enter, and trust that the right model will handle it. The decision-making burden shifts from me to the system, where it belongs.&lt;/p&gt;
&lt;h2 id="parallels-to-apple-s-design"&gt;Parallels to Apple's design&lt;/h2&gt;&lt;p&gt;This reminds me of something I read about Apple's design philosophy during the Jony Ive era. The idea more than making things look beautiful, it was about removing friction at every possible level, even in places users might not consciously notice.&lt;/p&gt;
&lt;p&gt;Think about the original iPhone's lack of a keyboard. Everyone said it was crazy, that people needed physical keys. But Apple understood that the mental model of "keyboard for typing" was actually limiting. By removing the physical keyboard, they freed up space for context-sensitive interfaces that could adapt to what you were actually trying to do.&lt;/p&gt;
&lt;p&gt;Cursor's automatic model routing feels like the same kind of thinking. Instead of optimizing the model picker interface, they eliminated the need for it entirely. The best interface is often no interface at all.&lt;/p&gt;
&lt;h2 id="the-broader-principle"&gt;The broader principle&lt;/h2&gt;&lt;p&gt;What makes this interesting isn't just that it saves me a few seconds per day. It's that it represents a shift in how we think about AI tool design. Instead of exposing the complexity of the underlying system to users, we can build intelligence into the routing layer itself.&lt;/p&gt;
&lt;p&gt;This has implications beyond just model selection. How many other micro-decisions are we forcing users to make that could be automated away? How many interface elements exist because we haven't figured out how to make them unnecessary?&lt;/p&gt;
&lt;p&gt;I suspect we'll see more of this pattern as AI tools mature. The first generation of AI interfaces were necessarily explicit: users needed to understand models, parameters, and context windows because the tools couldn't make those decisions reliably. But as the underlying systems get smarter, the interfaces can get simpler.&lt;/p&gt;
&lt;h2 id="the-invisible-improvements"&gt;The invisible improvements&lt;/h2&gt;&lt;p&gt;The best improvements are often the ones you don't notice consciously but feel in your workflow. Cursor's automatic model routing is exactly this kind of enhancement. I don't think about it while I'm coding, but I feel its absence when I use other tools that still require manual model selection.&lt;/p&gt;
&lt;p&gt;This is the kind of polish that compounds. Each eliminated micro-decision, each removed point of friction, each automated choice creates space for deeper focus on the work that actually matters. It's not revolutionary on its own, but it's part of building tools that feel like extensions of thought rather than obstacles to it.&lt;/p&gt;
&lt;p&gt;The question for other AI tool builders is: what other invisible friction exists in your interfaces? What decisions are you forcing users to make that your system could handle automatically? The model picker was just the beginning.&lt;/p&gt;
</content></entry><entry><title>Supercharge your coding agents with VSCode workspaces</title><link href="https://ericmjl.github.io/blog/2025/5/24/supercharge-your-coding-agents-with-vscode-workspaces/" rel="alternate"/><updated>2025-05-24T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:d299229c-1aa0-384c-a2e2-cb36c3d97384</id><content type="html">&lt;p&gt;I was building out my &lt;a href="https://github.com/ericmjl/building-with-llms-made-simple"&gt;LLM tutorial repository&lt;/a&gt; for &lt;a href="https://www.scipy2025.scipy.org/"&gt;SciPy 2025&lt;/a&gt; and found myself constantly switching between windows—improving the &lt;a href="https://github.com/ericmjl/llamabot"&gt;LlamaBot&lt;/a&gt; library in one window, then flipping to my tutorial repo in another to update examples that used the new features. Every time I added a new method or changed an API in LlamaBot, I had to remember to update the corresponding tutorial examples. The constant context switching was slowing me down and making it easy to miss places where the tutorial needed updates.&lt;/p&gt;
&lt;p&gt;Then I discovered something that changed how I code across repos: Workspaces! They aren't just convenient for organizing multiple repositories, they're also game-changers for coding agents in Cursor.&lt;/p&gt;
&lt;p&gt;When you add multiple repositories to the same workspace, your coding agent magically gains context across all your repos simultaneously. No more window switching, no more explaining relationships between codebases. Instead, your coding assistants can access code in multiple repositories at once.&lt;/p&gt;
&lt;p&gt;Here's how to set this up and why it matters.&lt;/p&gt;
&lt;h2 id="setting-up-your-first-multi-repo-workspace"&gt;Setting up your first multi-repo workspace&lt;/h2&gt;&lt;h3 id="step-1-create-the-workspace"&gt;Step 1: Create the workspace&lt;/h3&gt;&lt;p&gt;Open a blank Cursor/VSCode window and immediately save it as a workspace file (File → Save Workspace As). I recommend saving it outside any repository; I keep mine as a sibling directory to my repos, like &lt;code&gt;~/github/llm-scipy-tutorial.code-workspace&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Then, go to File → Add Folder to Workspace to add your first repository (my main tutorial project &lt;code&gt;~/github/building-with-llms-made-simple&lt;/code&gt;), and repeat for your second repo (the companion library I was improving, &lt;code&gt;~/github/llamabot&lt;/code&gt;). You'll see both folders appear in the Explorer sidebar.&lt;/p&gt;
&lt;h3 id="step-2-watch-the-magic-happen"&gt;Step 2: Watch the magic happen&lt;/h3&gt;&lt;p&gt;Here's where it gets interesting. Fire up Cursor's AI or GitHub Copilot and give it a specific prompt that references files across both repos. Try something like:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;"Look at @llamabot/llamabot/bot/simplebot.py and edit @building-with-llms-made-simple/notebooks/03_advanced_bot.py to update the StructuredBot example for document summarization."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;(If you're using VSCode instead of Cursor, just replace the @ symbols with #.)&lt;/p&gt;
&lt;p&gt;Your agent can now see both codebases simultaneously. It understands how your LlamaBot library works and can create coherent examples in your tutorial repo, suggesting coordinated changes across repos while maintaining consistency between your library code and tutorial examples.&lt;/p&gt;
&lt;h3 id="step-3-reopening-your-workspace"&gt;Step 3: Reopening your workspace&lt;/h3&gt;&lt;p&gt;Next time you open Cursor or VSCode, you'll see your workspace listed on the welcome screen under "Recent". Click it to instantly load all your repositories with the same folder structure and settings.&lt;/p&gt;
&lt;h2 id="quick-tips-that-make-this-even-better"&gt;Quick tips that make this even better&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Keep workspaces outside repositories:&lt;/strong&gt;
I always save workspace files as siblings to my repo directories, never inside them. This prevents workspace files from accidentally getting committed and keeps things clean when you're working across multiple projects.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;~/github/
├── llamabot/
├── building-with-llms-made-simple/
└── llm-scipy-tutorial.code-workspace
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;strong&gt;Quick tip on scale:&lt;/strong&gt;
You can add as many repositories as you need. I've had workspaces with multiple model experiment repos, shared data utilities, and production pipelines, and Cursor's agent could reference files across all of them. When you use &lt;code&gt;@workspace&lt;/code&gt; in Cursor, it considers every file in every repository. Fair warning though—I recently worked across 5 repos at work and my head was spinning even with LLM help. Sometimes less is more.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Be prescriptive with file references:&lt;/strong&gt;
Prompting across repos works best when you can pinpoint exactly which file to reference or edit. In Cursor, use &lt;code&gt;@&amp;lt;file&amp;gt;&lt;/code&gt; syntax, while in VSCode it's &lt;code&gt;#&amp;lt;file&amp;gt;&lt;/code&gt;. This helps the agent focus on the specific files you care about rather than wandering through your entire workspace.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Use this pattern strategically:&lt;/strong&gt;
This approach shines when your current project depends on functionality that was developed beforehand in other repositories. Think data science projects that depend on internal tools built by other teams, or tutorial repositories that need to stay consistent with the underlying library they're demonstrating. When you have models or analyses that depend on utilities, libraries, or frameworks developed separately, workspaces let your coding agent understand both the dependency and the dependent code simultaneously. For single-repo exploratory work, stick to regular folders.&lt;/p&gt;
&lt;p&gt;That's it. Next time you're coordinating changes across multiple data science repositories, set up a workspace and let your coding agent see the full picture.&lt;/p&gt;
</content></entry><entry><title>Why I'm excited for SciPy 2025!</title><link href="https://ericmjl.github.io/blog/2025/5/8/why-im-excited-for-scipy-2025/" rel="alternate"/><updated>2025-05-08T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:04f95c28-7e39-3643-bd47-944f82a804a3</id><content type="html">&lt;p&gt;&lt;a href="https://www.scipy2025.scipy.org/"&gt;SciPy 2025&lt;/a&gt; is just around the corner, and it's one of my favorite conferences of the year. I wanted to share why I'm looking forward to going to year's event in Tacoma, Seattle.&lt;/p&gt;
&lt;h2 id="back-to-the-pacific-northwest"&gt;Back to the Pacific Northwest&lt;/h2&gt;&lt;p&gt;SciPy returns to Seattle this year, specifically Tacoma. As someone whose hometown is Victoria, BC, the Pacific Northwest holds a special place in my heart. The PNW in summer is absolutely gorgeous, and beyond the conference itself, the area offers incredible food options and beautiful scenery. Seattle has some of the best Thai food you'll find anywhere—definitely worth exploring between sessions! Perfect for recharging during the conference.&lt;/p&gt;
&lt;p&gt;Also, I've got some of my favorite colleagues out there, and like last year, I hope to take the chance to visit them and build bridges across sites.&lt;/p&gt;
&lt;h2 id="the-community-of-tool-builders"&gt;The community of tool builders&lt;/h2&gt;&lt;p&gt;What makes SciPy special is the community. I'm a tool builder at heart, and there's nothing quite like mingling with the folks who create the scientific computing tools that power the Python ecosystem. These are the people whose libraries we import daily, and getting to network with them is invaluable.&lt;/p&gt;
&lt;h2 id="giving-back-through-financial-aid"&gt;Giving back through financial aid&lt;/h2&gt;&lt;p&gt;I've been one of the organizers running the financial aid program since around 2015—almost a decade now! This work is particularly meaningful to me because I was once a beneficiary of financial aid while in graduate school. Attending SciPy truly enabled my career, and I'm passionate about providing that same opportunity to others.&lt;/p&gt;
&lt;h2 id="my-tutorials-llms-and-network-analysis"&gt;My tutorials: LLMs and Network Analysis&lt;/h2&gt;&lt;p&gt;This year, I'm excited to be teaching not just one but two tutorials:&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/ericmjl/building-with-llms-made-simple"&gt;&lt;strong&gt;Building with LLMs Made Simple&lt;/strong&gt;&lt;/a&gt; (Day 1) - This new tutorial distills what I've learned over the past few years working with large language models both at Moderna and in my personal projects. We'll cover fundamental LLM interactions, extracting structured outputs from text, and if time permits, dive into retrieval-augmented generation (RAG). Rather than building ambitious systems, we'll focus on teaching practical fundamentals and exchanging ideas.&lt;/p&gt;
&lt;p&gt;&lt;a href="https://github.com/ericmjl/Network-Analysis-Made-Simple"&gt;&lt;strong&gt;Network Analysis Made Simple&lt;/strong&gt;&lt;/a&gt; (Day 2) - I've taught this tutorial many times, sometimes solo and sometimes with collaborators like &lt;a href="https://mriduls.com/"&gt;Mridul Seth&lt;/a&gt;. It focuses on thinking with graphs and applied network science. What I love about this subject is how versatile graph applications can be—from simpler use cases where graphs serve as powerful data models to more complex implementations like graph neural networks.&lt;/p&gt;
&lt;h2 id="code-sprints-where-the-magic-happens"&gt;Code sprints: where the magic happens&lt;/h2&gt;&lt;p&gt;Finally, I'm looking forward to the code sprints—two full days dedicated to contributing to the open-source tools that power our work. Every year I attend the sprints because they're simply fun! It's a great opportunity to give back to projects I use regularly and learn from other contributors. I've also written before why, if you're attending, code sprints are a worthwhile thing to attend -- check out the post &lt;a href="../../../3/17/why-you-should-take-part-in-the-scipy-sprints/"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;If you're on the fence about attending SciPy 2025, I can't recommend it enough. Meeting the tool makers, learning about new libraries, and being part of this vibrant community is an experience unlike any other conference. Hope to see you there!&lt;/p&gt;
</content></entry><entry><title>Wow, Modal!</title><link href="https://ericmjl.github.io/blog/2025/4/26/wow-modal/" rel="alternate"/><updated>2025-04-26T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:cd3c22f7-2acf-3b0a-a8c6-3281ae0de020</id><content type="html">&lt;p&gt;I've finally got back to playing with Modal, and I'm thoroughly impressed.&lt;/p&gt;
&lt;p&gt;Stuff you can do with Modal are a dime a dozen, and their docs are so good and chock full of real-life examples that I won't reiterate them here. I instead wanted to highlight some of the things I've tried that were useful to me.&lt;/p&gt;
&lt;h2 id="deploy-entire-web-apps-on-modal"&gt;Deploy entire web apps on Modal&lt;/h2&gt;&lt;p&gt;My recent experiments with Modal are to use it as a PaaS, where I deploy whole apps onto Modal backed by a SQLite database on a Modal Volume. This turns out to be an uber cost-efficient way to deploy stuff -- fully serverless with no constitutively-running server.&lt;/p&gt;
&lt;p&gt;My previous stack was Docker + a self-hosted Dokku instance on DigitalOcean, and while I was able to get it running after a few tries, it was hard for me to nail down the patterns because of the sheer number of manual steps involved. With Modal, the simplicity of configuration-as-Python (for someone who is familiar with Python) coupled with very simple CI/CD configuration makes this super easy for solo builders to quickly ship deployments on every commit.&lt;/p&gt;
&lt;p&gt;To show an example, I hosted a quick PyPI server with Basic Auth on Modal through &lt;a href="https://github.com/ericmjl/modal-pypi-server"&gt;this git repository&lt;/a&gt;. If you study the repository, you'll also see the endpoints created for registering a user (using an authentication token that should only be known by administrators). Moreover, the CI/CD pipelines allow me to re-deploy the app fresh on each commit to &lt;code&gt;main&lt;/code&gt;. While I didn't necessarily put the highest degree of security on it, the deployed PyPI server can get the job done if one just needs to quickly stand up one for internal packages.&lt;/p&gt;
&lt;h2 id="host-ollama-with-low-latency"&gt;Host Ollama with low latency&lt;/h2&gt;&lt;p&gt;One of the most exciting use cases I've found is deploying Ollama on Modal with remarkably low latency. After some experimentation with &lt;a href="https://github.com/ericmjl/ollama-on-modal/blob/main/endpoint_v2.py"&gt;this repository&lt;/a&gt;, I was able to get a super responsive setup by using a Modal volume to host the model weights.&lt;/p&gt;
&lt;p&gt;The architecture is straightforward but powerful:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Create a Modal function dedicated to downloading and pre-caching LLMs on a Modal volume&lt;/li&gt;
&lt;li&gt;Run an Ollama web endpoint on Modal that accesses these cached models&lt;/li&gt;
&lt;li&gt;Benefit from Modal's GPU infrastructure without the overhead of spinning up your own instances&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What makes this particularly compelling is the cost efficiency. Rather than maintaining a dedicated GPU instance 24/7, Modal lets you pay only for what you use while still keeping your models readily available thanks to the persistent volume storage. Ollama's container also loads very quickly from the persistent volume, giving me &lt;em&gt;at most&lt;/em&gt; seconds of wait time for larger models like Mistral Small 3.1 when I send an API call over the wire.&lt;/p&gt;
&lt;p&gt;To test this API, I did two experiments: (a) I connected to it using OpenWebUI (running locally, though in principle, this can be run on Modal as well), and (b) through LlamaBot. In both cases, all I needed to do was to provide &lt;code&gt;https://&amp;lt;my-modal-app-service-url&amp;gt;.modal.run&lt;/code&gt; (without a trailing &lt;code&gt;/&lt;/code&gt;!) as the API endpoint to hit, and it worked seamlessly:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;bot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;SimpleBot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;You are an expert user of Git.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;model_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ollama_chat/llama3&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;temperature&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;api_base&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;OLLAMA_API_BASE&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;stream&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;diff&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;diff --git a/llamabot/bot/model_dispatcher.py...&amp;#39;&lt;/span&gt;
&lt;span class="n"&gt;bot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;write_commit_message&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;diff&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The entire setup requires surprisingly little code—just define your volumes, create functions for model management, and set up the web endpoint. Modal handles all the complex infrastructure orchestration behind the scenes, letting you focus on actually using the models rather than maintaining them.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;My first contribution to the Modal examples gallery was this exact one! You can find it &lt;a href="https://github.com/modal-labs/modal-examples/pull/1138"&gt;here&lt;/a&gt;.&lt;/em&gt;&lt;/p&gt;
&lt;h2 id="spin-up-a-serverless-workstation"&gt;Spin up a "serverless workstation"&lt;/h2&gt;&lt;p&gt;I also thought of asking the question: can we create a serverless cloud-based workstation?&lt;/p&gt;
&lt;p&gt;If this term confuses you, let me explain.&lt;/p&gt;
&lt;p&gt;The heart of a workstation is nothing more than a persistent hard disk that stores user-level configuration inside their home directory. RAM, CPU, and GPU are fungible and swappable; what makes a "workstation" feel at home is the level of customization you can put into it to make it feel like your own machine. My gut told me that we should be able to do this as well on Modal, and the answer is yes!&lt;/p&gt;
&lt;p&gt;My implementation uses a Debian-based container with VSCode, Git, and both &lt;code&gt;uv&lt;/code&gt; and &lt;code&gt;pixi&lt;/code&gt; pre-installed. The magic happens with three Modal volumes that persist between sessions: one for VSCode settings, another for server data, and a third for my code repositories.&lt;/p&gt;
&lt;p&gt;VSCode runs in "tunnel" mode, which means I can securely access my development environment from any device with a web browser. I can clone repositories to the persistent &lt;code&gt;/root/_repos&lt;/code&gt; directory, install all my favorite extensions, and use Pixi to manage Python environments for different projects.&lt;/p&gt;
&lt;p&gt;To keep costs low, I configured the server to automatically time out after 5 minutes of inactivity. The flexibility to adjust RAM, CPU, and even GPU allocations based on what I'm working on has been incredibly useful—lightweight coding sessions can use minimal resources, while data processing tasks can scale up as needed.&lt;/p&gt;
&lt;p&gt;I wouldn't generally recommend doing this because you already have access to interactive GPU compute via Modal functions, so in principle, it's easy for you to primarily work out of your laptop and burst to the cloud through modal Functions. Nonetheless, it was fun to try to replicate some of the architectures that I've seen at work. From the Slack community channel, I can see there may be thought put into serverless-based interactive work, and I'm excited for that direction! And if you're curious about how I did it, check it out &lt;a href="https://github.com/ericmjl/modal-vscode-workstation"&gt;at this repo&lt;/a&gt;.&lt;/p&gt;
&lt;h2 id="what-modal-did-so-well"&gt;What Modal did so well&lt;/h2&gt;&lt;p&gt;What impressed me most about Modal is how they've dramatically simplified cloud infrastructure in several key ways. First, they made volume management intuitive—you can spin up persistent storage using code and easily transfer data to and from it with minimal fuss (e.g. using the CLI). The resource specification is refreshingly straightforward too; just declare what you need in Python code and Modal handles the rest.&lt;/p&gt;
&lt;p&gt;Modal truly shines in how it unifies abstractions that AWS spreads across multiple services. Instead of juggling between Lambda, Batch, ECS, and EC2, Modal gives you a single "function" concept that can transform into various types of web endpoints as needed. This simplification makes a world of difference for productivity.&lt;/p&gt;
&lt;p&gt;I appreciate their approach to secrets management. You specify env vars using &lt;code&gt;.env&lt;/code&gt; during the image build, and you specify secrets on the web UI. Their configuration-as-Python-code philosophy keeps makes it super intuitive to declare infrastructure as code, reminiscent of Marimo notebooks and Python scripts following PEP723 standards.&lt;/p&gt;
&lt;p&gt;Perhaps most underrated is how Modal makes debugging a program super simple. With a simple &lt;code&gt;modal shell&lt;/code&gt; command, I can inspect the environment directly. This might sound minor, but as someone who's wrestled with Linux systems before, knowing where everything lives is often the biggest hurdle to overcome. And to do it on the remote container itself, in a live session, is uber enabling.&lt;/p&gt;
&lt;p&gt;Finally... the pricing! Every other PaaS that I've seen charges a flat fee &lt;em&gt;per month per app&lt;/em&gt;. For an app that gets little usage but for which I want to be available on-demand, that's not worthwhile for me. With Modal's free tier, we have 8 web endpoints that we can deploy, for which their usage cost is entirely determined by &lt;em&gt;usage&lt;/em&gt; alone. That's the right way to do the cloud.&lt;/p&gt;
&lt;h2 id="show-me-the-repos-again"&gt;Show me the repos again&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://github.com/ericmjl/modal-pypi-server"&gt;Modal PyPI server&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ericmjl/ollama-on-modal/blob/main/endpoint_v2.py"&gt;Ollama on Modal&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://github.com/ericmjl/modal-vscode-workstation"&gt;Serverless workstation&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content></entry><entry><title>Good practices for AI-assisted development from a live protein calculator demo</title><link href="https://ericmjl.github.io/blog/2025/4/19/good-practices-for-ai-assisted-development-from-a-live-protein-calculator-demo/" rel="alternate"/><updated>2025-04-19T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:5d829b5a-e7a6-394c-b132-b9752c235a0c</id><content type="html">&lt;p&gt;&lt;a href="../../2/how-to-standardize-data-science-ways-of-working-to-unlock-your-teams-creativity/"&gt;In a previous blog post&lt;/a&gt;, I explored how standardization helps data science teams fly and actually unleashes creativity rather than constraining it. Here, I'd like to share my experience from a live coding demonstration I gave at BioIT World 2025 as part of the AI in Software Engineering track.&lt;/p&gt;
&lt;h2 id="what-i-built"&gt;What I built&lt;/h2&gt;&lt;p&gt;During my presentation, I built a protein mass spectrometry calculator tool live in front of the audience. This command-line tool mirrors the delivery model that I helped co-design at work, where we typically deliver our solutions as command-line tools. For this demonstration, I specifically chose to build a tool that simulates proteolytic digests and calculates mass spectra.&lt;/p&gt;
&lt;p&gt;The goal was to create a tool that could:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Accept a protein sequence input in various file formats&lt;/li&gt;
&lt;li&gt;Simulate proteolytic digestion&lt;/li&gt;
&lt;li&gt;Calculate what the mass spectra would look like&lt;/li&gt;
&lt;li&gt;Display results as a plot or PNG file&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I wanted this tool to have three interfaces:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;A Python library for notebook interaction&lt;/li&gt;
&lt;li&gt;A command-line interface using the Python library&lt;/li&gt;
&lt;li&gt;A web frontend backed by an API (though I didn't implement this during the demo)&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="starting-with-structure"&gt;Starting with structure&lt;/h2&gt;&lt;p&gt;I began by scaffolding a new project repository using my personal open-source tool called &lt;a href="./"&gt;&lt;code&gt;pyds-cli&lt;/code&gt;&lt;/a&gt;. This automatically creates a standardized project structure - something I emphasized is critical for collaborative data science work.&lt;/p&gt;
&lt;p&gt;"Standardization is really important for a data science team to fly," I explained. "We've standardized our ways of working such that every project looks pretty much exactly the same."&lt;/p&gt;
&lt;p&gt;The key design principle here is consistency in interface while maintaining flexibility in implementation. As I noted during the presentation, "the project looks the same, but underneath the hood, what methods you use, what plotting packages you like, what algorithms you choose, they don't matter. You do what you need." This standardization means that when someone else needs to jump in and help with a project, they can do so without wasting time figuring out how things are organized. The familiar structure lets them focus immediately on the substance of the work rather than deciphering a new system.&lt;/p&gt;
&lt;p&gt;Far from constraining creativity, this standardization enables it by removing unnecessary cognitive overhead. Data scientists can concentrate on solving problems instead of reinventing project structures and interfaces for each new initiative.&lt;/p&gt;
&lt;h2 id="design-first-code-second"&gt;Design first, code second&lt;/h2&gt;&lt;p&gt;Instead of diving straight into "vibe coding" (coding without clear direction), I started with a design document. I dictated my requirements to the AI assistant:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The functionality I needed&lt;/li&gt;
&lt;li&gt;The three interfaces I wanted&lt;/li&gt;
&lt;li&gt;The overall architecture&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The AI generated a comprehensive design document that included:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Project overview&lt;/li&gt;
&lt;li&gt;Functional requirements&lt;/li&gt;
&lt;li&gt;Architecture design&lt;/li&gt;
&lt;li&gt;Key modules (parsers, digestion simulation, mass spec calculation, visualization)&lt;/li&gt;
&lt;li&gt;Usage patterns&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;As I reviewed this document, I noted areas where I lacked expertise: "I don't have enough expertise to evaluate this. Got to talk with a biologist or mass spec expert." This self-awareness is crucial when working with AI tools to ensure that you don't end up building something with a fundamental logical error.&lt;/p&gt;
&lt;h2 id="implementing-with-ai-assistance"&gt;Implementing with AI assistance&lt;/h2&gt;&lt;p&gt;After reviewing the design, I switched to using the AI agent mode to implement the library and command-line interface. The agent created:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Parsers for protein sequences&lt;/li&gt;
&lt;li&gt;A digestion module with support for various enzymes&lt;/li&gt;
&lt;li&gt;Mass spec calculation functionality&lt;/li&gt;
&lt;li&gt;Visualization capabilities&lt;/li&gt;
&lt;li&gt;A command-line interface&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What would have taken me about three days of work was accomplished in minutes. However, I emphasized that this wasn't the end of the process - I would normally spend at least an hour or two reviewing the code to ensure it was correct and identify knowledge gaps.&lt;/p&gt;
&lt;h2 id="the-final-product"&gt;The final product&lt;/h2&gt;&lt;p&gt;When I tried to demonstrate the tool's functionality, I ran into some issues - a common occurrence in live demos! In my case, it was a package dependency problem. As I joked to the audience, "It is these moments like these that make me go, alright, I'm going to pull out the cooking show style thing and switch over to the pre-baked, already-working thing that I have prepared for everybody."&lt;/p&gt;
&lt;p&gt;This "cooking show mode" saved the demo, allowing me to showcase the working version I had prepared beforehand:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Calculating protein mass&lt;/li&gt;
&lt;li&gt;Performing trypsin digestion&lt;/li&gt;
&lt;li&gt;Outputting mass/charge values&lt;/li&gt;
&lt;li&gt;Generating a visualization of the mass spectrum&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;To the audience's applause, I concluded with the lessons learned.&lt;/p&gt;
&lt;h2 id="key-lessons-learned"&gt;Key lessons learned&lt;/h2&gt;&lt;p&gt;Throughout this exercise, I noted several important principles for effective AI collaboration. &lt;strong&gt;Context is key&lt;/strong&gt; - the more contextual information you provide to AI, the better its output. As Michael Smallegen had mentioned in his preceding lightning talk, trying to vibe code from scratch with minimal context would inevitably result in "spaghetti" code that's difficult to maintain.&lt;/p&gt;
&lt;p&gt;Even as data scientists, we need to &lt;strong&gt;apply software development judgment&lt;/strong&gt; to the process. Data scientists who can build the operational software around their analytical work minimize handoff delays and dependencies on other teams. AI assistance significantly lowers the energy barrier to making this software work, but without critical oversight and software development principles, AI-generated code can quickly become unmaintainable. Throughout the demo, I found myself constantly evaluating the code against established patterns and identifying areas where I lacked expertise and needed to consult with specialists.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Interactive communication enhances development&lt;/strong&gt;. During the demo, I discovered that dictating my requirements was faster than typing, and AI tools acted like "an intellectual sparring partner" to help clarify what I really needed. Even when my initial instructions weren't perfectly formed, the back-and-forth conversation helped refine my thinking as I worked through the problem.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Documentation emerged as another crucial advantage with these patterns&lt;/strong&gt;. One powerful aspect of this development process was getting comprehensive documentation without having to write it myself - a task that often gets neglected in rapid development cycles.&lt;/p&gt;
&lt;p&gt;My ex-colleague Matt Whitfield, who now works at Dash Bio, has taken this approach further with &lt;strong&gt;requirements-driven development&lt;/strong&gt;, where requirements become part of the codebase itself in the form of documentation, serving as context for both humans and AI. This seamless integration of specifications and implementation creates a powerful feedback loop.&lt;/p&gt;
&lt;p&gt;When used thoughtfully, AI tools can dramatically accelerate development while still allowing us to apply our domain expertise and judgment. The key is &lt;strong&gt;maintaining human agency&lt;/strong&gt; in the process - using AI as a powerful assistant rather than letting it take over completely.&lt;/p&gt;
&lt;h2 id="one-more-thing-the-repository"&gt;One more thing: the repository&lt;/h2&gt;&lt;p&gt;I put the repository up here for anyone who wants to explore it: &lt;a href="https://github.com/ericmjl/protein-mass-spec-calculator"&gt;https://github.com/ericmjl/protein-mass-spec-calculator&lt;/a&gt;.&lt;/p&gt;
</content></entry><entry><title>Wow, Marimo!</title><link href="https://ericmjl.github.io/blog/2025/4/8/wow-marimo/" rel="alternate"/><updated>2025-04-08T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:4f404eba-ce9f-3b10-9762-638922802eea</id><content type="html">&lt;p&gt;I have been using Marimo notebooks recently, and I'm thoroughly impressed. There are many benefits to using &lt;code&gt;marimo&lt;/code&gt;. The biggest are fully reactive notebooks, such that if you change a cell and execute it, all cells that depend on it will automatically re-execute, and self-contained notebook environments, so you never have to create a scratch environment to run a notebook. If you haven't started test-driving Marimo to see whether it works for you, I think it's time to start experimenting!&lt;/p&gt;
&lt;h2 id="run-marimo-without-ever-installing-it"&gt;Run Marimo... without ever installing it&lt;/h2&gt;&lt;p&gt;Thanks to &lt;code&gt;uv&lt;/code&gt;, we can run &lt;code&gt;marimo&lt;/code&gt; without ever needing to explicitly install it. This is a major upgrade from installing it with &lt;code&gt;pip&lt;/code&gt;/&lt;code&gt;conda&lt;/code&gt; and having to remember where on my &lt;a href="https://ericmjl.github.io/essays-on-data-science/software-skills/environment-variables/"&gt;PATH environment variable&lt;/a&gt; &lt;code&gt;marimo&lt;/code&gt; is. No more asking the question, "Did I install it in my base conda environment? Or was it installed in another env?"&lt;/p&gt;
&lt;p&gt;To do this, with &lt;code&gt;uv&lt;/code&gt; installed on your &lt;code&gt;PATH&lt;/code&gt;:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;uvx&lt;span class="w"&gt; &lt;/span&gt;marimo&lt;span class="w"&gt; &lt;/span&gt;edit&lt;span class="w"&gt; &lt;/span&gt;--sandbox&lt;span class="w"&gt; &lt;/span&gt;/path/to/notebook.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The notebook will get created automatically if it doesn't exist.&lt;/p&gt;
&lt;p&gt;What I love about this is that I get to use the latest &lt;code&gt;marimo&lt;/code&gt; all the time. And I don't have to think about what version of &lt;code&gt;marimo&lt;/code&gt; I'm carrying around in my local machine. &lt;code&gt;uv&lt;/code&gt; is providing the equivalent of a serverless API for CLI tools.&lt;/p&gt;
&lt;h2 id="ensure-notebooks-carry-their-own-environment"&gt;Ensure notebooks carry their own environment&lt;/h2&gt;&lt;p&gt;Marimo notebooks can be self-contained, with Python dependencies fully-specified in-file with &lt;a href="https://packaging.python.org/en/latest/specifications/inline-script-metadata/"&gt;PEP723-compatible in-line script metadata&lt;/a&gt;. The way to ensure that this is done is by running exactly the command above, with &lt;code&gt;--sandbox&lt;/code&gt; being the key. Additionally, if you add packages to the self-contained notebook via UI, they automagically get added into the in-line script metadata. The resulting file looks something like this:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# /// script&lt;/span&gt;
&lt;span class="c1"&gt;# requires-python = &amp;quot;&amp;gt;=3.12&amp;quot;&lt;/span&gt;
&lt;span class="c1"&gt;# dependencies = [&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;anthropic==0.49.0&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;arviz==0.21.0&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;joypy==0.2.6&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;jupyter-core==5.7.2&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;marimo&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;matplotlib==3.10.1&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;nbformat==5.10.4&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;numba==0.61.0&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;numpy==2.1.0&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;nutpie==0.14.3&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;pandas==2.2.3&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;pymc==5.21.1&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;seaborn==0.13.2&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;tqdm==4.67.1&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;# ]&lt;/span&gt;
&lt;span class="c1"&gt;# ///&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;marimo&lt;/span&gt;

&lt;span class="n"&gt;__generated_with&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;0.12.2&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;marimo&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;App&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;medium&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="nd"&gt;@app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cell&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;_&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;marimo&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;mo&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;mo&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;

&lt;span class="c1"&gt;# ...the rest of the notebook below...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Also, you can specify alternate sources in-line, with this example coming from my Network Analysis Made Simple repository:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# /// script&lt;/span&gt;
&lt;span class="c1"&gt;# requires-python = &amp;quot;&amp;gt;=3.13&amp;quot;&lt;/span&gt;
&lt;span class="c1"&gt;# dependencies = [&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;ipython==9.1.0&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;marimo&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;matplotlib==3.10.1&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;nams==0.0.2&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;networkx==3.4.2&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;numpy==2.2.5&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;pyprojroot==0.3.0&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;tqdm==4.67.1&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;# ]&lt;/span&gt;
&lt;span class="c1"&gt;# [[tool.uv.index]]&lt;/span&gt;
&lt;span class="c1"&gt;# name = &amp;quot;ericmjl-personal-packages&amp;quot;&lt;/span&gt;
&lt;span class="c1"&gt;# url = &amp;quot;https://ericmjl--pypiserver-server.modal.run/simple/&amp;quot;&lt;/span&gt;
&lt;span class="c1"&gt;# explicit = true&lt;/span&gt;
&lt;span class="c1"&gt;# [tool.uv.sources]&lt;/span&gt;
&lt;span class="c1"&gt;# nams = { index = &amp;quot;ericmjl-personal-packages&amp;quot; }&lt;/span&gt;
&lt;span class="c1"&gt;# ///&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="install-local-package-in-editable-mode"&gt;Install local package in editable mode&lt;/h2&gt;&lt;p&gt;You can interact with your local package in editable mode with Marimo notebooks. To do so you can add the package using the &lt;code&gt;packages&lt;/code&gt; tab in the UI:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;-e&lt;span class="w"&gt; &lt;/span&gt;&amp;lt;package-name&amp;gt;&lt;span class="w"&gt; &lt;/span&gt;@&lt;span class="w"&gt; &lt;/span&gt;.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It will be installed in editable mode, and it will be added to the in-line script metadata:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# /// script&lt;/span&gt;
&lt;span class="c1"&gt;# requires-python = &amp;quot;&amp;gt;=3.12&amp;quot;&lt;/span&gt;
&lt;span class="c1"&gt;# dependencies = [&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;&amp;lt;package-name&amp;gt;==&amp;lt;version-in-pyproject.toml&amp;gt;&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;marimo&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     ...&lt;/span&gt;
&lt;span class="c1"&gt;# ]&lt;/span&gt;
&lt;span class="c1"&gt;# ///&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="enable-ai-assistance-with-marimo-notebooks"&gt;Enable AI assistance with Marimo notebooks&lt;/h2&gt;&lt;p&gt;At this point, for anyone experienced enough in code writing, writing code with AI assistance is pretty much table stakes. (If you are just starting out, I would still encourage you to seek out a human mentor to teach you good patterns for writing reliable code!) Marimo has support for AI-assisted coding, and though it has some rough spots, I think it's worth taking a look at.&lt;/p&gt;
&lt;p&gt;To enable AI assistance, you'll need an API key for one of the major API providers (OpenAI, Anthropic, Google), and you can enable code completion using GitHub Copilot, Codeium, or Ollama (custom).&lt;/p&gt;
&lt;p&gt;That said, having gotten used to Cursor's more interactive style of coding assistance, I found Marimo's implementation of AI assistance to be a tad constraining. I can't do multi-cell edits, for example, and the connection to GitHub Copilot (for inline assistance) often shows an error connecting. My workaround for now has been to write a bunch of cells in Marimo, and then switch over to Cursor to directly edit the &lt;code&gt;.py&lt;/code&gt; file (e.g. to condense it to be less verbose, or correct inconsistencies I might have accumulated).&lt;/p&gt;
&lt;h2 id="run-a-marimo-notebook-directly-from-the-web"&gt;Run a marimo notebook directly from the web&lt;/h2&gt;&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;uvx&lt;span class="w"&gt; &lt;/span&gt;marimo&lt;span class="w"&gt; &lt;/span&gt;edit&lt;span class="w"&gt; &lt;/span&gt;--sandbox&lt;span class="w"&gt; &lt;/span&gt;https://url.to/your/notebook.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is so, so, so, so powerful and convenient! Simply point &lt;code&gt;marimo&lt;/code&gt; to an existing URL that constitutes the notebook, and &lt;code&gt;marimo&lt;/code&gt; will clone it and run it within a sandboxed environment for you. There is also an option there to run it within a Dockerized container of its own too, so it'll be entirely fenced off from your system.&lt;/p&gt;
&lt;h2 id="export-marimo-notebooks-as-markdown-jupyter-notebook-style"&gt;Export Marimo notebooks as Markdown, Jupyter notebook-style&lt;/h2&gt;&lt;p&gt;When I wrote my blog post on the use of the &lt;a href="https://ericmjl.github.io/blog/2025/4/3/bayesian-superiority-estimation-with-r2d2-priors-a-practical-guide-for-protein-screening/"&gt;R2D2 prior and Bayesian probability of superiority calculation&lt;/a&gt;, I used &lt;code&gt;marimo&lt;/code&gt; alongside &lt;code&gt;uvx&lt;/code&gt; to write the post with prose alongside code. One thing I wanted to do was to export it as a Markdown file with cell execution outputs, but Marimo's markdown exports don't carry the capability to do so natively.&lt;/p&gt;
&lt;p&gt;Pre-requisites:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Make sure that your notebooks contain &lt;code&gt;nbformat&lt;/code&gt; as part of its PEP723-style dependency declaration, and&lt;/li&gt;
&lt;li&gt;Make sure you have &lt;code&gt;uv&lt;/code&gt; installed on your system.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Firstly, export Marimo notebook while including outputs to a Jupyter notebook:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;uvx&lt;span class="w"&gt; &lt;/span&gt;marimo&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nb"&gt;export&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;ipynb&lt;span class="w"&gt; &lt;/span&gt;/path/to/notebook.py&lt;span class="w"&gt; &lt;/span&gt;-o&lt;span class="w"&gt; &lt;/span&gt;/path/to/notebook.ipynb&lt;span class="w"&gt; &lt;/span&gt;--include-outputs&lt;span class="w"&gt; &lt;/span&gt;--sandbox
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Secondly, export the Jupyter notebook to Markdown:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;uvx&lt;span class="w"&gt; &lt;/span&gt;--with&lt;span class="w"&gt; &lt;/span&gt;nbconvert&lt;span class="w"&gt; &lt;/span&gt;--from&lt;span class="w"&gt; &lt;/span&gt;jupyter-core&lt;span class="w"&gt; &lt;/span&gt;jupyter&lt;span class="w"&gt; &lt;/span&gt;nbconvert&lt;span class="w"&gt; &lt;/span&gt;--to&lt;span class="w"&gt; &lt;/span&gt;markdown&lt;span class="w"&gt; &lt;/span&gt;/path/to/notebook.ipynb
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And in this way, your notebook will be exportable to Markdown with outputs from the notebook cells. It took me a little while to figure this out, but now that I did, I'm glad I have it as an option, as I can now write entire notebooks in Marimo notebooks and have them exportable to my blog or eBooks.&lt;/p&gt;
&lt;h2 id="serve-a-marimo-notebook-as-a-modal-app"&gt;Serve a marimo notebook as a Modal app&lt;/h2&gt;&lt;p&gt;As it turns out, with Modal's ability to serve up &lt;em&gt;any&lt;/em&gt; arbitrary web server, we can deploy Marimo notebooks to Modal easily.&lt;/p&gt;
&lt;p&gt;Given a Marimo notebook that looks like this:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# file: demo.py&lt;/span&gt;
&lt;span class="c1"&gt;# /// script&lt;/span&gt;
&lt;span class="c1"&gt;# requires-python = &amp;quot;&amp;gt;=3.12&amp;quot;&lt;/span&gt;
&lt;span class="c1"&gt;# dependencies = [&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;anthropic==0.49.0&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;#     &amp;quot;pandas==2.2.3&amp;quot;,&lt;/span&gt;
&lt;span class="c1"&gt;# ]&lt;/span&gt;
&lt;span class="c1"&gt;# ///&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;marimo&lt;/span&gt;

&lt;span class="n"&gt;__generated_with&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;0.12.2&amp;quot;&lt;/span&gt;
&lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;marimo&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;App&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;width&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;medium&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;layout_file&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;layouts/demo.grid.json&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="nd"&gt;@app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cell&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;_&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;pandas&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;pd&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;,)&lt;/span&gt;


&lt;span class="nd"&gt;@app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cell&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;_&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Name&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;John&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Emma&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Michael&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Sarah&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;David&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Age&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;35&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;28&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;32&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;City&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;New York&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;London&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Paris&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Tokyo&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;Berlin&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="s1"&gt;&amp;#39;Salary&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;50000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;60000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;75000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;55000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;65000&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;df&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;


&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="vm"&gt;__name__&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;__main__&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;run&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And a Modal deployment script that looks like this:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# file: deployment.py&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;modal&lt;/span&gt;

&lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;modal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Image&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;debian_slim&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pip_install&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;uv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_local_file&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;demo.py&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/app/demo.py&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;add_local_dir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;layouts&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;/app/layouts&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;copy&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;workdir&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;/app&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;app&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;modal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;App&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;marimo-app&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;


&lt;span class="nd"&gt;@app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;function&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;allow_concurrent_inputs&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nd"&gt;@modal&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;web_server&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;8000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;startup_timeout&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;60&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;marimo_app&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Hello, World!&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;subprocess&lt;/span&gt;

    &lt;span class="c1"&gt;# Port must match the web_server port, and host must be 0.0.0.0 for this to work.&lt;/span&gt;
    &lt;span class="n"&gt;cmd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;uvx marimo run demo.py --sandbox --port 8000 --host 0.0.0.0 --headless&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;subprocess&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Popen&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;cmd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shell&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;One can iterate quickly on the notebook in Marimo, and then once you're ready, check that the deployment works (for fast iteration):&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;modal&lt;span class="w"&gt; &lt;/span&gt;serve&lt;span class="w"&gt; &lt;/span&gt;deployment.py
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And to do the final deployment (best done via GitHub CI/CD):&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;CI/CD&lt;/span&gt;

&lt;span class="nt"&gt;on&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;push&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;branches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;main&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;pull_request&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;branches&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;main&lt;/span&gt;

&lt;span class="nt"&gt;jobs&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;deploy&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Deploy&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;runs-on&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ubuntu-latest&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;env&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="nt"&gt;MODAL_TOKEN_ID&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;${{ secrets.MODAL_TOKEN_ID }}&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="nt"&gt;MODAL_TOKEN_SECRET&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;${{ secrets.MODAL_TOKEN_SECRET }}&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="nt"&gt;DEPLOY_ENV&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;${{ github.event_name == &amp;#39;pull_request&amp;#39; &amp;amp;&amp;amp; &amp;#39;test&amp;#39; || &amp;#39;main&amp;#39; }}&lt;/span&gt;

&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Checkout Repository&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;uses&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;actions/checkout@v4&lt;/span&gt;

&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Install the latest version of uv&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;uses&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;astral-sh/setup-uv@v5&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;with&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="nt"&gt;version&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;latest&amp;quot;&lt;/span&gt;

&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Install Modal&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;|&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;uv tool install modal&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Deploy job&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;|&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;echo &amp;quot;Deploying with $DEPLOY_ENV environment&amp;quot;&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;modal deploy deployment.py -e $DEPLOY_ENV&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This gives you an outlet to deploy Marimo apps in a completely serverless fashion. Super good for runtime costs!&lt;/p&gt;
&lt;p&gt;But Modal apps can run in WASM already, though, so why bother? It turns out not every Python package is built with WASM as a target or has issues running in WASM (PyMC is one of the latter), so this gives us an alternative way of sharing notebooks with others.&lt;/p&gt;
&lt;h2 id="what-one-needs-to-get-used-to"&gt;What one needs to get used to&lt;/h2&gt;&lt;p&gt;Here are some things that I know I had to get used to with Marimo:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The keybindings are different from Jupyter notebooks. I can't just &lt;code&gt;Esc+a&lt;/code&gt; or &lt;code&gt;Esc+b&lt;/code&gt; to quickly add a new cell above or below my current cell. &lt;code&gt;Cmd+Enter&lt;/code&gt; is what's used, not &lt;code&gt;Ctrl+Enter&lt;/code&gt;. Having default keybindings that were different was a weird design choice, and I would love to see keybinding "profiles" enabled on Marimo that let me quickly switch them out to be Jupyter-compatible. Specifically, the ones I really want to have are:&lt;ol&gt;
&lt;li&gt;&lt;code&gt;Esc+a&lt;/code&gt;/&lt;code&gt;Esc+b&lt;/code&gt; to quickly switch from cell editing mode to cell selection mode, and then to add above/below current cell,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Enter&lt;/code&gt; to go from cell selection mode to cell editing mode,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Ctrl+Enter&lt;/code&gt; to execute a cell,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;Cmd+I&lt;/code&gt; or &lt;code&gt;Cmd+K&lt;/code&gt; to reveal the prompt for cell generation mode with AI,&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Reactive execution takes some time to get used to, but once you do, you'll never want to work without it. It's a bit like AI-assisted coding at this point.&lt;/li&gt;
&lt;li&gt;AI-assistance requires you to decide whether to pass in the whole notebook as context or not. I think it should be turned on by default.&lt;/li&gt;
&lt;li&gt;Marimo has a UI builder built-in, which is nice, but is a bit clunky to get used to. Perhaps it's because I'm not used to building UIs, and prefer building CLIs. I can definitely see others finding it to be a wholesome front-end solution though. It's nice to have the toolkit built into the notebook system.&lt;/li&gt;
&lt;/ol&gt;
</content></entry><entry><title>From data chaos to statistical clarity: A laboratory transformation story</title><link href="https://ericmjl.github.io/blog/2025/4/5/from-data-chaos-to-statistical-clarity-a-laboratory-transformation-story/" rel="alternate"/><updated>2025-04-05T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:649a0711-3320-32f8-b2d7-aff811a98710</id><content type="html">&lt;h2 id="the-breaking-point"&gt;The Breaking Point&lt;/h2&gt;&lt;p&gt;Dr. Chen's team was drowning in spreadsheets. Again.&lt;/p&gt;
&lt;p&gt;"I don't understand why these position effects keep showing up," muttered Li, the senior scientist, as she manually flagged another set of outlier wells on their latest assay. "We're spending more time cleaning data than running experiments."&lt;/p&gt;
&lt;p&gt;Their high-throughput screening lab had followed the same workflow for years: carefully plan the biological aspects, run plate-based assays, then spend days wrestling with the resulting data before any actual analysis could begin.&lt;/p&gt;
&lt;p&gt;"Remember when that vendor rep insisted we put all our controls in column 1?" remarked Jamal, gesturing to their plate layout diagram. "Said their software couldn't handle distributed controls?"&lt;/p&gt;
&lt;p&gt;The team nodded grimly. That single decision had cascaded into countless hours of manual data correction as edge effects and position-dependent variations routinely skewed their results. Their analysis scripts had evolved into a jungle of if-else statements attempting to address every new edge case.&lt;/p&gt;
&lt;p&gt;When they approached the research digital team about automating their analysis pipeline, the response was disheartening but predictable.&lt;/p&gt;
&lt;p&gt;"This can't be automated reliably," Maya, the team's embedded data scientist from the research digital team, explained. "There are too many subjective decisions buried in your process. We'd just be encoding your workarounds, not solving the actual problems."&lt;/p&gt;
&lt;h2 id="the-turning-point"&gt;The Turning Point&lt;/h2&gt;&lt;p&gt;The breakthrough came during a seminar on Bayesian statistics. Dr. Chen realized their problem wasn't poor analysis—it was addressing experimental design flaws after data collection instead of preventing them from the start.&lt;/p&gt;
&lt;p&gt;"What if we completely reimagined our workflow?" he proposed at the next team meeting. "Rather than retrofitting solutions to poorly designed experiments, we could address these problems before they occur."&lt;/p&gt;
&lt;p&gt;The team was skeptical. Their current approach, despite its flaws, was familiar. But the hours spent manually curating data had become unsustainable, and they knew something had to change.&lt;/p&gt;
&lt;p&gt;Dr. Chen decided to reach out to Maya again. "I know you said our current workflow can't be reliably automated," he explained, "but we're ready to rebuild from the ground up. Would you consider working more closely with us?"&lt;/p&gt;
&lt;p&gt;Maya hesitated. "What's different this time?"&lt;/p&gt;
&lt;p&gt;"We're not trying to automate our existing process anymore," Dr. Chen explained. "We want to redesign our experiments with statistical principles from the start."&lt;/p&gt;
&lt;p&gt;Maya's expression changed. As a data scientist, she'd spent years trying to salvage analyses from poorly designed experiments. The opportunity to shape the experimental design itself was rare.&lt;/p&gt;
&lt;p&gt;"That changes everything," she said, nodding slowly. "If you're willing to rethink your plate layouts and measurement protocols based on statistical considerations... I'm in."&lt;/p&gt;
&lt;p&gt;Maya agreed, intrigued by the opportunity to apply statistical principles from the beginning rather than patching an existing system.&lt;/p&gt;
&lt;h2 id="rebuilding-from-first-principles"&gt;Rebuilding From First Principles&lt;/h2&gt;&lt;p&gt;Their transformation began with mapping the sources of variation in their experiments:&lt;/p&gt;
&lt;p&gt;First, they tackled the position effects problem. Against the vendor's recommendations, they designed a distributed control system, placing reference samples strategically throughout each plate rather than restricting them to dedicated columns.&lt;/p&gt;
&lt;p&gt;"But the software can't handle this layout," protested Li, recalling the vendor's warning.&lt;/p&gt;
&lt;p&gt;"Then we'll write our own analysis scripts," Dr. Chen countered. "One that treats position effects as systematically measurable phenomena rather than nuisance variables."&lt;/p&gt;
&lt;p&gt;They ran several plates with only control samples to create a detailed map of well position biases. The results were striking.&lt;/p&gt;
&lt;p&gt;"Look at this heatmap," Maya pointed out during their analysis review. "It's not just the column 1 effect we've been fighting. The entire plate edge shows elevated values compared to the center wells."&lt;/p&gt;
&lt;p&gt;&lt;img src="plate-effects.webp" alt="Heatmap showing plate position effects with elevated values (darker purple) around the edges and lower values (yellow/green) in the center of the 96-well plate"&gt;&lt;/p&gt;
&lt;p&gt;"The edge wells are consistently showing higher values," Dr. Chen observed, tracing the dark purple regions along the perimeter of the plate. "And it's not uniform—some corners are worse than others."&lt;/p&gt;
&lt;p&gt;This visualization became the foundation for a hierarchical Bayesian model that could account for position-specific variation while maximizing plate space for test samples.&lt;/p&gt;
&lt;p&gt;"Now we can quantify and correct for these position effects systematically," Maya explained. "Rather than manually flagging outliers, we're modeling the actual physical phenomena causing the variation."&lt;/p&gt;
&lt;p&gt;Next, they addressed their outlier problem. Instead of manually flagging extreme values—a process fraught with subjectivity—they needed a more principled approach.&lt;/p&gt;
&lt;p&gt;"I've been reading about robust statistical methods," Jamal shared during a team meeting. "What if we use t-distributed rather than Gaussian residuals in our model?"&lt;/p&gt;
&lt;p&gt;He pulled up a figure from a Bayesian statistics handbook. "Look at this comparison between standard Gaussian regression and robust regression using Student-t likelihood."&lt;/p&gt;
&lt;p&gt;&lt;img src="robust-regression.webp" alt="Comparison of Gaussian vs. robust Student-t regression showing how outliers (red X marks) severely skew the Gaussian fit (yellow dashed line) while the robust fit (solid orange line) maintains the true trend"&gt;&lt;/p&gt;
&lt;p&gt;"See how the outliers—these red X marks—pull the standard regression line way off?" Jamal explained. "But the robust approach stays true to the actual trend. We could apply this same principle to our plate data."&lt;/p&gt;
&lt;p&gt;"So the model effectively down-weights outliers based on their probability," Dr. Chen summarized, "which means we don't have to manually decide what's an outlier anymore."&lt;/p&gt;
&lt;p&gt;"Exactly," Jamal nodded. "The heavier tails of the t-distribution naturally accommodate occasional extreme values."&lt;/p&gt;
&lt;p&gt;It was a few weeks later that Maya made another crucial contribution. After attending a computational statistics conference, she returned excited about a novel approach.&lt;/p&gt;
&lt;p&gt;"I just learned about something called the R2D2 prior," she announced during their weekly meeting. "It could be perfect for our situation and would complement the robust estimation approach Jamal introduced."&lt;/p&gt;
&lt;p&gt;The team looked at her blankly.&lt;/p&gt;
&lt;p&gt;"R2-D2? Like the Star Wars robot?" asked Li with a skeptical smile.&lt;/p&gt;
&lt;p&gt;Maya laughed. "R2D2 stands for 'R-squared Dirichlet Distribution.' It's a statistical prior that could elegantly solve several of our problems at once."&lt;/p&gt;
&lt;p&gt;"Is there a practical implementation?" Jamal asked. "Or is this just theoretical?"&lt;/p&gt;
&lt;p&gt;"Actually, I found a great implementation in PyMC," Maya replied, pulling up her laptop. "There's even a detailed tutorial online about using it for protein screening experiments - very similar to our work. I can share &lt;a href="../../3/bayesian-superiority-estimation-with-r2d2-priors-a-practical-guide-for-protein-screening/"&gt;the link&lt;/a&gt; with everyone."&lt;/p&gt;
&lt;p&gt;She explained how the R2D2 prior could be combined with their robust estimation approach to yield three major benefits:&lt;/p&gt;
&lt;p&gt;"First, while Jamal's t-distribution handles outliers in the measurements themselves, the R2D2 prior works on a different level - it provides a principled way to model the variances in our system," Maya explained, sketching a diagram on the whiteboard. "Second, R2D2 provides automatic variance decomposition, which means we can quantify how much variation comes from different sources—plate position, reagent batch, technician, or true biological signal."&lt;/p&gt;
&lt;p&gt;&lt;img src="team-meeting.webp" alt="AI-generated photo of Maya explaining the R2D2 prior to the team"&gt;&lt;/p&gt;
&lt;p&gt;&lt;em&gt;AI-generated photo of Maya explaining the R2D2 prior to the team, with Dr. Chen sneaking a glance at the photographer.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;"And the third benefit?" Dr. Chen asked.&lt;/p&gt;
&lt;p&gt;"It gives us a built-in measure of model performance," Maya replied. "We can objectively evaluate how well our statistical approach is working without circular reasoning."&lt;/p&gt;
&lt;p&gt;"So the t-distribution and R2D2 prior are complementary?" Jamal asked.&lt;/p&gt;
&lt;p&gt;"Exactly," Maya nodded. "The t-distribution makes our model robust against outliers in individual measurements, while the R2D2 prior helps us understand and decompose the sources of variation across the entire experiment. Together, they give us a complete statistical framework."&lt;/p&gt;
&lt;p&gt;The team was intrigued but cautious. "This sounds promising, but will it actually work with our messy biological data?" Jamal asked.&lt;/p&gt;
&lt;p&gt;"Let's find out," Maya suggested. "I can implement a prototype using our control plate data."&lt;/p&gt;
&lt;p&gt;Maya spent several days meticulously analyzing their historical data, cataloging the different patterns of outliers and position effects that had appeared over the past year.&lt;/p&gt;
&lt;p&gt;"I want to make sure our model accounts for all the edge cases we've seen before," she explained to the team. "If we build it right the first time, we won't need to keep patching it later."&lt;/p&gt;
&lt;p&gt;Her careful preparation paid off. When she finally implemented the initial model combining the t-distribution likelihood with the R2D2 prior, it captured nearly all the patterns they'd observed in their historical data. The statistical framework was sound from the start.&lt;/p&gt;
&lt;p&gt;"The model looks great conceptually," Jamal observed as they reviewed the first results. "But it's taking almost an hour to run for a single plate."&lt;/p&gt;
&lt;p&gt;Maya nodded. "The computational efficiency needs work, but the statistical model itself is solid. We're already capturing the position effects and handling outliers correctly." She made a few targeted optimizations to the sampling approach, preserving the core statistical properties while dramatically reducing computation time.&lt;/p&gt;
&lt;p&gt;There were adjustments needed on the experimental side too. Lab technicians accustomed to the old workflow initially questioned the more distributed control layout. "It feels wrong to put controls in the middle of the plate," one technician admitted. But as they saw the resulting data quality improvements, skepticism quickly gave way to enthusiasm.&lt;/p&gt;
&lt;h2 id="the-payoff-efficiency-fidelity-automation"&gt;The Payoff: Efficiency, Fidelity, Automation&lt;/h2&gt;&lt;p&gt;Six months after implementing their new approach, the transformation was undeniable.&lt;/p&gt;
&lt;p&gt;"I haven't manually excluded a well in weeks," Li remarked during their progress meeting. What once required hours of subjective decision-making now happened automatically through their robust statistical framework.&lt;/p&gt;
&lt;p&gt;The robust R2D2 model had delivered on its promise. Outliers were now handled systematically rather than subjectively. The variance decomposition revealed that nearly 30% of their measurement variation came from plate position effects - knowledge that helped them further refine their experimental design.&lt;/p&gt;
&lt;p&gt;Perhaps most surprisingly, the model performance metrics highlighted which assays were most reliable and which needed further optimization. This led to targeted improvements in their least robust protocols.&lt;/p&gt;
&lt;p&gt;Their analysis pipeline ran without manual intervention, eliminating tedious data cleaning sessions. Scientists were freed to focus on interpretation rather than data wrangling. Reproducibility improved dramatically when analysis decisions were encoded in statistical models rather than arising from individual choices.&lt;/p&gt;
&lt;p&gt;Maya, initially hesitant to automate their process, was now a regular collaborator. "This is exactly how statistical methods should be applied," she told Dr. Chen. "Built into the experimental design from day one, not tacked on as an afterthought."&lt;/p&gt;
&lt;p&gt;The resulting system preserved hierarchical relationships between samples and maintained proper uncertainty quantification throughout.&lt;/p&gt;
&lt;h2 id="lessons-from-their-journey"&gt;Lessons From Their Journey&lt;/h2&gt;&lt;p&gt;Looking back, Dr. Chen's team realized their most powerful statistical tool wasn't sophisticated software or advanced algorithms—it was thoughtful experimental design from the very beginning, combined with principled statistical methods like the R2D2 prior.&lt;/p&gt;
&lt;p&gt;By anticipating sources of variation upfront and building robust statistical approaches, they had not only saved countless hours but had fundamentally improved the quality of their science.&lt;/p&gt;
&lt;p&gt;Their experience demonstrates a powerful truth: experiments designed with analysis in mind produce cleaner data from the start. The extra effort at the bench, coupled with the right statistical framework, translated to exponential savings throughout the data lifecycle.&lt;/p&gt;
&lt;p&gt;For research teams facing similar challenges, the lesson is clear—statistical thinking should inform your initial experimental approach rather than serving as an afterthought. Your future self will thank you for the foresight!&lt;/p&gt;
</content></entry><entry><title>Bayesian Superiority Estimation with R2D2 Priors: A Practical Guide for Protein Screening</title><link href="https://ericmjl.github.io/blog/2025/4/3/bayesian-superiority-estimation-with-r2d2-priors-a-practical-guide-for-protein-screening/" rel="alternate"/><updated>2025-04-03T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:db2ae135-3999-3bd2-a81e-3f5dbc5da2fc</id><content type="html">&lt;p&gt;Recently, I've been thinking and writing a lot about statistics.
It's because good statistical practice is both under-rated, under-taught, and under-valued
amongst machine learning practitioners &lt;em&gt;and&lt;/em&gt; laboratory scientists,
and yet it underpins the ability of machine learning practitioners in life sciences
to build high performing machine learning models that accelerate decisions in the lab.
It also enables laboratory scientists to design experiments that yield interpretable,
and actionable results.&lt;/p&gt;
&lt;p&gt;My prior experience doing the full spectrum of laboratory and computational science
is one of the reasons why it pains me to see potentially good data go to waste
due to poor experimental design and statistical analysis.
Without good statistical practice underlying the data generating process --
and by that I mean good experimental design,
and explicit quantification of uncertainty --
all ML models become equal: equally bad.&lt;/p&gt;
&lt;p&gt;In this blog post, I want to show you how to use Bayesian methods
to tackle two critical questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Is our experimental setup actually measuring the effect we care about
(vs. experimental noise)?&lt;/li&gt;
&lt;li&gt;Which candidates are truly superior to others?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;As always, I go back to my favorite example: screening proteins.
But as you read, note the generalities: they aren't specific to protein screening at all!&lt;/p&gt;
&lt;hr&gt;
&lt;blockquote&gt;&lt;p&gt;Note: The original &lt;code&gt;marimo&lt;/code&gt; notebook can be found &lt;a href="protein_estimation.py"&gt;here&lt;/a&gt;.
If there are discrepancies between my blog post and the original marimo notebook,
I note that the original notebook is correct.
To run it, download it, and then execute:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;uvx&lt;span class="w"&gt; &lt;/span&gt;marimo&lt;span class="w"&gt; &lt;/span&gt;edit&lt;span class="w"&gt; &lt;/span&gt;--sandbox&lt;span class="w"&gt; &lt;/span&gt;https://ericmjl.github.io/blog/2025/4/3/bayesian-superiority-estimation-with-r2d2-priors-a-practical-guide-for-protein-screening/protein_estimation.py
&lt;/pre&gt;&lt;/div&gt;
&lt;/blockquote&gt;
&lt;p&gt;When screening hundreds of molecules, proteins, or interventions,
two critical questions often arise:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Is our experimental setup actually measuring the effect we care about
(vs. experimental noise)?&lt;/li&gt;
&lt;li&gt;Which candidates are truly superior to others?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;In this tutorial, we'll tackle both challenges using a practical example:
a protein screening experiment with fluorescence readouts.
We'll show how &lt;strong&gt;R2D2 priors&lt;/strong&gt; help interpret variance decomposition,
and how &lt;strong&gt;Bayesian superiority calculation&lt;/strong&gt;
enables robust ranking of candidates.
Both techniques generalize to drug discovery, material science,
or any domain requiring rigorous comparison of multiple alternatives.&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;pymc&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;pm&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="the-protein-screening-example"&gt;The Protein Screening Example&lt;/h2&gt;&lt;p&gt;We'll use a dataset with fluorescence measurements for over 100 proteins
across multiple experiments and replicates, with an experimental design that includes:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A control protein present in all experiments and replicates&lt;/li&gt;
&lt;li&gt;"Crossover" proteins measured across all experiments&lt;/li&gt;
&lt;li&gt;Unique test proteins in each experiment&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This design is common in high-throughput screening where measuring all proteins in all conditions is impractical.
We'll implement our analysis using PyMC, a powerful probabilistic programming framework for Bayesian modeling.&lt;/p&gt;
&lt;h2 id="generating-synthetic-data"&gt;Generating Synthetic Data&lt;/h2&gt;&lt;p&gt;To demonstrate our approach, we'll generate synthetic data that mimics a realistic protein screening experiment with:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3 experiments with 2 replicates each&lt;/li&gt;
&lt;li&gt;A control protein and crossover proteins present in all experiments&lt;/li&gt;
&lt;li&gt;Test proteins unique to each experiment&lt;/li&gt;
&lt;li&gt;Systematic experiment effects (batch effects)&lt;/li&gt;
&lt;li&gt;Replicate-level variation&lt;/li&gt;
&lt;li&gt;Measurement noise&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;numpy&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;pandas&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="c1"&gt;# Set random seed for reproducibility&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;42&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Define parameters&lt;/span&gt;
&lt;span class="n"&gt;n_experiments&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;n_replicates&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;
&lt;span class="n"&gt;n_proteins_per_exp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;40&lt;/span&gt;
&lt;span class="n"&gt;n_crossover&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt;

&lt;span class="c1"&gt;# Create protein names&lt;/span&gt;
&lt;span class="n"&gt;control&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Control&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;crossover_proteins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Crossover_&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_crossover&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;
&lt;span class="n"&gt;other_proteins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Protein_&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)]&lt;/span&gt;

&lt;span class="c1"&gt;# Base fluorescence values&lt;/span&gt;
&lt;span class="n"&gt;base_values&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{}&lt;/span&gt;
&lt;span class="n"&gt;base_values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Control&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1000&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;crossover_proteins&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;base_values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;other_proteins&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;base_values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;200&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Create experiment effects&lt;/span&gt;
&lt;span class="n"&gt;exp_effects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_experiments&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;rep_effects&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_experiments&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_replicates&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="c1"&gt;# Generate data&lt;/span&gt;
&lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[]&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;exp&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_experiments&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="c1"&gt;# Select proteins for this experiment&lt;/span&gt;
    &lt;span class="n"&gt;exp_proteins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;control&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;crossover_proteins&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;other_proteins&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;rep&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_replicates&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;protein&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;exp_proteins&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="c1"&gt;# Add noise and effects&lt;/span&gt;
            &lt;span class="n"&gt;value&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="n"&gt;base_values&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;protein&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;exp_effects&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;rep_effects&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rep&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.05&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

            &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;append&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="p"&gt;{&lt;/span&gt;
                    &lt;span class="s2"&gt;&amp;quot;Experiment&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Exp_&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;exp&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s2"&gt;&amp;quot;Replicate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Rep_&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;rep&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s2"&gt;&amp;quot;Protein&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;protein&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                    &lt;span class="s2"&gt;&amp;quot;Fluorescence&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;value&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                &lt;span class="p"&gt;}&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Convert to DataFrame&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border="1" class="dataframe"&gt;
  &lt;thead&gt;
    &lt;tr style="text-align: right;"&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;Experiment&lt;/th&gt;
      &lt;th&gt;Replicate&lt;/th&gt;
      &lt;th&gt;Protein&lt;/th&gt;
      &lt;th&gt;Fluorescence&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;Exp_1&lt;/td&gt;
      &lt;td&gt;Rep_1&lt;/td&gt;
      &lt;td&gt;Control&lt;/td&gt;
      &lt;td&gt;1087.476281&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;Exp_1&lt;/td&gt;
      &lt;td&gt;Rep_1&lt;/td&gt;
      &lt;td&gt;Crossover_0&lt;/td&gt;
      &lt;td&gt;1054.176224&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;Exp_1&lt;/td&gt;
      &lt;td&gt;Rep_1&lt;/td&gt;
      &lt;td&gt;Crossover_1&lt;/td&gt;
      &lt;td&gt;955.647739&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;Exp_1&lt;/td&gt;
      &lt;td&gt;Rep_1&lt;/td&gt;
      &lt;td&gt;Crossover_2&lt;/td&gt;
      &lt;td&gt;1091.751188&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;Exp_1&lt;/td&gt;
      &lt;td&gt;Rep_1&lt;/td&gt;
      &lt;td&gt;Crossover_3&lt;/td&gt;
      &lt;td&gt;1189.344109&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;205&lt;/th&gt;
      &lt;td&gt;Exp_3&lt;/td&gt;
      &lt;td&gt;Rep_2&lt;/td&gt;
      &lt;td&gt;Protein_85&lt;/td&gt;
      &lt;td&gt;1765.149428&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;206&lt;/th&gt;
      &lt;td&gt;Exp_3&lt;/td&gt;
      &lt;td&gt;Rep_2&lt;/td&gt;
      &lt;td&gt;Protein_86&lt;/td&gt;
      &lt;td&gt;1595.422298&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;207&lt;/th&gt;
      &lt;td&gt;Exp_3&lt;/td&gt;
      &lt;td&gt;Rep_2&lt;/td&gt;
      &lt;td&gt;Protein_87&lt;/td&gt;
      &lt;td&gt;1889.585595&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;208&lt;/th&gt;
      &lt;td&gt;Exp_3&lt;/td&gt;
      &lt;td&gt;Rep_2&lt;/td&gt;
      &lt;td&gt;Protein_88&lt;/td&gt;
      &lt;td&gt;1394.395041&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;209&lt;/th&gt;
      &lt;td&gt;Exp_3&lt;/td&gt;
      &lt;td&gt;Rep_2&lt;/td&gt;
      &lt;td&gt;Protein_89&lt;/td&gt;
      &lt;td&gt;1411.831297&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;210 rows × 4 columns&lt;/p&gt;
&lt;/div&gt;&lt;p&gt;In the synthetic dataset we've created, we simulated:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;3 experiments with 2 replicates each&lt;/li&gt;
&lt;li&gt;A control protein and 4 crossover proteins present in all experiments&lt;/li&gt;
&lt;li&gt;100 other proteins distributed across experiments&lt;/li&gt;
&lt;li&gt;Multiplicative experiment effects (mean=1, sd=0.3)&lt;/li&gt;
&lt;li&gt;Multiplicative replicate effects (mean=1, sd=0.1)&lt;/li&gt;
&lt;li&gt;Multiplicative measurement noise (mean=1, sd=0.05)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This structure simulates a typical screening setup
where batch effects between experiments are stronger than replicate variation,
and both contribute significantly to the observed fluorescence values.
The setting mirrors real experimental challenges
where we need to separate biological signal from technical noise.&lt;/p&gt;
&lt;h2 id="examining-the-raw-data"&gt;Examining the Raw Data&lt;/h2&gt;&lt;p&gt;Before modeling, let's visualize the control and crossover proteins to understand experimental variation:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;seaborn&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;sns&lt;/span&gt;

&lt;span class="c1"&gt;# Filter for Control and Crossover samples&lt;/span&gt;
&lt;span class="n"&gt;mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Protein&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;str&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;contains&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Control|Crossover&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;filtered_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;mask&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="c1"&gt;# Create the swarm plot&lt;/span&gt;
&lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;swarmplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;filtered_df&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Experiment&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;y&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Fluorescence&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;hue&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Protein&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;8&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Activity for Controls and Crossover Samples&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Fluorescence&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gca&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src="protein_estimation_8_0.webp" alt="png"&gt;&lt;/p&gt;
&lt;p&gt;Notice the dramatic shift in fluorescence values across experiments.
Experiment 3 shows substantially higher fluorescence readings (around 1500-2000 units)
compared to Experiments 1 and 2 (mostly below 1300 units).&lt;/p&gt;
&lt;p&gt;This pattern illustrates a common challenge in high-throughput screening:
significant batch effects between experiments that can mask the true biological signal.
Without accounting for these experimental factors, we might incorrectly attribute
higher activity to proteins simply because they were measured in Experiment 3.&lt;/p&gt;
&lt;p&gt;Our modeling approach needs to account for both between-experiment and within-experiment
sources of variation to accurately compare proteins across the entire dataset.&lt;/p&gt;
&lt;h2 id="the-r2d2-prior-for-variance-decomposition"&gt;The R2D2 Prior for Variance Decomposition&lt;/h2&gt;&lt;p&gt;The &lt;a href="https://arxiv.org/abs/2111.10718"&gt;R2D2 prior (R-squared Dirichlet decomposition)&lt;/a&gt; provides an interpretable framework
for variance decomposition by placing a prior on the Bayesian coefficient of determination ($R^2$),
which then induces priors on individual parameters.&lt;/p&gt;
&lt;p&gt;The core idea is simple but powerful:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Place a $\text{Beta}(a,b)$ prior on $R^2$ (the proportion of variance explained by the model)&lt;/li&gt;
&lt;li&gt;Induce a prior on the global variance parameter representing total variance&lt;/li&gt;
&lt;li&gt;Decompose the global variance into components via a Dirichlet distribution&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This approach has key advantages over traditional hierarchical models:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Instead of specifying arbitrary priors for each variance component separately, we control
total explained variance through a single interpretable parameter ($R^2$)&lt;/li&gt;
&lt;li&gt;The variance partition becomes meaningful through the Dirichlet distribution&lt;/li&gt;
&lt;li&gt;The hierarchical nature ensures regularization of variance estimates&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here's our implementation of the model:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;_&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c1"&gt;# create categorical indices&lt;/span&gt;
    &lt;span class="n"&gt;exp_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Categorical&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Experiment&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;codes&lt;/span&gt;
    &lt;span class="n"&gt;rep_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Categorical&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Replicate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;codes&lt;/span&gt;
    &lt;span class="n"&gt;prot_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Categorical&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Protein&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;codes&lt;/span&gt;

    &lt;span class="c1"&gt;# define coords for dimensions&lt;/span&gt;
    &lt;span class="n"&gt;coords&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;experiment&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Experiment&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;replicate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Replicate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;protein&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Protein&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;component&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;experiment&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;replicate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;protein&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;coords&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;coords&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="c1"&gt;# 1. residual variance (σ²) stands alone&lt;/span&gt;
        &lt;span class="n"&gt;sigma_squared&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HalfNormal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sigma_squared&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;residual_sd&lt;/span&gt;   &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;residual_sd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sigma_squared&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="c1"&gt;# 2. R² prior and induced total signal variance W&lt;/span&gt;
        &lt;span class="n"&gt;r_squared&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Beta&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;r_squared&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;beta&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;W&lt;/span&gt;         &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;W&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma_squared&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;r_squared&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;r_squared&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# 3. split W among the three signal components&lt;/span&gt;
        &lt;span class="n"&gt;props&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Dirichlet&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;props&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ones&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;component&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;exp_var&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rep_var&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prot_var&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;props&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;W&lt;/span&gt;

        &lt;span class="c1"&gt;# component standard deviations&lt;/span&gt;
        &lt;span class="n"&gt;exp_sd&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;exp_sd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;exp_var&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;rep_sd&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rep_sd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;rep_var&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
        &lt;span class="n"&gt;prot_sd&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prot_sd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;math&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sqrt&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;prot_var&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

        &lt;span class="c1"&gt;# 4. group effects&lt;/span&gt;
        &lt;span class="n"&gt;exp_effect&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;exp_effect&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;exp_sd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;experiment&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;rep_effect&lt;/span&gt;  &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;rep_effect&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;rep_sd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;experiment&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;replicate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;prot_effect&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prot_effect&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;prot_sd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;dims&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;protein&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# global mean on log scale&lt;/span&gt;
        &lt;span class="n"&gt;global_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;global_mean&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;7&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# expected value for each observation&lt;/span&gt;
        &lt;span class="n"&gt;y_hat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="n"&gt;global_mean&lt;/span&gt;
            &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;exp_effect&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;exp_idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;rep_effect&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;exp_idx&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;rep_idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;prot_effect&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;prot_idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# optional: compute implied model R² for checking&lt;/span&gt;
        &lt;span class="n"&gt;model_r2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Deterministic&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;model_r2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;exp_var&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;rep_var&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;prot_var&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;exp_var&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;rep_var&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;prot_var&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;sigma_squared&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# 5. likelihood uses only the residual_sd&lt;/span&gt;
        &lt;span class="n"&gt;y&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Normal&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;y&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;mu&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;y_hat&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;sigma&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;residual_sd&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
            &lt;span class="n"&gt;observed&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;log&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Fluorescence&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

        &lt;span class="c1"&gt;# sample&lt;/span&gt;
        &lt;span class="n"&gt;trace&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pm&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tune&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1000&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;return_inferencedata&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;nuts_sampler&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;nutpie&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trace&lt;/span&gt;

&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trace&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="model-convergence-and-variance-analysis"&gt;Model Convergence and Variance Analysis&lt;/h2&gt;&lt;p&gt;After fitting our model, zero divergent transitions confirm good convergence.&lt;/p&gt;
&lt;p&gt;Let's examine how variance is partitioned across components:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;trace&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sample_stats&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;diverging&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sum&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div&gt;&lt;svg style="position: absolute; width: 0; height: 0; overflow: hidden"&gt;
&lt;defs&gt;
&lt;symbol id="icon-database" viewBox="0 0 32 32"&gt;
&lt;path d="M16 0c-8.837 0-16 2.239-16 5v4c0 2.761 7.163 5 16 5s16-2.239 16-5v-4c0-2.761-7.163-5-16-5z"&gt;&lt;/path&gt;
&lt;path d="M16 17c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z"&gt;&lt;/path&gt;
&lt;path d="M16 26c-8.837 0-16-2.239-16-5v6c0 2.761 7.163 5 16 5s16-2.239 16-5v-6c0 2.761-7.163 5-16 5z"&gt;&lt;/path&gt;
&lt;/symbol&gt;
&lt;symbol id="icon-file-text2" viewBox="0 0 32 32"&gt;
&lt;path d="M28.681 7.159c-0.694-0.947-1.662-2.053-2.724-3.116s-2.169-2.030-3.116-2.724c-1.612-1.182-2.393-1.319-2.841-1.319h-15.5c-1.378 0-2.5 1.121-2.5 2.5v27c0 1.378 1.122 2.5 2.5 2.5h23c1.378 0 2.5-1.122 2.5-2.5v-19.5c0-0.448-0.137-1.23-1.319-2.841zM24.543 5.457c0.959 0.959 1.712 1.825 2.268 2.543h-4.811v-4.811c0.718 0.556 1.584 1.309 2.543 2.268zM28 29.5c0 0.271-0.229 0.5-0.5 0.5h-23c-0.271 0-0.5-0.229-0.5-0.5v-27c0-0.271 0.229-0.5 0.5-0.5 0 0 15.499-0 15.5 0v7c0 0.552 0.448 1 1 1h7v19.5z"&gt;&lt;/path&gt;
&lt;path d="M23 26h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"&gt;&lt;/path&gt;
&lt;path d="M23 22h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"&gt;&lt;/path&gt;
&lt;path d="M23 18h-14c-0.552 0-1-0.448-1-1s0.448-1 1-1h14c0.552 0 1 0.448 1 1s-0.448 1-1 1z"&gt;&lt;/path&gt;
&lt;/symbol&gt;
&lt;/defs&gt;
&lt;/svg&gt;
&lt;style&gt;/* CSS stylesheet for displaying xarray objects in jupyterlab.
 *
 */

:root {
  --xr-font-color0: var(--jp-content-font-color0, rgba(0, 0, 0, 1));
  --xr-font-color2: var(--jp-content-font-color2, rgba(0, 0, 0, 0.54));
  --xr-font-color3: var(--jp-content-font-color3, rgba(0, 0, 0, 0.38));
  --xr-border-color: var(--jp-border-color2, #e0e0e0);
  --xr-disabled-color: var(--jp-layout-color3, #bdbdbd);
  --xr-background-color: var(--jp-layout-color0, white);
  --xr-background-color-row-even: var(--jp-layout-color1, white);
  --xr-background-color-row-odd: var(--jp-layout-color2, #eeeeee);
}

html[theme="dark"],
html[data-theme="dark"],
body[data-theme="dark"],
body.vscode-dark {
  --xr-font-color0: rgba(255, 255, 255, 1);
  --xr-font-color2: rgba(255, 255, 255, 0.54);
  --xr-font-color3: rgba(255, 255, 255, 0.38);
  --xr-border-color: #1f1f1f;
  --xr-disabled-color: #515151;
  --xr-background-color: #111111;
  --xr-background-color-row-even: #111111;
  --xr-background-color-row-odd: #313131;
}

.xr-wrap {
  display: block !important;
  min-width: 300px;
  max-width: 700px;
}

.xr-text-repr-fallback {
  /* fallback to plain text repr when CSS is not injected (untrusted notebook) */
  display: none;
}

.xr-header {
  padding-top: 6px;
  padding-bottom: 6px;
  margin-bottom: 4px;
  border-bottom: solid 1px var(--xr-border-color);
}

.xr-header &gt; div,
.xr-header &gt; ul {
  display: inline;
  margin-top: 0;
  margin-bottom: 0;
}

.xr-obj-type,
.xr-array-name {
  margin-left: 2px;
  margin-right: 10px;
}

.xr-obj-type {
  color: var(--xr-font-color2);
}

.xr-sections {
  padding-left: 0 !important;
  display: grid;
  grid-template-columns: 150px auto auto 1fr 0 20px 0 20px;
}

.xr-section-item {
  display: contents;
}

.xr-section-item input {
  display: inline-block;
  opacity: 0;
  height: 0;
}

.xr-section-item input + label {
  color: var(--xr-disabled-color);
}

.xr-section-item input:enabled + label {
  cursor: pointer;
  color: var(--xr-font-color2);
}

.xr-section-item input:focus + label {
  border: 2px solid var(--xr-font-color0);
}

.xr-section-item input:enabled + label:hover {
  color: var(--xr-font-color0);
}

.xr-section-summary {
  grid-column: 1;
  color: var(--xr-font-color2);
  font-weight: 500;
}

.xr-section-summary &gt; span {
  display: inline-block;
  padding-left: 0.5em;
}

.xr-section-summary-in:disabled + label {
  color: var(--xr-font-color2);
}

.xr-section-summary-in + label:before {
  display: inline-block;
  content: "►";
  font-size: 11px;
  width: 15px;
  text-align: center;
}

.xr-section-summary-in:disabled + label:before {
  color: var(--xr-disabled-color);
}

.xr-section-summary-in:checked + label:before {
  content: "▼";
}

.xr-section-summary-in:checked + label &gt; span {
  display: none;
}

.xr-section-summary,
.xr-section-inline-details {
  padding-top: 4px;
  padding-bottom: 4px;
}

.xr-section-inline-details {
  grid-column: 2 / -1;
}

.xr-section-details {
  display: none;
  grid-column: 1 / -1;
  margin-bottom: 5px;
}

.xr-section-summary-in:checked ~ .xr-section-details {
  display: contents;
}

.xr-array-wrap {
  grid-column: 1 / -1;
  display: grid;
  grid-template-columns: 20px auto;
}

.xr-array-wrap &gt; label {
  grid-column: 1;
  vertical-align: top;
}

.xr-preview {
  color: var(--xr-font-color3);
}

.xr-array-preview,
.xr-array-data {
  padding: 0 5px !important;
  grid-column: 2;
}

.xr-array-data,
.xr-array-in:checked ~ .xr-array-preview {
  display: none;
}

.xr-array-in:checked ~ .xr-array-data,
.xr-array-preview {
  display: inline-block;
}

.xr-dim-list {
  display: inline-block !important;
  list-style: none;
  padding: 0 !important;
  margin: 0;
}

.xr-dim-list li {
  display: inline-block;
  padding: 0;
  margin: 0;
}

.xr-dim-list:before {
  content: "(";
}

.xr-dim-list:after {
  content: ")";
}

.xr-dim-list li:not(:last-child):after {
  content: ",";
  padding-right: 5px;
}

.xr-has-index {
  font-weight: bold;
}

.xr-var-list,
.xr-var-item {
  display: contents;
}

.xr-var-item &gt; div,
.xr-var-item label,
.xr-var-item &gt; .xr-var-name span {
  background-color: var(--xr-background-color-row-even);
  margin-bottom: 0;
}

.xr-var-item &gt; .xr-var-name:hover span {
  padding-right: 5px;
}

.xr-var-list &gt; li:nth-child(odd) &gt; div,
.xr-var-list &gt; li:nth-child(odd) &gt; label,
.xr-var-list &gt; li:nth-child(odd) &gt; .xr-var-name span {
  background-color: var(--xr-background-color-row-odd);
}

.xr-var-name {
  grid-column: 1;
}

.xr-var-dims {
  grid-column: 2;
}

.xr-var-dtype {
  grid-column: 3;
  text-align: right;
  color: var(--xr-font-color2);
}

.xr-var-preview {
  grid-column: 4;
}

.xr-index-preview {
  grid-column: 2 / 5;
  color: var(--xr-font-color2);
}

.xr-var-name,
.xr-var-dims,
.xr-var-dtype,
.xr-preview,
.xr-attrs dt {
  white-space: nowrap;
  overflow: hidden;
  text-overflow: ellipsis;
  padding-right: 10px;
}

.xr-var-name:hover,
.xr-var-dims:hover,
.xr-var-dtype:hover,
.xr-attrs dt:hover {
  overflow: visible;
  width: auto;
  z-index: 1;
}

.xr-var-attrs,
.xr-var-data,
.xr-index-data {
  display: none;
  background-color: var(--xr-background-color) !important;
  padding-bottom: 5px !important;
}

.xr-var-attrs-in:checked ~ .xr-var-attrs,
.xr-var-data-in:checked ~ .xr-var-data,
.xr-index-data-in:checked ~ .xr-index-data {
  display: block;
}

.xr-var-data &gt; table {
  float: right;
}

.xr-var-name span,
.xr-var-data,
.xr-index-name div,
.xr-index-data,
.xr-attrs {
  padding-left: 25px !important;
}

.xr-attrs,
.xr-var-attrs,
.xr-var-data,
.xr-index-data {
  grid-column: 1 / -1;
}

dl.xr-attrs {
  padding: 0;
  margin: 0;
  display: grid;
  grid-template-columns: 125px auto;
}

.xr-attrs dt,
.xr-attrs dd {
  padding: 0;
  margin: 0;
  float: left;
  padding-right: 10px;
  width: auto;
}

.xr-attrs dt {
  font-weight: normal;
  grid-column: 1;
}

.xr-attrs dt:hover span {
  display: inline-block;
  background: var(--xr-background-color);
  padding-right: 10px;
}

.xr-attrs dd {
  grid-column: 2;
  white-space: pre-wrap;
  word-break: break-all;
}

.xr-icon-database,
.xr-icon-file-text2,
.xr-no-icon {
  display: inline-block;
  vertical-align: middle;
  width: 1em;
  height: 1.5em !important;
  stroke-width: 0;
  stroke: currentColor;
  fill: currentColor;
}
&lt;/style&gt;&lt;pre class='xr-text-repr-fallback'&gt;&amp;lt;xarray.DataArray &amp;#x27;diverging&amp;#x27; ()&amp;gt; Size: 8B
array(0)&lt;/pre&gt;&lt;div class='xr-wrap' style='display:none'&gt;&lt;div class='xr-header'&gt;&lt;div class='xr-obj-type'&gt;xarray.DataArray&lt;/div&gt;&lt;div class='xr-array-name'&gt;'diverging'&lt;/div&gt;&lt;/div&gt;&lt;ul class='xr-sections'&gt;&lt;li class='xr-section-item'&gt;&lt;div class='xr-array-wrap'&gt;&lt;input id='section-c99a31a0-17a2-456b-bcbf-974a2ca5805f' class='xr-array-in' type='checkbox' checked&gt;&lt;label for='section-c99a31a0-17a2-456b-bcbf-974a2ca5805f' title='Show/hide data repr'&gt;&lt;svg class='icon xr-icon-database'&gt;&lt;use xlink:href='#icon-database'&gt;&lt;/use&gt;&lt;/svg&gt;&lt;/label&gt;&lt;div class='xr-array-preview xr-preview'&gt;&lt;span&gt;0&lt;/span&gt;&lt;/div&gt;&lt;div class='xr-array-data'&gt;&lt;pre&gt;array(0)&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class='xr-section-item'&gt;&lt;input id='section-b8c16d3c-86d4-4e87-8354-9c81177af604' class='xr-section-summary-in' type='checkbox' disabled &gt;&lt;label for='section-b8c16d3c-86d4-4e87-8354-9c81177af604' class='xr-section-summary'  title='Expand/collapse section'&gt;Coordinates: &lt;span&gt;(0)&lt;/span&gt;&lt;/label&gt;&lt;div class='xr-section-inline-details'&gt;&lt;/div&gt;&lt;div class='xr-section-details'&gt;&lt;ul class='xr-var-list'&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class='xr-section-item'&gt;&lt;input id='section-1c2fb95c-816e-4ef2-a1d5-b5b59328304b' class='xr-section-summary-in' type='checkbox' disabled &gt;&lt;label for='section-1c2fb95c-816e-4ef2-a1d5-b5b59328304b' class='xr-section-summary'  title='Expand/collapse section'&gt;Indexes: &lt;span&gt;(0)&lt;/span&gt;&lt;/label&gt;&lt;div class='xr-section-inline-details'&gt;&lt;/div&gt;&lt;div class='xr-section-details'&gt;&lt;ul class='xr-var-list'&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/li&gt;&lt;li class='xr-section-item'&gt;&lt;input id='section-0dd3a477-2091-44af-9871-42810eeac5dd' class='xr-section-summary-in' type='checkbox' disabled &gt;&lt;label for='section-0dd3a477-2091-44af-9871-42810eeac5dd' class='xr-section-summary'  title='Expand/collapse section'&gt;Attributes: &lt;span&gt;(0)&lt;/span&gt;&lt;/label&gt;&lt;div class='xr-section-inline-details'&gt;&lt;/div&gt;&lt;div class='xr-section-details'&gt;&lt;dl class='xr-attrs'&gt;&lt;/dl&gt;&lt;/div&gt;&lt;/li&gt;&lt;/ul&gt;&lt;/div&gt;&lt;/div&gt;&lt;p&gt;In the ridgeline plot below, we see that unexplained variance contributes minimally to total variation,
while experiment and replicate effects—which ideally should contribute little—are actually
significant contributors to the readout variation. This serves as a metric of laboratory consistency.&lt;/p&gt;
&lt;p&gt;Ideally, the protein variation (our biological signal of interest) should be the dominant source
of variation. This analysis suggests that improving experimental protocols to reduce batch effects
would substantially improve our signal-to-noise ratio.&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;arviz&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;az&lt;/span&gt;

&lt;span class="n"&gt;axes_posterior_props&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;az&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trace&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;props&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;grid&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="n"&gt;axes_posterior_props&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;experiment&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axes_posterior_props&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;replicate&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axes_posterior_props&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;protein&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;axes_posterior_props&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;flatten&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;unexplained&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src="protein_estimation_15_0.webp" alt="png"&gt;&lt;/p&gt;
&lt;p&gt;These posterior plots show the distributions of the variance components. Each represents the proportion of total variance attributed to that component. We can see clear differences in the contributions from each source.&lt;/p&gt;
&lt;p&gt;We can also look at the total $R^2$ value, which represents the proportion of variance explained by the model:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;az&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_posterior&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trace&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;var_names&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;model_r2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;r_squared&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;pre&gt;Axes(0.125,0.110;0.352x0.770)
Axes(0.548,0.110;0.352x0.770)&lt;/pre&gt;&lt;br&gt;&lt;img src='data:image/png;base64,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'/&gt;&lt;/p&gt;
&lt;p&gt;Taken together, we can interpret that the model fits the data very well (&lt;code&gt;model_r2&lt;/code&gt; close to 1), but it is concerning to me that protein only explains 19% of the variation in readout, while experiment and replicate explains more than 70% of the output variation, which signals to me that the measurements are not particularly tight, and a lot could be done to control experiment-to-experiment variation. Additionally, the fact that our &lt;code&gt;model_r2&lt;/code&gt; and &lt;code&gt;r_squared&lt;/code&gt; parameters are self-consistent is reassuring as well.&lt;/p&gt;
&lt;h2 id="protein-activity-estimates"&gt;Protein Activity Estimates&lt;/h2&gt;&lt;p&gt;Now that we've decomposed the variance and accounted for experimental effects,
let's examine the protein activity estimates—the "true" biological signal after
removing experimental noise:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;ax&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;az&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;plot_forest&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;trace&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prot_effect&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;ax&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;set_xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;log(protein activity)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src="protein_estimation_20_0.webp" alt="png"&gt;&lt;/p&gt;
&lt;p&gt;The forest plot displays posterior distributions of protein activity (log scale),
with horizontal lines representing 94% credible intervals.&lt;/p&gt;
&lt;p&gt;A key challenge emerges: despite similar uncertainty across proteins,
overlapping credible intervals make it difficult to determine which proteins
are truly superior. Simply ranking by posterior means could lead us to prioritize
proteins with slightly higher point estimates when uncertainty makes their actual
superiority ambiguous.&lt;/p&gt;
&lt;p&gt;This is a fundamental limitation of ranking by point estimates alone:
it fails to properly account for uncertainty. A protein with a slightly lower mean
but narrower credible intervals might be a better candidate than one with a higher
mean but wider uncertainty bounds.&lt;/p&gt;
&lt;h2 id="why-not-calculate-the-effect-sizes"&gt;Why not calculate the effect sizes?&lt;/h2&gt;&lt;p&gt;While effect sizes quantify difference magnitudes, they have important limitations:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;They still have posterior distributions with uncertainty&lt;/li&gt;
&lt;li&gt;They require an arbitrary reference protein&lt;/li&gt;
&lt;li&gt;Scale interpretation is subjective&lt;/li&gt;
&lt;li&gt;They don't directly answer "which is better?"&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is where the probability of superiority calculation shines: it integrates over
the entire posterior distribution to directly answer our key question:
"What is the probability that protein A is better than protein B?"&lt;/p&gt;
&lt;p&gt;The probability of superiority:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;For each posterior sample, compares protein A's activity to protein B's&lt;/li&gt;
&lt;li&gt;Counts the proportion of samples where A &amp;gt; B&lt;/li&gt;
&lt;li&gt;Results in P(A &amp;gt; B) - a single number from 0 to 1&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This approach integrates over uncertainty, directly answers our question,
avoids arbitrary references, and produces an intuitive metric for decision-making.&lt;/p&gt;
&lt;p&gt;Let's illustrate this with a comparison between two specific proteins:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;_&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c1"&gt;# Get posterior samples&lt;/span&gt;
    &lt;span class="n"&gt;prot_activity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;trace&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prot_effect&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
    &lt;span class="n"&gt;prot_activity_flat&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prot_activity&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;prot_activity&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;

    &lt;span class="c1"&gt;# Get protein names&lt;/span&gt;
    &lt;span class="n"&gt;protein_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;trace&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;protein&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;

    &lt;span class="c1"&gt;# Choose two proteins to compare&lt;/span&gt;
    &lt;span class="n"&gt;protein1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Protein_12&amp;quot;&lt;/span&gt;  &lt;span class="c1"&gt;# A high performer&lt;/span&gt;
    &lt;span class="n"&gt;protein2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Protein_66&amp;quot;&lt;/span&gt;  &lt;span class="c1"&gt;# Another high performer&lt;/span&gt;

    &lt;span class="n"&gt;idx1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;protein_names&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;protein1&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;idx2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;protein_names&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;protein2&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c1"&gt;# Extract their posterior samples&lt;/span&gt;
    &lt;span class="n"&gt;samples1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prot_activity_flat&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;idx1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;samples2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;prot_activity_flat&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;idx2&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c1"&gt;# Calculate differences for each posterior sample&lt;/span&gt;
    &lt;span class="n"&gt;differences&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;samples1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;samples2&lt;/span&gt;

    &lt;span class="c1"&gt;# Calculate superiority probability&lt;/span&gt;
    &lt;span class="n"&gt;prob_superiority&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;differences&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Plot the posterior of differences&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="c1"&gt;# Histogram of differences&lt;/span&gt;
    &lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;histplot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;differences&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;bins&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;30&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Add vertical line at zero&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;axvline&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;r&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;linestyle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;--&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Shade the area where protein1 &amp;gt; protein2&lt;/span&gt;
    &lt;span class="n"&gt;positive_mask&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;differences&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fill_between&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;differences&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;positive_mask&lt;/span&gt;&lt;span class="p"&gt;]),&lt;/span&gt;
        &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gca&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_ylim&lt;/span&gt;&lt;span class="p"&gt;()[&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;  &lt;span class="c1"&gt;# Half height for visibility&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;green&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;P(&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;protein1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; &amp;gt; &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;protein2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;) = &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;prob_superiority&lt;/span&gt;&lt;span class="si"&gt;:&lt;/span&gt;&lt;span class="s2"&gt;.3f&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Activity Difference (&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;protein1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; - &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;protein2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Frequency&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;Posterior Distribution of Activity Difference &amp;quot;&lt;/span&gt;
        &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Between &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;protein1&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; and &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;protein2&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src="protein_estimation_23_0.webp" alt="png"&gt;&lt;/p&gt;
&lt;p&gt;This visualization demonstrates the core concept. The green shaded area represents
the proportion of posterior samples where the first protein outperforms the second.
This proportion is the probability of superiority.&lt;/p&gt;
&lt;p&gt;Rather than reducing our rich posterior distributions to point estimates or effect sizes
that still require interpretation, the superiority probability directly integrates over
all uncertainty to answer our precise question: "How likely is it that this protein is
better than that one?"&lt;/p&gt;
&lt;p&gt;Now let's calculate this for all pairs of proteins to create a comprehensive superiority matrix:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;tqdm.auto&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;tqdm&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;_&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="n"&gt;n_proteins&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;trace&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prot_effect&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;shape&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="n"&gt;prot_activity&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;trace&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prot_effect&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;reshape&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_proteins&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;superiority_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;n_proteins&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;n_proteins&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;tqdm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_proteins&lt;/span&gt;&lt;span class="p"&gt;)):&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;n_proteins&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
            &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;!=&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
                &lt;span class="n"&gt;superiority_matrix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                    &lt;span class="n"&gt;prot_activity&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;prot_activity&lt;/span&gt;&lt;span class="p"&gt;[:,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
                &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;superiority_matrix&lt;/span&gt;

&lt;span class="n"&gt;superiority_matrix&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This superiority matrix gives us, for each pair of proteins (i, j), the probability
that protein i has higher activity than protein j, incorporating all model uncertainty.&lt;/p&gt;
&lt;p&gt;The calculation yields a probability interpretation: "There's an 85% chance that
protein A is superior to protein B" rather than the frequentist "protein A is
significantly better than protein B with p&amp;lt;0.05."&lt;/p&gt;
&lt;p&gt;Let's visualize this matrix:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Create heatmap&lt;/span&gt;
&lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;heatmap&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;superiority_matrix&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;annot&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;cmap&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;YlOrRd&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;fmt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;.2f&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Superiority Matrix&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gca&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src="protein_estimation_27_0.webp" alt="png"&gt;&lt;/p&gt;
&lt;p&gt;We can rank proteins by their average probability of superiority across all comparisons:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;_&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c1"&gt;# Calculate average probability of superiority and sort proteins&lt;/span&gt;
    &lt;span class="n"&gt;avg_superiority&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;superiority_matrix&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;axis&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;protein_names&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;df&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Protein&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;unique&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;superiority_df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;DataFrame&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Protein&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;protein_names&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;Avg_Superiority&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;avg_superiority&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sorted_superiority&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;superiority_df&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sort_values&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;Avg_Superiority&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ascending&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;head&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Create plot&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;12&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="c1"&gt;# For each protein, plot individual points and mean line&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;protein&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sorted_superiority&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Protein&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]):&lt;/span&gt;
        &lt;span class="n"&gt;protein_idx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;where&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;protein_names&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="n"&gt;protein&lt;/span&gt;&lt;span class="p"&gt;)[&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;protein_probs&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;superiority_matrix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;protein_idx&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;protein_probs&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;protein_probs&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;hlines&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;avg_superiority&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;protein_idx&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="mf"&gt;0.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mf"&gt;0.25&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;color&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;red&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xticks&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sorted_superiority&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt; &lt;span class="n"&gt;sorted_superiority&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Protein&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;rotation&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;90&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Probability of Superiority&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Distribution of Superiority Probabilities by Protein&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tight_layout&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;despine&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gca&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src="protein_estimation_29_0.webp" alt="png"&gt;&lt;/p&gt;
&lt;p&gt;This ranking differs from what we might conclude by examining forest plots alone.
The superiority metric directly quantifies the probability that one protein
outperforms others, properly accounting for the full posterior distribution
and uncertainty in each comparison.&lt;/p&gt;
&lt;p&gt;To better understand how protein activity relates to superiority probability,
let's compare their posterior mean activity with two superiority measures:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;_&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt;
    &lt;span class="c1"&gt;# Calculate probability of superiority distribution for each protein&lt;/span&gt;
    &lt;span class="n"&gt;superiority_dist&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;concatenate&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="n"&gt;superiority_matrix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="n"&gt;j&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;superiority_matrix&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="p"&gt;:]])&lt;/span&gt;
        &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;j&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;enumerate&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;len&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;superiority_matrix&lt;/span&gt;&lt;span class="p"&gt;)))&lt;/span&gt;
    &lt;span class="p"&gt;]&lt;/span&gt;

    &lt;span class="c1"&gt;# Get protein activity statistics from trace&lt;/span&gt;
    &lt;span class="n"&gt;protein_activity_mean&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;trace&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;posterior&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prot_effect&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dim&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;chain&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;draw&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;values&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="c1"&gt;# Create scatter plot&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;figure&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;figsize&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

    &lt;span class="c1"&gt;# Plot points&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;protein_activity_mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;superiority_dist&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;mean p(superior)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;scatter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="n"&gt;protein_activity_mean&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dist&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;q&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;dist&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;superiority_dist&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
        &lt;span class="n"&gt;alpha&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.6&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;label&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;minimum p(superior)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;xlabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Posterior Mean Protein Activity (log scale)&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;ylabel&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Probability of Superiority&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;legend&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;title&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Protein Activity vs Probability of Superiority&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;sns&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;despine&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;plt&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gca&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;

&lt;span class="n"&gt;_&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;img src="protein_estimation_31_0.webp" alt="png"&gt;&lt;/p&gt;
&lt;p&gt;This plot reveals that:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The relationship between activity and superiority is non-linear&lt;/li&gt;
&lt;li&gt;Proteins with similar activities can have different superiority probabilities
depending on the certainty of their estimates&lt;/li&gt;
&lt;li&gt;The minimum probability of superiority provides a conservative decision-making
measure—a protein with high minimum superiority is more likely to be the superior
candidate&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;When screening candidates, two questions are critical: "Are we measuring what matters?"
and "Which candidates are truly superior?" Traditional approaches using point estimates
and p-values inadequately address these questions when dealing with experimental noise
and multiple comparisons.&lt;/p&gt;
&lt;p&gt;A Bayesian model with explicitly modeled terms offers a powerful alternative:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;R2D2 priors&lt;/strong&gt; decompose variance into interpretable components, revealing how
much signal comes from the biological effect versus experimental artifacts.
This guides concrete improvements to experimental protocols.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Bayesian superiority calculation&lt;/strong&gt; directly quantifies the probability that
one candidate outperforms others, properly accounting for uncertainty and
avoiding the pitfalls of simple rank ordering.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These techniques transform screening data into actionable insights and apply to
any domain requiring robust comparison of multiple candidates under noisy conditions:
drug discovery, materials science, A/B testing, clinical trials, and more.&lt;/p&gt;
&lt;p&gt;Bayesian methods move us beyond simplistic "winners and losers" to a nuanced
understanding of which candidates are most likely to succeed, with what degree
of certainty, and how we can improve our measurement process itself.&lt;/p&gt;
&lt;p&gt;And more meta-level: none of this needs fancy names.
It's just logic and math, applied to data.
Wasn't that what statistics was supposed to be?&lt;/p&gt;
&lt;p&gt;&lt;em&gt;With thanks to Jackie Valeri for sanity-checking the code while also learning the ins-and-outs of the R2D2 prior,
and Virgile Andreani for first introducing me to the R2D2 prior!&lt;/em&gt;&lt;/p&gt;
</content></entry><entry><title>How to standardize Data Science ways of working to unlock your team's creativity</title><link href="https://ericmjl.github.io/blog/2025/4/2/how-to-standardize-data-science-ways-of-working-to-unlock-your-teams-creativity/" rel="alternate"/><updated>2025-04-02T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:f8a525d5-e6b3-3332-9401-d5144077095d</id><content type="html">&lt;p&gt;On 2 April 2025, I had the opportunity to speak at BioIT World about my experiences building data science teams and the associated tooling at Moderna. Unlike most speakers, I ditched the slides and went for a more interactive approach – that's just my style. Why subject everyone to 25 minutes of me blabbering when we could have an engaging discussion instead?&lt;/p&gt;
&lt;p&gt;For those who couldn't attend, I wanted to share the key insights from my talk on how we've made our data science team fly at Moderna. These lessons come from my experience as a data science team lead, where I manage a team of six (including myself), serving Moderna's 600-person research organization, and where we share tools with other data science teams totaling 12, serving a wide remit across Moderna’s ~6000-person organization.&lt;/p&gt;
&lt;h2 id="the-mission"&gt;The Mission&lt;/h2&gt;&lt;p&gt;First, a bit about us: my home team's mission is to "make science run at the speed of thought and to quantify the unquantified." I joined Moderna in summer 2021 – too late to profit from the pandemic, so I'm really there for the science, not the money! Part of making science run at the speed of thought is to make sure we data scientists have the tools and practices at hand that enable &lt;em&gt;us&lt;/em&gt; to work at the speed of thought. That is where standardization comes in.&lt;/p&gt;
&lt;h2 id="talk-overview"&gt;Talk Overview&lt;/h2&gt;&lt;p&gt;In my talk, I focused on three key aspects of standardizing data science workflows:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;How to design delivery models that serve stakeholder, customer, and/or collaborator needs&lt;/li&gt;
&lt;li&gt;Ways to make best practices the path of least resistance&lt;/li&gt;
&lt;li&gt;Strategies for implementing standards on the path of least resistance&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I shared concrete examples from my experience at Moderna, but also encouraged the audience to discuss these topics with their neighbors so we could learn from the collective wisdom in the room. My goal was for everyone to leave with practical ideas they could implement right away—regardless of their team or org size. I will admit, the room was relatively quiet at first (I think most people came not expecting my talk delivery format), but gradually the room warmed up.&lt;/p&gt;
&lt;h3 id="1-standardized-delivery-models"&gt;1. Standardized Delivery Models&lt;/h3&gt;&lt;p&gt;The first thing to address is the question: what exactly does your data science team deliver? And I don't mean "insights" – that's not a specific enough answer. I'm talking about concrete work products. Something tangible that people can interact with or mull over.&lt;/p&gt;
&lt;p&gt;Before choosing delivery formats, it's essential to understand the "jobs to be done" (from Clayton Christensen's Innovator's Solution) – what are stakeholders actually trying to accomplish with your outputs? Delivery models should be tailored to what your company actually needs, not just what's trendy.&lt;/p&gt;
&lt;p&gt;At Moderna, we deliver two things as a data science team:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Python packages&lt;/strong&gt;: Reusable components that encapsulate data science work that other computational and data scientists can reuse.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Compute tasks&lt;/strong&gt;: CLI tools that serve as a wrapper for computational workflows, which are deployed on the cloud and accessible via web UI, RESTful API, and Python client. These serve both technical and non-technical users.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We made this decision early on to avoid building things like dashboards, which in my opinion are where data science projects go to die. Dashboards should be built by the people who actually want to see the data. I've also been in the position of promising a UI and suffering the maintenance costs later. It was not a happy time. So we cut out building any kind of user interface from our work products, preferring to leave this to professional front-end engineers.&lt;/p&gt;
&lt;p&gt;Yet at the same time, there is a strong "engineering" component to our work, because we want to reduce the friction that occurs from handover. I've heard stories in finance of data science and quant teams completing their prototype in a notebook within a month, but the ML engineering team needing 8 months to productionize it because the deployment language was different (Java, not Python), the runtime environment was different, and the engineering team was not intimately familiar with the problem domain, and so they were delayed on the appropriate suite of tests to write. So at Moderna, every data scientist must understand that our return on investment is only realized if we package our work as software, and we avoid these problems associated with handoff by ensuring that we have the tools to make deployment easy.&lt;/p&gt;
&lt;p&gt;Moreover, I don't care if you know how to build a fancy Transformer ML model in a Jupyter notebook. If your work cannot be operationalized hands-free without your involvement, you have not delivered a return on investment on your time. Software is how we scale labour!&lt;/p&gt;
&lt;p&gt;The benefits? Consistency, reusability, and very clear paths from exploration to production. Expectations with stakeholders are crystal clear, yet what we deliver is flexible enough to work with a variety of collaborators.&lt;/p&gt;
&lt;h3 id="2-making-best-practices-the-path-of-least-resistance"&gt;2. Making Best Practices the Path of Least Resistance&lt;/h3&gt;&lt;p&gt;Consistent organization and automation reduces cognitive load and frees up mental energy for creative problem-solving. Our philosophy is simple: "Make the right thing the easy thing to do."&lt;/p&gt;
&lt;p&gt;We've implemented several tactical best practices that make a big difference:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;One project, one code repository&lt;/strong&gt;: Every project gets its own code repository – you can break this rule when you know the rule, but it's a good starting point.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Standardized project scaffolding&lt;/strong&gt;: We've invested in command-line tools (not unlike &lt;a href="https://ericmjl.github.io/pyds-cli/"&gt;&lt;code&gt;pyds-cli&lt;/code&gt;&lt;/a&gt;) that, after a quick questionnaire, generate a full Python project package structure with:&lt;ul&gt;
&lt;li&gt;Dedicated locations for tests (signaling that testing is expected)&lt;/li&gt;
&lt;li&gt;Documentation templates (encouraging thorough documentation)&lt;/li&gt;
&lt;li&gt;Proper code organization with examples of submodules&lt;/li&gt;
&lt;li&gt;Configuration files&lt;/li&gt;
&lt;li&gt;Pre-configured CI/CD pipelines that run on every commit&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;We invested early on in &lt;strong&gt;automated workflows&lt;/strong&gt; that reduce friction, such as:&lt;ul&gt;
&lt;li&gt;Automatic installation of pre-commit hooks for code quality&lt;/li&gt;
&lt;li&gt;Automated authentication for internal systems&lt;/li&gt;
&lt;li&gt;Tools that manage cloud workstation usage (e.g., auto-shutdown for workstations)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Documentation&lt;/strong&gt;: We invest heavily in documentation, running quarterly "docathons" to ensure our work is adequately documented for future team members. As I like to say, "Software scales labor, documentation scales brains."&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Modern dependency management&lt;/strong&gt; with tools like pixi and uv (we've evolved from conda to mamba to reduce build times, and now we're transitioning to pixi/uv)&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;&lt;p&gt;Software scales labor, documentation scales brains!&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;These standardized practices reduce cognitive overhead. If I jump into a colleague's project, I immediately know where to look for documentation and tests, which helps me onboard faster than having one-on-one sessions. Shared idioms and patterns make it easier to jump in and help each other, fostering stronger team dynamics.&lt;/p&gt;
&lt;p&gt;I would note that the tech stack is merely one piece of the whole puzzle. The practices we build around the tech stack are another integral portion. The goal is for scientists to spend less time on configuration and more time on science. The investment of my time hopefully yields thirty, sixty, or a hundredfold in terms of what we give back to our teammates.&lt;/p&gt;
&lt;p&gt;The best part? We enable all of this at extremely low software licensing spend — our only vendor spends on our technology stack for data scientists are on AWS, GitHub, and GitHub Copilot.&lt;/p&gt;
&lt;h3 id="3-implementation-strategy"&gt;3. Implementation Strategy&lt;/h3&gt;&lt;p&gt;Change management principles apply to workflow standardization. Successful implementation requires both technical and cultural elements.&lt;/p&gt;
&lt;p&gt;Our standardization journey at Moderna involved several key elements:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Leadership buy-in was critical&lt;/strong&gt;: The support chain included my direct manager Andrew Giessel and Dave Johnson, our Chief Data and AI Officer. When I implemented our standardized project structure, Dave personally reviewed the pull request. His comment – "I love the user-facing emojis" – showed me he had actually read my code and was supportive of this change.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Evolution through multiple iterations&lt;/strong&gt;: We started with a basic template that codified our best practices, each time making things better. Because we have so many repos flying around, we don't worry about upgrading all of them at once, but instead opportunistically update them when we get back to touching them.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Empowering others to contribute&lt;/strong&gt;: If someone finds a pain point, I guide them through the process for improving our command-line interface tools. The benefit is that shared knowledge grows and spreads.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Documentation of ways-of-working&lt;/strong&gt;: We leverage our two-day quarterly docathons to improve documentation for the long haul.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The command-line tooling we've developed isn't just my project – it's now shared with the ML platform team, and data scientists actively make code changes to it. We "dog food" our own tools, which is incredibly empowering. You've got to give power back to the frontliners so they can make the changes they need to move at the speed they want.&lt;/p&gt;
&lt;p&gt;Sustainable change requires leadership support, frontline champions, visible benefits, and evolution at a pace teams can absorb.&lt;/p&gt;
&lt;h2 id="balancing-standards-with-innovation"&gt;Balancing Standards with Innovation&lt;/h2&gt;&lt;p&gt;The perceived tension between standardization and creativity is often a false dichotomy. Well-designed standards actually create freedom through constraints.&lt;/p&gt;
&lt;p&gt;At Moderna, we've found this balance by being intentional about where we apply standards:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Areas where we apply standardization&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Project structure and organization&lt;/li&gt;
&lt;li&gt;Deployment pipelines and processes&lt;/li&gt;
&lt;li&gt;Documentation formats and expectations&lt;/li&gt;
&lt;li&gt;Code review and quality check workflows&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Areas where we preserve flexibility&lt;/strong&gt;:&lt;ul&gt;
&lt;li&gt;Algorithm selection and implementation&lt;/li&gt;
&lt;li&gt;Analysis approaches and methodologies&lt;/li&gt;
&lt;li&gt;Visualization choices and design&lt;/li&gt;
&lt;li&gt;Local development environment preferences&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The key insight is &lt;strong&gt;focusing standardization on interfaces, not implementations&lt;/strong&gt;. You're free to use whatever Python packages you need, but by standardizing on the language and the structure, we can make it easier to jump in and help each other.&lt;/p&gt;
&lt;p&gt;In doing so, we help make data science teams fly by:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Eliminating decision fatigue on routine aspects of projects&lt;/li&gt;
&lt;li&gt;Reducing cognitive load from context-switching between different project structures&lt;/li&gt;
&lt;li&gt;Automating repeatable processes, freeing mental energy for scientific challenges&lt;/li&gt;
&lt;li&gt;Enabling faster onboarding and collaboration, reducing time spent explaining project basics&lt;/li&gt;
&lt;li&gt;Creating predictable workflows where scientists can anticipate next steps rather than inventing them&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="ai-assisted-coding"&gt;AI-Assisted Coding&lt;/h2&gt;&lt;p&gt;Though not a focus of my talk, many asked about AI-assisted coding. This is completely ingrained at Moderna – everyone has access to ChatGPT Enterprise (that's public knowledge), and every developer who asks for GitHub Copilot gets it. We're developing extensive documentation on how to productively use AI assistance in daily coding.&lt;/p&gt;
&lt;p&gt;The testimonials are powerful. Just yesterday, I interviewed a DevOps engineer who's doing three to four people's worth of work and coasting because of GitHub Copilot in agent mode. It's the ultimate productivity tool if you know how to wield it properly.&lt;/p&gt;
&lt;h2 id="final-thoughts"&gt;Final Thoughts&lt;/h2&gt;&lt;p&gt;What's particularly powerful about our approach is that it can be implemented with minimal vendor dependencies. At Moderna, our data science and deployment infrastructure is built almost entirely on open-source tools — our only significant software expenses are AWS, GitHub Enterprise, and GitHub Copilot. This deliberate choice to avoid vendor lock-in provides significant cost savings and the flexibility to adapt quickly as the scientific landscape changes.&lt;/p&gt;
&lt;p&gt;The ultimate goal isn't standardization for its own sake — it's creating an environment where data scientists can do their best thinking and most innovative work. When done right, good standards don't constrain scientists; they liberate them.&lt;/p&gt;
&lt;p&gt;It's taken three and a half years of my life at Moderna to build this culture, but it's been worth it. I'm optimistic about our future regardless of the stock price – I'm long on the technology, and concomitantly, I would love to see science run at the speed of thought!&lt;/p&gt;
&lt;p&gt;Thanks for coming to my TED Talk! :)&lt;/p&gt;
</content></entry><entry><title>Why you should take part in the SciPy sprints!</title><link href="https://ericmjl.github.io/blog/2025/3/17/why-you-should-take-part-in-the-scipy-sprints/" rel="alternate"/><updated>2025-03-17T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:8ef7ad6b-c4bd-3c32-96e0-8890ae046f8d</id><content type="html">&lt;p&gt;If you're a newcomer to the scientific Python open source community, hear me out: I would like to share with you why participating in sprints can be incredibly valuable, beneficial, and possibly even transformative for you!&lt;/p&gt;
&lt;p&gt;My first open source contribution was with the Matplotlib project at the SciPy 2015 sprints. I was assigned what seemed like a simple task: replace all PyLab imports in the examples gallery with proper import statements. Instead of this:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;matplotlib.pylab&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The examples would now have this:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;numpy&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;matplotlib.pyplot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;plt&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And I would make sure that each example that I changed ran correctly. Manually. By hand. One-by-one. Painstaking done for posterity.&lt;/p&gt;
&lt;p&gt;What started as a straightforward assignment turned into 70 pull requests over three months. But here's the thing - I learned so much during that process!&lt;/p&gt;
&lt;p&gt;First off, I got &lt;strong&gt;so many reps&lt;/strong&gt; with &lt;code&gt;git&lt;/code&gt; that by the end, I finally had the confidence to do &lt;code&gt;git&lt;/code&gt;-based software development on my own. When people say repetitions are the key to mastery, this rang absolutely true for me. My software skills improved dramatically. I witnessed firsthand how CI/CD was used to automatically check code, which I later applied to my own open source projects.&lt;/p&gt;
&lt;p&gt;I also observed how people collaborate both in-person and remotely. This became invaluable when I later worked at Novartis, where half my time was spent working remotely with teams in Basel while I was in Cambridge. I was used to asynchronous communication. And because there was always the risk of being blocked on one thing and not knowing how to proceed for a few days, I got used to juggling multiple projects in parallel so that I would always have something moving forward.&lt;/p&gt;
&lt;p&gt;Perhaps most importantly, I got to meet Matplotlib maintainers! Through those interactions, I learned how to communicate with them - understanding what information they needed whenever I reported an issue and how to provide minimal reproducible examples. The face time with them and our consistent interactions through those pull requests built trust, making it easier to approach maintainers when I needed help. And a side effect is that as my time in the open source world grew, I could build that same trust with other open source project maintainers.&lt;/p&gt;
&lt;p&gt;This way of working with maintainers and understanding their perspective has helped me tremendously throughout my career. It's also been a great networking opportunity and an excellent way to learn new skills - especially for data scientists who can benefit significantly from stronger software development skills.&lt;/p&gt;
&lt;p&gt;For students especially (as I was back in 2015), sprints provide an invaluable opportunity to connect with the people developing the tools that you may rely on. You get to become more familiar with Python packages you might use in your day job while simultaneously giving back to the codebases you care about.&lt;/p&gt;
&lt;p&gt;If you're looking to make a permanent impact, contributing to software is a great way to do it. My impact on Matplotlib was helping people understand they shouldn't use PyLab by eliminating PyLab examples, reducing support questions for the developers.&lt;/p&gt;
&lt;p&gt;Here's my encouragement to you: book an extra day or two at your hotel, delay your return flight, and join the sprints. Network, write some code, and have fun. It could be an experience that absolutely changes the arc of your career trajectory!&lt;/p&gt;
</content></entry><entry><title>The art of finesse as a data scientist</title><link href="https://ericmjl.github.io/blog/2025/3/16/the-art-of-finesse-as-a-data-scientist/" rel="alternate"/><updated>2025-03-16T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:dcf20a87-5652-36ed-ab47-a60236c73c9f</id><content type="html">&lt;p&gt;I was reading &lt;a href="https://newsletter.weskao.com/"&gt;Wes Kao's newsletter&lt;/a&gt; recently when she brought up this &lt;a href="https://newsletter.weskao.com/p/the-unspoken-skill-of-finesse"&gt;concept of "finesse."&lt;/a&gt; This idea immediately clicked for me. In data science, finesse distinguishes exceptional practitioners from the merely competent. It's the subtle skill that allows data scientists to navigate complex challenges while consistently delivering value.&lt;/p&gt;
&lt;h2 id="what-finesse-looks-like-in-practice"&gt;What finesse looks like in practice&lt;/h2&gt;&lt;p&gt;Finesse in data science manifests in several interconnected ways. At its core, it's about making smart decisions about where to focus your efforts, communicating progress effectively, and maintaining momentum even when faced with obstacles. Let me walk through how this plays out in real work situations.&lt;/p&gt;
&lt;h2 id="recognizing-and-escaping-rabbit-holes"&gt;Recognizing and escaping rabbit holes&lt;/h2&gt;&lt;p&gt;Every data scientist has experienced it—what should have been a quick debugging task somehow consumes your entire week. You find yourself four days deep in a technical rabbit hole with nothing tangible to show for it.&lt;/p&gt;
&lt;p&gt;Finesse means developing the self-awareness to recognize when you're stuck and the judgment to adjust course. Instead of persisting with diminishing returns, ask yourself: "Is this the best use of my time right now? What alternative approaches might get me to a useful result faster?"&lt;/p&gt;
&lt;p&gt;For example, when working on prioritizing protein mutations with a computation taking far longer than expected, finesse guides you to develop a parallel approach. Rather than just waiting, you tell collaborators: "While my comprehensive analysis runs in the background, here's an interim solution to review. Can you identify any concerns with these initial results?"&lt;/p&gt;
&lt;p&gt;This strategic pivot keeps the project moving forward while still pursuing the optimal solution. You've created a path to progress rather than allowing a technical challenge to become a complete roadblock.&lt;/p&gt;
&lt;h2 id="creating-tangible-progress-markers"&gt;Creating tangible progress markers&lt;/h2&gt;&lt;p&gt;The most visible demonstration of finesse is consistently showing progress through concrete evidence. Your stakeholders need to see the work advancing through tangible artifacts:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Visualizations that highlight initial insights&lt;/li&gt;
&lt;li&gt;Written summaries documenting your current understanding&lt;/li&gt;
&lt;li&gt;Regularly updated documentation of your approach&lt;/li&gt;
&lt;li&gt;Working prototypes, even if they represent simplified solutions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Your collaborators aren't simply curious—they're accountable to others and depend on your work to meet their own commitments. They need evidence of progress they can share with their stakeholders.&lt;/p&gt;
&lt;p&gt;Consider your manager specifically. They directly justify the resources allocated to your work. As one of the most expensive investments in the company, you need to demonstrate continuous value creation. Without visible progress markers, this becomes nearly impossible, putting both your manager and your project at risk.&lt;/p&gt;
&lt;h2 id="working-backwards-from-meaningful-milestones"&gt;Working backwards from meaningful milestones&lt;/h2&gt;&lt;p&gt;A powerful technique for maintaining momentum is identifying meaningful intermediate milestones and working backwards from them. Ask yourself: "What halfway point would give my stakeholders confidence we're on the right track?"&lt;/p&gt;
&lt;p&gt;This approach forces you to break complex problems into demonstrable chunks. Each milestone provides an opportunity to gather feedback, validate assumptions, and adjust course if necessary.&lt;/p&gt;
&lt;p&gt;Rather than waiting until you have the perfect model, share preliminary results that show promise. Validate your data processing pipeline before completing the full analysis. These intermediate deliverables build credibility and trust with your team while ensuring you're headed in the right direction.&lt;/p&gt;
&lt;h2 id="adapting-with-purpose-and-justification"&gt;Adapting with purpose and justification&lt;/h2&gt;&lt;p&gt;Finesse requires purposeful adaptability. When evidence suggests your current approach isn't optimal, make calculated adjustments while keeping your ultimate objective in focus.&lt;/p&gt;
&lt;p&gt;These adaptations must be defensible and based on what you've learned. When changing course, clearly articulate:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What you've discovered that prompted this change&lt;/li&gt;
&lt;li&gt;Why the new direction is more promising&lt;/li&gt;
&lt;li&gt;How it still connects to your ultimate goal&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This balance between persistence and flexibility demonstrates maturity and judgment. You're neither rigidly sticking to a failing approach nor capriciously jumping between methods without justification.&lt;/p&gt;
&lt;h2 id="overcoming-technical-roadblocks-creatively"&gt;Overcoming technical roadblocks creatively&lt;/h2&gt;&lt;p&gt;Technical limitations test your finesse perhaps more than anything else. When your stack is outdated or your environment creates blockers, finding creative workarounds becomes essential.&lt;/p&gt;
&lt;p&gt;Apply finesse by temporarily using alternative tools, simplifying your approach to work within current constraints, or creating mock-ups of what would be possible once technical issues are resolved. The goal is to ensure technical challenges don't halt all progress, even if you can't completely solve them immediately.&lt;/p&gt;
&lt;p&gt;This creative problem-solving shows initiative and demonstrates your commitment to delivering value despite obstacles. It builds confidence with stakeholders who see your ability to navigate complex technical environments.&lt;/p&gt;
&lt;h2 id="cultivating-a-network-for-timely-assistance"&gt;Cultivating a network for timely assistance&lt;/h2&gt;&lt;p&gt;A crucial aspect of finesse is knowing whom in your organizational network to approach when you need specific help, rather than solely relying on your line management to make connections. This requires proactively building relationships across teams and disciplines over time.&lt;/p&gt;
&lt;p&gt;When facing a technical challenge or needing domain expertise, finesse means already knowing which colleague might have encountered a similar issue or possesses the knowledge you need. This network doesn't materialize overnight—it requires deliberate cultivation through regular cross-team interactions outside formal meetings, following up on casual conversations with meaningful exchanges, offering help to others before you need assistance yourself, and demonstrating genuine interest in colleagues' work.&lt;/p&gt;
&lt;p&gt;By building this network proactively, you can quickly tap into collective organizational knowledge when challenges arise. This approach often resolves problems faster than waiting for formal channels and demonstrates resourcefulness that builds confidence with stakeholders.&lt;/p&gt;
&lt;h2 id="mastering-the-essential-balances"&gt;Mastering the essential balances&lt;/h2&gt;&lt;p&gt;Underlying all these aspects of finesse is the ability to maintain several critical balances. You must develop judgment about when to persist through difficulty versus when to pivot to a different approach. You need to balance perfectionism against progress, delivering valuable insights incrementally rather than waiting for the complete solution. Maintaining scientific integrity while making your work accessible requires balancing technical depth with clear communication. And following a clear strategy while incorporating new information demands a balance between planning and adaptation.&lt;/p&gt;
&lt;p&gt;Mastering these balances takes time and conscious effort, but they represent the foundation of effective professional judgment in data science.&lt;/p&gt;
&lt;h2 id="developing-your-own-finesse"&gt;Developing your own finesse&lt;/h2&gt;&lt;p&gt;Finesse develops through experience, reflection, and deliberate practice. When facing obstacles, I've learned to ask myself: "What would a data scientist with finesse do right now?" The answer usually involves delivering incremental value while maintaining progress toward the comprehensive solution.&lt;/p&gt;
&lt;p&gt;What's important to understand is that finesse isn't typically an innate talent. It's a skill cultivated through intentional development over time. For some who may have natural inclinations toward strategic thinking or communication, this might take months to years to refine. For others starting from scratch, it can be a journey spanning years or even decades.&lt;/p&gt;
&lt;p&gt;The most effective way to develop finesse is through a combination of deliberate daily practice, mentorship, and reflective analysis. Consciously apply finesse principles to everyday challenges to build the muscle memory needed for these approaches to become second nature. Learn directly from those who already demonstrate these skills to find models to emulate and shortcuts past common pitfalls. Regularly examine your approaches and outcomes to identify improvement opportunities. Study how colleagues with finesse navigate similar situations to discover practical templates for your own work.&lt;/p&gt;
&lt;p&gt;The approaches I've outlined reflect what I've observed in my day-to-day work. Effective data scientists demonstrate finesse by balancing persistence with adaptability, communicating progress transparently, and finding creative paths forward when faced with technical limitations.&lt;/p&gt;
&lt;p&gt;Yet finesse manifests in countless ways beyond what I've described here. I'm curious - what are other ways you see finesse showing up in data science roles?&lt;/p&gt;
</content></entry><entry><title>A blueprint for data-driven molecule engineering</title><link href="https://ericmjl.github.io/blog/2025/3/6/a-blueprint-for-data-driven-molecule-engineering/" rel="alternate"/><updated>2025-03-06T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:62d4fd48-0e31-338d-8bb1-3fe5d806b33f</id><content type="html">&lt;p&gt;Recently, I've been writing about my thoughts on data science in the biotech world, especially for those who are molecule hunters. You can find some of those posts below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2025/2/23/reliable-biological-data-requires-physical-quantities-not-statistical-artifacts/"&gt;Reliable biological data requires physical quantities, not statistical artifacts&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2025/1/19/why-data-from-preclinical-biotech-lab-experiments-make-machine-learning-challenging/"&gt;Why data from preclinical biotech lab experiments make machine learning challenging&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/11/22/a-modest-proposal-for-data-catalogues-at-biotechs/"&gt;A modest proposal for data catalogues at biotechs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/25/the-human-dimension-to-clean-distributable-and-documented-data-science-code/"&gt;The Human Dimension to Clean, Distributable, and Documented Data Science Code&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/18/keys-to-effective-collaborative-data-science/"&gt;Keys to effective collaborative data science&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This post kicks off a series on how cross-functional molecule design teams can achieve operational speed and efficiency. Through the lens of a fictitious startup, Catalyst Therapeutics (any similarities to real companies are coincidental), I'll share insights drawn from patterns and challenges that I have seen and heard. This series represents a strategic playbook I've developed to accelerate molecule discovery to what I call "the speed of thought."&lt;/p&gt;
&lt;p&gt;Here's my goal: I want to illustrate that none of this has to be fancy. &lt;strong&gt;It just has to be a stable enough crank to be turned into a flywheel.&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Throughout this series, I'll address the crucial aspects every biotech data science team must master: data capture and engineering, with statistical modeling at its core, supercharged with machine learning and proper experimental design. Catalyst's mission—developing novel protein binders for an oncology target that has eluded conventional methods—serves as the perfect vehicle to demonstrate the principles in this playbook. If your interest is piqued, read on, and I hope you enjoy it :).&lt;/p&gt;
&lt;h2 id="the-catalyst-therapeutics-story-designing-a-novel-protein-binder"&gt;The Catalyst Therapeutics story: designing a novel protein binder&lt;/h2&gt;&lt;p&gt;When I first walked into their lab, I met the core team who would become the heroes of this story:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Maya&lt;/strong&gt;, the protein engineer with a decade of wet lab experience and a healthy skepticism about computational methods. "Models are nice," she often said, "but proteins don't read papers."&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Dev&lt;/strong&gt;, the junior scientist fresh from a top-tier PhD program, eager to apply cutting-edge techniques but still learning the gap between academic benchmarks and real-world biology.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Sophie&lt;/strong&gt;, the pragmatic data scientist with stints at three different biotechs—one acquired, one failed, and one struggling with scaling challenges. As she told me, "I've seen enough ways this can go wrong across different contexts. I'm here to avoid repeating those mistakes."&lt;/p&gt;
&lt;p&gt;This team was about to embark on a campaign to develop a novel protein scaffold that could bind to their target with high specificity and favorable biophysical properties. What made their approach work—when so many similar efforts fail—was their integrated approach to experimental design from day one.&lt;/p&gt;
&lt;h2 id="first-meeting-setting-the-foundation"&gt;First Meeting: Setting the Foundation&lt;/h2&gt;&lt;p&gt;I sat in on their kickoff meeting, where Sophie immediately steered the conversation away from fancy algorithms.&lt;/p&gt;
&lt;p&gt;"Before we talk about deep learning or any computational methods," she said, "let's map out what we're measuring and how we'll account for variables that have nothing to do with our protein sequences."&lt;/p&gt;
&lt;p&gt;Maya nodded enthusiastically. "In my last role, we spent six months optimizing the wrong property because our assay was actually measuring something else entirely."&lt;/p&gt;
&lt;p&gt;They spent the entire first day diagramming their experimental workflow on a whiteboard, identifying every potential confounder:&lt;/p&gt;
&lt;p&gt;"If we use different plates on different days, we need to track that," Sophie noted, creating a column in her spreadsheet.&lt;/p&gt;
&lt;p&gt;"The position in the plate matters too," Dev added. "Edge wells behave differently due to evaporation."&lt;/p&gt;
&lt;p&gt;"And don't forget who's running the experiment," Maya said. "Different hands, different results."&lt;/p&gt;
&lt;p&gt;What struck me was how Sophie approached this not as a statistics problem but as a practical data collection issue. She wasn't trying to impress anyone with complex experimental designs—she was making sure their foundation was solid.&lt;/p&gt;
&lt;p&gt;"Every variable we don't account for now becomes noise that we can't remove later," she explained. "And noise puts a hard ceiling on what our models can achieve."&lt;/p&gt;
&lt;h2 id="the-experimental-design-takes-shape"&gt;The experimental design takes shape&lt;/h2&gt;&lt;p&gt;Over the next week, the team developed an experimental plan:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Every plate would include control proteins in the same positions—a practical compromise since truly randomized placement would be statistically ideal but experimentally infeasible&lt;/li&gt;
&lt;li&gt;Each experimental run would include samples from previous runs to detect day-to-day drift&lt;/li&gt;
&lt;li&gt;They included standardized control samples that both Maya and Dev would process independently once a week to quantify operator effects, also taking advantage of these controls to measure well position effects&lt;/li&gt;
&lt;li&gt;They created detailed metadata sheets that captured everything from reagent lot numbers to ambient temperature&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;"Slow is smooth, and smooth is fast," Maya said when Dev worried about the extra controls. "Taking time to set this up properly now will save us months of troubleshooting later."&lt;/p&gt;
&lt;p&gt;When I asked Dev if this felt like overkill, his response was telling: "Sophie showed us simulations of how much signal we'd lose if we didn't do this. The extra work now saves months later."&lt;/p&gt;
&lt;p&gt;These simulations revealed Sophie's deeper approach to experimental design. Rather than focusing on traditional power calculations to determine sample sizes, she had modeled how each source of variability would impact their ability to detect meaningful signals about molecular properties.&lt;/p&gt;
&lt;p&gt;"Think about it this way," Sophie explained to me later. "Every time we run an experiment, we're not just collecting data points. We're updating our beliefs about how these molecules behave. That's why this is fundamentally a Bayesian problem."&lt;/p&gt;
&lt;p&gt;What Catalyst understood—that biotech data science teams may miss—is that effective experimental design in molecular discovery is about maximizing the information we gain from each experiment while explicitly accounting for sources of uncertainty. Sophie's approach treated every experimental result as a way to update their beliefs about molecular properties, requiring them to carefully model all sources of noise that could obscure the signal they actually cared about.&lt;/p&gt;
&lt;p&gt;Sophie explained her philosophy: "In biotech, we're not trying to publish a paper with p-values and hypothesis tests. We're building a Bayesian model that explicitly represents all the factors influencing our measurements. The traditional approach asks 'what's the probability of seeing this data given my hypothesis?' But we need to instead ask, 'what's the probability my molecule has this property given all the confounding factors in my data?' That means our north star is signal-to-noise ratio and explicit modeling of uncertainty, not statistical significance."&lt;/p&gt;
&lt;h2 id="initial-data-collection-reveals-biases-in-data"&gt;Initial data collection reveals biases in data&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Week 2:&lt;/strong&gt; Two weeks into their campaign, the first data started coming in. During their analysis meeting, Sophie projected a heatmap showing expression levels across their first set of variants.&lt;/p&gt;
&lt;p&gt;"See this pattern?" she asked, pointing to subtly higher values on the right side of each plate on the control plates. "This is a systematic bias in the reader. Thankfully we planned to control for well position. If not, we might have thought these sequences were actually better."&lt;/p&gt;
&lt;p&gt;Maya looked concerned. "So we throw out the data?"&lt;/p&gt;
&lt;p&gt;"No," Sophie smiled. "Because we designed for this. We can model the positional effect and subtract it out." She pulled up her statistical model, which included terms for plate, position, date, and operator.&lt;/p&gt;
&lt;p&gt;This moment illustrated something crucial: good experimental design means experiments where you can identify and account for imperfections. No perfect experiment exists, but if we can capture as much data as possible up-front, we afford ourselves the chance of controlling for those confounding imperfections.&lt;/p&gt;
&lt;p&gt;The team continued their first round, generating expression and binding data for their initial library of 500 variants. Unlike many campaigns that rush to make thousands of variants, they deliberately kept this round small.&lt;/p&gt;
&lt;p&gt;"We need to validate our measurement pipeline before scaling," Sophie explained. "A thousand noisy measurements are worth less than a hundred clean ones."&lt;/p&gt;
&lt;h2 id="statistical-estimation-of-molecular-property"&gt;Statistical estimation of molecular property&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Week 4:&lt;/strong&gt; One month in, they had enough data to build their first statistical models. I watched Sophie work through this methodically:&lt;/p&gt;
&lt;p&gt;"The first thing we need to do is to build a protein property estimation model that regresses out all our known confounders," she explained to the team. "We're not trying to predict anything yet—we're just trying to get clean property estimates for each sequence, while quantifying the fundamental noise level that we have not measured."&lt;/p&gt;
&lt;p&gt;She walked them through her model, which looked approximately like:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Binding Observation = f(Sequence) + Expression Level + Plate + Position + Date + Operator + (Instrument × Date) + ε
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;"This is crucial to understand," Sophie explained. "Our binding observation is the sum of the true property value dictated by the sequence—that's the f(Sequence) part—plus the effect of expression level, plus all these shifts induced by experimental factors, plus &lt;code&gt;ε&lt;/code&gt;, which represents the remaining noise intrinsic to the system that cannot be explained by the rest of the factors."&lt;/p&gt;
&lt;p&gt;"Notice that last term," she pointed out. "We think the instrument behaves differently on different days, so we have an interaction term to capture that."&lt;/p&gt;
&lt;p&gt;Dev seemed confused. "Couldn't a machine learning model figure this out automatically?"&lt;/p&gt;
&lt;p&gt;Sophie shook her head. "ML models can find patterns, but they can't tell you which patterns matter for your science and which are just experimental artifacts. We need to be explicit about our assumptions."&lt;/p&gt;
&lt;p&gt;When she fitted the model, it provided estimates for how much each factor contributed to the measurements. The plate effect was substantial. The position effect was moderate. The operator effect was surprisingly small.&lt;/p&gt;
&lt;p&gt;"Good news," Sophie told Maya and Dev. "You two are performing the assay very consistently. That's one less factor to worry about."&lt;/p&gt;
&lt;p&gt;This analysis gave them clean property estimates for each sequence, with confounders regressed out. But it also provided something equally valuable: a measure of their experimental noise.&lt;/p&gt;
&lt;p&gt;"Our statistical analysis shows that the unexplained variance,  &lt;code&gt;ε&lt;/code&gt;, is about 1.3 in our binding measurement, while the total variance of our measurements is about 3.0," Sophie noted. "That means our theoretical maximum variance explained can be calculated as 1 - (variance of noise/total variance of measurements), which gives us about 56.7%. No model, no matter how sophisticated, can exceed that ceiling. That's our measurement limit."&lt;/p&gt;
&lt;p&gt;This reality check sparked immediate action from the team. Maya studied the plate effect data and suggested modifications to their protocol. "If we pre-equilibrate the plates for 30 minutes at room temperature before reading, we might reduce that variance," she hypothesized. Meanwhile, Dev focused on the position effect, designing a randomization scheme that would distribute key variants across different plate positions in future experiments.&lt;/p&gt;
&lt;p&gt;"Every bit of noise we eliminate now increases what our models can learn later," Sophie reminded them. "Let's implement these improvements for our next round."&lt;/p&gt;
&lt;p&gt;This reality check is something many teams miss. If your experimental precision is low, no amount of modeling sophistication will overcome it.&lt;/p&gt;
&lt;h2 id="predictive-modeling-of-molecular-properties"&gt;Predictive modeling of molecular properties&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Week 6:&lt;/strong&gt; With clean property estimates in hand, the team moved to the predictive modeling phase. Sophie started simple:&lt;/p&gt;
&lt;p&gt;"Before we try anything fancy, let's establish a baseline with a random forest on one-hot encoded sequences," she told the team. "This gives us the simplest possible pipeline from sequence to prediction, with minimal choices to debug."&lt;/p&gt;
&lt;p&gt;She showed them her approach in code, explaining both the model source definition and the training procedure:&lt;/p&gt;
&lt;p&gt;"There are two distinct parts to this exercise. First, our model source—the code that defines how we transform sequences and what algorithm we use. Second, our model version—the actual trained model with parameters fit to our specific dataset."&lt;/p&gt;
&lt;p&gt;Sophie's baseline consisted of:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A simple flattened one-hot encoding for sequence featurization&lt;/li&gt;
&lt;li&gt;A default random forest implementation with minimal hyperparameter tuning&lt;/li&gt;
&lt;li&gt;A clear evaluation protocol that would remain consistent across iterations&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;"This entire pipeline should take under an hour to implement and train, especially with AI assistance," she explained. "It establishes our lower bound on expected performance. I bet a deep neural network model could work, but it would be quite a bit more time to code up and verify that it's working correctly. This model gives me a baseline to work with for our prioritization efforts."&lt;/p&gt;
&lt;p&gt;When the results came back, the model explained about 30% of the variance in binding affinity.&lt;/p&gt;
&lt;p&gt;"Is that good?" Dev asked.&lt;/p&gt;
&lt;p&gt;"It's promising," Sophie replied. "Given our experimental noise and total measurement variance, the theoretical maximum is about 56.7%. We're capturing more than half of the explainable signal, which is reasonable for a first pass."&lt;/p&gt;
&lt;p&gt;Dev continued, "I also heard there are other featurization methods, would you want to test-drive them?"&lt;/p&gt;
&lt;p&gt;"Maybe, but I'm more concerned about our computational workflow being scaffolded out end-to-end," Sophie replied., "and modularizing it such that we can target individual components for improvements later on. The more important thing is for us to find binders; we can test the effect of computational design workflows opportunistically later."&lt;/p&gt;
&lt;p&gt;Maya had a practical question, asking, "Can we use it to find better binders?"&lt;/p&gt;
&lt;p&gt;"That's the real test," Sophie agreed. "But there's another benefit to this simple baseline—it helps us diagnose whether our modeling task is even feasible. If we'd gotten variance explained of 0.1 or 0.2, we'd probably need to revisit our statistical estimation or experimental design. The baseline tells us that this is a tractable problem."&lt;/p&gt;
&lt;p&gt;What I appreciated about Sophie's approach was her pragmatism. She wasn't trying to publish a methods paper—she was trying to help the team find better molecules. She later quickly evaluated several model architectures, but always with an eye toward practical utility rather than theoretical sophistication.&lt;/p&gt;
&lt;p&gt;"A model that explains 5% more variance but takes a week longer to build isn't worth it at this stage," she explained. "Speed of iteration matters more than squeezing out the last bits of performance. We version both our code and our trained models, but we don't let perfect be the enemy of good."&lt;/p&gt;
&lt;h2 id="generation-and-prioritization"&gt;Generation and prioritization&lt;/h2&gt;&lt;p&gt;&lt;strong&gt;Week 8:&lt;/strong&gt; Two months into the campaign, the team was ready to generate new candidates. Sophie convened a strategy meeting where each member brought different ideas to the table: computational predictions from her model, Maya's structural insights, and Dev's literature-derived variants.&lt;/p&gt;
&lt;p&gt;"We have budget for about 1,000 new sequences," Maya said. "Given our lab capacity, reagent costs, and screening throughput, that's our practical limit for this round. Let's allocate our experimental real estate thoughtfully instead of just picking the top model predictions."&lt;/p&gt;
&lt;p&gt;Sophie nodded in agreement. "A mentor of mine who was a computational chemist used to say, 'Show me your hypothesis in the form of a molecule.' That's what we're doing here—translating our different hypotheses into actual sequences we can test."&lt;/p&gt;
&lt;p&gt;After a focused afternoon of debate, they divided their budget:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;512 sequences for model-guided optimization&lt;/li&gt;
&lt;li&gt;243 for testing Maya's structural hypotheses&lt;/li&gt;
&lt;li&gt;217 for exploring diverse regions of sequence space&lt;/li&gt;
&lt;li&gt;28 for controls and replicates&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This wasn't arbitrary allocation, and in a later post I will detail the method by which they arrived at this budget allocation. Sophie reminded them that their model explained 30% variance against a theoretical maximum of 56.7% - "Good enough to guide us, but not to trust blindly."&lt;/p&gt;
&lt;p&gt;Maya argued for her structural approaches: "The model finds patterns but doesn't understand physics." Dev pushed for diversity of mutations sourced from the literature: "That's how we'll improve the model most efficiently." Sophie mediated, pushing them to quantify their confidence in each approach.&lt;/p&gt;
&lt;p&gt;They ultimately balanced exploitation (model-guided sequences) with exploration (structural hypotheses and diversity) while maintaining experimental quality control. This embodied a key principle: models serve the science, not vice versa. The goal was finding molecules that would actually work, not optimizing model metrics.&lt;/p&gt;
&lt;h2 id="learning-and-iteration"&gt;Learning and iteration&lt;/h2&gt;&lt;p&gt;The results from their second round were enlightening. The model-guided sequences showed improved binding, but two of Maya's structural hypotheses produced even better variants.&lt;/p&gt;
&lt;p&gt;"This is why we need both approaches," Sophie explained during their review. "The model found the local optimum, but Maya's intuition helped us jump to a different part of the fitness landscape."&lt;/p&gt;
&lt;p&gt;What was equally impressive was the impact of their experimental improvements. Thanks to Maya's plate pre-equilibration protocol and Dev's position randomization scheme, the unexplained assay variability had decreased from 56.7% to just 31.4%.&lt;/p&gt;
&lt;p&gt;"Look at this," Sophie highlighted during their review meeting. "Our noise floor dropped substantially. The theoretical maximum variance we can explain is now 68.6% instead of 56.7%."&lt;/p&gt;
&lt;p&gt;They updated their models with the new data, and the predictive performance improved substantially. The model now explained nearly 47.2% of the variance—a significant improvement in absolute terms, but even more impressive relative to their new theoretical maximum.&lt;/p&gt;
&lt;p&gt;"The model is getting better because we're giving it better data," Sophie pointed out. "This is the virtuous cycle we want: better experiments lead to better models lead to better designs lead to better experiments."&lt;/p&gt;
&lt;p&gt;Dev nodded appreciatively. "It's not just about more data—it's about cleaner data."&lt;/p&gt;
&lt;p&gt;"Exactly," Sophie agreed. "We're not just climbing the hill faster; we've actually made the hill higher."&lt;/p&gt;
&lt;h2 id="lessons-for-biotech-data-science-leads"&gt;Lessons for biotech data science leads&lt;/h2&gt;&lt;p&gt;Catalyst's story illustrates several principles that biotech founders should embrace:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1. Integration is more important than specialization&lt;/strong&gt; Sophie's ability to work across experimental design, statistics, and machine learning was more valuable than deep expertise in any one area. Look for data scientists who can bridge these domains rather than hiring separate specialists.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2. Experimental design sets your ceiling&lt;/strong&gt; No computational method can extract information that isn't in your data. Invest heavily in experimental design before scaling data collection.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3. Control what you can, measure what you can't&lt;/strong&gt; You can't eliminate all experimental variability, but you can measure and account for it. Track everything that might confound your measurements, or find plausible and logical surrogates for them.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4. Know your noise floor&lt;/strong&gt; Measure your experimental precision early. It tells you the theoretical maximum performance of any predictive model.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;5. Start simple, add complexity carefully&lt;/strong&gt; Begin with baseline models that establish performance benchmarks. Add complexity &lt;em&gt;only when it demonstrably improves decisions, not metrics&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;6. Balance computation and intuition&lt;/strong&gt; The most powerful approaches combine computational methods with human expertise and intuition.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;7. Allocate experimental resources thoughtfully&lt;/strong&gt; Don't just test the top predictions from your model. Allocate your experimental budget across different strategies.&lt;/p&gt;
&lt;hr&gt;
&lt;p&gt;In the next parts of this series, we'll see how:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The team designed the experiment collaboratively and why that was important,&lt;/li&gt;
&lt;li&gt;The use of Bayesian estimation was crucial for generating archival data,&lt;/li&gt;
&lt;li&gt;Sophie approached model building with speed and pragmatism put together (hint: AI was used!), and&lt;/li&gt;
&lt;li&gt;The team specifically arrived at that design budget allocation.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Stay tuned for more!&lt;/p&gt;
</content></entry><entry><title>How to fix PyPI upload errors related to license metadata</title><link href="https://ericmjl.github.io/blog/2025/3/1/how-to-fix-pypi-upload-errors-related-to-license-metadata/" rel="alternate"/><updated>2025-03-01T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:407cc55a-f470-3479-aa66-55a4f8ed5339</id><content type="html">&lt;h2 id="background"&gt;Background&lt;/h2&gt;&lt;p&gt;I use a &lt;a href="https://github.com/ericmjl/llamabot/blob/main/.github/workflows/release-python-package.yaml"&gt;GitHub Actions workflow&lt;/a&gt; to automate the release process for my Python packages. The workflow was designed to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Bump the version number&lt;/li&gt;
&lt;li&gt;Automatically generate release notes using &lt;a href="https://github.com/ericmjl/llamabot"&gt;LlamaBot&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Build the package using &lt;code&gt;uv&lt;/code&gt;, a blazing-fast Python package installer and resolver&lt;/li&gt;
&lt;li&gt;Upload to PyPI&lt;/li&gt;
&lt;li&gt;Create a GitHub release&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I chose &lt;code&gt;uv&lt;/code&gt; for my CI/CD pipelines because it's significantly faster than pip and provides more reliable dependency resolution. However, even with &lt;code&gt;uv&lt;/code&gt;'s modern tooling, I encountered this error:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;error: Upload failed with status code 400 Bad Request. Server says: 400 license-file introduced in metadata version 2.4
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="the-problem"&gt;The problem&lt;/h2&gt;&lt;p&gt;The error occurred because I was using setuptools as my build backend, which has less mature support for newer PEP 621 metadata features like the &lt;code&gt;license&lt;/code&gt; field with a &lt;code&gt;file&lt;/code&gt; specification. This led me down a path of discovery about modern Python packaging tools. Even though I was using &lt;code&gt;uv&lt;/code&gt; for package installation, the underlying build system still needed modernization.&lt;/p&gt;
&lt;h2 id="the-solution-switch-to-hatchling"&gt;The solution: Switch to Hatchling&lt;/h2&gt;&lt;h3 id="pre-requisites"&gt;Pre-requisites&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;A Python package with a &lt;code&gt;pyproject.toml&lt;/code&gt; file&lt;/li&gt;
&lt;li&gt;GitHub Actions workflow for package publishing&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="steps"&gt;Steps&lt;/h3&gt;&lt;ol&gt;
&lt;li&gt;Update your &lt;code&gt;pyproject.toml&lt;/code&gt; build system specification from setuptools to Hatchling:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;[build-system]&lt;/span&gt;
&lt;span class="n"&gt;requires&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;hatchling&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;build-backend&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;hatchling.build&amp;quot;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ol&gt;
&lt;li&gt;Keep your existing license specification:&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;[project]&lt;/span&gt;
&lt;span class="c1"&gt;# ... other fields ...&lt;/span&gt;
&lt;span class="n"&gt;license&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="n"&gt;file&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;LICENSE&amp;quot;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;That's it! The error should be resolved because Hatchling has better support for modern packaging metadata.&lt;/p&gt;
&lt;h2 id="explanation"&gt;Explanation&lt;/h2&gt;&lt;p&gt;The Python packaging ecosystem has been evolving rapidly. Newer tools like Hatchling are built from the ground up to support modern packaging standards and provide better defaults. The error I encountered was just one symptom of using an older tool (setuptools) that's still catching up with newer metadata specifications.&lt;/p&gt;
&lt;h2 id="references"&gt;References&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://hatch.pypa.io/1.7/why/"&gt;Why Hatch?&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://packaging.python.org/"&gt;Python Packaging User Guide&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://discuss.python.org/t/good-packaging-setuptools-whatever-tutorial/19378"&gt;Python Discourse Discussion on Build Backends&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
</content></entry><entry><title>Reliable biological data requires physical quantities, not statistical artifacts</title><link href="https://ericmjl.github.io/blog/2025/2/23/reliable-biological-data-requires-physical-quantities-not-statistical-artifacts/" rel="alternate"/><updated>2025-02-23T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:3b919526-2f66-3042-96c8-b641678c3184</id><content type="html">&lt;p&gt;In my work with building machine learning models in the life sciences, I've noticed a recurring pattern that needs addressing. We might try to use sequence to predict a quantity that is normalized against a control (that might then be normalized again against some baseline), and we might be asked to make our model predict a statistical quantity that is an artifact of our experimental noise and has nothing to do with sequence. I argue that this is fundamentally flawed. Let me explain by highlighting a prototypical example.&lt;/p&gt;
&lt;h2 id="a-prototypical-example"&gt;A prototypical example&lt;/h2&gt;&lt;p&gt;A team is looking at developing novel non-antibody binders using modern protein design tools (such as ProteinMPNN and RFDiffusion). It being a new startup, at the behest of company management, the laboratory scientists design the experiment such that only controls are replicated across plates, while test proteins were not -- all to save on cost and iteration time.&lt;/p&gt;
&lt;p&gt;As such, we only have a single value readout per test protein. To make matters worse, this being a novel non-antibody binder, we have no non-antibody binders to act as positive controls, and can only look at a positive control antibody instead.&lt;/p&gt;
&lt;p&gt;To get around this problem, the data scientist, who was not involved in the experiment design but was involved in analysis of the data, decides to pool the negative controls, estimate a distribution for the control, and calculate a one-sided p-value of the test protein against the pooled control distributions. He does the same for the positive controls. The more extreme the p-value of the test protein against the negative controls, the better the protein is.&lt;/p&gt;
&lt;p&gt;The two numbers that get recorded for data archival purposes is something like this:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Sequence&lt;/th&gt;
&lt;th&gt;Neg control p-value&lt;/th&gt;
&lt;th&gt;Pos control p-value&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;MKLLTVFLGLLLLWPGAQS...&lt;/td&gt;
&lt;td&gt;0.003&lt;/td&gt;
&lt;td&gt;0.891&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;DVQLVESGGGLVQPGGSL...&lt;/td&gt;
&lt;td&gt;0.008&lt;/td&gt;
&lt;td&gt;0.756&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;EVQLLESGGGLVKPGGSL...&lt;/td&gt;
&lt;td&gt;0.001&lt;/td&gt;
&lt;td&gt;0.945&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;QVQLQESGPGLVKPSQTL...&lt;/td&gt;
&lt;td&gt;0.015&lt;/td&gt;
&lt;td&gt;0.682&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MALWMRLLPLLALLALWG...&lt;/td&gt;
&lt;td&gt;0.042&lt;/td&gt;
&lt;td&gt;0.523&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Single replicate binding measurements were deemed not sufficiently reliable to record as archival data, and as such, but the massaged version of that data (in the form of p-values) gave 1-3 hits, so the team felt confident in the decision-making process and went ahead with this. The data scientists decided to train a machine learning model to predict two p-values, and use those p-values to prioritize new non-antibody designs.&lt;/p&gt;
&lt;h2 id="whoa-hold-your-horses-here.."&gt;Whoa, hold your horses here...&lt;/h2&gt;&lt;p&gt;(If you're looking at the state of affairs I just described and are puzzled, confused, and perhaps even incredulous at why this is even happening, just know that you're not alone -- it's why I'm writing this down. Bear with me!)&lt;/p&gt;
&lt;p&gt;Let's think carefully about the long-term state of this data. It being the company crown jewels, we need to make sure it has the following qualities:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It's easily interpretable: as few pieces of experiment-specific background knowledge are necessary to understand&lt;/li&gt;
&lt;li&gt;It is comparable with other datasets &lt;em&gt;with as few caveats as possible&lt;/em&gt;.&lt;/li&gt;
&lt;li&gt;It is easy to document and justify the design choices in the data that are present.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I would like to argue that the team collectively failed the data scientist and circumstances forced him into poor choices.&lt;/p&gt;
&lt;h2 id="what-s-wrong-with-p-values-as-archival-data"&gt;What's wrong with p-values as archival data?&lt;/h2&gt;&lt;p&gt;&lt;em&gt;A LOT!&lt;/em&gt; p-values are a statistical artifact, a property of the noise of your measurement system. If you store a p-value (as the estimated quantity) next to a sequence in a table, the next unsuspecting data scientist and member of leadership looking at the table will think that sequence can be used to predict p-value, and that is &lt;em&gt;completely illogical&lt;/em&gt;!&lt;/p&gt;
&lt;p&gt;Here's why: The p-value depends on the noise of the measurement system in the negative controls and positive controls. That noise level can evolve over time as laboratory experimenters get better and better at ferreting out sources of noise in their system. If the noise of the system gets progressively reduced, the controls will have tighter bounds. In turn, your non-control measurements may appear to have a smaller and smaller p-value, thereby giving the illusion that you are gaining more and more hits, when in fact all you've done is just made your control distribution tighter.&lt;/p&gt;
&lt;p&gt;Moreover, storing of p-values as archival data fails the test of easy interpretability and comparability. Proper data archival requires three key pieces of context:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The exact experimental protocol used (which can change over time)&lt;/li&gt;
&lt;li&gt;The statistical analysis method used to process the raw data&lt;/li&gt;
&lt;li&gt;The identity of controls used for comparison&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Without versioning both protocols and analysis code, and without explicitly storing control identities, data becomes increasingly difficult to interpret over time. Even small changes in laboratory technique or analysis methods can dramatically affect p-values, making historical comparisons meaningless.&lt;/p&gt;
&lt;p&gt;Some might argue that p-values are the standard in biological research and have served us well for decades. However, just because something is standard practice doesn't make it optimal. Many fields, from physics to engineering, have evolved beyond relative measurements to absolute physical quantities, leading to more reproducible and cumulative progress.&lt;/p&gt;
&lt;h2 id="what-s-wrong-with-predicting-p-values-from-sequence"&gt;What's wrong with predicting p-values from sequence?&lt;/h2&gt;&lt;p&gt;&lt;em&gt;EVERYTHING!&lt;/em&gt; Once again, p-values are a statistical artifact. A p-value is more heavily influenced by the noise in your experimental system than the sequence that you're trying to design. And if your laboratory scientists are also improving their laboratory technique and reducing the amount of uncertainty that are present in the measurement, you, the data scientist, are going to be fooled by the fact that your p-values are getting smaller and smaller, thinking that your models are getting better and better.&lt;/p&gt;
&lt;h2 id="let-s-not-fool-ourselves-here"&gt;Let's not fool ourselves here&lt;/h2&gt;&lt;p&gt;Richard Feynman once said:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The first principle is that you must not fool yourself—and you are the easiest person to fool.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;When we introduce a complex statistical procedure into the mix like I just described above, it's easy to fool ourselves into thinking that we're doing something that handles the complexity of the data.&lt;/p&gt;
&lt;p&gt;Unfortunately, the reality is this: The simpler our systems, the easier it is for us to wrangle, understand, interpret, and wield them. Complex statistical transformations and normalizations introduce more opportunities for error, make our data harder to interpret, and obscure the underlying physical relationships we're trying to understand. Every layer of statistical manipulation we add is another layer where we can introduce bias, lose information, or make incorrect assumptions.&lt;/p&gt;
&lt;p&gt;But let's be clear: "simple" doesn't mean "simplistic." As Einstein famously advised,&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;Everything should be made as simple as possible, but no simpler.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;While we should avoid unnecessary statistical complexity, we still need appropriate models to handle experimental variation and uncertainty. The key is choosing models that maintain the physical interpretability of our measurements while accounting for the inherent structure of our experimental system.&lt;/p&gt;
&lt;p&gt;When we keep our systems as simple as possible -- measuring physical quantities directly and minimizing transformations -- we maintain a clearer connection to the actual biology we're studying and make our results more easily reproducible and trustworthy.&lt;/p&gt;
&lt;h2 id="how-do-we-solve-this-problem"&gt;How do we solve this problem?&lt;/h2&gt;&lt;p&gt;So what's the path forward? Instead of adding layers of statistical complexity to compensate for poor experimental design and non-physical measurements, we need to focus on three fundamental improvements:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;better experimental design,&lt;/li&gt;
&lt;li&gt;direct measurement of quantities with physical units, and&lt;/li&gt;
&lt;li&gt;appropriate statistical estimation methods.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Let's examine each of these solutions in detail.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Firstly, we have to design better experiments.&lt;/strong&gt; Basic design of experiments dictates that we should have at least three replicates for each test protein we're measuring; if budget constrained, it's fine to do two replicates while acknowledging that we'll have noisier estimates. If we're doing plate-based assays that involve incubation, then it is imperative to do at least a single experiment of controls-only to measure positional variation that can be regressed out later.&lt;/p&gt;
&lt;p&gt;Secondly, we should avoid collecting &lt;em&gt;relative&lt;/em&gt; quantities as much as possible and instead &lt;strong&gt;collect data with physical units.&lt;/strong&gt; With physical quantities that have units (e.g. time, concentration, velocity, distance, counts), there are fewer layers of calculation that go into the final collected quantity that need to be documented, and that makes for a simpler system of data collection. Quantities for physical units are also easier to interpret without specific knowledge of an experiment. Finally, machine learning models trained on quantities with physical units are also mentally easier to connect to the laboratory experiment itself, making it easier to trust the model if it's performing well.&lt;/p&gt;
&lt;p&gt;(On that point, try explaining to a laboratory scientist, who is used to measuring concentrations, that you will now predict for them the "z-score transformation of binding for the target of interest" instead -- and watch out for the puzzled look that envelopes their expression.)&lt;/p&gt;
&lt;p&gt;While some normalization may still be necessary to arrive at physical quantities (e.g., using calibration curves), these transformations should be standardized, documented, and result in interpretable physical units rather than relative or statistical quantities.&lt;/p&gt;
&lt;p&gt;And if you're a card-carrying statistician, you'll know that as soon as you take a ratio of estimators, all of the "nice properties" you've derived for those estimators go out the window. But being a practically self-trained Bayesian with no formal training in statistics, I'm not in the business of deriving estimators, I'm in the business of building generative probabilistic models, which leads me to the third point...&lt;/p&gt;
&lt;p&gt;Thirdly, armed with replicates and physical quantities, we should &lt;strong&gt;build the simplest possible Bayesian estimation models of those quantities from replicate measurement data&lt;/strong&gt;. This gives us an opportunity to quantify the amount of noise that is present inside the experimental system, while also simultaneously doing a principled aggregation of replicate measurements. The presence of at least one more replicate affords us the ability to generate an estimate of uncertainty for each test protein. Bayesian estimation models also force us to be explicit about the distribution from which we believe our data were generated, including batch effects and experimental variation. For example, in a binding assay, we might model:&lt;/p&gt;
&lt;p&gt;$$\text{measurement} = \text{true_binding_strength} + \text{batch_effect} + \text{measurement_noise}$$&lt;/p&gt;
&lt;p&gt;By keeping our models simple and grounded in physical reality, we create data that stands the test of time -- data that future scientists can build upon without having to decode layers of statistical manipulation.&lt;/p&gt;
&lt;h2 id="the-speed-of-light-as-an-analogy"&gt;The speed of light as an analogy&lt;/h2&gt;&lt;p&gt;If you think about it, the speed of light and its history of measurement provides a useful, if imperfect, analogy here. The first known measurement of the speed of light was roughly in the 1600s and has been refined ever since. The methods have changed, and the precision of the estimate has as well, but we're able to compare these over time and show how progress has been made because we chose to store the speed of light with physical units rather than as a dimensionless relative quantity.&lt;/p&gt;
&lt;p&gt;Of course, biological measurements face challenges that physics doesn't - day-to-day variability in living systems, batch effects, and environmental sensitivities that necessitate internal controls. But this actually strengthens the argument for physical units: when we need to account for these biological variables, it's far better to do so explicitly (e.g., by measuring and correcting for batch effects) while maintaining physical units, rather than obscuring them through relative measurements and p-values.&lt;/p&gt;
&lt;p&gt;Thinking bigger picture, biology has suffered immensely from a lack of influence from the world of metrology (the science of measurement itself). This is why we have relative quantities and normalization to controls, which themselves are noisy, as a widespread practice amongst life scientists.&lt;/p&gt;
&lt;h2 id="a-different-story"&gt;A different story&lt;/h2&gt;&lt;p&gt;To cap things off, let me retell the story of the non-antibody binder design campaign where they decide to do things in a better way.&lt;/p&gt;
&lt;p&gt;The team decides to collect the data with duplicate measurements, striking a compromise between the cost of running an experiment and the reliability of the data collected. It is now twice as expensive per data point but also at least twice as reliable as without replicate measurements. While this initial investment might seem steep, the team recognizes that unreliable data is ultimately more expensive: failed follow-up experiments, missed opportunities, and wasted development time cost far more than getting the data right the first time.&lt;/p&gt;
&lt;p&gt;Because this is a binding experiment, the team decides upfront to collect data in units of concentration, which is a canonical unit for binding strength.&lt;/p&gt;
&lt;p&gt;The data are collected, yielding the following table, which now has four columns: sequence, binding strength (in nM), and uncertainty (in nM), and estimation model source code commit hash, and assay protocol version.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Sequence&lt;/th&gt;
&lt;th&gt;Binding Strength (nM)&lt;/th&gt;
&lt;th&gt;Uncertainty (nM)&lt;/th&gt;
&lt;th&gt;Model Hash&lt;/th&gt;
&lt;th&gt;Protocol Version&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;MSYQQKFLDAIAKMTGWVD...&lt;/td&gt;
&lt;td&gt;42.3&lt;/td&gt;
&lt;td&gt;±3.1&lt;/td&gt;
&lt;td&gt;a7bc93e&lt;/td&gt;
&lt;td&gt;v1.2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PWHEVFKKLREAYGLSKTV...&lt;/td&gt;
&lt;td&gt;156.8&lt;/td&gt;
&lt;td&gt;±12.4&lt;/td&gt;
&lt;td&gt;a7bc93e&lt;/td&gt;
&lt;td&gt;v1.2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;MDKKWLNALRQFYEQHPSL...&lt;/td&gt;
&lt;td&gt;28.9&lt;/td&gt;
&lt;td&gt;±2.8&lt;/td&gt;
&lt;td&gt;a7bc93e&lt;/td&gt;
&lt;td&gt;v1.2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;GSRVLWEAIKKFPGDYTNV...&lt;/td&gt;
&lt;td&gt;89.5&lt;/td&gt;
&lt;td&gt;±7.2&lt;/td&gt;
&lt;td&gt;a7bc93e&lt;/td&gt;
&lt;td&gt;v1.2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;PNWQYFKDLAKSTRGVMEK...&lt;/td&gt;
&lt;td&gt;203.7&lt;/td&gt;
&lt;td&gt;±15.6&lt;/td&gt;
&lt;td&gt;a7bc93e&lt;/td&gt;
&lt;td&gt;v1.2.0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;The team decides that this assay is important enough for further investment that they decide to systematically record the uncertainty in controls over time, thus providing a benchmark on how well the experiments are being conducted, while also giving the machine learner an upper bound on their model's performance — &lt;a href="https://pubs.acs.org/doi/full/10.1021/jm300131x"&gt;it logically cannot be any better than experimental noise&lt;/a&gt;, otherwise there is overtraining happening.&lt;/p&gt;
&lt;p&gt;When prioritizing what protein to use as a basis for further engineering, the only thing that the data scientist needs to do is to calculate the probability of superiority of one protein against other proteins and to pick the one that has the highest probability of superiority — and all of these are calculations easily doable using posterior samples from Bayesian estimation. Unlike p-values which only tell us about statistical significance against controls, probability of superiority directly compares the estimated binding strengths between proteins while accounting for measurement uncertainty.&lt;/p&gt;
&lt;p&gt;The data scientist then designs a million new proteins &lt;em&gt;in silico&lt;/em&gt; using ProteinMPNN, RFDiffusion, and institutes some preliminary filters using ESMFold, AlphaFold, and finally custom-designed code. He then trains a probabilistic machine learning model on the first round of non-antibody binders, and then uses it to prioritize new binders, according to their probability of superiority amongst each other. These are then synthesized by the laboratory team and tested again, allowing the data scientist to properly compare the accuracy of his probabilistic machine learning model.&lt;/p&gt;
&lt;p&gt;All in all, through their choices, the team effectively said,&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;"Slow is smooth, and smooth is fast."&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Just like the US Navy Seals. And yes, this team will run fast &lt;em&gt;and&lt;/em&gt; run far!&lt;/p&gt;
</content></entry><entry><title>Let me ship you the Python you need</title><link href="https://ericmjl.github.io/blog/2025/2/17/let-me-ship-you-the-python-you-need/" rel="alternate"/><updated>2025-02-17T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:a7a93222-289b-3cfb-b949-601ed131e803</id><content type="html">&lt;p&gt;Recently, I watched Peter Wang's PyBay talk, &lt;a href="https://www.youtube.com/watch?v=qA7NVwmx3gw"&gt;The Five Demons of Python Packaging That Fuel Our Persistent Nightmare&lt;/a&gt;, and have been reflecting on a quote from Peter's talk:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;...I don't care you'll get a Python we'll get you the Python you need what's the thing you want to do&lt;/p&gt;
&lt;/blockquote&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/qA7NVwmx3gw?si=LJHDuLLQo6kpdU9x&amp;amp;start=2183" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen&gt;&lt;/iframe&gt;&lt;p&gt;&lt;br&gt;
&lt;br&gt;&lt;/p&gt;
&lt;p&gt;It's the difference between first worrying about "what Python do you have", instead of worrying about "what Python do you need".&lt;/p&gt;
&lt;p&gt;&lt;code&gt;uv&lt;/code&gt;, and in particular &lt;code&gt;uvx&lt;/code&gt;, really adopts this paradigm. When I install a tool using &lt;code&gt;uv tool&lt;/code&gt; or run a tool using &lt;code&gt;uvx &amp;lt;toolname&amp;gt;&lt;/code&gt;, I can specify the exact Python interpreter version that I'd like:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;uv&lt;span class="w"&gt; &lt;/span&gt;tool&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;&amp;lt;toolname&amp;gt;&lt;span class="w"&gt; &lt;/span&gt;--python&lt;span class="w"&gt; &lt;/span&gt;&amp;lt;version_number&amp;gt;
uvx&lt;span class="w"&gt; &lt;/span&gt;&amp;lt;toolname&amp;gt;&lt;span class="w"&gt; &lt;/span&gt;--python&lt;span class="w"&gt; &lt;/span&gt;&amp;lt;version_number&amp;gt;&lt;span class="w"&gt; &lt;/span&gt;...tool&lt;span class="w"&gt; &lt;/span&gt;args&lt;span class="w"&gt; &lt;/span&gt;here...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This is incredibly liberating as a user who is looking to install globally-available tools.&lt;/p&gt;
&lt;p&gt;It's come to the point that for one-off work, I now do one of the following.&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# If I&amp;#39;m in the mood for marimo:&lt;/span&gt;
uvx&lt;span class="w"&gt; &lt;/span&gt;marimo&lt;span class="w"&gt; &lt;/span&gt;edit&lt;span class="w"&gt; &lt;/span&gt;--sandbox&lt;span class="w"&gt; &lt;/span&gt;/path/to/notebook.py

&lt;span class="c1"&gt;# If I&amp;#39;m in the mood for jlab:&lt;/span&gt;
uvx&lt;span class="w"&gt; &lt;/span&gt;juv&lt;span class="w"&gt; &lt;/span&gt;run&lt;span class="w"&gt; &lt;/span&gt;/path/to/notebook.ipynb
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;code&gt;uvx&lt;/code&gt; coupled with &lt;code&gt;marimo&lt;/code&gt;/&lt;code&gt;juv&lt;/code&gt; and PEP723 now helps invert the process of getting started. There's no more finagling with first creating a Python environment and then getting to work. Instead, we just get to work and install packages on an as-needed basis, done on-the-fly. &lt;code&gt;marimo&lt;/code&gt; and &lt;code&gt;juv&lt;/code&gt; will &lt;em&gt;ship you the Python that you need to do what you want to do&lt;/em&gt;. That's pretty rad.&lt;/p&gt;
</content></entry><entry><title>Lightening the LlamaBot</title><link href="https://ericmjl.github.io/blog/2025/2/7/lightening-the-llamabot/" rel="alternate"/><updated>2025-02-07T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:619735c8-02d0-36ba-a4b1-5426415f8b11</id><content type="html">&lt;p&gt;In my recent work with LlamaBot, I faced a challenge that many developers might recognize: dependency bloat. What started as a simple bot framework had grown into a monolithic beast that pulled in everything but the kitchen sink. Let me share how I tackled this issue and what I learned along the way.&lt;/p&gt;
&lt;h2 id="the-weight-of-dependencies"&gt;The weight of dependencies&lt;/h2&gt;&lt;p&gt;When I looked at LlamaBot's dependency chain, it was extensive: Panel, Bokeh, LanceDB, ChromaDB, Tantivy, Astor, PyperClip, PromptToolkit - the list went on. Being honest with myself, I had gotten lazy with dependencies. The consequence was that every LlamaBot installation involved downloading seven gigabytes worth of packages. 7 GB!! All to just use either the git hooks (&lt;code&gt;llamabot git hooks&lt;/code&gt;) CLI, or just to use &lt;code&gt;StructuredBot&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This is what the dependencies looked like before, on commit &lt;code&gt;5a1f67d96bbcbd031fdc7f8888aabb0a1f225e10&lt;/code&gt; (which you can also view &lt;a href="https://github.com/ericmjl/llamabot/blob/5a1f67d96bbcbd031fdc7f8888aabb0a1f225e10/pyproject.toml"&gt;here&lt;/a&gt;):&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;dependencies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;openai&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;panel&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;jupyter_bokeh&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bokeh&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loguru&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pyperclip&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;astor&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;python-dotenv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;typer&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gitpython&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pyprojroot&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;frozenlist&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;beautifulsoup4&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rich&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pyzotero&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;case-converter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prompt-toolkit&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sh&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pre-commit&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;beartype&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;litellm&amp;gt;=1.59.1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# this is necessary to guarantee that ollama_chat models are supported for structured outputs&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;python-slugify&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pydantic&amp;gt;=2.0&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pdfminer.six&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rank-bm25&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lancedb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;chromadb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tantivy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;numpy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# https://github.com/ericmjl/llamabot/issues/56&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;python-frontmatter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;diskcache&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;nbformat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;alembic&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;jinja2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;fastapi&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;uvicorn&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sentence-transformers&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tenacity&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;python-multipart&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;httpx&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tqdm&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;docker&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;duckduckgo-search&amp;gt;=7.2.1,&amp;lt;8&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;markdownify&amp;gt;=0.14.1,&amp;lt;0.15&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;# For secure script execution&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="p"&gt;...&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The real kicker came from dependencies like ChromaDB, which pulled in sentence-transformers, which then required PyTorch. On Linux systems, this cascade would drag in all the CUDA packages too. It was overkill for what most users needed.&lt;/p&gt;
&lt;h2 id="breaking-down-the-monolith"&gt;Breaking down the monolith&lt;/h2&gt;&lt;p&gt;I started by mapping out where each dependency was actually used. As an example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;CLI components: pyzotero, Rich, GitPython, nbformat&lt;/li&gt;
&lt;li&gt;RAG functionality: ChromaDB and related packages&lt;/li&gt;
&lt;li&gt;Agent features: Docker, python-docker, markdownify, dotgosearch&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For context, I developed a few prototype CLI applications within LlamaBot to test drive the user experience of some tools I had in mind. Most of them, it turns out, I don't use, and at some future date, I may just deprecate them. Others, however, like the git commit writer, are now fully integrated into my workflow that I can't do without it.&lt;/p&gt;
&lt;p&gt;An interesting realization hit me: the RAG (Retrieval-Augmented Generation) corner of LlamaBot wasn't getting much use. While cosine similarity RAG had its moment in the spotlight, it wasn't as revolutionary as I had thought it would be -- it turns out to still be useful for small context window models, but with many models hitting 128K to 2M token context lengths, and with other advances in RAG (e.g. GraphRAG), it may become less and less useful. But it still lives around just in case it's handy, as there are use cases for small LMs.&lt;/p&gt;
&lt;p&gt;Finally, the Agent features were new and experimental, so it didn't make sense to ship them as part of the core dependencies.&lt;/p&gt;
&lt;h2 id="the-refactoring-strategy"&gt;The refactoring strategy&lt;/h2&gt;&lt;p&gt;For the core of LlamaBot, I decided to focus on &lt;code&gt;SimpleBot&lt;/code&gt; and &lt;code&gt;StructuredBot&lt;/code&gt; - the core components that delivered the most value. After all, I find myself using &lt;code&gt;StructuredBot&lt;/code&gt; the most. The strategy? Move non-essential imports outside of &lt;code&gt;SimpleBot&lt;/code&gt; and &lt;code&gt;StructuredBot&lt;/code&gt; into try-except blocks. Yes, this meant more try-except blocks throughout the codebase, but the trade-off was worth it. Here's an example of what this looks like:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nc"&gt;LanceDBDocStore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;AbstractDocumentStore&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;A document store for LlamaBot that wraps around LanceDB.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="fm"&gt;__init__&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;table_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;storage_path&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Path&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;home&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;.llamabot&amp;quot;&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;lancedb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;lancedb&lt;/span&gt;
            &lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;lancedb.embeddings&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;EmbeddingFunctionRegistry&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;get_registry&lt;/span&gt;
            &lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;lancedb.pydantic&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;LanceModel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Vector&lt;/span&gt;
        &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;ImportError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ImportError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;LanceDB is required for LanceDBDocStore. &amp;quot;&lt;/span&gt;
                &lt;span class="s2"&gt;&amp;quot;Please `pip install llamabot[rag]` to use the LanceDB document store.&amp;quot;&lt;/span&gt;
            &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Cursor absolutely helped here; I was able to eliminate a lot of repetitive error message typing through Cursor's tab-completion.&lt;/p&gt;
&lt;p&gt;In the CLI, I went a step further. Instead of importing optional dependencies at the module level, I moved imports into specific functions. This meant managing imports at runtime rather than import time. Here's an example from the my experimental Zotero chat CLI tool:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@app&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;command&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;chat&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;typer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Argument&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;The thing you want to chat about.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;sync&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;bool&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;typer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Option&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;help&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Whether or not to synchronize the Zotero library.&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;model_name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;default_language_model&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt;
&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Chat with a paper.&lt;/span&gt;

&lt;span class="sd"&gt;    :param query: A paper to search for, whether by title, author, or other metadata.&lt;/span&gt;
&lt;span class="sd"&gt;    :param sync: Whether or not to synchronize the Zotero library.&lt;/span&gt;
&lt;span class="sd"&gt;    :param model_name: The name of the model to use.&lt;/span&gt;
&lt;span class="sd"&gt;    &amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;caseconverter&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;snakecase&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;ImportError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ImportError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;caseconverter is not installed. Please install it with `pip install llamabot[cli]`.&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;

    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;prompt_toolkit&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;prompt&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="ne"&gt;ImportError&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ImportError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
            &lt;span class="s2"&gt;&amp;quot;prompt_toolkit is not installed. Please install it with `pip install llamabot[cli]`.&amp;quot;&lt;/span&gt;
        &lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="new-dependency-specification"&gt;New dependency specification&lt;/h2&gt;&lt;p&gt;With these changes, I could change my &lt;code&gt;pyproject.toml&lt;/code&gt; dependency specification to look like this:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;dependencies&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;openai&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;loguru&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;python-dotenv&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;typer&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pyprojroot&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;beautifulsoup4&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;litellm&amp;gt;=1.59.1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# this is necessary to guarantee that ollama_chat models are supported for structured outputs&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;python-slugify&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pydantic&amp;gt;=2.0&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;numpy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="c1"&gt;# https://github.com/ericmjl/llamabot/issues/56&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;jinja2&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;fastapi&amp;gt;=0.104.0&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;# This version requires Python &amp;gt;=3.8&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;uvicorn&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tenacity&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;python-multipart&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;httpx&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tqdm&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sqlalchemy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pdfminer.six&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="p"&gt;...&lt;/span&gt;

&lt;span class="k"&gt;[project.optional-dependencies]&lt;/span&gt;
&lt;span class="n"&gt;notebooks&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ics&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tzlocal&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;chonkie[all]&amp;gt;=0.2.2,&amp;lt;0.3&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;ui&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;panel&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;jupyter_bokeh&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;bokeh&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;rag&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;lancedb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;chromadb&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;tantivy&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rank-bm25&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;agent&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;docker&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;duckduckgo-search&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;markdownify&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;cli&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pyzotero&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;nbformat&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;python-frontmatter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;rich&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gitpython&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;prompt-toolkit&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;case-converter&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;pyperclip&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;astor&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;all&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;llamabot[notebooks,ui,rag,agent,cli]&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The new dependency specification is much more organized!&lt;/p&gt;
&lt;h2 id="measuring-success"&gt;Measuring success&lt;/h2&gt;&lt;p&gt;The results were dramatic. I created two Dockerfiles, one for the full suite of optional dependencies, and one without. Here's what the two Dockerfiles looked like:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;# filename: llamabot.all.Dockerfile&lt;/span&gt;
&lt;span class="c"&gt;# All optional dependencies&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;ghcr.io/astral-sh/uv:python3.12-bookworm-slim&lt;/span&gt;

&lt;span class="k"&gt;COPY&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;.&lt;span class="w"&gt; &lt;/span&gt;/src

&lt;span class="k"&gt;RUN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;uv&lt;span class="w"&gt; &lt;/span&gt;venv&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;uv&lt;span class="w"&gt; &lt;/span&gt;pip&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;/src&lt;span class="o"&gt;[&lt;/span&gt;all&lt;span class="o"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c"&gt;# filename: llamabot.bare.Dockerfile&lt;/span&gt;
&lt;span class="c"&gt;# Only core dependencies.&lt;/span&gt;
&lt;span class="k"&gt;FROM&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;ghcr.io/astral-sh/uv:python3.12-bookworm-slim&lt;/span&gt;

&lt;span class="k"&gt;COPY&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;.&lt;span class="w"&gt; &lt;/span&gt;/src

&lt;span class="k"&gt;RUN&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;uv&lt;span class="w"&gt; &lt;/span&gt;venv&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="se"&gt;\&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;uv&lt;span class="w"&gt; &lt;/span&gt;pip&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;/src
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;To build the containers, I used the following commands:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docker&lt;span class="w"&gt; &lt;/span&gt;build&lt;span class="w"&gt; &lt;/span&gt;-t&lt;span class="w"&gt; &lt;/span&gt;llamabot.bare&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;llamabot.bare.Dockerfile&lt;span class="w"&gt; &lt;/span&gt;.
docker&lt;span class="w"&gt; &lt;/span&gt;build&lt;span class="w"&gt; &lt;/span&gt;-t&lt;span class="w"&gt; &lt;/span&gt;llamabot.all&lt;span class="w"&gt; &lt;/span&gt;-f&lt;span class="w"&gt; &lt;/span&gt;llamabot.all.Dockerfile&lt;span class="w"&gt; &lt;/span&gt;.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And to view the container sizes:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;❯&lt;span class="w"&gt; &lt;/span&gt;docker&lt;span class="w"&gt; &lt;/span&gt;images&lt;span class="w"&gt; &lt;/span&gt;llamabot.bare&lt;span class="w"&gt; &lt;/span&gt;--format&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;{{.Size}}&amp;quot;&lt;/span&gt;
390MB

❯&lt;span class="w"&gt; &lt;/span&gt;docker&lt;span class="w"&gt; &lt;/span&gt;images&lt;span class="w"&gt; &lt;/span&gt;llamabot.all&lt;span class="w"&gt; &lt;/span&gt;--format&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;{{.Size}}&amp;quot;&lt;/span&gt;
&lt;span class="m"&gt;6&lt;/span&gt;.75GB
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;The numbers don't lie -- we have an approximately 20X smaller container size by removing unnecessary dependencies. I'm all for anything that lets me squeeze out 80% of performance with 20% of effort!&lt;/p&gt;
&lt;h1 id=""&gt;#&lt;/h1&gt;&lt;h2 id="securing-the-changes"&gt;Securing the changes&lt;/h2&gt;&lt;p&gt;I would consider these changes to be somewhat brittle; it relies on a developer's knowledge of the codebase to know that something should be considered core v.s. non-core. To secure these changes for the future, I added safeguards:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;PR tests that verify SimpleBot and StructuredBot functionality&lt;/li&gt;
&lt;li&gt;A bare package test that runs against Gemma 2B&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The PR tests look like this:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# File: pr-tests.yaml&lt;/span&gt;
&lt;span class="nn"&gt;...&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="nt"&gt;bare-package-test&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;runs-on&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;ubuntu-latest&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;strategy&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="nt"&gt;matrix&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;python-version&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="nv"&gt;3.11&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;3.12&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;environment-type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;[&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;pixi&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;#39;uv&amp;#39;&lt;/span&gt;&lt;span class="p p-Indicator"&gt;]&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;steps&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Checkout repository&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;uses&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;actions/checkout@v4&lt;/span&gt;

&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Install pixi&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;uses&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;prefix-dev/setup-pixi@v0.8.1&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;if&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;${{ matrix.environment-type == &amp;#39;pixi&amp;#39; }}&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;with&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="nt"&gt;pixi-version&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;latest&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="nt"&gt;cache&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;true&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="nt"&gt;cache-write&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;${{ github.event_name == &amp;#39;push&amp;#39; &amp;amp;&amp;amp; github.ref_name == &amp;#39;main&amp;#39; }}&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="nt"&gt;environments&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;bare&lt;/span&gt;

&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Install uv&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;curl -LsSf https://astral.sh/uv/install.sh | sh&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;if&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;${{ matrix.environment-type == &amp;#39;uv&amp;#39; }}&lt;/span&gt;

&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Set up Python&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;uv venv llamabot-env --python ${{ matrix.python-version }}&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;if&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;${{ matrix.environment-type == &amp;#39;uv&amp;#39; }}&lt;/span&gt;

&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Install gemma2:2b&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;|&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;sudo apt-get update&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;sudo apt-get install -y curl&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;curl -fsSL https://ollama.com/install.sh | sh&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;sleep 3&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;ollama pull gemma2:2b&lt;/span&gt;

&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Test running llamabot (pixi)&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;if&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;${{ matrix.environment-type == &amp;#39;pixi&amp;#39; }}&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;|&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;pixi run -e bare python scripts/yo.py&lt;/span&gt;

&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;Test running llamabot (uv)&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;if&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;${{ matrix.environment-type == &amp;#39;uv&amp;#39; }}&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;run&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="p p-Indicator"&gt;|&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;source llamabot-env/bin/activate&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;uv pip install .&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="no"&gt;python scripts/yo.py&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And that infamous &lt;code&gt;yo.py&lt;/code&gt;:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;A simple script to test the llamabot package.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;llamabot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;lmb&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;pydantic&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BaseModel&lt;/span&gt;

&lt;span class="n"&gt;bot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SimpleBot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;yo&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ollama_chat/gemma2:2b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;sup?&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;


&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nc"&gt;Person&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseModel&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;A person.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;

    &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;
    &lt;span class="n"&gt;age&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;


&lt;span class="n"&gt;sbot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StructuredBot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;yo&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ollama_chat/gemma2:2b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;pydantic_model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;Person&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sbot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;what is your name and age?&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Essentially, I wanted to guarantee that this script's behaviour worked whenever we installed the package without optional dependencies. That's what motivated this test.&lt;/p&gt;
&lt;h2 id="looking-forward"&gt;Looking forward&lt;/h2&gt;&lt;p&gt;Even before this refactor, I've found myself gravitating more toward &lt;code&gt;StructuredBot&lt;/code&gt; than &lt;code&gt;SimpleBot&lt;/code&gt;. Its abstractions build naturally on &lt;code&gt;SimpleBot&lt;/code&gt;'s patterns, making it more versatile for my needs.&lt;/p&gt;
&lt;p&gt;The refactoring exercise taught me valuable lessons about dependency management and the importance of regular codebase maintenance. Sometimes, less really is more - especially when "more" means downloading half the Python ecosystem.&lt;/p&gt;
&lt;p&gt;Now, I have a leaner, more focused tool that does exactly what it needs to do, without the bloat. That's a win in my book!&lt;/p&gt;
</content></entry><entry><title>PyData Boston/Cambridge Talk @ Moderna: What makes an agent?</title><link href="https://ericmjl.github.io/blog/2025/1/31/pydata-bostoncambridge-talk-moderna-what-makes-an-agent/" rel="alternate"/><updated>2025-01-31T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:47bf3f8b-761f-3e7d-bb55-8d0d6f395f31</id><content type="html">&lt;p&gt;&lt;em&gt;This is a written and edited version of a talk I gave at PyData Boston/Cambridge on 29 Jan 2025, hosted at Moderna's new office buildings for the first time ever.&lt;/em&gt;  This talk will likely leave us with more questions than answers about "what makes an agent." But I hope it also leaves us with more focused questions and frameworks for thinking about "what makes an agent".&lt;/p&gt;
&lt;p&gt;It is based loosely on &lt;a href="https://ericmjl.github.io/blog/2025/1/4/what-makes-an-agent/"&gt;my blog post&lt;/a&gt;. The original notebook that I presented is available &lt;a href="https://github.com/ericmjl/llamabot/blob/main/talks/pydata-boston-2025/what-makes-an-agent.py"&gt;within the LlamaBot repo&lt;/a&gt;.&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;llamabot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;lmb&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="let-s-start-with-llm-bots"&gt;Let's start with LLM bots&lt;/h2&gt;&lt;p&gt;This is a helpful framing to anchor our discussion of agents.&lt;/p&gt;
&lt;p&gt;LlamaBot is a Python package that I made to pedagogically explore the landscape of LLMs as the field evolves. Intentionally designed to lag 1-2 steps behind innovations and incorporate only what turns out to be robust. LlamaBot implements a variety of LLM bots, the simplest being the &lt;code&gt;SimpleBot&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;SimpleBot&lt;/code&gt; makes clear the basic anatomy of an LLM API call:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;1️⃣ A system prompt:&lt;/strong&gt; maintained over all calls.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;2️⃣ A model name:&lt;/strong&gt; specifying which model to chat with.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;3️⃣ A user's prompt:&lt;/strong&gt; your input into the LLM's internals.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;4️⃣ The response:&lt;/strong&gt; returned by autoregressive next-token prediction.&lt;/p&gt;
&lt;p&gt;In &lt;code&gt;SimpleBot&lt;/code&gt;, that looks like this:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;flight_bot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SimpleBot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;system_prompt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;You are a helpful travel assistant.&lt;/span&gt;
&lt;span class="s2"&gt;    You provide information about flights between cities.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;model_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ollama_chat/llama3.2:3b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Simple interaction&lt;/span&gt;
&lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;flight_bot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Find me the prices for flights from San Francisco to Tokyo.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And the response looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;I'd be happy to help you find flight prices from San Francisco (SFO) to Tokyo.

Please note that flight prices can vary depending on several factors such as the time of year, demand, airline, and availability. I'll provide you with some general price ranges for flights from SFO to Tokyo, but keep in mind that these may not be exact prices for your specific travel dates.

Here are some approximate price ranges for flights from San Francisco (SFO) to Tokyo:

**Economy Class:**

* Low-cost carriers like ANA, Japan Airlines, and All Nippon Airways: $500-$800 USD
* Mid-range airlines like United Airlines, Delta Air Lines, and American Airlines: $700-$1,200 USD
* Premium economy or business class: $1,200-$2,500 USD

**Premium Economy Class:**

* Low-cost carriers: $900-$1,400 USD
* Mid-range airlines: $1,200-$1,800 USD
* Premium economy or business class: $1,800-$3,000 USD

**Business Class:**

* Low-cost carriers: Not available
* Mid-range airlines: $2,500-$4,000 USD
* Premium business class: $4,000-$6,000 USD

To get a more accurate estimate of flight prices from SFO to Tokyo, I recommend checking with the following airlines or online travel agencies:

1. Expedia
2. Kayak
3. Skyscanner
4. Google Flights
5. ANA (All Nippon Airways)
6. Japan Airlines

Please enter your specific travel dates and preferred airline to get a more accurate quote.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Together with the audience, we evaluate that response together. &lt;em&gt;Is this an agent?&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Audience commentary:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Not an agent: didn't take action apart from introspection of weights.&lt;/li&gt;
&lt;li&gt;Not an agent: no external APIs called.&lt;/li&gt;
&lt;li&gt;Yes: agent doesn't mean it has to have a tool, anything that gives usable information is an agent.&lt;/li&gt;
&lt;li&gt;Yes (to an extent): Follows up with a question/prompt back to the human.&lt;/li&gt;
&lt;li&gt;No: agent should make decisions. This LLM call did not make any decisions. Decision-making outsourced to user.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;My thoughts are as follows: &lt;code&gt;SimpleBot&lt;/code&gt;s are heavily reliant on what's in the training data; they are just calling the LLM directly, with no memory of historical chat information. Given the range of regular human experiences in North America, I would venture to guess that it is rare for us to go outside of any base LLM's training data, unless we hit very specialized and niche topics not covered on the internet. (Try thinking of some!) It's hard to argue that this would have the properties of agency, which we would associate with an agent.&lt;/p&gt;
&lt;h2 id="agents-that-give-us-structure"&gt;Agents that give us structure?&lt;/h2&gt;&lt;p&gt;Let's try a different view of agents. What if we said, "Agents give you information structured the way you wanted it?" Let's start with that as a working definition.&lt;/p&gt;
&lt;p&gt;LlamaBot implements structured generation. To use it, you need to provide a Pydantic model, effectively a form that you want the bot to fill, and pair it with a &lt;code&gt;StructuredBot&lt;/code&gt;. &lt;code&gt;StructuredBot&lt;/code&gt;s have the same basic requirements as &lt;code&gt;SimpleBot&lt;/code&gt;s, which are a system prompt and model name, but also need to have a Pydantic model passed in. When called, it will return an instance of that Pydantic model.&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;pydantic&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;BaseModel&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;

&lt;span class="k"&gt;class&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nc"&gt;FlightInfo&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;BaseModel&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;origin&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;
    &lt;span class="n"&gt;destination&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;
    &lt;span class="n"&gt;duration&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Duration in hours&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Price in USD&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;recommended_arrival&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;Field&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="o"&gt;...&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Recommended time to arrive at airport before flight, in hours&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;flight_bot_structured&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;StructuredBot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;system_prompt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;You are a helpful travel assistant that provides&lt;/span&gt;
&lt;span class="s2"&gt;    structured information about flights.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;model_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ollama_chat/llama3.2:3b&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;pydantic_model&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;FlightInfo&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Now we get structured data&lt;/span&gt;
&lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;flight_bot_structured&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;Find me the prices for flights from San Francisco to Tokyo.&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;##########&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;The price of the flight is $&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;. We recommend that you arrive at the airport &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;result&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;recommended_arrival&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt; hours before the flight.&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Its response looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{ "origin": "San Francisco", "destination": "Tokyo", "duration": 11, "price": 550, "recommended_arrival": 12 }
##########
The price of the flight is $550.0. We recommend that you arrive at the airport 12.0 hours before the flight.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="audience-discussion-is-this-an-agent"&gt;Audience discussion: Is this an agent?&lt;/h3&gt;&lt;p&gt;I specifically asked, where does this fall short of an agent's definition?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;No: still like RAG. Doesn't read the situation, is this a good time to buy? Still a simple answer.&lt;/li&gt;
&lt;li&gt;No: it would independently decide what to look at, where to look.&lt;/li&gt;
&lt;li&gt;Yes: Based on the working definition, yes, and the nature of decision-making means the LLM is doing rapid-fire retrieval from memorized data. Recommended arrival time &lt;strong&gt;is&lt;/strong&gt; an added recommendation that was not part of the original query intent.&lt;/li&gt;
&lt;li&gt;No: Is the definition even correct? Why isn't a SQL query an agent, under that definition?&lt;/li&gt;
&lt;li&gt;No: within this context, the &lt;strong&gt;latent&lt;/strong&gt; query intent might be to buy tix, but this bot doesn't help me do it.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In summary, this bot still misses the crucial elements of (a) autonomy/agency, and (b) intent guessing.&lt;/p&gt;
&lt;h3 id="structured-generation-gets-us-midway"&gt;Structured generation gets us midway&lt;/h3&gt;&lt;p&gt;It affords us more &lt;strong&gt;control&lt;/strong&gt; over what we are asking an LLM to generate. The response is constrained.&lt;/p&gt;
&lt;p&gt;But it seems like it may still lack a crucial element of an "agent". I'm glad the audience shred up this definition! 🤗&lt;/p&gt;
&lt;h2 id="what-if-we-said-an-agent-needed-to-interact-externally"&gt;What if we said, "An agent needed to interact externally"?&lt;/h2&gt;&lt;p&gt;Here is my attempt, within LlamaBot, to try to nail down what an agent actually is. This flight agent-like bot helps me plan out trips, using one tool, the &lt;code&gt;search_internet&lt;/code&gt; tool.&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;llamabot.bot.agentbot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;search_internet&lt;/span&gt;

&lt;span class="n"&gt;travel_agent&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AgentBot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;system_prompt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&amp;quot;&amp;quot;You are a travel planning assistant. Help users plan their trips&lt;/span&gt;
&lt;span class="s2"&gt;    by searching flights, hotels, and checking weather.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="n"&gt;functions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;search_internet&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;model_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gpt-4o&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Now we can handle complex queries&lt;/span&gt;
&lt;span class="n"&gt;travel_response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;travel_agent&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;What&amp;#39;s the flight time from San Francisco to Tokyo?&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s2"&gt;##########&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;travel_response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Its response looks like this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{"tool_name":"search_internet","tool_args":[{"name":"search_term","value":"flight time from San Francisco to Tokyo"}],"use_cached_results":[]}
search_internet({'search_term': 'flight time from San Francisco to Tokyo'}) -&amp;gt; 3394908c, {'https://www.flightsfrom.com/SFO-HND': '�(�\x00 ~������\x17\x1e�\x02�.IwѕZ\x0c�\x1cI\\S\x1c\x1d9��d���7aZ��Dٚݝ\x14�R~ ...(truncated)...'}
{"tool_name":"agent_finish","tool_args":[{"name":"message","value":"The flight time from San Francisco to Tokyo is approximately 10 hours and 48 minutes to 11 hours and 10 minutes for a non-stop flight. This duration can vary slightly depending on the specific flight path and weather conditions."}],"use_cached_results":[]}
agent_finish({'message': 'The flight time from San Francisco to Tokyo is approximately 10 hours and 48 minutes to 11 hours and 10 minutes for a non-stop flight. This duration can vary slightly depending on the specific flight path and weather conditions.'}) -&amp;gt; a106bcaa, The flight time from San Francisco to Tokyo is approximately 10 hours and 48 minutes to 11 hours and 10 minutes for a non-stop flight. This duration can vary slightly depending on the specific flight path and weather conditions.

##########
The flight time from San Francisco to Tokyo is approximately 10 hours and 48 minutes to 11 hours and 10 minutes for a non-stop flight. This duration can vary slightly depending on the specific flight path and weather conditions.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Looks like this could be an agent, but let's see what the audience said.&lt;/p&gt;
&lt;h3 id="audience-discussion-is-this-really-an-agent"&gt;Audience discussion: is this really an agent?&lt;/h3&gt;&lt;p&gt;I asked the audience to disregard the fact that I called this an &lt;code&gt;AgentBot&lt;/code&gt;. Just because I said so doesn't make it so!&lt;/p&gt;
&lt;p&gt;Where are the holes in this definition of an agent?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Getting closer: agent will do research, in this example, it is the use of a search tool, and processing the text to give a response; the task could have been clearer for end user.&lt;/li&gt;
&lt;li&gt;Yes: it has introduced a new error mode. A new way to be wrong, need someone to blame for this.&lt;/li&gt;
&lt;li&gt;New component here: this bot has agency, decides "do I call this function or not".&lt;/li&gt;
&lt;li&gt;Principal-Agent problem: agent does something that may not be what the Principal wanted it to do.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The audience also had more questions raised:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Q: Is part of the definition of an agent the fact that it is going to interact with a human?&lt;/li&gt;
&lt;li&gt;Q: In this implementation of search, is the LLM always going to do the function call?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It's at this point that I knew that we had more focused questions from the audience, and that my first goal was accomplished: to get the audience to think more critically about the definition of an agent.&lt;/p&gt;
&lt;hr&gt;
&lt;h2 id="design-patterns-should-this-be-an-agent"&gt;Design patterns: should this be an agent?&lt;/h2&gt;&lt;p&gt;We discussed what an agent is. Now, let's assume that we know what an agent is. (This is an assumption!) If so, how should we design agents, and even then, should it be an agent?&lt;/p&gt;
&lt;p&gt;I presented to the audience a minimally complex example: a restaurant bill calculator. It's got a few key characteristics:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;l calculation is computable (and hence easily verifiable).&lt;/li&gt;
&lt;li&gt;There is sufficient variation in the kinds of questions we can ask.&lt;/li&gt;
&lt;li&gt;We can easily implement multiple designs.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="agentbot-implementation"&gt;AgentBot implementation&lt;/h3&gt;&lt;p&gt;Let's start with an implementation based on AgentBot.&lt;/p&gt;
&lt;p&gt;We have two tools, which are nothing more than Python functions that are decorated with a &lt;code&gt;@tool&lt;/code&gt;  decorator. They are, namely, &lt;code&gt;calculate_total_with_tip&lt;/code&gt; and &lt;code&gt;split_bill&lt;/code&gt;.&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Define the tools&lt;/span&gt;
&lt;span class="nd"&gt;@lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tool&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;calculate_total_with_tip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bill_amount&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tip_rate&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;tip_rate&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;tip_rate&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;ValueError&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Tip rate must be between 0 and 1.0&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;bill_amount&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tip_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nd"&gt;@lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tool&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;split_bill&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_amount&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_people&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;total_amount&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_people&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, we create the AgentBot:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Create the bot&lt;/span&gt;
&lt;span class="n"&gt;bot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AgentBot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;system_prompt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;system&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;You are my assistant with respect to restaurant bills.&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;functions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;calculate_total_with_tip&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;split_bill&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;model_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gpt-4o&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, let's try a few calculations, the first one being just calculating the total with tip:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Calculate total with tip&lt;/span&gt;
&lt;span class="n"&gt;calculate_total_only_prompt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;My dinner was $2300 without tips. Calculate my total with an 18% tip.&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;resp&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;calculate_total_only_prompt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We get this output:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{"tool_name":"calculate_total_with_tip","tool_args":[{"name":"bill_amount","value":2300},{"name":"tip_rate","value":0.18}],"use_cached_results":[]}
calculate_total_with_tip({'bill_amount': 2300, 'tip_rate': 0.18}) -&amp;gt; bdc66afd, 2714.0
{"tool_name":"agent_finish","tool_args":[{"name":"message","value":"The total amount for your dinner, including an 18% tip, is $2714.00."}],"use_cached_results":[]}
agent_finish({'message': 'The total amount for your dinner, including an 18% tip, is $2714.00.'}) -&amp;gt; 73ec736f, The total amount for your dinner, including an 18% tip, is $2714.00.
The total amount for your dinner, including an 18% tip, is $2714.00.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This looks right! And if we try another:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Split the bill&lt;/span&gt;

&lt;span class="n"&gt;split_bill_only_prompt&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;My dinner was $2300 in total, I added an 18&lt;/span&gt;&lt;span class="si"&gt;% g&lt;/span&gt;&lt;span class="s2"&gt;ratuity, split the bill between 20 people.&amp;quot;&lt;/span&gt;

&lt;span class="n"&gt;resp2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;bot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;split_bill_only_prompt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We get this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{"tool_name":"calculate_total_with_tip","tool_args":[{"name":"bill_amount","value":2300},{"name":"tip_rate","value":0.18}],"use_cached_results":[]}
calculate_total_with_tip({'bill_amount': 2300, 'tip_rate': 0.18}) -&amp;gt; bdc66afd, 2714.0
{"tool_name":"split_bill","tool_args":[{"name":"total_amount","value":null},{"name":"num_people","value":20}],"use_cached_results":[{"arg_name":"total_amount","hash_key":"bdc66afd"}]}
split_bill({'num_people': 20, 'total_amount': 2714.0}) -&amp;gt; 2a75a8f8, 135.7
{"tool_name":"agent_finish","tool_args":[{"name":"message","value":"The total bill including an 18% gratuity is $2714. When split between 20 people, each person needs to pay $135.70."}],"use_cached_results":[]}
agent_finish({'message': 'The total bill including an 18% gratuity is $2714. When split between 20 people, each person needs to pay $135.70.'}) -&amp;gt; f92ef2d0, The total bill including an 18% gratuity is $2714. When split between 20 people, each person needs to pay $135.70.
The total bill including an 18% gratuity is $2714. When split between 20 people, each person needs to pay $135.70.
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id="couldn-t-it-have-been-a-python-function"&gt;Couldn't it have been a Python function?&lt;/h3&gt;&lt;p&gt;Should this have been an agent? After all, we very well could have done this instead:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;calculate_bill&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;total_amount&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tip_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_people&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_amount&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tip_percentage&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_people&lt;/span&gt;

&lt;span class="n"&gt;calculate_bill&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2300&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;18&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# gives us 135.7&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="is-there-a-way-to-make-that-python-function-more-flexible"&gt;Is there a way to make that Python function more flexible?&lt;/h3&gt;&lt;p&gt;But this is a very restrictive implementation. What if we didn't have any division to do?&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;calculate_bill_v2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;total_amount&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tip_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_people&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;tip_percentage&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;total_amount&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;total_amount&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tip_percentage&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="mi"&gt;100&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;num_people&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;total_amount&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_people&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;total_amount&lt;/span&gt;

&lt;span class="n"&gt;calculate_bill_v2&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;2714&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;20&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# also gives us 135.7&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Some commentary from me: it's the same functionality, but with more flexibility. The scope of inputs is more variable, in the form of almost anything you want... but not with natural language. And this point, by the way, seems to distinguish between an LLM agent and a Python program.&lt;/p&gt;
&lt;h3 id="but-couldn-t-reasoning-models-do-this"&gt;But couldn't reasoning models do this?&lt;/h3&gt;&lt;p&gt;What if we did this instead?&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;r1_bot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SimpleBot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;You are a smart bill calculator.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;model_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;ollama_chat/deepseek-r1:latest&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;r1_bot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="s2"&gt;&amp;quot;My dinner was $2300 in total, with 18% tip. Split the bill between 20 people. Respond with only the number.&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;We get this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;&amp;lt;think&amp;gt;
First, I need to calculate the total amount of the dinner including an 18% tip.

The original cost is \$2300. To find the tip, I'll multiply this by 18%.

\$2300 × 0.18 = \$414

Adding the tip to the original cost gives:

\$2300 + \$414 = \$2714

Next, I need to split this total amount equally among 20 people.

So, I'll divide \$2714 by 20.

\$2714 ÷ 20 = \$135.70

Therefore, each person should contribute \$135.70.
&amp;lt;/think&amp;gt;

Sure! Let's break down the calculation step by step:

**Step 1: Calculate the Total with Tip**

- **Original Cost:** \$2,300
- **Tip Percentage:** 18%

\[
\text{Tip} = 2,\!300 \times 0.18 = 414
\]

\[
\text{Total with Tip} = 2,\!300 + 414 = 2,\!714
\]
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This didn't involve any tools that the agent had to interact with, so it seems like problems that only involve reasoning don't necessitate building an agent.&lt;/p&gt;
&lt;h3 id="what-if-we-didn-t-have-to-provide-any-tools-instead"&gt;What if we didn't have to provide any tools instead?&lt;/h3&gt;&lt;p&gt;Or what if, we asked the agent to write and execute its own code? (This follows HuggingFace's definition of an agent.)&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;llamabot.bot.agentbot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;write_and_execute_script&lt;/span&gt;

&lt;span class="c1"&gt;# Create the bot&lt;/span&gt;
&lt;span class="n"&gt;autonomous_code_bot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AgentBot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;system_prompt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;system&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;You are my assistant with respect to restaurant bills.&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;functions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;write_and_execute_script&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
    &lt;span class="n"&gt;model_name&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;gpt-4o&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Split the bill&lt;/span&gt;
&lt;span class="n"&gt;autonomous_code_bot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;split_bill_only_prompt&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;And with this implementation, we get:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{"tool_name":"write_and_execute_script","tool_args":[{"name":"code","value":"# Calculate the total bill including gratuity and then split it among 20 people\n\ntotal_bill = 2300\n\ngratitude_percentage = 18\n\n# Calculate the total amount including gratuity\ntotal_with_gratuity = total_bill + (total_bill * gratitude_percentage / 100)\n\n# Split the total amount among 20 people\namount_per_person = total_with_gratuity / 20\n\nprint(amount_per_person)"}],"use_cached_results":[]}---SCRIPT EXECUTION RESULT---
135.7
---SCRIPT EXECUTION RESULT---

write_and_execute_script({'code': '# Calculate the total bill including gratuity and then split it among 20 people\n\ntotal_bill = 2300\n\ngratitude_percentage = 18\n\n# Calculate the total amount including gratuity\ntotal_with_gratuity = total_bill + (total_bill * gratitude_percentage / 100)\n\n# Split the total amount among 20 people\namount_per_person = total_with_gratuity / 20\n\nprint(amount_per_person)'}) -&amp;gt; 18b90e4d, 135.7
{"tool_name":"agent_finish","tool_args":[{"name":"message","value":"The total dinner bill was $2300, and with an 18% gratuity added, the total becomes $2714. When this total is split between 20 people, each person needs to pay $135.70."}],"use_cached_results":[]}
agent_finish({'message': 'The total dinner bill was $2300, and with an 18% gratuity added, the total becomes $2714. When this total is split between 20 people, each person needs to pay $135.70.'}) -&amp;gt; 0cfe8442, The total dinner bill was $2300, and with an 18% gratuity added, the total becomes $2714. When this total is split between 20 people, each person needs to pay $135.70.
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;In all cases above, our bots are able to answer the question. But they all approached the question in different ways. It seems like the problems in which we would want an agent aren't the kinds that can be solved by SimpleBots (of various models) alone.&lt;/p&gt;
&lt;h3 id="dissection"&gt;Dissection&lt;/h3&gt;&lt;p&gt;If we think carefully about the distinction between a Python function, its stochastic variant, and an LLM agent, we might make the following observations:&lt;/p&gt;
&lt;h4 id="functions"&gt;Functions&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;Functions are written to accomplish a goal.&lt;/li&gt;
&lt;li&gt;Functions have an input signature, a body, and a return.&lt;/li&gt;
&lt;li&gt;Function inputs are constrained to the types that are accepted; they cannot be natural language.&lt;/li&gt;
&lt;li&gt;Function program flow is deterministic.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id="stochastic-functions"&gt;Stochastic Functions&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;Stochastic functions have non-deterministic flow control, resulting in a distribution of possible outputs.&lt;/li&gt;
&lt;/ol&gt;
&lt;h4 id="llm-agents"&gt;LLM Agents&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;Are non-deterministic in flow control.&lt;/li&gt;
&lt;li&gt;Rely on structured outputs internally.&lt;/li&gt;
&lt;li&gt;Allow for natural language inputs.&lt;/li&gt;
&lt;li&gt;Nonetheless accomplish a goal.&lt;/li&gt;
&lt;/ol&gt;
&lt;h3 id="what-should-be-an-agent"&gt;What should be an agent?&lt;/h3&gt;&lt;p&gt;Anthropic has &lt;a href="https://www.anthropic.com/research/building-effective-agents"&gt;guidance&lt;/a&gt;.&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;When building applications with LLMs, we recommend finding the simplest solution possible, and only increasing complexity when needed. &lt;strong&gt;This might mean not building agentic systems at all.&lt;/strong&gt; Agentic systems often trade latency and cost for better task performance, and you should consider when this tradeoff makes sense.&lt;/p&gt;
&lt;p&gt;When more complexity is warranted, workflows offer predictability and consistency for well-defined tasks, whereas agents are the better option when flexibility and model-driven decision-making are needed at scale. For many applications, however, optimizing single LLM calls with retrieval and in-context examples is usually enough.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, can you build it with a regular Python program first? If so, maybe just start there.&lt;/p&gt;
&lt;p&gt;And from my own blog:&lt;/p&gt;
&lt;blockquote&gt;&lt;h2 id="a-roadmap-for-designing-agent-applications"&gt;A roadmap for designing agent applications&lt;/h2&gt;&lt;p&gt;I've found that the most effective way to design agent applications is to progressively relax constraints on inputs and execution order.&lt;/p&gt;
&lt;h3 id="start-with-a-deterministic-program"&gt;Start with a deterministic program&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Design your application as you would with regular API calls&lt;/li&gt;
&lt;li&gt;Define clear input/output specifications&lt;/li&gt;
&lt;li&gt;Implement core functionality with standard programming patterns&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="relax-input-constraints"&gt;Relax input constraints&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Accept natural language input and convert it to structured parameters for function calls&lt;/li&gt;
&lt;li&gt;Enable autonomous function calling based on natural language understanding&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="relax-execution-order-constraints"&gt;Relax execution order constraints&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;Only necessary when natural language inputs are varied enough to require different execution paths&lt;/li&gt;
&lt;li&gt;Allow flexible ordering of operations when needed&lt;/li&gt;
&lt;li&gt;Enable dynamic selection of which functions to call&lt;/li&gt;
&lt;li&gt;Maintain boundaries around available actions while allowing flexibility in their use&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This progressive relaxation approach helps us transition from traditional deterministic programming to an agent-driven paradigm where execution order is non-deterministic and inputs are natural language.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="more-perspectives"&gt;More perspectives&lt;/h2&gt;&lt;p&gt;Finally, I concluded with a few more perspectives for others to consider.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://www.linkedin.com/posts/philipp-schmid-a6a2bb196_function-calling-is-not-solved-yet-a-new-activity-7288821311613591552-i5dH"&gt;Function calling is not solved&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://drive.google.com/file/d/1oEjiRCTbd54aSdB_eEe3UShxLBWK9xkt/view"&gt;Google's definition of agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://x.com/AymericRoucher/status/1874116324898598934"&gt;HuggingFace's definition of agents&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2025/1/4/what-makes-an-agent/"&gt;My previous blog post on What makes an Agent&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;In this talk, we went through two main questions:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;What makes an agent?&lt;/li&gt;
&lt;li&gt;What should be an agent?&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;I am pretty confident that I left the audience with more questions than answers. But I also am confident that those questions were more focused and specific than they were before the talk. I hope that's the same for you, the reader of this blog!&lt;/p&gt;
</content></entry><entry><title>Why data from preclinical biotech lab experiments make machine learning challenging</title><link href="https://ericmjl.github.io/blog/2025/1/19/why-data-from-preclinical-biotech-lab-experiments-make-machine-learning-challenging/" rel="alternate"/><updated>2025-01-19T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:a2ce019f-507f-312c-8f29-63972b78f8fc</id><content type="html">&lt;p&gt;Biotech teams keep running into the same wall. It starts innocently enough with a promising discovery: a public dataset that appears to perfectly align with their research objectives, to identify molecules with a particular function. The excitement builds quickly. A data scientist, who is matrixed into the team, envisions combining it with internal data (about to be generated) to build better models and accelerate their research. But what begins with enthusiasm inevitably leads to hard lessons about the unique challenges of preclinical biotech data. Let's explore this prototypical story in more detail.&lt;/p&gt;
&lt;h2 id="the-initial-promise-finding-public-data"&gt;The initial promise: finding public data&lt;/h2&gt;&lt;p&gt;Midway through the project, an apparently perfect dataset appears. It's cleanly organized with binary classifications (active/inactive) and enough examples to train a solid classifier -- 1000+ molecules are included, a luxury number in preclinical laboratory experiments.&lt;/p&gt;
&lt;p&gt;Initial models look promising. Validation metrics are strong. Cross-validation results encourage further development. Everyone aligns behind the opportunity, and both project and department leadership provide enthusiastic support! The laboratory team is ready to generate data that, in all likelihood, could be combined with the existing public data. Together with the data scientist, the team designs the experiment and gets ready to execute on it.&lt;/p&gt;
&lt;p&gt;However, as the team dives deeper into the experiment design, complexities emerge that challenge their initial optimism.&lt;/p&gt;
&lt;h2 id="when-reality-crashes-in-fundamental-challenges"&gt;When reality crashes in: fundamental challenges&lt;/h2&gt;&lt;p&gt;When planning out the measurement experiment, fundamental challenges with data compatibility emerge. These aren't just technical hurdles - they're systemic issues that any data scientist supporting preclinical work needs to overcome.&lt;/p&gt;
&lt;h3 id="challenge-1-reconciling-binary-and-continuous-measurements"&gt;Challenge 1: reconciling binary and continuous measurements&lt;/h3&gt;&lt;p&gt;The first roadblock appears immediately. The lab team can generate continuous-valued activity measurements. But these aren't binary, as they were in the public data! How do you reconcile binary classifications with continuous measurements? What seems like a straightforward thresholding problem quickly spirals into deeper questions.&lt;/p&gt;
&lt;p&gt;What threshold did the original researchers use? Their methods mention a cutoff, but it turns out their dataset is an amalgamation of many other datasets, each with their own binary classification values. Different labs define "active" behavior differently, and in this case, these definitions were poorly documented; even when they are documented, the experimental conditions behind those definitions may differ dramatically from the new team's setup. We are now in a logical deadlock.&lt;/p&gt;
&lt;h3 id="challenge-2-hunting-invisible-variables"&gt;Challenge 2: hunting invisible variables&lt;/h3&gt;&lt;p&gt;Next, certain experimental factors were not recorded in the public dataset, making them difficult to compare against the internal data. These include temperature, pH levels, incubation times, and buffer composition for dissolving the molecule -- potentially important process information that was not captured.&lt;/p&gt;
&lt;p&gt;Do these matter? They may play a role, but without the data recorded, it's impossible to know. Much of this information remains forever lost, buried in lab notebooks that will never see the light of day. (This, in case you weren't aware, represents a fundamental problem in biotech data collection!)&lt;/p&gt;
&lt;h3 id="challenge-3-navigating-the-domain-shift-maze"&gt;Challenge 3: navigating the domain shift maze&lt;/h3&gt;&lt;p&gt;The most insidious challenge lurks in domain shifts between experiments. The team is working with HEK293 cells. The public dataset, however, used HEK293T cells. The similar names suggest compatibility, but reality proves far more complex: dramatic differences can exist between these cell lines!&lt;/p&gt;
&lt;p&gt;&lt;em&gt;For those who didn't know, HEK293 and HEK293T cells are both derived from human embryonic kidney cells, but HEK293T cells contain an additional gene expressing the SV40 Large T antigen. This key difference makes HEK293T cells capable of episomal replication of plasmids containing the SV40 origin, resulting in higher transfection efficiency (of the external DNA added, it'll take up more) and protein expression levels (overall, more protein produced) compared to standard HEK293 cells.&lt;/em&gt;&lt;/p&gt;
&lt;p&gt;Bridging this domain shift demands careful validation through "bridging experiments." Only then can teams confidently build hybrid models combining both datasets. But practical constraints often fight against these technical requirements. Funding for the project may be running out. Or there may be an important deadline that leads the team to conclude that it isn't high enough of a priority to do so.&lt;/p&gt;
&lt;h3 id="challenge-4-wrestling-with-biological-complexity"&gt;Challenge 4: wrestling with biological complexity&lt;/h3&gt;&lt;p&gt;All these challenges stem from a fundamental truth: biological systems are inherently complex. Teams aren't working with static, controlled systems. They're dealing with living organisms that change over time. These systems interact in non-linear ways. Their behavior often defies expectations.&lt;/p&gt;
&lt;p&gt;Teams are forced to make critical assumptions, like stability of their cell lines over time. A card-carrying biologist would laugh at this assumption -- and yet it is the best we have! And what's the alternative? Comprehensive genetic sequencing at every passage? The cost and time requirements make this impossible for most research programs.&lt;/p&gt;
&lt;h2 id="switching-from-a-machine-learning-perspective-to-a-decision-support-perspective"&gt;Switching from a machine learning perspective to a decision support perspective&lt;/h2&gt;&lt;p&gt;And so the data scientist is left scratching their head. Clearly, it's not the right idea to threshold and binarize the lab's continuous measurements and then smash them together as new rows in the data table from the public dataset. And ignoring the public data feels deeply wrong too, as if we're passing up a huge opportunity to gain leverage from publicly available information. What are we supposed to do under these circumstances?&lt;/p&gt;
&lt;p&gt;In reflecting on this prototypical story, which I've seen everywhere I've worked, it's pointed me towards a more nuanced approach to machine learning in biotech. Force-fitting disparate datasets into a single model is a dead end. My description of the options reveals how the data scientist is thinking about the issue as a machine learning problem rather than as a decision support problem. I suspect other digital teams, in their fervor to support machine learning or due to their prior bias as data scientists, may end up thinking about the problem the same way, with consequences for the final design of their data platform.&lt;/p&gt;
&lt;p&gt;If we take the decision support perspective, we quickly realize that it's illogical to try building a single model that combines the two datasets. Instead, we should build two separate models and combine their results in a decision fusion step guided by human judgment about which model is more reliable. There are multiple ways to approach this fusion: we could use weighted ranking between the two models, calculate a weighted average of their predictions, or employ Bayesian model averaging, to combine the two models' predictions into a single prediction. It might look like this:&lt;/p&gt;
&lt;pre class="mermaid"&gt;
flowchart LR
    subgraph Data1[Public Dataset]
        D1[Binary Classifications]
        E1[Experimental Context 1]
    end

    subgraph Data2[Internal Dataset]
        D2[Continuous Measurements]
        E2[Experimental Context 2]
        B[Bridging Experiments]
    end

    subgraph Models[Model Predictions]
        D1 --&gt; M1[Model 1 Predictions]
        E1 --&gt; M1
        D2 --&gt; M2[Model 2 Predictions]
        E2 --&gt; M2
        B --&gt; M2
    end

    subgraph WeightingLayer[Model Weighting Layer]
        H1[Human Expertise]
        note1[Understanding of:&lt;br/&gt;- Experimental conditions&lt;br/&gt;- Model limitations&lt;br/&gt;- Data quality&lt;br/&gt;- Biological context]
        H1 --- note1

        H1 --&gt; W1[Weight Assignment]
        M1 --&gt; W1
        M2 --&gt; W1
    end

    W1 --&gt; F[Final Molecule Prioritization]

    note2[Bridging experiments help&lt;br/&gt;determine appropriate&lt;br/&gt;weighting between models]
    note2 -.-&gt; WeightingLayer
&lt;/pre&gt;&lt;p&gt;As illustrated above, each dataset gets its own model, maintaining statistical and logical integrity. At the decision fusion layer, the team combines these model predictions with human knowledge about experimental conditions, model limitations, and biological context to do a final weighted selection of molecules to work with.&lt;/p&gt;
&lt;p&gt;This strategy naturally leads us into the realm of multi-criteria decision analysis (MCDA) and hybrid decision support systems. Instead of relying solely on machine learning outputs, these methods formally incorporate multiple evidence sources - including human expertise, experimental contexts, and various model predictions.&lt;/p&gt;
&lt;p&gt;Success in this framework doesn't require perfect dataset integration -- which comes from thinking about the problem as a machine learning problem. Rather, it comes from building thoughtful decision support systems that help researchers weigh evidence from multiple sources. For the community of ML builders in biotech, this opens exciting new opportunities! The next generation of tools might look less like monolithic predictive engines and more like hybrid systems that empower researchers to navigate complex trade-offs and effectively combine evidence from diverse experiments.&lt;/p&gt;
&lt;p&gt;By embracing this nuanced perspective, we can harness the power of machine learning while respecting the inherent complexity of biological systems. This balanced approach, I believe, will be key to accelerating discovery in the fast-moving world of biotech research.&lt;/p&gt;
</content></entry><entry><title>Writing at the speed of thought</title><link href="https://ericmjl.github.io/blog/2025/1/13/writing-at-the-speed-of-thought/" rel="alternate"/><updated>2025-01-13T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:787d9321-2199-3b52-9fc4-e4dc2577105a</id><content type="html">&lt;p&gt;In recent months, I've been dealing with wrist pain that made typing increasingly difficult. What started as a limitation, though, led me to discover something unexpected: a better way to write that not only solved my physical constraints but revolutionized how I capture my thoughts.&lt;/p&gt;
&lt;h2 id="the-challenge-when-typing-becomes-your-enemy"&gt;The challenge: When typing becomes your enemy&lt;/h2&gt;&lt;p&gt;The frustration of having ideas trapped in your head while your body won't cooperate is real. Typing had become tedious and painful, creating a genuine barrier between my thoughts and their expression. But beyond the physical limitations, I realized typing itself was always a bottleneck – my fingers simply couldn't keep up with my brain.&lt;/p&gt;
&lt;h2 id="finding-freedom-in-voice"&gt;Finding freedom in voice&lt;/h2&gt;&lt;p&gt;The solution came through dictation. While macOS's native dictation works, it left me with a lot of manual cleanup work. Through a class by Jason Liu, I discovered Better Dictation, and I also found VoicePal for my phone through Ali Abdaal's YouTube channel. Both tools are game-changers – they can handle spelled-out words intelligently, returning the correct spelling of technical terms or names without showing the letter-by-letter dictation. With either tool, I finally had a system that could truly keep up with my thought process while minimizing the need for manual editing.&lt;/p&gt;
&lt;p&gt;What makes this approach powerful isn't just about avoiding physical pain – it's about capturing what I call "nonlinear ideas." When I'm typing, if I go down one train of thought and something else bubbles up, I face a frustrating choice: either lose my original thread while trying to capture the new idea, or lose the new idea while trying to finish my current thought. Speaking allows me to quickly address new thoughts and return to my original path without losing either.&lt;/p&gt;
&lt;h2 id="my-current-writing-stack"&gt;My current writing stack&lt;/h2&gt;&lt;p&gt;Here's what works for me:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;I use VoicePal for initial dictation, which automatically cleans up filler words and makes the raw content more readable.&lt;/li&gt;
&lt;li&gt;This cleaned-up text goes into Claude (Anthropic's AI) for initial editing while preserving my voice and style.&lt;/li&gt;
&lt;li&gt;Final edits happen in Cursor, where I can make minor tweaks if needed.&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="the-ai-question-keeping-it-authentic"&gt;The AI question: Keeping it authentic&lt;/h2&gt;&lt;p&gt;The common concern about AI writing tools isn't quite that they strip away personality – it's that they tend to generate generic, templated-sounding text. I've found two key elements that help me avoid this. First, through in-context learning – showing the AI examples of my writing style – I can keep the tone and flow that I've developed through years of debate, public speaking, and blogging. This foundation of finding my voice through traditional means, without AI assistance, has been crucial.&lt;/p&gt;
&lt;p&gt;But style is just the surface. The real key to authentic writing with AI is in the editing process. When I critically review and edit AI-generated drafts, I'm not just proofreading – I'm actively shaping the text to match the true substance of what's in my head. This is where ownership really happens. The AI helps get ideas on the page, but through editing, I ensure the final text genuinely reflects what I'm trying to say.&lt;/p&gt;
&lt;p&gt;So yes, you've been reading AI-generated text – but not for the reasons you might think. I'm not trying to be another content creator churning out quick posts for monetization. I'm using AI because I have real physical constraints now, and I need to find efficient ways to keep writing while managing my wrist pain. Sometimes constraints lead us to better solutions.&lt;/p&gt;
&lt;h2 id="the-freedom-to-think"&gt;The freedom to think&lt;/h2&gt;&lt;p&gt;What started as a solution to physical pain has become something much more valuable: a way to write at the speed of thought. It's not just about accessibility – it's about removing the barriers between our ideas and their expression. For me, that's absolutely huge. Being able to capture my thoughts painlessly and naturally has significantly improved not just my writing, but my quality of life.&lt;/p&gt;
&lt;h2 id="embracing-new-ways-to-create"&gt;Embracing new ways to create&lt;/h2&gt;&lt;p&gt;Sometimes our greatest innovations come from working around limitations. While I started this journey looking for a way to write through pain, I've ended up discovering a method that's actually superior to my old way of working. The combination of dictation and AI assistance hasn't just helped me continue writing – it's helped me write better, think more clearly, and express ideas more naturally than ever before.&lt;/p&gt;
&lt;p&gt;If you're facing similar challenges, whether they're physical limitations or just the frustration of thoughts moving faster than your fingers, I encourage you to experiment with these tools. The technology exists now to remove the barriers between your thoughts and their expression. All that's left is to find the combination that works for you and to let your ideas flow freely.&lt;/p&gt;
</content></entry><entry><title>A practical guide to securing secrets in data science projects</title><link href="https://ericmjl.github.io/blog/2025/1/10/a-practical-guide-to-securing-secrets-in-data-science-projects/" rel="alternate"/><updated>2025-01-10T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:42a4be8f-507b-3f80-b58e-2f7f6a454e88</id><content type="html">&lt;p&gt;I've made my share of mistakes with secrets management - from accidentally committing API keys in Jupyter notebooks to storing passwords in plain text. Today, I want to share what I've learned about keeping secrets secure in our data science workflows.&lt;/p&gt;
&lt;p&gt;Let's face it: as data scientists, we deal with a lot of sensitive credentials. API keys for data vendors, database passwords, cloud service tokens - the list goes on. Here's my practical approach to handling these securely.&lt;/p&gt;
&lt;h2 id="start-local-the-power-of-environment-variables"&gt;Start local: the power of environment variables&lt;/h2&gt;&lt;p&gt;The foundation of my secret management starts with environment variables. I've found that using &lt;code&gt;direnv&lt;/code&gt; combined with &lt;code&gt;.env&lt;/code&gt; files is a game-changer for local development. Here's the how and why.&lt;/p&gt;
&lt;p&gt;First, install &lt;a href="https://direnv.net/"&gt;&lt;code&gt;direnv&lt;/code&gt;&lt;/a&gt; in your shell. This tool automatically loads environment variables when you enter a directory and unloads them when you leave. This is way more secure than setting globals in your &lt;code&gt;.bashrc&lt;/code&gt; file, where any program in your user space could potentially access them.&lt;/p&gt;
&lt;p&gt;In my projects, I create a &lt;code&gt;.env&lt;/code&gt; file with permissions set to 400 (meaning only I can read it):&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;chmod&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;400&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;.env
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Inside this file, I store my secrets:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nv"&gt;DATA_VENDOR_API_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;abc123
&lt;span class="nv"&gt;DB_PASSWORD&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;supersecret
&lt;span class="nv"&gt;AWS_ACCESS_KEY&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;xyz789
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Now, here's an important technical detail: if your Jupyter notebooks are launched in an environment where &lt;code&gt;direnv&lt;/code&gt; is already active, the environment variables will be automatically available through &lt;code&gt;os.getenv()&lt;/code&gt;. You won't need any additional setup:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;os&lt;/span&gt;
&lt;span class="n"&gt;api_key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;DATA_VENDOR_API_KEY&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;  &lt;span class="c1"&gt;# Works if direnv is active&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;However, if your notebooks launch without &lt;code&gt;direnv&lt;/code&gt; support (which is common in many setups), you can fall back to &lt;code&gt;python-dotenv&lt;/code&gt;:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;dotenv&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;load_dotenv&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;os&lt;/span&gt;

&lt;span class="n"&gt;load_dotenv&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;  &lt;span class="c1"&gt;# Backup for when direnv isn&amp;#39;t available&lt;/span&gt;
&lt;span class="n"&gt;api_key&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;os&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;getenv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;DATA_VENDOR_API_KEY&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;I generally include &lt;code&gt;python-dotenv&lt;/code&gt; in my projects anyway - it's a lightweight safeguard that ensures my code works across different environments, whether &lt;code&gt;direnv&lt;/code&gt; is available or not.&lt;/p&gt;
&lt;h2 id="version-control-the-first-line-of-defense"&gt;Version control: the first line of defense&lt;/h2&gt;&lt;p&gt;One crucial habit I've developed is to immediately add &lt;code&gt;.env&lt;/code&gt; to my &lt;code&gt;.gitignore&lt;/code&gt; file. This prevents me from accidentally committing secrets to version control. Here's what I add to &lt;code&gt;.gitignore&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;.env # &amp;lt;---this is the key thing to add!

# These are also good to add!
*.pem
*credentials*
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="preventing-commits-with-pre-commit-hooks"&gt;Preventing commits with pre-commit hooks&lt;/h2&gt;&lt;p&gt;While &lt;code&gt;.gitignore&lt;/code&gt; helps prevent committing specific files, I've learned that having an automated check for accidental API key commits is invaluable. I use a pre-commit hook that scans for potential API keys in my code before they ever make it into a commit.&lt;/p&gt;
&lt;p&gt;Here's the pre-commit hook I use (you can find it &lt;a href="https://github.com/ericmjl/api-key-precommit"&gt;here on GitHub&lt;/a&gt;):&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;repos&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;repo&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;https://github.com/ericmjl/api-key-precommit&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;rev&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;v0.1.0&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="nt"&gt;hooks&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;      &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nt"&gt;id&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;api-key-checker&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="nt"&gt;args&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;--pattern&amp;quot;&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;[a-zA-Z0-9]{32,}&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;# Catches generic API keys&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;--pattern&amp;quot;&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;api[_-]key[_-][a-zA-Z0-9]{16,}&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;# API keys with prefix&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;--pattern&amp;quot;&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;sk-[A-Za-z0-9]{48}&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;# OpenAI-style keys&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;--pattern&amp;quot;&lt;/span&gt;
&lt;span class="w"&gt;          &lt;/span&gt;&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="s"&gt;&amp;quot;ghp_[A-Za-z0-9]{36}&amp;quot;&lt;/span&gt;&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="c1"&gt;# GitHub tokens&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This hook has saved me numerous times. Based on the configuration, you can catch keys like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generic 32+ character keys&lt;/li&gt;
&lt;li&gt;OpenAI API keys&lt;/li&gt;
&lt;li&gt;GitHub personal access tokens&lt;/li&gt;
&lt;li&gt;Slack tokens&lt;/li&gt;
&lt;li&gt;And any other patterns you configure it to catch&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Hot tip: use a local LLM to help you generate the pattern. You can copy/paste the key a few times in a plain text editor, flip around 4-5 characters in there, and then ask it for the regex pattern. Much faster than trying to decipher it yourself! Here's the prompt:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;I have the following example API keys (not real, obfuscated):

sk-38qv80ehlkjsh4t8-dfo9387hanlc
sk-38qv80ehl3jsh4se-dfe9387haelc
sk-319voi4nwp8uryv4;jnyqr8gy[d9u63t
sk-490408tyklnldsf-498324uohsiuaodf

Return for me a regex pattern that can match these keys and beyond.
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once you have this set up, the pre-commit hook will automatically scan your code before each commit. If it finds anything that looks like an API key, it'll stop the commit process and point out exactly where the potential exposure is. This gives you a chance to remove any secrets before they ever make it into version control - which has saved me countless times from accidentally exposing sensitive information.&lt;/p&gt;
&lt;p&gt;I've found that combining this with &lt;code&gt;.gitignore&lt;/code&gt; creates a double-layered defense against accidentally exposing secrets. The &lt;code&gt;.gitignore&lt;/code&gt; prevents committing known secret-containing files, while the pre-commit hook catches any secrets that might slip into your actual code.&lt;/p&gt;
&lt;h2 id="sharing-secrets-in-collaborative-projects"&gt;Sharing secrets in collaborative projects&lt;/h2&gt;&lt;p&gt;First, let me address something critical: if you absolutely must share a secret with a teammate (which should be rare), never do it through chat, email, or externally hosted services. I've learned to use self-destroying notes through open source tools like Cryptgeon, which you can deploy on your internal network. Here's the thing - NEVER trust externally hosted secret-sharing services. You have no idea what they're doing with your data behind the scenes!&lt;/p&gt;
&lt;p&gt;While I've built my own tool (&lt;a href="http://pigeon.ericmjl.com/"&gt;pigeon notes&lt;/a&gt;, which I'll open source later this year), I actually recommend you don't trust my hosted version either. The safest approach is to deploy your own instance of an open source tool like &lt;a href="https://github.com/cupcakearmy/cryptgeon"&gt;Cryptgeon&lt;/a&gt; on your internal infrastructure. This way, you have full control and visibility over how your sensitive data is handled.&lt;/p&gt;
&lt;p&gt;Now, for managing secrets in your development workflow, I've found that almost every major vendor platform that involves code execution provides some form of secrets management. For example:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;GitHub provides Secrets features for CI/CD pipelines&lt;/li&gt;
&lt;li&gt;Workspace platforms like Domino or Nebari have their own secrets management systems&lt;/li&gt;
&lt;li&gt;Cloud providers like AWS offer services like Secrets Manager to interact with their services&lt;/li&gt;
&lt;li&gt;Most compute platforms come with built-in ways to handle secrets securely&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The key is to check your platform's documentation and invest time in understanding its secrets management capabilities. Instead of hard-coding secrets directly in your notebooks or scripts (which is both insecure and a common mistake), leverage these built-in features - they're usually well-tested and integrated with the platform's security model. While it might take some initial setup time, it's worth learning how to store and access secrets properly within each environment you work in.&lt;/p&gt;
&lt;h2 id="what-if-i-accidentally-commit-a-secret"&gt;What if I accidentally commit a secret?&lt;/h2&gt;&lt;p&gt;When a secret gets committed (it happens), the first step is to immediately rotate any exposed credentials - this minimizes the window of vulnerability. Then comes the cleanup: BFG Repo Cleaner helps scrub the secret from Git history. Install it using homebrew (&lt;code&gt;brew install bfg&lt;/code&gt;) or download the jar file. With BFG installed, run:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;bfg&lt;span class="w"&gt; &lt;/span&gt;--replace-text&lt;span class="w"&gt; &lt;/span&gt;secrets.txt&lt;span class="w"&gt; &lt;/span&gt;your-repo.git
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Where &lt;code&gt;secrets.txt&lt;/code&gt; contains the patterns of secrets you want to remove. After BFG does its work, force push the cleaned repository:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;git&lt;span class="w"&gt; &lt;/span&gt;push&lt;span class="w"&gt; &lt;/span&gt;origin&lt;span class="w"&gt; &lt;/span&gt;--force
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Let the team know they'll need to re-clone the repository fresh - any local copies still contain the secret in their history. How long it takes depends on your repository's history and complexity, so coordinate with your team to pause development until the cleanup is complete to avoid commit conflicts.&lt;/p&gt;
&lt;h2 id="best-practices-i-ve-learned-the-hard-way"&gt;Best practices I've learned the hard way&lt;/h2&gt;&lt;p&gt;Through trial and error, I've developed these habits:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Never hardcode secrets in Jupyter notebooks, even in "temporary" analysis&lt;/li&gt;
&lt;li&gt;Rotate API keys when platforms notify me about expiration or when something breaks&lt;/li&gt;
&lt;li&gt;Use pre-commit hooks to catch potential secret leaks before they happen&lt;/li&gt;
&lt;li&gt;Keep my secrets scoped to specific projects using &lt;code&gt;direnv&lt;/code&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h2 id="following-proven-paths"&gt;Following proven paths&lt;/h2&gt;&lt;p&gt;Everything I've described here isn't unique to data science - it's exactly how software developers have been building applications for years. The principles behind the &lt;a href="https://12factor.net/"&gt;12-factor app methodology&lt;/a&gt;, which has guided software development for over a decade, apply just as well to our data science work. Whether we're building a web service or training a machine learning model, we're still writing code, and the same best practices around configuration and secrets management apply.&lt;/p&gt;
&lt;h2 id="a-note-on-security-layers"&gt;A note on security layers&lt;/h2&gt;&lt;p&gt;Secrets management, like Swiss cheese - each security measure has holes, but when layered together, they create a robust defense. While AWS Secrets Manager might be overkill for some projects, even simple practices like using &lt;code&gt;.env&lt;/code&gt; files and proper permissions can significantly improve security.&lt;/p&gt;
&lt;p&gt;It only takes one exposed secret to potentially compromise your data or services. As data scientists, we handle sensitive information daily, so taking these precautions isn't just good practice - it's essential.&lt;/p&gt;
</content></entry><entry><title>What makes an agent?</title><link href="https://ericmjl.github.io/blog/2025/1/4/what-makes-an-agent/" rel="alternate"/><updated>2025-01-04T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:92003528-4cb2-33a5-a01d-b64d159d9d07</id><content type="html">&lt;p&gt;In &lt;a href="https://ericmjl.github.io/blog/2024/12/15/how-llamabots-new-agent-features-simplify-complex-task-automation/"&gt;a previous blog post&lt;/a&gt;,
I explored how LlamaBot's agent features can simplify complex task automation.
Here, I'd like to explore what exactly makes an agent.&lt;/p&gt;
&lt;h2 id="my-working-definition"&gt;My working definition&lt;/h2&gt;&lt;p&gt;In my view, an LLM agent is a system that demonstrates
the appearance of making goal-directed decisions autonomously.
It operates within a scoped set of tasks it can perform,
without requiring pre-specification of the order in which these tasks are executed.
Three key attributes set it apart:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Goal-oriented non-determinism&lt;/strong&gt;: The agent makes decisions dynamically rather than following a pre-defined sequence of actions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decision-making flow control&lt;/strong&gt;: Decisions adapt based on context, resembling a "thinking" process.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Natural language interfaces&lt;/strong&gt;: Inputs and outputs are in plain language, making interactions feel intuitive.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;A familiar way to understand these attributes
is to think about a flight booking agent you might call on the phone.
They handle diverse requests
("I want to fly to Tokyo next month" or
"I need the cheapest flight to London this weekend"),
make decisions based on context
(checking different airlines, considering your preferences),
and communicate naturally.
The parallel helps clarify what we mean by agent-like behavior -
though with one key distinction.
While a human agent exercises genuine autonomy in their decision-making,
our LLM agents create the &lt;em&gt;appearance&lt;/em&gt; of autonomy and flexibility
through careful engineering of their design and tools.&lt;/p&gt;
&lt;p&gt;These attributes combine to give agents their apparent autonomy
and their flexibility in handling diverse inputs.
With these principles in mind,
I designed &lt;a href="https://ericmjl.github.io/llamabot/tutorials/agentbot/"&gt;LlamaBot's AgentBot&lt;/a&gt;
to make creating and deploying agents simple, flexible, and powerful.&lt;/p&gt;
&lt;h2 id="a-key-insight-the-guiding-principle"&gt;A key insight: The guiding principle&lt;/h2&gt;&lt;p&gt;Through my experience with website scraping and other projects,
I've developed what I believe is a crucial principle for when to use agents:
You really don't want to use agents
until you have &lt;em&gt;variable scope input&lt;/em&gt;
paired with a &lt;em&gt;constrained scope of actions&lt;/em&gt; that could be taken.&lt;/p&gt;
&lt;p&gt;This principle really emerged from my observations
of what makes for successful applications of agents.
In fact, I've noticed that an agent's effectiveness -
that is, the probability it chooses the correct action -
appears to be highly dependent on two key factors:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;The number of options available to it (the more constrained, the better)&lt;/li&gt;
&lt;li&gt;The vagueness of the incoming instructions (the clearer, the better)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This relationship between input clarity and action constraints isn't just theoretical.
I've also observed that an agent's ability to clarify these actions when needed
plays a significant role in its success.
When an agent can effectively navigate between understanding variable inputs
and selecting from constrained actions,
it performs at its best.&lt;/p&gt;
&lt;h2 id="industry-perspectives-on-agents"&gt;Industry perspectives on agents&lt;/h2&gt;&lt;p&gt;While my observations come from hands-on experience
building and deploying agents,
it's worth examining how others in the field think about agents.
These diverse perspectives
help contextualize the practical principles I've discovered
and offer additional frameworks for thinking about agent design.&lt;/p&gt;
&lt;h3 id="anthropic-s-architectural-distinction"&gt;Anthropic's architectural distinction&lt;/h3&gt;&lt;p&gt;&lt;a href="https://www.anthropic.com/research/building-effective-agents"&gt;Anthropic's perspective on agents&lt;/a&gt;
particularly resonates with my thinking
about variable scope inputs and constrained actions.&lt;/p&gt;
&lt;p&gt;They make an important architectural distinction
in how they categorize agent systems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Workflows&lt;/strong&gt;: Systems where LLMs and tools are orchestrated through predefined code paths&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Agents&lt;/strong&gt;: Systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;What's particularly compelling about their approach
is how they emphasize finding the simplest solution possible.
They explicitly note that agent systems often trade latency and cost
for better task performance -
a tradeoff that needs careful consideration.
This aligns perfectly with my observations:
simple scales, while complexity doesn't.
Building a Rube Goldberg machine
is a recipe for long-term disaster.&lt;/p&gt;
&lt;p&gt;Their framework suggests using workflows
for predictability and consistency in well-defined tasks,
while reserving agents for situations
where flexibility and model-driven decision-making are needed at scale.
This maps well to my principle about variable scope inputs -
workflows for fixed patterns,
agents for variable ones.&lt;/p&gt;
&lt;h3 id="google-s-comprehensive-framework"&gt;Google's comprehensive framework&lt;/h3&gt;&lt;p&gt;Google's definition,
outlined in their &lt;a href="https://drive.google.com/file/d/1oEjiRCTbd54aSdB_eEe3UShxLBWK9xkt/view?usp=sharing"&gt;whitepaper&lt;/a&gt;,
takes a more expansive view
while still complementing these ideas.
They define a generative AI agent
as an application that attempts to achieve goals
by observing and acting upon the world
using available tools.
Their framework emphasizes several key characteristics
that help clarify when to use agents:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Autonomy&lt;/strong&gt;: Agents can act independently of human intervention when provided with proper goals or objectives&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Proactive Planning&lt;/strong&gt;: They can reason about next steps even without explicit instruction sets&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tool Integration&lt;/strong&gt;: They emphasize how agents extend beyond standalone models by interfacing with external tools and systems&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Observation and Action&lt;/strong&gt;: Agents operate in a cycle of observing their environment, making decisions, and taking actions&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This aligns with my observations about constrained action spaces -
Google's framework suggests that
while agents should be autonomous in their decision-making,
they still operate within a defined set of tools and capabilities.
Their emphasis on the observation-action cycle
particularly resonates with how I've seen agents succeed in practice:
they need clear feedback loops
between their actions and their environment
to make effective decisions.&lt;/p&gt;
&lt;h3 id="huggingface-s-paradigm"&gt;HuggingFace's paradigm&lt;/h3&gt;&lt;p&gt;An intriguing new perspective in the field,
&lt;a href="https://x.com/AymericRoucher/status/1874116324898598934"&gt;announced by Aymeric Roucher&lt;/a&gt; from HuggingFace,
proposes having the agent write the code it needs to execute
before actually executing it.
This approach introduces flexibility to the agent compared to my own definition,
and should theoretically be much more powerful.
As with anything that executes code,
it also requires careful consideration of security implications.
If an agent can write and execute its own code,
it becomes vulnerable to prompt injection attacks
that could lead to malicious code written
that gets executed with potentially disastrous consequences.
The ideal implementation would run such code in isolated environments
that can't affect the host system -
similar to how we use containers and sandboxes in other contexts.&lt;/p&gt;
&lt;p&gt;It's important to distinguish this paradigm
from agents that help us write prototype applications or production code.
Here, we're talking about agents writing throwaway code -
small, single-use scripts that the agent uses to process data or perform calculations,
get the output, and then decide on its next action.
This is fundamentally different from code generation for applications;
it's about giving agents the ability
to dynamically create and execute computational tools
as part of their reasoning process.&lt;/p&gt;
&lt;h2 id="real-world-applications"&gt;Real-world applications&lt;/h2&gt;&lt;p&gt;Let's look at some examples
of how agents might work.
Being someone from the life sciences,
I have a little bit of a bias,
so that's where my line of thinking is.
Before we go to the vaccines examples,
I think it's important
to think about what a minimally complex example
might look like.&lt;/p&gt;
&lt;h3 id="a-minimal-example-bill-calculation-bot"&gt;A minimal example: Bill calculation bot&lt;/h3&gt;&lt;p&gt;Let's look at a concrete example:
a bill calculation bot.
This is a minimally complex,
easily understandable example of variable scope input with constrained actions.
The bot might:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Calculate tips&lt;/li&gt;
&lt;li&gt;Split bills among people&lt;/li&gt;
&lt;li&gt;Provide itemized breakdowns in table format
What makes this a good use case for agents
is that we're not limiting ourselves
to a single input (total amount)
and single output (split amount per person).
Instead,
users might want different combinations of actions.
Some might want to calculate the total and split it,
others might want to add a tip first, then split,
and some might just want the total.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="life-sciences-example-literature-research-assistant"&gt;Life sciences example: Literature research assistant&lt;/h3&gt;&lt;p&gt;For a more sophisticated real-world example,
consider a literature research assistant agent
in the life sciences.
This kind of agent perfectly embodies the principle
of variable scope input
with constrained actions:&lt;/p&gt;
&lt;p&gt;The agent might handle research queries like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;"Find recent papers about CRISPR in cancer therapy"&lt;/li&gt;
&lt;li&gt;"What's the latest on protein folding algorithms?"&lt;/li&gt;
&lt;li&gt;"Who else is citing this methodology paper?"&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Each query could trigger different combinations of constrained actions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Searching multiple databases (PubMed, bioRxiv, etc.)&lt;/li&gt;
&lt;li&gt;Filtering by publication date, citation count, or journal impact factor&lt;/li&gt;
&lt;li&gt;Cross-referencing author networks&lt;/li&gt;
&lt;li&gt;Extracting methodology sections&lt;/li&gt;
&lt;li&gt;Generating citation summaries&lt;/li&gt;
&lt;li&gt;Identifying key figures&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What makes this a compelling use case for agents
is how the inputs can vary wildly
(from broad research questions to specific citation queries)
while the actions remain constrained
to a well-defined set of database operations and text analysis tools.
The agent needs to dynamically choose
which combination of these actions
will best serve the researcher's intent.&lt;/p&gt;
&lt;h3 id="life-sciences-example-protocol-optimization-assistant"&gt;Life sciences example: Protocol optimization assistant&lt;/h3&gt;&lt;p&gt;Another life sciences example
would be a protocol optimization agent
that assists with experimental design.
Researchers might input anything
from "How should I modify this Western blot protocol
for a very low-abundance protein?"
to "What's the best transfection method
for these primary neurons?"
The agent would work with a constrained set of actions like:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Searching and comparing protocols from methods sections in published papers&lt;/li&gt;
&lt;li&gt;Identifying key differences between similar protocols across labs&lt;/li&gt;
&lt;li&gt;Extracting specific modifications for edge cases (e.g., low abundance proteins, difficult-to-transfect cells)&lt;/li&gt;
&lt;li&gt;Compiling troubleshooting notes from supplementary materials&lt;/li&gt;
&lt;li&gt;Cross-referencing protocol modifications with success rates reported in papers&lt;/li&gt;
&lt;li&gt;Aggregating user experiences from protocol repositories like protocols.io&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;What makes this realistic
is that it mirrors what scientists actually do -
we spend hours scouring papers and supplements,
comparing different labs' approaches,
and piecing together what modifications
might work for our specific case.
The agent operates within the constrained action space
of literature analysis and comparison,
but can handle the variable scope of input questions
that arise during protocol optimization.&lt;/p&gt;
&lt;p&gt;These examples from life sciences highlight
how agents can handle complex, variable inputs
while working within carefully defined action constraints.&lt;/p&gt;
&lt;h3 id="a-roadmap-for-designing-agent-applications"&gt;A roadmap for designing agent applications&lt;/h3&gt;&lt;p&gt;I've found that the most effective way
to design agent applications
is to progressively relax constraints
on inputs and execution order.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Start with a deterministic program&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Design your application as you would with regular API calls&lt;/li&gt;
&lt;li&gt;Define clear input/output specifications&lt;/li&gt;
&lt;li&gt;Implement core functionality with standard programming patterns&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Relax input constraints&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Accept natural language input and convert it to structured parameters for function calls&lt;/li&gt;
&lt;li&gt;Enable autonomous function calling based on natural language understanding&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Relax execution order constraints&lt;/strong&gt;&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Only necessary when natural language inputs are varied enough to require different execution paths&lt;/li&gt;
&lt;li&gt;Allow flexible ordering of operations when needed&lt;/li&gt;
&lt;li&gt;Enable dynamic selection of which functions to call&lt;/li&gt;
&lt;li&gt;Maintain boundaries around available actions while allowing flexibility in their use&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This progressive relaxation approach
helps us transition
from traditional deterministic programming
to an agent-driven paradigm
where execution order is non-deterministic
and inputs are natural language.&lt;/p&gt;
&lt;h2 id="looking-forward"&gt;Looking forward&lt;/h2&gt;&lt;p&gt;After diving into all these different perspectives on agents,
I keep coming back to a key insight:
the real power of agents
isn't about making them completely autonomous.
It's about finding that sweet spot
where they can handle all sorts of different inputs
while working with a well-defined set of actions.&lt;/p&gt;
&lt;p&gt;Think about the examples we've looked at -
from simple bill-splitting
to complex protocol optimization.
In each case,
the agent's effectiveness comes
from having clear boundaries around what it can do,
paired with the flexibility to handle a limited scope of questions or requests.
This is what makes agents genuinely useful
in real-world contexts.&lt;/p&gt;
&lt;p&gt;I believe this is the direction
we should be pushing in:
not trying to make agents do everything,
but rather getting really good
at defining the right scope and tools
for specific use cases.
After all, the most successful applications of agents
will be the ones
that solve real problems really well.&lt;/p&gt;
</content></entry><entry><title>What I learned blogging every week for one year</title><link href="https://ericmjl.github.io/blog/2024/12/31/what-i-learned-blogging-every-week-for-one-year/" rel="alternate"/><updated>2024-12-31T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:3ac60220-8c36-30e3-aaa9-d6c94f211e47</id><content type="html">&lt;p&gt;A challenge I set for myself this year
was to write a blog post on average once every week for the year,
regardless of what's going on.
This meant writing 52 blog posts in total.
As of December 19, I've hit 53, meaning that I've accomplished my goals!&lt;/p&gt;
&lt;h2 id="what-i-learned"&gt;What I learned&lt;/h2&gt;&lt;p&gt;Here's what I've learned so far.&lt;/p&gt;
&lt;h3 id="consistency-is-king"&gt;Consistency is king!&lt;/h3&gt;&lt;p&gt;One of the biggest lessons I learned from this year-long blogging journey
is that consistency truly is king.
By committing to writing weekly,
I noticed my sense of what to write about
became increasingly refined and sharpened over time.
The regular practice helped me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Develop a better radar for interesting technical topics worth sharing&lt;/li&gt;
&lt;li&gt;Improve my ability to explain complex concepts clearly&lt;/li&gt;
&lt;li&gt;Build a more systematic approach to documenting my learning journey&lt;/li&gt;
&lt;li&gt;Create a sustainable writing habit that feels natural rather than forced&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="build-ai-assistance-tools-and-use-them"&gt;Build AI-assistance tools and use them&lt;/h3&gt;&lt;p&gt;Throughout this journey,
I found myself building quite a bit of tooling
to support my blogging workflow,
most of it done with AI assistance.
Some notable examples include:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Blog Banner Generation&lt;/strong&gt;:
I developed a system that uses DALL-E
to automatically generate watercolor-style banner images for each post,
maintaining a consistent visual identity
while saving time on design.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Social Media Integration&lt;/strong&gt;:
I built tools that use LLMs to help craft:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Twitter posts that effectively summarize and promote new blog entries&lt;/li&gt;
&lt;li&gt;Substack newsletters that engage my subscriber base&lt;/li&gt;
&lt;li&gt;LinkedIn updates that resonate with my professional network&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;Content Enhancement&lt;/strong&gt;:
I created automated systems for:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Generating relevant tags for better post categorization&lt;/li&gt;
&lt;li&gt;Creating compelling summaries that capture the essence of each post&lt;/li&gt;
&lt;li&gt;Suggesting related content links to improve site navigation&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This tooling journey has been particularly valuable as it helped me:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Further hone my prompting practices with LLMs&lt;/li&gt;
&lt;li&gt;Develop better intuition for designing programs
that incorporate LLMs as integral components&lt;/li&gt;
&lt;li&gt;Create a more efficient and consistent blogging workflow&lt;/li&gt;
&lt;li&gt;Experiment with different approaches to human-AI collaboration&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The experience of building these tools
has not only made my blogging more efficient
but has also sharpened my intuition for
how to effectively integrate AI into creative workflows
while maintaining human oversight and editorial control.&lt;/p&gt;
&lt;h2 id="wordcloud"&gt;Wordcloud&lt;/h2&gt;&lt;p&gt;And here is the obligatory wordcloud of the blog posts:&lt;/p&gt;
&lt;p&gt;&lt;img src="wordcloud.webp" alt="Wordcloud of blog posts"&gt;&lt;/p&gt;
&lt;h2 id="reflections"&gt;Reflections&lt;/h2&gt;&lt;p&gt;One year of blogging once a week,
with regular posts on LinkedIn, Twitter, Substack, and Bluesky,
have left my colleagues jokingly calling me a content creator.
Jokes aside, this year has seen a significant focus on LLMs in my writing,
as evidenced by the numerous posts in the LLMs &amp;amp; Data Science Tooling section.
This reflects the broader excitement and rapid developments in the LLM space.
At the same time,
my core interests continue to lie at the intersection of life sciences and computation -
you can see this in the many posts about protein language models,
biological applications of deep learning,
and data science in biotech organizations.
My professional goal is to make discovery science run at the speed of thought
and quantify the unquantified.
This means leveraging advances in AI and computation
to accelerate scientific discovery,
while developing tools that help scientists
measure and understand previously intangible aspects of biology.
After all, the most transformative breakthroughs happen
when we remove the friction from scientific exploration
and shine light on unexplored territory.&lt;/p&gt;
&lt;h2 id="blog-posts-by-theme"&gt;Blog Posts by Theme&lt;/h2&gt;&lt;p&gt;To help you navigate through these posts more easily,
I've organized them by major themes below.
You'll notice some posts appear under multiple themes -
that's because many topics naturally overlap,
reflecting the interconnected nature of modern data science work.
Whether you're interested in LLMs, leadership, tooling, or biology,
you'll find relevant content in these curated lists.&lt;/p&gt;
&lt;h3 id="biology-chemistry"&gt;Biology &amp;amp; Chemistry&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/7/success-factors-for-data-science-teams-in-biotech/"&gt;Success Factors for Data Science Teams in Biotech (2024-02-07)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/18/dashboard-ready-data-is-often-machine-learning-ready-data/"&gt;Dashboard-ready data is often machine learning-ready data (2024-02-18)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/3/10/mixtral-8x7b-instruct-works-on-an-old-gtx1080/"&gt;Mixtral-8x7b-Instruct works on an old GTX1080! (2024-03-10)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/5/5/data-science-in-the-biotech-research-organization/"&gt;Data Science in the Biotech Research Organization (2024-05-05)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/5/12/paper-review-design-of-highly-functional-genome-editors-by-modeling-the-universe-of-crispr-cas-sequences/"&gt;Paper Review: Design of highly functional genome editors by modeling the universe of CRISPR-Cas sequences (2024-05-12)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/5/16/how-to-control-pymol-from-jupyter-notebooks/"&gt;How to control PyMOL from Jupyter notebooks (2024-05-16)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/5/27/multi-modality-deep-learning/"&gt;Multi-modality Deep Learning (2024-05-27)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/6/8/the-neural-von-mises-mixture-model/"&gt;The Neural Von Mises Mixture Model (2024-06-08)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/7/26/a-survey-of-how-to-use-protein-language-models-for-protein-design-part-1/"&gt;A survey of how to use protein language models for protein design: Part 1 (2024-07-26)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/8/2/a-survey-of-how-to-use-protein-language-models-for-protein-design-part-2/"&gt;A survey of how to use protein language models for protein design: Part 2 (2024-08-02)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/8/9/a-survey-of-how-to-use-protein-language-models-for-protein-design-part-3/"&gt;A survey of how to use protein language models for protein design: Part 3 (2024-08-09)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/11/22/a-modest-proposal-for-data-catalogues-at-biotechs/"&gt;A modest proposal for data catalogues at biotechs (2024-11-22)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="career-advice"&gt;Career Advice&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/1/11/github-actions-secrets-need-to-be-explicitly-declared/"&gt;GitHub Actions secrets need to be explicitly declared (2024-01-11)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/25/how-to-keep-sharp-with-technical-skills-as-a-data-science-team-lead/"&gt;How to keep sharp with technical skills as a data science team lead (2024-02-25)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/29/your-first-90-days-at-work-what-should-you-do/"&gt;Your first 90 days at work - what should you do? (2024-02-29)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/3/9/industry-career-panel-for-phd-students-and-post-docs-at-mit/"&gt;From Academia to Industry: Career Advice from MIT Industry Careers Panel (2024-03-09)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/3/23/how-to-organize-and-motivate-a-biotech-data-science-team/"&gt;How to organize and motivate a biotech data science team (2024-03-23)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/6/26/hire-for-communication-skills-not-conversational-skills/"&gt;Hire for communication skills, not conversational skills (2024-06-26)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/7/14/conference-report-scipy-2024/"&gt;Conference report: SciPy 2024 (2024-07-14)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/9/what-brings-you-joy-at-work/"&gt;What brings you joy at work? (2024-10-09)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/12/17/how-to-thrive-and-not-just-survive-during-organizational-change/"&gt;How to thrive, and not just survive, during organizational change (2024-12-17)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="data-science-practice-leadership"&gt;Data Science Practice &amp;amp; Leadership&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/1/28/exploratory-data-analysis-isnt-open-ended/"&gt;Exploratory data analysis isn’t open-ended (2024-01-28)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/7/success-factors-for-data-science-teams-in-biotech/"&gt;Success Factors for Data Science Teams in Biotech (2024-02-07)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/18/dashboard-ready-data-is-often-machine-learning-ready-data/"&gt;Dashboard-ready data is often machine learning-ready data (2024-02-18)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/25/how-to-keep-sharp-with-technical-skills-as-a-data-science-team-lead/"&gt;How to keep sharp with technical skills as a data science team lead (2024-02-25)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/3/23/how-to-organize-and-motivate-a-biotech-data-science-team/"&gt;How to organize and motivate a biotech data science team (2024-03-23)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/4/5/how-to-grow-software-development-skills-in-a-data-science-team/"&gt;How to grow software development skills in a data science team (2024-04-05)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/4/7/pyds-cli-version-040-released/"&gt;pyds-cli version 0.4.0 released! (2024-04-07)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/4/17/how-llms-can-accelerate-data-science/"&gt;How LLMs can accelerate data science (2024-04-17)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/5/5/data-science-in-the-biotech-research-organization/"&gt;Data Science in the Biotech Research Organization (2024-05-05)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/6/18/headache-free-portable-and-reproducible-handling-of-data-access-and-versioning/"&gt;Headache-free, portable, and reproducible handling of data access and versioning (2024-06-18)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/6/26/hire-for-communication-skills-not-conversational-skills/"&gt;Hire for communication skills, not conversational skills (2024-06-26)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/6/30/two-years-of-docathons-insights-and-lessons-learned/"&gt;Two years of docathons: Insights and lessons learned (2024-06-30)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/7/2/use-native-formats-when-storing-data/"&gt;Use native formats when storing data (2024-07-02)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/7/14/conference-report-scipy-2024/"&gt;Conference report: SciPy 2024 (2024-07-14)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/6/building-pigeon-secure-notes-in-under-15-minutes-of-coding/"&gt;Building Pigeon Secure Notes in under 15 minutes of coding (2024-10-06)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/9/what-brings-you-joy-at-work/"&gt;What brings you joy at work? (2024-10-09)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/18/keys-to-effective-collaborative-data-science/"&gt;Keys to effective collaborative data science (2024-10-18)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/25/the-human-dimension-to-clean-distributable-and-documented-data-science-code/"&gt;The Human Dimension to Clean, Distributable, and Documented Data Science Code (2024-10-25)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/11/22/a-modest-proposal-for-data-catalogues-at-biotechs/"&gt;A modest proposal for data catalogues at biotechs (2024-11-22)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="data-science-tooling"&gt;Data Science Tooling&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/1/10/evolving-llamabot/"&gt;Evolving LlamaBot (2024-01-10)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/1/11/github-actions-secrets-need-to-be-explicitly-declared/"&gt;GitHub Actions secrets need to be explicitly declared (2024-01-11)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/1/15/your-embedding-model-can-be-different-from-your-text-generation-model/"&gt;Your embedding model can be different from your text generation model (2024-01-15)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/1/28/exploratory-data-analysis-isnt-open-ended/"&gt;Exploratory data analysis isn’t open-ended (2024-01-28)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/1/an-incomplete-and-opinionated-survey-of-llm-tooling/"&gt;An (incomplete and opinionated) survey of LLM tooling (2024-02-01)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/7/success-factors-for-data-science-teams-in-biotech/"&gt;Success Factors for Data Science Teams in Biotech (2024-02-07)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/18/dashboard-ready-data-is-often-machine-learning-ready-data/"&gt;Dashboard-ready data is often machine learning-ready data (2024-02-18)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/21/llamabot-with-ollama-on-my-home-virtual-private-network/"&gt;LlamaBot with Ollama on my home virtual private network (2024-02-21)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/3/10/mixtral-8x7b-instruct-works-on-an-old-gtx1080/"&gt;Mixtral-8x7b-Instruct works on an old GTX1080! (2024-03-10)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/3/24/llamabot-040-released/"&gt;Llamabot 0.4.0 Released! (2024-03-24)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/4/5/how-to-grow-software-development-skills-in-a-data-science-team/"&gt;How to grow software development skills in a data science team (2024-04-05)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/4/7/pyds-cli-version-040-released/"&gt;pyds-cli version 0.4.0 released! (2024-04-07)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/4/9/how-to-make-distributable-pre-commit-hooks/"&gt;How to make distributable pre-commit hooks (2024-04-09)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/4/17/how-llms-can-accelerate-data-science/"&gt;How LLMs can accelerate data science (2024-04-17)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/5/16/how-to-control-pymol-from-jupyter-notebooks/"&gt;How to control PyMOL from Jupyter notebooks (2024-05-16)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/5/27/multi-modality-deep-learning/"&gt;Multi-modality Deep Learning (2024-05-27)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/6/1/how-to-manage-cuda-libraries-within-conda-environments/"&gt;How to manage CUDA libraries within Conda environments (2024-06-01)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/6/8/the-neural-von-mises-mixture-model/"&gt;The Neural Von Mises Mixture Model (2024-06-08)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/6/18/headache-free-portable-and-reproducible-handling-of-data-access-and-versioning/"&gt;Headache-free, portable, and reproducible handling of data access and versioning (2024-06-18)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/6/30/two-years-of-docathons-insights-and-lessons-learned/"&gt;Two years of docathons: Insights and lessons learned (2024-06-30)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/7/2/use-native-formats-when-storing-data/"&gt;Use native formats when storing data (2024-07-02)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/7/14/conference-report-scipy-2024/"&gt;Conference report: SciPy 2024 (2024-07-14)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/8/16/its-time-to-try-out-pixi/"&gt;It's time to try out pixi! (2024-08-16)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/8/25/dissecting-the-esm3-model-architecture/"&gt;Dissecting the ESM3 Model Architecture (2024-08-25)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/8/31/llamabot-now-has-structuredbot/"&gt;LlamaBot now has StructuredBot! (2024-08-31)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/9/6/on-writing-llm-evals-in-pytest/"&gt;On writing LLM evals in pytest (2024-09-06)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/9/14/cursor-is-a-jetpack-for-coders/"&gt;Cursor is a jetpack for coders (2024-09-14)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/9/15/sync-github-secrets-with-your-env-and-gh-cli/"&gt;Sync GitHub secrets with your .env and gh CLI (2024-09-15)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/9/19/how-to-set-up-pixi-with-codeartifacts/"&gt;How to set up Pixi with CodeArtifacts (2024-09-19)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/9/23/recreating-shortwhale-with-ai-assisted-coding/"&gt;Recreating Shortwhale with AI-Assisted Coding (2024-09-23)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/9/27/explain-your-jupyter-notebooks-using-llamabot/"&gt;Explain your Jupyter notebooks using LlamaBot (2024-09-27)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/6/building-pigeon-secure-notes-in-under-15-minutes-of-coding/"&gt;Building Pigeon Secure Notes in under 15 minutes of coding (2024-10-06)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/9/what-brings-you-joy-at-work/"&gt;What brings you joy at work? (2024-10-09)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/18/keys-to-effective-collaborative-data-science/"&gt;Keys to effective collaborative data science (2024-10-18)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/20/cursor-did-a-one-shot-rewrite-of-a-panel-app-i-built/"&gt;Cursor did a one-shot rewrite of a Panel app I built (2024-10-20)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/25/the-human-dimension-to-clean-distributable-and-documented-data-science-code/"&gt;The Human Dimension to Clean, Distributable, and Documented Data Science Code (2024-10-25)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/11/2/introducing-new-local-llamabot-logging-features/"&gt;Introducing new (local) LlamaBot logging features (2024-11-02)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/11/8/disposable-environments-for-ad-hoc-analyses/"&gt;Disposable environments for ad-hoc analyses (2024-11-08)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/11/14/deploying-ollama-on-modal/"&gt;Deploying Ollama on Modal (2024-11-14)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/11/22/a-modest-proposal-for-data-catalogues-at-biotechs/"&gt;A modest proposal for data catalogues at biotechs (2024-11-22)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/12/15/how-llamabots-new-agent-features-simplify-complex-task-automation/"&gt;How LlamaBot's new agent features simplify complex task automation (2024-12-15)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/12/16/5-retrieval-strategies-to-boost-your-rag-systems-performance/"&gt;5 retrieval strategies to boost your RAG system's performance (2024-12-16)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/12/20/accurately-extract-text-from-research-literature-pdfs-with-nougat-ocr-and-docling/"&gt;Accurately extract text from research literature PDFs with Nougat-OCR and Docling (2024-12-20)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id="llms"&gt;LLMs&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/1/10/evolving-llamabot/"&gt;Evolving LlamaBot (2024-01-10)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/1/15/your-embedding-model-can-be-different-from-your-text-generation-model/"&gt;Your embedding model can be different from your text generation model (2024-01-15)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/1/an-incomplete-and-opinionated-survey-of-llm-tooling/"&gt;An (incomplete and opinionated) survey of LLM tooling (2024-02-01)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/21/llamabot-with-ollama-on-my-home-virtual-private-network/"&gt;LlamaBot with Ollama on my home virtual private network (2024-02-21)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/3/10/mixtral-8x7b-instruct-works-on-an-old-gtx1080/"&gt;Mixtral-8x7b-Instruct works on an old GTX1080! (2024-03-10)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/3/24/llamabot-040-released/"&gt;Llamabot 0.4.0 Released! (2024-03-24)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/4/17/how-llms-can-accelerate-data-science/"&gt;How LLMs can accelerate data science (2024-04-17)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/5/12/paper-review-design-of-highly-functional-genome-editors-by-modeling-the-universe-of-crispr-cas-sequences/"&gt;Paper Review: Design of highly functional genome editors by modeling the universe of CRISPR-Cas sequences (2024-05-12)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/5/16/how-to-control-pymol-from-jupyter-notebooks/"&gt;How to control PyMOL from Jupyter notebooks (2024-05-16)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/5/27/multi-modality-deep-learning/"&gt;Multi-modality Deep Learning (2024-05-27)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/6/8/the-neural-von-mises-mixture-model/"&gt;The Neural Von Mises Mixture Model (2024-06-08)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/7/26/a-survey-of-how-to-use-protein-language-models-for-protein-design-part-1/"&gt;A survey of how to use protein language models for protein design: Part 1 (2024-07-26)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/8/2/a-survey-of-how-to-use-protein-language-models-for-protein-design-part-2/"&gt;A survey of how to use protein language models for protein design: Part 2 (2024-08-02)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/8/9/a-survey-of-how-to-use-protein-language-models-for-protein-design-part-3/"&gt;A survey of how to use protein language models for protein design: Part 3 (2024-08-09)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/8/25/dissecting-the-esm3-model-architecture/"&gt;Dissecting the ESM3 Model Architecture (2024-08-25)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/8/31/llamabot-now-has-structuredbot/"&gt;LlamaBot now has StructuredBot! (2024-08-31)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/9/6/on-writing-llm-evals-in-pytest/"&gt;On writing LLM evals in pytest (2024-09-06)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/9/14/cursor-is-a-jetpack-for-coders/"&gt;Cursor is a jetpack for coders (2024-09-14)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/9/23/recreating-shortwhale-with-ai-assisted-coding/"&gt;Recreating Shortwhale with AI-Assisted Coding (2024-09-23)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/9/27/explain-your-jupyter-notebooks-using-llamabot/"&gt;Explain your Jupyter notebooks using LlamaBot (2024-09-27)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/20/cursor-did-a-one-shot-rewrite-of-a-panel-app-i-built/"&gt;Cursor did a one-shot rewrite of a Panel app I built (2024-10-20)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/11/2/introducing-new-local-llamabot-logging-features/"&gt;Introducing new (local) LlamaBot logging features (2024-11-02)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/11/14/deploying-ollama-on-modal/"&gt;Deploying Ollama on Modal (2024-11-14)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/12/15/how-llamabots-new-agent-features-simplify-complex-task-automation/"&gt;How LlamaBot's new agent features simplify complex task automation (2024-12-15)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/12/16/5-retrieval-strategies-to-boost-your-rag-systems-performance/"&gt;5 retrieval strategies to boost your RAG system's performance (2024-12-16)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ericmjl.github.io/blog/2024/12/20/accurately-extract-text-from-research-literature-pdfs-with-nougat-ocr-and-docling/"&gt;Accurately extract text from research literature PDFs with Nougat-OCR and Docling (2024-12-20)&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="blog-posts"&gt;Blog Posts&lt;/h2&gt;&lt;p&gt;And for completeness, here's a list of all the blog posts I've written up till 29 December 2024.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Date&lt;/th&gt;
&lt;th&gt;Title&lt;/th&gt;
&lt;th&gt;Categories&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;2024-01-10&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/1/10/evolving-llamabot/"&gt;Evolving LlamaBot&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-01-11&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/1/11/github-actions-secrets-need-to-be-explicitly-declared/"&gt;GitHub Actions secrets need to be explicitly declared&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Tooling, Career Advice&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-01-15&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/1/15/your-embedding-model-can-be-different-from-your-text-generation-model/"&gt;Your embedding model can be different from your text generation model&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-01-28&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/1/28/exploratory-data-analysis-isnt-open-ended/"&gt;Exploratory data analysis isn’t open-ended&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-02-01&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/1/an-incomplete-and-opinionated-survey-of-llm-tooling/"&gt;An (incomplete and opinionated) survey of LLM tooling&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-02-07&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/7/success-factors-for-data-science-teams-in-biotech/"&gt;Success Factors for Data Science Teams in Biotech&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Data Science Tooling, Biology &amp;amp; Chemistry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-02-18&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/18/dashboard-ready-data-is-often-machine-learning-ready-data/"&gt;Dashboard-ready data is often machine learning-ready data&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Data Science Tooling, Biology &amp;amp; Chemistry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-02-21&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/21/llamabot-with-ollama-on-my-home-virtual-private-network/"&gt;LlamaBot with Ollama on my home virtual private network&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-02-25&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/25/how-to-keep-sharp-with-technical-skills-as-a-data-science-team-lead/"&gt;How to keep sharp with technical skills as a data science team lead&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Career Advice&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-02-29&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/2/29/your-first-90-days-at-work-what-should-you-do/"&gt;Your first 90 days at work - what should you do?&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Career Advice&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-03-09&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/3/9/industry-career-panel-for-phd-students-and-post-docs-at-mit/"&gt;From Academia to Industry: Career Advice from MIT Industry Careers Panel&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Career Advice&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-03-10&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/3/10/mixtral-8x7b-instruct-works-on-an-old-gtx1080/"&gt;Mixtral-8x7b-Instruct works on an old GTX1080!&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling, Biology &amp;amp; Chemistry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-03-23&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/3/23/how-to-organize-and-motivate-a-biotech-data-science-team/"&gt;How to organize and motivate a biotech data science team&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Career Advice&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-03-24&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/3/24/llamabot-040-released/"&gt;Llamabot 0.4.0 Released!&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-04-05&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/4/5/how-to-grow-software-development-skills-in-a-data-science-team/"&gt;How to grow software development skills in a data science team&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-04-07&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/4/7/pyds-cli-version-040-released/"&gt;pyds-cli version 0.4.0 released!&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-04-09&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/4/9/how-to-make-distributable-pre-commit-hooks/"&gt;How to make distributable pre-commit hooks&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-04-17&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/4/17/how-llms-can-accelerate-data-science/"&gt;How LLMs can accelerate data science&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Practice &amp;amp; Leadership, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-05-05&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/5/5/data-science-in-the-biotech-research-organization/"&gt;Data Science in the Biotech Research Organization&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Biology &amp;amp; Chemistry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-05-12&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/5/12/paper-review-design-of-highly-functional-genome-editors-by-modeling-the-universe-of-crispr-cas-sequences/"&gt;Paper Review: Design of highly functional genome editors by modeling the universe of CRISPR-Cas sequences&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Biology &amp;amp; Chemistry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-05-16&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/5/16/how-to-control-pymol-from-jupyter-notebooks/"&gt;How to control PyMOL from Jupyter notebooks&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling, Biology &amp;amp; Chemistry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-05-27&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/5/27/multi-modality-deep-learning/"&gt;Multi-modality Deep Learning&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling, Biology &amp;amp; Chemistry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-06-01&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/6/1/how-to-manage-cuda-libraries-within-conda-environments/"&gt;How to manage CUDA libraries within Conda environments&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-06-08&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/6/8/the-neural-von-mises-mixture-model/"&gt;The Neural Von Mises Mixture Model&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling, Biology &amp;amp; Chemistry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-06-18&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/6/18/headache-free-portable-and-reproducible-handling-of-data-access-and-versioning/"&gt;Headache-free, portable, and reproducible handling of data access and versioning&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-06-26&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/6/26/hire-for-communication-skills-not-conversational-skills/"&gt;Hire for communication skills, not conversational skills&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Career Advice&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-06-30&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/6/30/two-years-of-docathons-insights-and-lessons-learned/"&gt;Two years of docathons: Insights and lessons learned&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-07-02&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/7/2/use-native-formats-when-storing-data/"&gt;Use native formats when storing data&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-07-14&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/7/14/conference-report-scipy-2024/"&gt;Conference report: SciPy 2024&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Data Science Tooling, Career Advice&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-07-26&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/7/26/a-survey-of-how-to-use-protein-language-models-for-protein-design-part-1/"&gt;A survey of how to use protein language models for protein design: Part 1&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Biology &amp;amp; Chemistry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-08-02&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/8/2/a-survey-of-how-to-use-protein-language-models-for-protein-design-part-2/"&gt;A survey of how to use protein language models for protein design: Part 2&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Biology &amp;amp; Chemistry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-08-09&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/8/9/a-survey-of-how-to-use-protein-language-models-for-protein-design-part-3/"&gt;A survey of how to use protein language models for protein design: Part 3&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Biology &amp;amp; Chemistry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-08-16&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/8/16/its-time-to-try-out-pixi/"&gt;It's time to try out pixi!&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-08-25&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/8/25/dissecting-the-esm3-model-architecture/"&gt;Dissecting the ESM3 Model Architecture&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-08-31&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/8/31/llamabot-now-has-structuredbot/"&gt;LlamaBot now has StructuredBot!&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-09-06&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/9/6/on-writing-llm-evals-in-pytest/"&gt;On writing LLM evals in pytest&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-09-14&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/9/14/cursor-is-a-jetpack-for-coders/"&gt;Cursor is a jetpack for coders&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-09-15&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/9/15/sync-github-secrets-with-your-env-and-gh-cli/"&gt;Sync GitHub secrets with your .env and gh CLI&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-09-19&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/9/19/how-to-set-up-pixi-with-codeartifacts/"&gt;How to set up Pixi with CodeArtifacts&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-09-23&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/9/23/recreating-shortwhale-with-ai-assisted-coding/"&gt;Recreating Shortwhale with AI-Assisted Coding&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-09-27&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/9/27/explain-your-jupyter-notebooks-using-llamabot/"&gt;Explain your Jupyter notebooks using LlamaBot&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-10-06&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/6/building-pigeon-secure-notes-in-under-15-minutes-of-coding/"&gt;Building Pigeon Secure Notes in under 15 minutes of coding&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-10-09&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/9/what-brings-you-joy-at-work/"&gt;What brings you joy at work?&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Data Science Tooling, Career Advice&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-10-18&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/18/keys-to-effective-collaborative-data-science/"&gt;Keys to effective collaborative data science&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-10-20&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/20/cursor-did-a-one-shot-rewrite-of-a-panel-app-i-built/"&gt;Cursor did a one-shot rewrite of a Panel app I built&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-10-25&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/10/25/the-human-dimension-to-clean-distributable-and-documented-data-science-code/"&gt;The Human Dimension to Clean, Distributable, and Documented Data Science Code&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-11-02&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/11/2/introducing-new-local-llamabot-logging-features/"&gt;Introducing new (local) LlamaBot logging features&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-11-08&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/11/8/disposable-environments-for-ad-hoc-analyses/"&gt;Disposable environments for ad-hoc analyses&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-11-14&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/11/14/deploying-ollama-on-modal/"&gt;Deploying Ollama on Modal&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-11-22&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/11/22/a-modest-proposal-for-data-catalogues-at-biotechs/"&gt;A modest proposal for data catalogues at biotechs&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Data Science Practice &amp;amp; Leadership, Data Science Tooling, Biology &amp;amp; Chemistry&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-12-15&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/12/15/how-llamabots-new-agent-features-simplify-complex-task-automation/"&gt;How LlamaBot's new agent features simplify complex task automation&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-12-16&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/12/16/5-retrieval-strategies-to-boost-your-rag-systems-performance/"&gt;5 retrieval strategies to boost your RAG system's performance&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-12-17&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/12/17/how-to-thrive-and-not-just-survive-during-organizational-change/"&gt;How to thrive, and not just survive, during organizational change&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;Career Advice&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;2024-12-20&lt;/td&gt;
&lt;td&gt;&lt;a href="https://ericmjl.github.io/blog/2024/12/20/accurately-extract-text-from-research-literature-pdfs-with-nougat-ocr-and-docling/"&gt;Accurately extract text from research literature PDFs with Nougat-OCR and Docling&lt;/a&gt;&lt;/td&gt;
&lt;td&gt;LLMs, Data Science Tooling&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</content></entry><entry><title>Accurately extract text from research literature PDFs with Nougat-OCR and Docling</title><link href="https://ericmjl.github.io/blog/2024/12/20/accurately-extract-text-from-research-literature-pdfs-with-nougat-ocr-and-docling/" rel="alternate"/><updated>2024-12-20T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:796a57f6-f7d7-3ffd-8210-95464f6f88ab</id><content type="html">&lt;p&gt;Parsing published literature into plain text is a task that seems deceptively simple.
In reality, PDFs can be notoriously difficult to work with,
especially when they include elements like equations, tables, and figures.
If you're working with large language models (LLMs)
or just trying to extract data for analysis,
the standard text extraction tools often leave
significant amounts of useful context behind.
Recently, I explored two tools
&lt;strong&gt;Nougat-OCR&lt;/strong&gt; by Facebook Research and &lt;strong&gt;Docling&lt;/strong&gt; by IBM
to address this problem more effectively.&lt;/p&gt;
&lt;h2 id="the-problem-what-vanilla-tools-miss"&gt;The problem: What vanilla tools miss&lt;/h2&gt;&lt;p&gt;Standard methods for extracting text from PDFs
often work well for plain paragraphs
but stumble when it comes to three critical areas:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Equations&lt;/strong&gt;: PDFs store equations as images or special font renderings, making it challenging to extract them accurately as structured text.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Tables&lt;/strong&gt;: Extracting tables often results in misaligned columns or garbled data, losing the relationships between rows and columns.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Figures&lt;/strong&gt;: Figures and diagrams frequently get ignored or reduced to low-quality placeholders, stripping away valuable visual context.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;Given these challenges, I wanted to find tools that could improve text extraction for equations, tables, and figures. Here’s what I found.&lt;/p&gt;
&lt;h2 id="nougat-ocr-accurate-equation-and-table-extraction"&gt;Nougat-OCR: Accurate equation and table extraction&lt;/h2&gt;&lt;p&gt;&lt;a href="https://github.com/facebookresearch/nougat"&gt;Nougat-OCR&lt;/a&gt;
is a tool developed by Facebook Research
that focuses on converting scientific PDFs into structured text,
including support for equations and tables.
Its installation and usage are straightforward.&lt;/p&gt;
&lt;h3 id="installation"&gt;Installation&lt;/h3&gt;&lt;p&gt;I used the &lt;code&gt;uv&lt;/code&gt; package manager to set up Nougat-OCR:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;uv&lt;span class="w"&gt; &lt;/span&gt;tool&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;nougat-ocr&lt;span class="w"&gt; &lt;/span&gt;--python&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;.12&lt;span class="w"&gt; &lt;/span&gt;--with&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nv"&gt;transformers&lt;/span&gt;&lt;span class="o"&gt;==&lt;/span&gt;&lt;span class="m"&gt;4&lt;/span&gt;.38.2
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Once installed, the &lt;code&gt;nougat&lt;/code&gt; command becomes available on your system PATH.&lt;/p&gt;
&lt;h3 id="usage"&gt;Usage&lt;/h3&gt;&lt;p&gt;To extract text from a PDF, run the following command:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;nougat&lt;span class="w"&gt; &lt;/span&gt;data/curve-sim.pdf&lt;span class="w"&gt; &lt;/span&gt;&amp;gt;&lt;span class="w"&gt; &lt;/span&gt;data/curve-sim.mmd
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This processes the PDF and redirects the extracted text into a Markdown file.&lt;/p&gt;
&lt;h3 id="key-strengths"&gt;Key strengths&lt;/h3&gt;&lt;p&gt;Nougat-OCR handles equations and tables impressively well.
For example,
consider this equation from the paper
&lt;em&gt;A curve similarity approach to parallelism testing in bioassay&lt;/em&gt;:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="sb"&gt;\[&lt;/span&gt;&lt;span class="nb"&gt;f&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nv"&gt;\theta&lt;/span&gt;&lt;span class="nb"&gt;_{i},x&lt;/span&gt;&lt;span class="o"&gt;)=&lt;/span&gt;&lt;span class="nb"&gt;a_{i}&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nv"&gt;\frac&lt;/span&gt;&lt;span class="nb"&gt;{&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;b_{i}&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nb"&gt;a_{i}&lt;/span&gt;&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="nb"&gt;}{&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="o"&gt;+&lt;/span&gt;&lt;span class="nv"&gt;\exp\{&lt;/span&gt;&lt;span class="nb"&gt;d_{i}&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;x&lt;/span&gt;&lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="nv"&gt;\log&lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;c_{i}&lt;/span&gt;&lt;span class="o"&gt;))&lt;/span&gt;&lt;span class="nv"&gt;\}&lt;/span&gt;&lt;span class="nb"&gt;}&lt;/span&gt;&lt;span class="nv"&gt;\,&lt;/span&gt;&lt;span class="nb"&gt;. &lt;/span&gt;&lt;span class="nv"&gt;\tag&lt;/span&gt;&lt;span class="nb"&gt;{&lt;/span&gt;&lt;span class="m"&gt;1&lt;/span&gt;&lt;span class="nb"&gt;}&lt;/span&gt;&lt;span class="s"&gt;\]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;It also processes tables cleanly. Here’s an example table:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;\begin&lt;/span&gt;&lt;span class="nb"&gt;{&lt;/span&gt;table&lt;span class="nb"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;\begin&lt;/span&gt;&lt;span class="nb"&gt;{&lt;/span&gt;tabular&lt;span class="nb"&gt;}{&lt;/span&gt;l c c c c c c&lt;span class="nb"&gt;}&lt;/span&gt; &lt;span class="k"&gt;\hline&lt;/span&gt; &lt;span class="k"&gt;\hline&lt;/span&gt;
&lt;span class="nb"&gt;&amp;amp;&lt;/span&gt; &lt;span class="k"&gt;\multicolumn&lt;/span&gt;&lt;span class="nb"&gt;{&lt;/span&gt;3&lt;span class="nb"&gt;}{&lt;/span&gt;c&lt;span class="nb"&gt;}{&lt;/span&gt;Reference&lt;span class="nb"&gt;}&lt;/span&gt; &lt;span class="nb"&gt;&amp;amp;&lt;/span&gt; &lt;span class="k"&gt;\multicolumn&lt;/span&gt;&lt;span class="nb"&gt;{&lt;/span&gt;3&lt;span class="nb"&gt;}{&lt;/span&gt;c&lt;span class="nb"&gt;}{&lt;/span&gt;Sample&lt;span class="nb"&gt;}&lt;/span&gt; &lt;span class="k"&gt;\\&lt;/span&gt; &lt;span class="k"&gt;\cline&lt;/span&gt;&lt;span class="nb"&gt;{&lt;/span&gt;2-7&lt;span class="nb"&gt;}&lt;/span&gt;
Concentration &lt;span class="nb"&gt;&amp;amp;&lt;/span&gt; 1 &lt;span class="nb"&gt;&amp;amp;&lt;/span&gt; 2 &lt;span class="nb"&gt;&amp;amp;&lt;/span&gt; 3 &lt;span class="nb"&gt;&amp;amp;&lt;/span&gt; 1 &lt;span class="nb"&gt;&amp;amp;&lt;/span&gt; 2 &lt;span class="nb"&gt;&amp;amp;&lt;/span&gt; 3 &lt;span class="k"&gt;\\&lt;/span&gt;
&lt;span class="k"&gt;\hline&lt;/span&gt;
125,000 &lt;span class="nb"&gt;&amp;amp;&lt;/span&gt; 2.086879 &lt;span class="nb"&gt;&amp;amp;&lt;/span&gt; 2.119145 &lt;span class="nb"&gt;&amp;amp;&lt;/span&gt; 2.273702 &lt;span class="nb"&gt;&amp;amp;&lt;/span&gt; 1.524275 &lt;span class="nb"&gt;&amp;amp;&lt;/span&gt; 1.438422 &lt;span class="nb"&gt;&amp;amp;&lt;/span&gt; 1.563780 &lt;span class="k"&gt;\\&lt;/span&gt;
... (truncated for brevity) ...
&lt;span class="k"&gt;\hline&lt;/span&gt; &lt;span class="k"&gt;\hline&lt;/span&gt;
&lt;span class="k"&gt;\end&lt;/span&gt;&lt;span class="nb"&gt;{&lt;/span&gt;tabular&lt;span class="nb"&gt;}&lt;/span&gt;
&lt;span class="k"&gt;\end&lt;/span&gt;&lt;span class="nb"&gt;{&lt;/span&gt;table&lt;span class="nb"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This table is extracted with alignment preserved,
making it ideal for further analysis.
However,
&lt;strong&gt;Nougat-OCR does not perform well with figures.&lt;/strong&gt;&lt;/p&gt;
&lt;h2 id="docling-extracting-figures-accurately"&gt;Docling: Extracting figures accurately&lt;/h2&gt;&lt;p&gt;For figures,
I turned to &lt;strong&gt;Docling&lt;/strong&gt; by IBM.
While Nougat-OCR shines at text-based elements like equations and tables,
Docling focuses on images and visual components.&lt;/p&gt;
&lt;h3 id="installation"&gt;Installation&lt;/h3&gt;&lt;p&gt;Like Nougat-OCR,
Docling can be installed with &lt;code&gt;uv&lt;/code&gt;:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;uv&lt;span class="w"&gt; &lt;/span&gt;tool&lt;span class="w"&gt; &lt;/span&gt;install&lt;span class="w"&gt; &lt;/span&gt;docling&lt;span class="w"&gt; &lt;/span&gt;--python&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;3&lt;/span&gt;.12
&lt;/pre&gt;&lt;/div&gt;
&lt;h3 id="usage"&gt;Usage&lt;/h3&gt;&lt;p&gt;To extract images from a PDF, run:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docling&lt;span class="w"&gt; &lt;/span&gt;data/curve-sim.pdf&lt;span class="w"&gt; &lt;/span&gt;&amp;gt;&lt;span class="w"&gt; &lt;/span&gt;data/curve-sim-figures.md
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Docling processes the PDF and outputs images as base64-encoded strings embedded in a Markdown file. For example:&lt;/p&gt;
&lt;p&gt;&lt;img src="data:image/png;base64,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" alt="Figure 1"&gt;&lt;/p&gt;
&lt;h3 id="key-strengths"&gt;Key strengths&lt;/h3&gt;&lt;p&gt;Docling faithfully extracts all figures and encodes them as reusable base64 strings, which can then be passed to a multimodal LLM for description or analysis.&lt;/p&gt;
&lt;h3 id="limitations"&gt;Limitations&lt;/h3&gt;&lt;p&gt;Docling does not handle equations well. For example, the earlier equation:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;f(&lt;span class="k"&gt;\theta_&lt;/span&gt;&lt;span class="nb"&gt;{&lt;/span&gt;i&lt;span class="nb"&gt;}&lt;/span&gt;,x)=a&lt;span class="nb"&gt;_{&lt;/span&gt;i&lt;span class="nb"&gt;}&lt;/span&gt;+...
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;was rendered incorrectly as:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;f θ i ; x ð Þ ¼ ai þ bi /C0 ai ð Þ 1 þ exp di x /C0 log ci ð Þ ð Þ f g : (1)
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id="combining-nougat-ocr-and-docling-a-complete-workflow"&gt;Combining Nougat-OCR and Docling: A complete workflow&lt;/h2&gt;&lt;p&gt;To get the best of both tools, I used a multi-step workflow:&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Extract text-based elements (equations, tables)&lt;/strong&gt; with Nougat-OCR:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;nougat&lt;span class="w"&gt; &lt;/span&gt;data/curve-sim.pdf&lt;span class="w"&gt; &lt;/span&gt;&amp;gt;&lt;span class="w"&gt; &lt;/span&gt;data/curve-sim.mmd
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Extract figures&lt;/strong&gt; with Docling:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;docling&lt;span class="w"&gt; &lt;/span&gt;data/curve-sim.pdf
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;Process figures with a multimodal LLM&lt;/strong&gt; like LlamaBot:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;base64&lt;/span&gt;
&lt;span class="n"&gt;image&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;base64&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;decode&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;img_string&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;description&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SimpleBot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;user&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;image&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;description&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;This approach ensures that you capture all critical components of a PDF—
equations, tables, and figures—
with minimal loss of context.&lt;/p&gt;
&lt;h2 id="next-steps-deploying-on-modal-for-scalable-preprocessing"&gt;Next steps: Deploying on Modal for scalable preprocessing&lt;/h2&gt;&lt;p&gt;Both Nougat-OCR and Docling benefit significantly from GPU acceleration,
especially when processing large volumes of PDFs.
To make this workflow more scalable and accessible,
my next step is to deploy these tools on &lt;a href="https://modal.com/"&gt;Modal&lt;/a&gt;,
a serverless platform that supports GPU-based processing.
By deploying Nougat and Docling as APIs on Modal,
I can:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Preprocess PDFs on-demand&lt;/strong&gt;: Use simple API calls to trigger text and figure extraction.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Leverage GPUs for performance&lt;/strong&gt;: Accelerate processing for large or complex PDFs.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Integrate with existing workflows&lt;/strong&gt;: Seamlessly use these APIs in multimodal LLM pipelines or downstream analysis.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This deployment will allow me to scale preprocessing tasks effortlessly
and unlock the full potential of structured PDF data.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;Parsing PDFs into structured plain text is more than just a convenience;
it's a necessity when working with LLMs or conducting scientific analysis.
By combining Nougat-OCR for text-based elements and Docling for visual content,
you can extract high-quality data from published literature.&lt;/p&gt;
&lt;p&gt;To make this solution scalable,
deploying these tools on Modal with GPU support
will ensure rapid, on-demand preprocessing through simple API calls.
This workflow allows you to retain equations, tables, and figures,
ensuring that no valuable knowledge is left behind.
As tools like Nougat-OCR and Docling continue to improve,
so too will our ability to make sense of complex, multimodal content.&lt;/p&gt;
</content></entry><entry><title>How to thrive, and not just survive, during organizational change</title><link href="https://ericmjl.github.io/blog/2024/12/17/how-to-thrive-and-not-just-survive-during-organizational-change/" rel="alternate"/><updated>2024-12-17T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:3afe2c35-44e4-317e-a5f6-6c39e6d9ed99</id><content type="html">&lt;p&gt;Organizational change can feel overwhelming, but with the right mindset and strategies, you can thrive—even in times of uncertainty. Whether it’s a restructuring, a leadership shift, or new priorities, the way you navigate change will shape your career trajectory. Having recently gone through a year of multiple larger-scale changes, I've had some thoughts that I wanted to share. Here are some practical insights to help you adapt and grow.&lt;/p&gt;
&lt;h2 id="focus-on-what-matters-your-colleagues"&gt;Focus on what matters: your colleagues&lt;/h2&gt;&lt;p&gt;We do our best work for the colleagues we care about. When things feel chaotic, lean into this mindset: block out the noise, and focus on the relationships that matter.&lt;/p&gt;
&lt;p&gt;Here’s the reality: companies are bound by rules and regulations that often prevent them from fully prioritizing your well-being. These can be things like accounting timelines and investor pressure, things that are entirely out of your control. Your colleagues, on the other hand, can be your greatest allies. By investing in these relationships, you’ll create a network of support that will carry you through tough times.&lt;/p&gt;
&lt;h2 id="keep-shipping-great-work"&gt;Keep shipping great work&lt;/h2&gt;&lt;p&gt;Amid change, the best way to stand out is by continuing to deliver. Roll up your sleeves and keep shipping awesome work. Consistently delight your customers, whether internal or external.&lt;/p&gt;
&lt;p&gt;Here’s why: people notice. When you show that you’re steady and reliable, even in uncertain times, you’ll earn respect and trust. And that kind of reputation can open doors—both within your company and beyond.&lt;/p&gt;
&lt;p&gt;I think we have something to learn from President Obama:&lt;/p&gt;
&lt;iframe width="560" height="315" src="https://www.youtube.com/embed/YNY4UFaHbP4?si=_tkyjLX25IWCfF0L" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen&gt;&lt;/iframe&gt;&lt;h2 id="build-your-own-optionality"&gt;Build your own optionality&lt;/h2&gt;&lt;p&gt;Right from day one of your job, don’t wait for your manager to carve a path for you. Be proactive:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Build relationships across teams and departments.&lt;/li&gt;
&lt;li&gt;Stay visible by sharing your work and offering to help others.&lt;/li&gt;
&lt;li&gt;Keep nurturing your external network—opportunities often come from unexpected places.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;By taking charge of your own opportunities, you’ll have more options to pivot when changes arise.&lt;/p&gt;
&lt;h2 id="stay-grounded-in-the-present"&gt;Stay grounded in the present&lt;/h2&gt;&lt;p&gt;“Do not worry about tomorrow, for tomorrow will worry about itself.”&lt;/p&gt;
&lt;p&gt;If this quote resonates with you, you already know its source—and if not, a quick search will uncover its wisdom. The message is simple: don’t get paralyzed by what-ifs. Focus on what you can control today. By staying present and calm, you’ll be better equipped to handle whatever comes next.&lt;/p&gt;
&lt;h2 id="think-long-term-cultivate-your-career-committee"&gt;Think long-term: cultivate your "career committee"&lt;/h2&gt;&lt;p&gt;One of the best ways to navigate change is to have a trusted group of advisors guiding you. Think of it like a PhD committee—but for your career:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Identify 3–4 people whose opinions you trust.&lt;/li&gt;
&lt;li&gt;These could be mentors, peers, or leaders who have a clear view of the organization.&lt;/li&gt;
&lt;li&gt;Check in with them periodically to get advice, stay aligned, and anticipate changes before they happen.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Having this “career committee” will help you make more informed decisions and ensure you stay on track, even as the organization evolves.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;Organizations are made in the image of their leaders. Which means there is no right or wrong organizational structure ever, there's just the one one that works for the leader in charge. When leadership changes, organizations will change. This is a fact of work life, and the earlier that we are accustomed to this fact, the better.&lt;/p&gt;
&lt;p&gt;Change is inevitable, but how you respond to it is what sets you apart. By focusing on relationships, delivering great work, and staying proactive about your career, you can turn organizational shifts into opportunities for growth!&lt;/p&gt;
</content></entry><entry><title>5 retrieval strategies to boost your RAG system's performance</title><link href="https://ericmjl.github.io/blog/2024/12/16/5-retrieval-strategies-to-boost-your-rag-systems-performance/" rel="alternate"/><updated>2024-12-16T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:894784fc-3c9a-36a7-944f-344f2ad23942</id><content type="html">&lt;p&gt;&lt;!-- Include Mermaid.js --&gt;&lt;/p&gt;
&lt;script type="module"&gt;
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs';
    mermaid.initialize({ startOnLoad: true });
&lt;/script&gt;&lt;p&gt;Retrieval-Augmented Generation (RAG) is gaining traction
as a powerful paradigm for large language models (LLMs).&lt;/p&gt;
&lt;p&gt;At its core,
RAG combines two key processes:
retrieval,
which fetches relevant information from an external source,
and generation,
where the model uses that information to produce a response.&lt;/p&gt;
&lt;p&gt;This allows systems to provide more accurate,
context-aware answers beyond what the model was trained on.&lt;/p&gt;
&lt;p&gt;In Jason Liu's blog post,
&lt;a href="https://jxnl.co/writing/2023/09/17/rag-is-more-than-embeddings/"&gt;&lt;em&gt;RAG is more than just embedding search&lt;/em&gt;&lt;/a&gt;,
he makes a compelling argument:
Retrieval in RAG systems goes far beyond vector similarity searches.&lt;/p&gt;
&lt;p&gt;Expanding on this idea,
we'll explore a basic ontology of retrieval methods,
breaking down their features,
use cases,
and practical examples.&lt;/p&gt;
&lt;h2 id="methods-of-retrieval"&gt;Methods of retrieval&lt;/h2&gt;&lt;p&gt;Retrieval methods in RAG can be broadly categorized based on their level of automation and how they interact with data. Below is a table summarizing the key approaches:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;&lt;tr&gt;
&lt;th&gt;Method Name&lt;/th&gt;
&lt;th&gt;Automation&lt;/th&gt;
&lt;th&gt;Example&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Human-Curated&lt;/td&gt;
&lt;td&gt;Manual&lt;/td&gt;
&lt;td&gt;Humans copy/paste known context into ChatGPT’s chat box, which is then used to generate a response. Lack of automation makes this non-scalable&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Exact Keyword Search&lt;/td&gt;
&lt;td&gt;Automated&lt;/td&gt;
&lt;td&gt;Use a bot to extract or enhance a query with keywords, which are in turn used to retrieve texts with exact keyword matching.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fuzzy Keyword Search&lt;/td&gt;
&lt;td&gt;Automated&lt;/td&gt;
&lt;td&gt;Using keywords, perform fuzzy text search of each term to identify which texts to return.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Vector Similarity Search&lt;/td&gt;
&lt;td&gt;Automated&lt;/td&gt;
&lt;td&gt;Using the full-length query, or enhanced versions of the query, identify either the top K text chunks with embedding vector similarity or those above a threshold.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Knowledge Graph-based Retrieval&lt;/td&gt;
&lt;td&gt;Automated&lt;/td&gt;
&lt;td&gt;Using information on a knowledge graph, retrieve text chunks based on graph structure, statistics, or information.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Each method has unique strengths and weaknesses, and understanding these nuances is key to designing an effective RAG system.&lt;/p&gt;
&lt;h2 id="deep-dive-into-retrieval-methods"&gt;Deep dive into retrieval methods&lt;/h2&gt;&lt;h3 id="human-curated"&gt;Human-Curated&lt;/h3&gt;&lt;p&gt;Human-curated retrieval involves manually gathering the necessary context for a query.
The user selects specific information from a source,
such as copying a definition or relevant text,
and supplies it directly to the system.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: A user researching quantum mechanics copies a specific definition of "quantum entanglement" into ChatGPT
before asking follow-up questions.
The context is manually curated
and acts as the retrieval mechanism.&lt;/p&gt;
&lt;div class="mermaid"&gt;
graph TD;
    User[User Curates Context] --&gt; ChatGPT[ChatGPT];
    ChatGPT --&gt; Response[Generate Response Based on Context];
&lt;/div&gt;&lt;p&gt;LlamaBot's documentation writer uses this in the form of pointers to source code:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nn"&gt;---&lt;/span&gt;
&lt;span class="nt"&gt;diataxis_type&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;one of &amp;quot;tutorial&amp;quot;, &amp;quot;explanation&amp;quot;, &amp;quot;reference&amp;quot;, or &amp;quot;how-to guide&amp;quot;&lt;/span&gt;
&lt;span class="w"&gt;  &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;triggers retrieval of diataxis&amp;#39;s explainer regarding each.&lt;/span&gt;
&lt;span class="nt"&gt;intents&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;&amp;lt;human prompts go here&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;linked_files&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="p p-Indicator"&gt;-&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="l l-Scalar l-Scalar-Plain"&gt;/path/to/file.ext (triggers automatic reading of such text)&lt;/span&gt;
&lt;span class="nn"&gt;---&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;When to Use&lt;/strong&gt;: This method excels in scenarios
where the user has domain knowledge
and can curate precise context manually.
However,
it doesn't scale well for larger datasets
or automated systems.&lt;/p&gt;
&lt;h3 id="exact-keyword-search"&gt;Exact Keyword Search&lt;/h3&gt;&lt;p&gt;Exact keyword search relies on identifying specific words or phrases in the query
and using them to retrieve matching documents.
It is a straightforward method
that works well for structured data
and predefined terminologies.&lt;/p&gt;
&lt;div class="mermaid"&gt;
graph TD;
    Query[User Query] --&gt; Keywords[Extract Keywords];
    Keywords --&gt; Database[Search Database];
    Database --&gt; Results[Exact Matches Returned];
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: A chatbot receives the query "Python error handling."
The system extracts the keywords "Python" and "error handling"
to retrieve matching documentation.&lt;/p&gt;
&lt;p&gt;Within LlamaBot,
you can stuff texts into a lightweight BM25DocStore
and perform keyword-based retrieval in memory:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;llamabot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;lmb&lt;/span&gt;

&lt;span class="n"&gt;docstore&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;BM25DocStore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text_chunks&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;retrieved&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;docstore&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;retrieve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;keyword&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;When to Use&lt;/strong&gt;: This is effective for structured,
well-tagged datasets
where precision is key.
However,
it can miss relevant results
if keywords are absent
or phrased differently.&lt;/p&gt;
&lt;h3 id="fuzzy-keyword-search"&gt;Fuzzy Keyword Search&lt;/h3&gt;&lt;p&gt;Fuzzy keyword search extends exact search
by allowing for minor variations,
typos,
or alternate phrasings in the query.
It's particularly useful
for dealing with noisy
or unstructured data.&lt;/p&gt;
&lt;div class="mermaid"&gt;
graph TD;
    Query[User Query] --&gt; FuzzyLogic[Apply Fuzzy Matching];
    FuzzyLogic --&gt; Database[Search Database];
    Database --&gt; Results[Fuzzy Matches Returned];
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: A user searches for "data cleaning" in a system,
and fuzzy search identifies relevant results
like "data preprocessing"
or "cleaning datasets."&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When to Use&lt;/strong&gt;: Useful for noisy
or unstructured datasets.
Fuzzy searches are tolerant to typos
and alternate phrasings
but may retrieve less precise results.&lt;/p&gt;
&lt;h3 id="vector-similarity-search"&gt;Vector Similarity Search&lt;/h3&gt;&lt;p&gt;Vector similarity search uses dense embeddings
to compare the semantic meaning of a query
with chunks of text.
This method is particularly popular in modern systems
because it captures semantic relationships
beyond exact keyword matches,
making it highly effective
for natural language understanding.
Unlike keyword-based methods,
vector search can retrieve contextually relevant information
even when phrasing varies significantly,
making it a critical tool
for handling ambiguous
or flexible queries.
This method identifies top matches
by calculating the similarity
between the query's embedding
and stored vectors in a database.&lt;/p&gt;
&lt;div class="mermaid"&gt;
graph TD;
    Query[User Query] --&gt; Embed[Generate Query Embedding];
    Embed --&gt; Compare[Compare with Vector Database];
    Compare --&gt; Results[Retrieve Top K Matches];
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: Given the query "Explain neural networks,"
the system generates a vector embedding of the query
and retrieves top K similar documents
based on cosine similarity.&lt;/p&gt;
&lt;p&gt;Within LlamaBot,
you can use LanceDB or ChromaDB locally
to store vector embeddings of text chunks.
After chunking:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;docstore&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LanceDBDocStore&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;text_chunks&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;retrieved&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;docstore&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;retrieve&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;query&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;When to Use&lt;/strong&gt;: This is ideal
for semantically rich,
large-scale datasets.
It captures nuanced relationships
but requires computationally expensive indexing
and may struggle with highly specific queries
or vague, open-ended prompts
like "tell me what's going on here,"
which lack semantic alignment
with specific text chunks.&lt;/p&gt;
&lt;h3 id="knowledge-graph-based-retrieval"&gt;Knowledge Graph-based Retrieval&lt;/h3&gt;&lt;p&gt;Knowledge graph-based retrieval
uses structured graphs
that represent relationships
between entities.
Queries trigger graph traversals
to fetch contextually connected information
based on the graph's structure.&lt;/p&gt;
&lt;div class="mermaid"&gt;
graph TD;
    Query[User Query] --&gt; Graph[Traverse Knowledge Graph];
    Graph --&gt; Entities[Fetch Related Entities];
    Entities --&gt; RelatedTexts[Retrieve Texts from Entities];
    RelatedTexts --&gt; Results[Output Retrieved Texts];
&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: A query about "Albert Einstein"
triggers traversal of a knowledge graph
to fetch connected entities
like "Theory of Relativity"
or "Nobel Prize 1921."&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;When to Use&lt;/strong&gt;: Best suited for structured datasets
where relationships between entities
are explicitly defined.
While powerful,
it requires the upfront construction of a knowledge graph,
which can be expensive.&lt;/p&gt;
&lt;h3 id="combining-retrieval-methods"&gt;Combining retrieval methods&lt;/h3&gt;&lt;div class="mermaid"&gt;
graph TD;
    Query[User Query] --&gt; KnowledgeGraph[Knowledge Graph Traversal];
    KnowledgeGraph --&gt; EnhancedQuery[Enhanced Query Generation];
    EnhancedQuery --&gt; VectorSearch[Vector Similarity Search];
    EnhancedQuery --&gt; FuzzyKeywordSearch[Fuzzy Keyword Search];
    EnhancedQuery --&gt; ExactKeywordSearch[Exact Keyword Search];
    VectorSearch --&gt; UnifiedResults[Unified Retrieved Results];
    FuzzyKeywordSearch --&gt; UnifiedResults[Unified Retrieved Results];
    ExactKeywordSearch --&gt; UnifiedResults[Unified Retrieved Results];
&lt;/div&gt;&lt;p&gt;In real-world scenarios,
a combination of retrieval methods
often provides the most robust solution.
However,
blending these methods
comes with its own challenges.
Managing computational costs
can become a concern,
especially when multiple methods
(e.g., vector search and graph traversal)
need to run sequentially or in parallel.
Prioritizing retrieved results
is another challenge,
as combining outputs
requires careful scoring or ranking
to ensure relevance
without overwhelming the LLM
with redundant information for generation.&lt;/p&gt;
&lt;p&gt;For example,
a system might:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Use a knowledge graph to identify related entities or terms to refine a vague query.&lt;/li&gt;
&lt;li&gt;Generate natural language queries enhanced with these terms for embedding-based vector similarity searches.&lt;/li&gt;
&lt;li&gt;Extract keywords from the enhanced query for fuzzy and exact keyword searches.&lt;/li&gt;
&lt;li&gt;Retrieve relevant texts from all methods and merge them into a unified set of results.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;Example&lt;/strong&gt;: Consider a user asking,
"Tell me what's going on here,"
with no clear context.
The system could use a knowledge graph
to expand this vague query,
identifying related concepts or entities.
For instance,
if the context involves neural networks,
the graph might suggest terms
like "gradient descent" or "backpropagation."
These terms could:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Form part of an enhanced natural language query, such as "How does gradient descent relate to neural networks?" for embedding-based searches.&lt;/li&gt;
&lt;li&gt;Generate keywords like "gradient," "neural," and "backpropagation" for exact or fuzzy keyword searches.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Results from all retrieval methods
would then be merged and ranked
to provide the most relevant set of texts
for answering the query.&lt;/p&gt;
&lt;p&gt;This layered approach
balances broad semantic understanding
with precision and contextual depth.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;Retrieval is the cornerstone of RAG systems,
and the choice of method
can profoundly impact performance
and user experience.
While vector similarity search
is a popular default,
it is not a one-size-fits-all solution.
Depending on the data structure,
query complexity,
and computational resources,
methods like fuzzy search,
exact keyword search,
or even manual curation
may be more appropriate.
Knowledge graph-based retrieval
adds another layer of sophistication
for specific use cases.&lt;/p&gt;
&lt;p&gt;Crucially, combining these methods can unlock even greater potential.
By layering techniques,
such as vector similarity,
keyword filtering,
and knowledge graph traversal,
you can design retrieval systems that are both broad and precise.
The key takeaway?
Stay flexible and adaptable with your retrieval strategies.
By combining methods like vector search,
keyword filtering,
and knowledge graph traversal,
you can create systems that are both precise and versatile.
Effective RAG design requires a nuanced understanding
of retrieval methods and their trade-offs.
By leveraging the right mix of strategies,
you can unlock the full potential
of retrieval-augmented generation.&lt;/p&gt;
</content></entry><entry><title>How LlamaBot's new agent features simplify complex task automation</title><link href="https://ericmjl.github.io/blog/2024/12/15/how-llamabots-new-agent-features-simplify-complex-task-automation/" rel="alternate"/><updated>2024-12-15T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:f9a23969-2fcc-38da-9f22-13e44f1c3705</id><content type="html">&lt;h2 id="a-pythonic-way-to-build-agents"&gt;A Pythonic way to build agents&lt;/h2&gt;&lt;p&gt;The concept of "agents" has been swirling in the AI space for a while now,
sometimes polarizing opinion.
Some see agents as the next logical step in automating complex tasks,
while others dismiss them as over-hyped,
glorified "just programming."
I believe the truth lies somewhere in between—agents are neither magical nor trivial.
Instead,
they're tools that,
when well-designed,
bring immense utility to the table.&lt;/p&gt;
&lt;p&gt;With LlamaBot's new agent capabilities,
I aimed to create an approachable yet powerful implementation for building agents.
This post dives into the core ideas that underpin these new features,
how they simplify agent construction,
and why I think they strike a balance between complexity and usability.&lt;/p&gt;
&lt;h2 id="what-makes-an-agent"&gt;What makes an agent?&lt;/h2&gt;&lt;p&gt;Let's start with the fundamentals.
What exactly is an agent?
My working definition is as follows:&lt;/p&gt;
&lt;p&gt;An &lt;strong&gt;LLM agent&lt;/strong&gt; is a system that demonstrates the &lt;em&gt;appearance&lt;/em&gt; of making goal-directed decisions autonomously.
It operates within a scoped set of tasks it can perform,
without requiring pre-specification of the order in which these tasks are executed.
Three attributes set it apart:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Goal-oriented non-determinism&lt;/strong&gt;: The agent makes decisions dynamically rather than following a pre-defined sequence of actions.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Decision-making flow control&lt;/strong&gt;: Decisions adapt based on context, resembling a "thinking" process.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Natural language interfaces&lt;/strong&gt;: Inputs and outputs are in plain language, making interactions feel intuitive.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;These attributes combine to give agents their apparent autonomy.
With these principles in mind,
I designed LlamaBot's &lt;code&gt;AgentBot&lt;/code&gt; to make creating and deploying agents simple,
flexible,
and powerful.&lt;/p&gt;
&lt;h2 id="the-heart-of-llamabot-s-new-agentbot"&gt;The heart of LlamaBot's new AgentBot&lt;/h2&gt;&lt;h3 id="decision-making-with-tools"&gt;Decision-making with tools&lt;/h3&gt;&lt;p&gt;At its core,
the &lt;code&gt;AgentBot&lt;/code&gt; orchestrates tools—Python functions annotated with the &lt;code&gt;@tool&lt;/code&gt; decorator.
Given a user prompt,
the bot selects which tools to use and in what order,
allowing for dynamic workflows.
This decision-making process is powered by an LLM
and is wrapped in a structured system prompt
that guides the agent toward making the right choices.
For example, consider the simple task of splitting a restaurant bill:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tool&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;calculate_total_with_tip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;bill_amount&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;tip_rate&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;bill_amount&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;tip_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nd"&gt;@lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tool&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;split_bill&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;total_amount&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_people&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;int&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;total_amount&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="n"&gt;num_people&lt;/span&gt;

&lt;span class="n"&gt;bot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AgentBot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;system_prompt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;system&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;You are my assistant with respect to restaurant bills.&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;functions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;calculate_total_with_tip&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;split_bill&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Here, the bot dynamically decides which tool to use based on the input.
For a prompt like &lt;em&gt;"My dinner was 2300 without tips. Calculate my total with an 18% tip and split the bill between 4 people,"&lt;/em&gt; the bot intelligently selects both &lt;code&gt;calculate_total_with_tip&lt;/code&gt; and &lt;code&gt;split_bill&lt;/code&gt;,
chaining them together to provide a complete answer:&lt;/p&gt;
&lt;blockquote&gt;&lt;p&gt;The total bill with an 18% tip is 2714.00,
and when split between 4 people,
each person should pay 678.50.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id="real-world-example-stock-market-analysis"&gt;Real-world example: Stock market analysis&lt;/h2&gt;&lt;p&gt;LlamaBot's new &lt;code&gt;AgentBot&lt;/code&gt; makes it easy to perform multi-step tasks like stock market analysis.
Here's an example where I created an agent to:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Scrape stock prices,&lt;/li&gt;
&lt;li&gt;Calculate percentiles,&lt;/li&gt;
&lt;li&gt;Detect outliers, and&lt;/li&gt;
&lt;li&gt;Summarize statistical insights.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;With this setup,
you can prompt the bot with natural language,
like: &lt;em&gt;"Analyze the last 100 days of MRNA stock prices"&lt;/em&gt;,
and receive detailed,
actionable insights in plain English.&lt;/p&gt;
&lt;h3 id="code-implementation"&gt;Code implementation&lt;/h3&gt;&lt;p&gt;Here's how the &lt;code&gt;AgentBot&lt;/code&gt; was constructed for this task:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;# Import necessary tools&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;llamabot&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;lmb&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;numpy&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;httpx&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;typing&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;Dict&lt;/span&gt;
&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;loguru&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;logger&lt;/span&gt;

&lt;span class="nd"&gt;@lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tool&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;scrape_stock_prices&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Scrape historical stock prices from Yahoo Finance API.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://query1.finance.yahoo.com/v8/finance/chart/&lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;range&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;100d&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;interval&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="s2"&gt;&amp;quot;1d&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;}&lt;/span&gt;
    &lt;span class="k"&gt;try&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="k"&gt;with&lt;/span&gt; &lt;span class="n"&gt;httpx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Client&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;client&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
            &lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;client&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
            &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;raise_for_status&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;json&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
            &lt;span class="n"&gt;prices&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;chart&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;result&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;indicators&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;quote&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;][&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;close&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
            &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;price&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;prices&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;price&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
    &lt;span class="k"&gt;except&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;httpx&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;HTTPError&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ne"&gt;KeyError&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="ne"&gt;IndexError&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="n"&gt;logger&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;error&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Error fetching stock data: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="k"&gt;raise&lt;/span&gt; &lt;span class="ne"&gt;Exception&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="sa"&gt;f&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;Failed to fetch data for &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;symbol&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;: &lt;/span&gt;&lt;span class="si"&gt;{&lt;/span&gt;&lt;span class="n"&gt;e&lt;/span&gt;&lt;span class="si"&gt;}&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="nd"&gt;@lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tool&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;calculate_percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Calculate the percentile value from a list of numbers.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;))&lt;/span&gt;

&lt;span class="nd"&gt;@lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tool&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;detect_outliers&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;],&lt;/span&gt; &lt;span class="n"&gt;threshold&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.5&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Detect outliers in data using the IQR method.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="n"&gt;q1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;25&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;q3&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;percentile&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mi"&gt;75&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;iqr&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q3&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;q1&lt;/span&gt;
    &lt;span class="n"&gt;lower_bound&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q1&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt; &lt;span class="n"&gt;threshold&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;iqr&lt;/span&gt;
    &lt;span class="n"&gt;upper_bound&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;q3&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;threshold&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="n"&gt;iqr&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;data&lt;/span&gt; &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="n"&gt;lower_bound&lt;/span&gt; &lt;span class="ow"&gt;or&lt;/span&gt; &lt;span class="n"&gt;x&lt;/span&gt; &lt;span class="o"&gt;&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;upper_bound&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;

&lt;span class="nd"&gt;@lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;tool&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;summarize_statistics&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;List&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt; &lt;span class="o"&gt;-&amp;gt;&lt;/span&gt; &lt;span class="n"&gt;Dict&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;]:&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Calculate basic statistical measures for a dataset.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;mean&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;mean&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;median&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;median&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;std&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;std&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;min&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;min&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
        &lt;span class="s2"&gt;&amp;quot;max&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;float&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;max&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;data&lt;/span&gt;&lt;span class="p"&gt;)),&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

&lt;span class="c1"&gt;# Create the stock analysis agent&lt;/span&gt;
&lt;span class="n"&gt;stats_bot&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;AgentBot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
    &lt;span class="n"&gt;system_prompt&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;lmb&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;system&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;You are a stock market analysis assistant. Help analyze stock price data&lt;/span&gt;
&lt;span class="sd"&gt;        by providing insights about their distribution, outliers, and basic statistics.&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
    &lt;span class="p"&gt;),&lt;/span&gt;
    &lt;span class="n"&gt;functions&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;
        &lt;span class="n"&gt;scrape_stock_prices&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;calculate_percentile&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;detect_outliers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="n"&gt;summarize_statistics&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;],&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# Using the agent for stock analysis&lt;/span&gt;
&lt;span class="n"&gt;response&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;stats_bot&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;
&lt;span class="w"&gt;    &lt;/span&gt;&lt;span class="sd"&gt;&amp;quot;&amp;quot;&amp;quot;Please analyze the last 100 days of MRNA stock prices:&lt;/span&gt;
&lt;span class="sd"&gt;    1. Scrape the price data&lt;/span&gt;
&lt;span class="sd"&gt;    2. Calculate the 90th percentile price&lt;/span&gt;
&lt;span class="sd"&gt;    3. Detect any price outliers&lt;/span&gt;
&lt;span class="sd"&gt;    4. Provide basic statistical summary&amp;quot;&amp;quot;&amp;quot;&lt;/span&gt;
&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;response&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;content&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;For the prompt &lt;em&gt;"Analyze the last 100 days of MRNA stock prices"&lt;/em&gt;, the bot responds:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;The analysis of the last 100 days of MRNA stock prices is complete. Here&amp;#39;s a summary of the findings:

1. **90th Percentile Price**: The 90th percentile price is approximately $90.38. This means that 90% of the closing prices over the last 100 days were below this value, indicating that prices above this are relatively rare.
2. **Outliers**: No outliers were detected in the data. This suggests that the price movements have been relatively stable without any extreme highs or lows that deviate significantly from the overall trend.
3. **Basic Statistical Summary**:
    - **Mean (Average) Price**: The average closing price was about $67.71.
    - **Median Price**: The median price, which is the middle value when all prices are sorted, was $63.93. This is slightly lower than the mean, indicating a few higher prices might be pulling the average up.
    - **Standard Deviation**: The standard deviation was $22.36, which measures the amount of variation or dispersion in the prices. A higher standard deviation indicates more variability in the stock price.
    - **Minimum Price**: The lowest closing price was $36.85.
    - **Maximum Price**: The highest closing price was $122.50.

Overall, the MRNA stock has shown some variability in its prices over the last 100 days, with a significant range between the minimum and maximum prices. However, the absence of outliers suggests that these price changes have been consistent with the general trend, without any unexpected spikes or drops.
&lt;/pre&gt;&lt;/div&gt;
&lt;h2 id="key-features-of-the-new-agentbot"&gt;Key features of the new AgentBot&lt;/h2&gt;&lt;p&gt;(a) &lt;strong&gt;Caching intermediate results&lt;/strong&gt;:The agent caches results like numerical arrays,
reducing redundant computation and saving time and cost.
Results are stored in a memory dictionary indexed by their SHA-256 hash keys,
ensuring efficient reuse of previously computed outcomes
and cost savings on output token usage.&lt;/p&gt;
&lt;p&gt;(b) &lt;strong&gt;Error handling as a first-class citizen&lt;/strong&gt;:
Errors are handled gracefully,
guiding users without derailing workflows.
The &lt;code&gt;return_error&lt;/code&gt; tool,
which is built-in to the agent,
allows the agent to identify issues explicitly,
providing developers with actionable feedback.&lt;/p&gt;
&lt;p&gt;(c) &lt;strong&gt;Dynamic tool selection&lt;/strong&gt;:
The agent adapts its actions based on context and task complexity.
It leverages a decision-making model
that determines the most relevant tools to call
and uses cached results dynamically where appropriate.&lt;/p&gt;
&lt;p&gt;(d) &lt;strong&gt;High-level APIs for streamlined usage&lt;/strong&gt;:
The API design ensures readability and ease of use.
By combining Pydantic models for structured inputs and outputs
with well-annotated Python functions,
developers can build powerful agents with minimal boilerplate.&lt;/p&gt;
&lt;h2 id="conclusion-why-agents-matter"&gt;Conclusion: Why agents matter&lt;/h2&gt;&lt;p&gt;LlamaBot's new agent capabilities offer developers
a practical toolset for managing complex workflows with ease.
Rather than reinventing the wheel,
&lt;code&gt;AgentBot&lt;/code&gt; focuses on integrating thoughtful design and simplicity,
helping users harness the strengths of existing frameworks
while tailoring solutions to specific needs.&lt;/p&gt;
&lt;p&gt;To explore these features further,
check out the &lt;a href="https://ericmjl.github.io/llamabot/tutorials/agentbot/"&gt;LlamaBot documentation&lt;/a&gt;.
Let's build something amazing together!&lt;/p&gt;
</content></entry><entry><title>A modest proposal for data catalogues at biotechs</title><link href="https://ericmjl.github.io/blog/2024/11/22/a-modest-proposal-for-data-catalogues-at-biotechs/" rel="alternate"/><updated>2024-11-22T00:00:00Z</updated><author><name>Eric J. Ma</name></author><id>urn:uuid:e0866ca3-1473-36a4-9b78-14bbf74f1efd</id><content type="html">&lt;h2 id="the-fatal-flaw"&gt;The fatal flaw&lt;/h2&gt;&lt;p&gt;Having now worked at two companies (Moderna and Novartis)
and discussed the matter with friends across the industry,
I have noticed a fatal flaw in data platform strategy
that I believe makes adoption very challenging from the beginning.&lt;/p&gt;
&lt;p&gt;Here's the flaw:
asking people to invest additional time in submitting data,
managing permissions to it,
cataloguing it,
and discovering it on a newly built system
&lt;em&gt;instead of leveraging the existing ways of working that people already have&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The build-out invariably looks like this:
a data platform team builds new software that sits in the cloud.
The product team, being product-driven, starts asking for "use cases".
The identified customers, i.e. the bench scientists, aren't exactly thrilled,
even if they understand the importance.
There's an eye of skepticism.
Your PhD-trained scientists, who may or may not necessarily be computationally-savvy,
give you that look that communicates one or more of the following questions:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Storage:&lt;/strong&gt; Now where do I actually store my data? Do I still get to keep it in SharePoint/OneDrive/Dropbox? Or is it now on your platform, in AWS/S3, where I don't get to have control over it?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Permissions:&lt;/strong&gt; Where do I manage permissions? On your new data platform? Why do I need to learn how to do that, when I can just use SharePoint/OneDrive/Dropbox? Do I really have to learn AWS?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Cataloguing:&lt;/strong&gt; Sure, I get the idea of "discoverability" and "reuse" and "FAIR" principles, but what benefit does it give me today? If someone else needs to ask me for data, won't it just increase my burden as an access gatekeeper?&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Discovering:&lt;/strong&gt; Yeah, okay, so I can go to your platform, or I can just ask my social network of people at the company, right? Are you saying you're building a search engine for data across the company?&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The team looks at those questions,
but being digital folks with an incentive to build a product,
they don't exactly have a satisfactory answer for each question.
The team instead sets success criteria that doesn't exactly resonate with the customers:
data are migrated from existing system to shiny new data platform,
and computational scientists can access it.
The team then does a lift and shift of some bulk RNAseq data
from three experiments that were run before.
Metrics are calculated:
"We moved hundreds of gigabytes of data off of on-prem storage into the cloud,
saving the company $X amount over three years!"
The data platform team calls this a success and moves onto the next use case.
No new RNAseq data are uploaded to the shiny new data platform because,
well, the sequencing core was just overwhelmed with other work
and didn't want the additional burden of cataloging and tracing where data came from,
and just wanted to have automation that piped RNAseq data back to their collaborators
for downstream analyses.&lt;/p&gt;
&lt;p&gt;So herein lies a massive problem with this approach:
How are you going to get buy-in for data cataloguing
if the data catalogue you build doesn't actually benefit your customer?
Your "buy-in" is going to be half-baked.
Focusing on the laboratory scientist as the customer,
but asking them to do additional work,
is laying the groundwork for building a product that allows individuals to fail upwards.
And that's just a waste of money.&lt;/p&gt;
&lt;h2 id="my-modest-proposal"&gt;My modest proposal&lt;/h2&gt;&lt;p&gt;My proposal starts with this:
your data platform should start as
&lt;em&gt;the exhaust pipe of data sharing interactions throughout the company&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;The files that are actively being shared
are probably the ones that need storage, cataloguing, and permissions the most.
These are also the ones with the highest amount of current institutional knowledge present.
Backing up, cataloguing, and maintaining a record of data production and consumption,
should be done in the background with zero intervention from anyone.&lt;/p&gt;
&lt;p&gt;My proposal continues with a different definition of the customer:
Start with your computational &amp;amp; data scientists!
They are the ones who are wrangling, well, &lt;em&gt;data&lt;/em&gt;.
Build tooling for them
that makes it easier for people to access files directly
on the platforms that are &lt;em&gt;already&lt;/em&gt; being used,
so they never need to ask collaborators to send files over.
Make it easy for them to write code like this:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;data_platform&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;read_onedrive&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;pandas&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="k"&gt;as&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;pd&lt;/span&gt;

&lt;span class="n"&gt;filepath&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;read_onedrive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;https://onedrive.share.url-goes.here/...&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;df&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;pd&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;read_csv&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;filepath&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Then, use that same toolset
to ensure that those files are being version controlled
in a backup location on the cloud.
Within that toolset,
inject the code necessary
to log the fact that:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Amy Landry, the computational scientist, asked Brendan Lee, the laboratory scientist, for access to&lt;/li&gt;
&lt;li&gt;&lt;code&gt;$ONEDRIVE_PATH/data/immunotherapy/ITX_47213/results.xlsx&lt;/code&gt;,&lt;/li&gt;
&lt;li&gt;with SHA256 hash &lt;code&gt;3191vh9ifsodi6gr2p498qvfn5y082gjcn&lt;/code&gt; (for versioning purposes)&lt;/li&gt;
&lt;li&gt;on date &lt;code&gt;2024-11-03 19:25:39&lt;/code&gt;,&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Here, the source of &lt;code&gt;read_onedrive&lt;/code&gt; would have the following signature, with an emphasis on the &lt;code&gt;autolog&lt;/code&gt; decorator:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nd"&gt;@autolog&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;read_onedrive&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;str&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="o"&gt;...&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;filepath&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Along the way, within the data access tooling,
&lt;code&gt;autolog&lt;/code&gt; can trigger an automatic backup of that file
to a centralized buckets in the cloud,
and create or update a catalogue entry
based on the hash of the file and the filename,
with known access permissions accurately logged.
Your data catalogue now has automatic entries being generated
with no effort on anybody's part whatsoever.
And the implementation sketch of &lt;code&gt;autolog&lt;/code&gt; may look like this:&lt;/p&gt;
&lt;div class="hll"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nn"&gt;data_platform&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;create_or_update_catalogue&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;log_permission&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;get_producer_consumers&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;trigger_backup&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;autolog&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;def&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="nf"&gt;wrapper&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
        &lt;span class="n"&gt;filepath&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;func&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;args&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt;&lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;url&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;kwargs&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;pop&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s2"&gt;&amp;quot;url&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
        &lt;span class="n"&gt;create_or_update_catalogue&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# &amp;lt;- triggers async job in cloud, doesn&amp;#39;t actually run on machine&lt;/span&gt;
        &lt;span class="n"&gt;trigger_backup&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;url&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="c1"&gt;# &amp;lt;- triggers async job in cloud, doesn&amp;#39;t actually run on machine&lt;/span&gt;
        &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;filepath&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="n"&gt;wrapper&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;Behind-the-scenes,
the cloud function that &lt;code&gt;create_or_update_catalogue&lt;/code&gt; triggers is managing a lot.
It has knowledge of your internal roster of people through ActiveDirectory or equivalent.
It as scoped read-only permissions to pull in the file (or folder)
and will automatically back it up in the cloud,
through a &lt;a href="https://en.wikipedia.org/wiki/Content-addressable_storage"&gt;content-addressed storage&lt;/a&gt; system.
It also keeps track of file versions through file and file set versioning,
keeping up-to-date the lineage of a file
each time the &lt;code&gt;data_platform&lt;/code&gt; package interacts with it.&lt;/p&gt;
&lt;p&gt;Under this circumstance,
the data catalogue is no longer a table:
it is a social graph!&lt;/p&gt;
&lt;h3 id="benefits"&gt;Benefits&lt;/h3&gt;&lt;p&gt;In doing so,
we start by automatically cataloging
the most actively shared data that exists.
Your data access tooling eliminates the need
for people to think about
"which exact copy of my data did Brendan share with me?"
It also eliminates the need
for computational and data scientists
to think about how they will manage their data locally;
it's always pulled in fresh from their colleagues' OneDrive.
Your use of content-addressed storage
allows you to identify duplicates floating around the company,
enabling one to flag such files and query the file owners for a resolution.
You've solved a computational/data scientist's data access problem
by making it more seamless than they would otherwise have to
(i.e. sharing files over MS Teams, downloading copies,
and storing them on their own filesystem).&lt;/p&gt;
&lt;p&gt;I can imagine more tooling built around this concept
of logging interactions as a side effect.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Was there a programmatic access of that data?
If so, log that interaction with a counter on the data catalogue!&lt;/li&gt;
&lt;li&gt;Was there a new sharing event that happened via OneDrive?
Log that event!&lt;/li&gt;
&lt;li&gt;Was the data programmatically accessed within a scalable computation platform?
Log the service account!
Also, auto log the execution run, source code, git commit hash, source repo, etc.,
and use an LLM to generate a plain text description of what's going on,
and make that generated description searchable on the platform.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A side effect is that we can also map interactions between colleagues
based on sharing of data files between one another --
and use that to identify potential data silos between departments that are showing up.
The network of data sharing interactions also provides a new opportunity for finding data:
"whom should I ask to get access to {{ this kind of data }}?"
Imagine the following case:
Colin finds out that Bob has access to Amy's &lt;code&gt;results.xlsx&lt;/code&gt; data,
and can now ping Amy directly on MS Teams for access.
Upon Amy sharing that data with Colin and Colin accessing the data via &lt;code&gt;read_sharepoint()&lt;/code&gt;,
the catalogue entry gets automatically updated as well
with Bob and Colin being known users with permission to access that file.
No new systems for Amy and Colin to deal with,
but the new permissions associated with that data
are automatically reflected in the catalog.&lt;/p&gt;
&lt;h2 id="implementation-and-cost-considerations"&gt;Implementation and Cost Considerations&lt;/h2&gt;&lt;p&gt;Building this kind of data cataloguing system
is surprisingly lightweight
compared to traditional data platforms.
The initial implementation would require a small, focused team:
a couple of backend engineers with system administration privileges
who understand cloud infrastructure
and can use LLMs to quickly parse vendor API documentation,
one Python developer to build the data access tooling rapidly with Cursor/Copilot Edits,
and a part-time product manager to gather user feedback.
With this lean team,
an MVP could be deployed within 1-3 months
targeting the most-used platform within the company.&lt;/p&gt;
&lt;p&gt;From a cost perspective,
this approach offers several compelling advantages.
Since we're building on top of existing storage infrastructure
rather than creating new systems,
we eliminate a major cost center right from the start.
Training costs are minimal
because users continue working with their familiar tools and workflows.
The maintenance burden is significantly reduced
by leveraging existing enterprise systems
rather than maintaining new infrastructure.
We also avoid the substantial costs
typically associated with data migration projects.
Support costs stay low
since users are working with tools they already know.&lt;/p&gt;
&lt;p&gt;The expected benefits of this approach are substantial!&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;100% automatic cataloging of shared data&lt;/li&gt;
&lt;li&gt;100% automated backups of files&lt;/li&gt;
&lt;li&gt;Improved data governance through automatic logging of ownership, sharing, and permissions&lt;/li&gt;
&lt;li&gt;Better collaboration through social graph discovery&lt;/li&gt;
&lt;li&gt;Automated file de-duplication&lt;/li&gt;
&lt;li&gt;Reduced risk of data silos&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Success can be measured through several key metrics.
We track the number of automatic catalog entries generated
to measure system adoption.
User surveys help quantify time saved in data discovery.
We monitor the number of unique users accessing data through the platform
to gauge reach.
The reduction in duplicate data storage shows improved efficiency.
Finally, we measure increases in cross-department data sharing
to evaluate improved collaboration.&lt;/p&gt;
&lt;h2 id="security-and-compliance-considerations"&gt;Security and Compliance Considerations&lt;/h2&gt;&lt;p&gt;This approach actually enhances security and compliance
compared to traditional methods.&lt;/p&gt;
&lt;p&gt;From a data governance perspective,
the system automatically logs all data access,
creating a comprehensive audit trail.
Permissions continue to be managed through existing enterprise systems
like OneDrive and SharePoint,
while data sharing is tracked through the social graph of interactions.
Version control is handled automatically through file hashing.&lt;/p&gt;
&lt;p&gt;When it comes to regulatory compliance,
we benefit from the fact that existing enterprise storage systems
like OneDrive and SharePoint
are typically already validated for regulatory compliance.
Our platform simply adds an additional layer of tracking
without disrupting these validated systems.
The automatic backup capabilities ensure
that data preservation requirements are consistently met.&lt;/p&gt;
&lt;p&gt;The approach also provides strong risk mitigation.
Since data is never manually moved outside of approved systems,
we significantly reduce security risks.
Centralized automatic backups minimizes the risk of data loss,
while the use of familiar tools for scientists
reduces the likelihood of shadow IT emerging.
Additionally, by clearly tracking who has access to what data,
we decrease the likelihood of data access issues arising.&lt;/p&gt;
&lt;h2 id="caveats"&gt;Caveats&lt;/h2&gt;&lt;p&gt;There are caveats to this solution:
we clearly aren't going to capture every single data file through this mechanism.
(At least we start somewhere though!)&lt;/p&gt;
&lt;p&gt;And of course,
something that may be mission critical might not have been shared.&lt;/p&gt;
&lt;p&gt;Also, this focus on interactions means there's a lot more work to be done on the data platform builders' side:
the team will end up needing to ask every data storage platform
(e.g. Benchling, OneDrive, Signals, Google Drive, internally built apps etc.)
for information about who is sharing what.
Before a vendor system can be onboarded,
they must have the ability to provide direct URIs
to &lt;em&gt;any&lt;/em&gt; resource within their system via API calls.&lt;/p&gt;
&lt;p&gt;People have to get used to not schlepping files around by email and chat,
and instead sharing direct links instead.
There will always be some kind of behaviour change needed.&lt;/p&gt;
&lt;p&gt;But my proposal here asks us to start with mapping interactions between people first.
And instead of asking people to take on the additional burden of being a data producer of a data product,
we just ask them to continue what they're doing anyways
and not have to interact with yet another system.&lt;/p&gt;
&lt;h2 id="change-management-strategy"&gt;Change Management Strategy&lt;/h2&gt;&lt;p&gt;Implementing this approach requires thoughtful change management,
even though we're minimizing disruption to existing workflows.
Here is how I would propose to move forward with this.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Phased Rollout&lt;/strong&gt;: We start with a single high-value team of computational scientists,
then expand to their immediate collaborators.
From there, we gradually roll out to other computational teams,
and finally provide optional tools for laboratory scientists.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Handling Resistance&lt;/strong&gt;: When dealing with existing data platform teams,
we position this as complementary to their work, not a replacement.
For IT teams,
we emphasize our use of existing validated systems and security controls.&lt;/p&gt;
&lt;p&gt;With scientists,
we focus on the social value that emerges from understanding data flow.
When they can see who else works with similar data,
they discover potential collaborators naturally.
The system helps them understand data provenance
by showing the chain of sharing that led to their current analysis.
Most importantly,
they can avoid duplicating work
by seeing who has already performed similar analyses.&lt;/p&gt;
&lt;p&gt;For management,
we demonstrate improved compliance
and provide insights into how data actually flows through the organization.
This visibility helps identify both successful collaboration patterns
and potential data silos that need addressing.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Communication Strategy&lt;/strong&gt;: Our communication strategy involves regular demos of successful data discovery
and monthly metrics on automatic cataloging.
We maintain clear documentation of benefits realized
and keep open feedback channels for improvement.&lt;/p&gt;
&lt;p&gt;The key is positioning this as an enhancement layer
that makes existing tools work better together,
rather than a new system that replaces current workflows.&lt;/p&gt;
&lt;h2 id="conclusion"&gt;Conclusion&lt;/h2&gt;&lt;p&gt;The path to better data management in biotech doesn't require massive infrastructure changes or behavioral shifts. Instead, by:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Starting Small&lt;/strong&gt;: Focus on computational scientists first&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Leveraging Existing Systems&lt;/strong&gt;: Build on top of current tools&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Automating Everything&lt;/strong&gt;: No manual cataloging required&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Following Social Patterns&lt;/strong&gt;: Let natural data sharing guide the system&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We can create a data platform that actually works, costs less, and delivers immediate value. The key insight is treating data cataloging as an interaction logging problem rather than a data product problem.&lt;/p&gt;
&lt;p&gt;For biotech leadership considering this approach:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Start with a small pilot team&lt;/li&gt;
&lt;li&gt;Measure success through automatic catalog growth&lt;/li&gt;
&lt;li&gt;Scale based on demonstrated value&lt;/li&gt;
&lt;li&gt;Keep the focus on enhancing, not replacing, existing workflows&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The result?
A data platform that grows organically with your organization,
supports compliance requirements,
and most importantly -
one that people will actually use.&lt;/p&gt;
</content></entry></feed>