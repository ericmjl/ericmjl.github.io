[
  {
    "id": "blog-agentic-eda",
    "url": "/blog/2026/2/13/agentic-eda/",
    "title": "How to stay in control when doing EDA with coding agents",
    "summary": "In this blog post, I share how coding agents can supercharge data analysis, but only if we stay in control. By slowing down, asking the right questions, and structuring sessions with journals and artifact gating, we avoid chaos and keep our scientific thinking sharp. I explain the skills and patterns that help teams focus on insights rather than just generating code. Curious how you can harness agent speed without losing your scientific edge?",
    "body": "Speed without control is just chaos. I've seen teammates compress a week and a half of analysis work into half a day using coding agents. That's a 5-10x speedup. But here's the thing: that speed only matters if you stay in the driver's seat. Otherwise you're not doing data science, you're just generating artifacts. The real unlock isn't that agents write code fast. It's that they can be guided through a structure that keeps you in control of the analysis. The problem isn't speed, it's agency Coding agents are eager. Give them a CSV file and they'll open it, generate a dozen plots, and dump a wall of code before you've finished describing what you're actually looking for. That feels productive. It isn't. The problem is that you've lost the thread. You didn't formulate a clear question. You didn't think through what the x-axis and y-axis should be. You're now reacting to whatever the agent produced, rather than steering toward an answer. I've developed a different approach, codified in two skills I use with my coding agents: scientific-eda for exploratory data analysis and ml-experimentation for machine learning experiments. The pattern is the same in both: slow down first, gate on artifacts (plots, tables, etc.), and structure the session so both you and the agent can follow what happened. Slow down first: the Socratic opening The first design principle is counterintuitive: slow down before you speed up. When you invoke the scientific-eda skill, the agent does not immediately load your data and start plotting. Instead, it asks you questions. What's the problem context? What are you hoping to learn or decide? What constraints matter? From the skill definition: Do not open the data file and start coding or plotting. Ask for or confirm: the problem context\u2014biological, chemical, or data-science question; what the user hopes to learn or decide; and any constraints. There's also an explicit guardrail: \"ask 'why' before executing.\" When you request a specific plot or table, the agent briefly asks what question or decision it serves. This isn't bureaucracy. It's alignment. The agent is checking that you've thought through the request before it spends your time executing it. This Socratic opening feels slower. But it prevents the far more common waste: generating plots you didn't need, going down rabbit holes you can't explain, and ending up with a folder full of artifacts and no clear answer. Gate everything on artifacts The second design principle is more specific: one artifact at a time. If you can't describe what you want, you're not ready to execute. The agent waits. You think. You describe. Then the agent generates exactly what you asked for. For a plot, this means articulating the x-axis, the y-axis, and what pattern you're looking for. What would confirm or refute your hypothesis? If you can't answer, the analysis isn't ready to run. For a table, this means specifying the columns, rows, and aggregation level. What comparison are you trying to make? What decision will this table inform? A vague request like \"show me the data\" isn't actionable. \"Show me the mean expression level by treatment group\" is. This is a forcing function for clarity. Describing an artifact precisely forces you to articulate the question you're actually asking. The tradeoff is worth it. You give up a bit of speed up front for precision in execution. And because the agent can generate code in seconds rather than minutes, the net result is still a massive speedup. My teammates went from 1.5-2 weeks to half a day. The precision tax is negligible compared to the execution dividend. The session structure Here's where structure becomes a feature, not overhead. Each analysis session is a timestamped folder. The naming convention is ISO datetime plus a descriptive slug: Let's walk through what each piece does. journal.md is the memory. Before each action, the agent reads the journal. After each action, it appends what happened. Entries get timestamped and tagged: for data structure discoveries, for visualizations, for observations, for suggested next steps. The journal is scannable. It's also the entry point for anyone (including future you) who wants to understand what happened without reading the code. plots/ holds all figures from the session. The skill specifies WebP format for smaller file sizes, though that's a minor detail. scripts/ contains disposable Python scripts. Each script has PEP723 inline metadata at the top, declaring its own dependencies. You run them with from the session folder. No environment wrangling. No \"which virtualenv am I in?\" confusion. One script, one plot. The structure serves two purposes. First, it gives the agent a clear protocol to follow: read journal, execute, append to journal, suggest next step. Second, it leaves a trace that any human can follow. You can read the plan, then the journal, then the report, and understand the entire analysis without touching the code. What changes in team conversations Somethi",
    "tags": [
      "agents",
      "ai",
      "data science",
      "exploratory data analysis",
      "workflow",
      "productivity"
    ],
    "pub_date": "2026-02-13",
    "type": "blog"
  },
  {
    "id": "blog-how-to-do-agentic-data-science",
    "url": "/blog/2026/2/1/how-to-do-agentic-data-science/",
    "title": "How to Do Agentic Data Science",
    "summary": "In this blog post, I share ten lessons I've learned from experimenting with agentic coding in data science, from setting clear goals and structuring projects to leveraging coding agents for faster iterations and better insights. I discuss practical tips like maintaining logs, generating diagnostic plots, and treating the agent as a partner in exploration. Curious how you can make AI your jazz partner in data science and boost your productivity?",
    "body": "Having tasted what agentic coding could look like for software development, I wanted to know what it would look like for data science - this meant training machine learning models and answering scientific questions. So I started experimenting, at work, and on my own at home as well. Here are ten lessons I've learned from my experiments thus far. 1. Be prescriptive in your prompting Similar to building software, you need to know exactly what you want and how you'll evaluate the outcome. The difference, however, is as follows: With software, you will often know what you need to build, but with data science, you can only know what hypotheses need to be verified, which means you will need to iterate your way to the answer. Nonetheless, it is possible to leverage coding agents to move quickly. The parallels are striking: if you frame each question you ask in terms of an observable outcome, you can set up your coding agent to write code that produces an output that can be evaluated for correctness, just like with software tests! Here, your ability to describe precisely the hypothesis you're exploring, and the ability to describe in precise language what the answer would look like if the hypothesis held true or not, are critical components of what enables the coding agent to figure out what needs to be counterfactually true (within the codebase or the data) in order for your hypothesis to hold true. Here is an example from my work. In a machine learning experiment with synthetic data, I wanted to hit 100% sequence editing performance. (It was synthetic data after all!) The coding agent hit a scenario where it was only doing 25%. With the clear goal in mind, it proposed edits to the code, edited the code, and re-ran experiments until it hit 100%. All without cheating; I know, because I checked! 2. Strong patterns in the file system The agent, like humans, needs a predictable place for experiments. Similar to how a software repo has a conventional layout (src/, tests/, and so on), your experiments need a conventional layout so the agent knows where to put things and where to look. Within the experimentation skill that I wrote, I instruct the coding agent to do its work inside an folder. Underneath that, for each experiment, we have datetime-prefixed subfolders, in which, there's a README file, a directory, a directory, a directory. Naming things logically helps, but the scheme matters more than the exact names. Coding agents will follow the patterns you already have. 3. Put logging instructions in AGENTS.md With software, the feature one ask's a coding agent to build either succeeds or fails, and this can be automatically verified using programmatically-runnable unit and integration tests. With data science, experimental runs produce logs and metrics, but aren't easily boolean pass/fail like software tests. In both cases, however, and the agent can introspect logs to figure out what to change! Your AGENTS.md file should include instructions for putting enough logging in place so the LLM can introspect what's going on during the experiment. I've written elsewhere about how to teach your coding agent with AGENTS.md and using AGENTS.md as repository memory for self-improving agents. Pair that with tools that run code in the terminal so the agent gets logs it can read. When the agent can read the logs, it can figure out what's wrong and what to change. In my work, logging and printing to terminal were what let my agent fix a masking strategy that was only yielding 25% correctness. It read the logs, proposed a fix, re-ran, and got to where we needed to be. No intervention on my part. A 3 day experiment became 20 minutes. 4. Give it report-writing skills The agent can write code and read mountains of logs, but you need something else: a human-readable summary of what it observed and what looked weird, so you can triage without re-reading every log. Give your coding agent instructions (e.g. in an agent skill) to write out in plain language what it observed during the model evaluation phase. It should read execution logs throughout the run. Tell it to write down anything that looks weird for follow-up. If something is off, it should say so. You get a readable summary and a list of things to dig into. For reports (e.g. ), encode in the skill that every table and every plot must be scrutinized. Ensure that plots are generated for every table, and that someone (you or the agent, with you verifying) carefully checks for inconsistencies between AI-generated plots and the tables they are supposed to reflect. The agent can miss things. It is valid to ask the AI to check its own work, but only if you have an idea of exactly where it is wrong and you tell it as such. Vague \"double-check this\" rarely helps; \"the values in figure 2 do not match the second column of table 1\" gives the agent something it can fix. 5. Have the agent keep an append-only journal of observations Within the skill, instruct the agent to keep a single file (e.g",
    "tags": [
      "agentic",
      "coding",
      "experiments",
      "logging",
      "reports",
      "journal",
      "plots",
      "iteration",
      "structure",
      "exploration"
    ],
    "pub_date": "2026-02-01",
    "type": "blog"
  },
  {
    "id": "blog-model-feel-fast-tests-and-ai-coding-that-stays-in-flow",
    "url": "/blog/2026/1/25/model-feel-fast-tests-and-ai-coding-that-stays-in-flow/",
    "title": "Model feel, fast tests, and AI coding that stays in flow",
    "summary": "In this blog post, I share my hands-on experience using AI coding models, focusing less on benchmarks and more on the day-to-day feel\u2014how model style, personality, and the right testing harness impact productivity and flow. I discuss the trade-offs between long-horizon autonomy and short-horizon iteration, and why a constructive, enthusiastic AI assistant matters as much as raw performance. Curious how the right mix of model and harness can transform your coding workflow?",
    "body": "Most of the conversation about AI coding models focuses on performance metrics. Benchmarks, evals, pass rates, latency. Useful stuff, but it misses the part that actually shapes my day-to-day: what it feels like to work with the model. Once you start using LLMs as coding agents, the qualitative experience becomes a throughput issue. It affects how often you intervene, how much you trust what is happening, and whether you stay in flow or spend your time cleaning up weird breakage. Two axes keep showing up for me. First is time horizon and supervision style: long-horizon autonomy versus short-horizon iteration. Second is personality and verbosity: how the model behaves when it is wrong, how much it narrates, and whether it stays constructive or spirals into apology loops. There is also a third ingredient that ends up mattering as much as the model: the agentic harness. By that I mean the tools and checks that the agent can run to verify it did not break behavior, and whether the harness gives you streaming and visual feedback\u2014a live trace of what the model is doing\u2014or leaves you staring at a spinner until the answer drops. Good harness beats model swapping more often than I expected. Long-horizon autonomy vs short-horizon iteration I call it \"Opus-feel\" when a model has that \"ask and it shall be given\" vibe with a longer time horizon. You describe what you want, it runs for a while, and it comes back with a plausible scaffold. It is great for momentum. I call it \"Sonnet-feel\" when a model leans toward shorter-horizon iteration. It works better when you are walking through a real codebase step by step, keeping changes small enough that you can validate what happened, correct course, and keep going. Another way to put it is that long-horizon autonomy pushes you toward a spec-and-review loop, while short-horizon iteration pushes you toward a steer-and-verify loop. Both can be productive. They just fail differently. In a sufficiently large codebase, you cannot rely solely on long-horizon autonomy where you ask for something with a vague description and hope it lands cleanly. You are not always guaranteed something well organized, especially when the job is refactoring rather than greenfield scaffolding. A concrete example for me came from Canvas chat. At the time, everything was tied to and . When I wanted to refactor things into plugins, I needed to dogfood a plugin pattern in the codebase itself. Long-horizon autonomy struggled here. It could generate a plugin pattern, but it was not great at the careful, incremental work of extracting behavior out of a monolith and into a clean plugin boundary. Walking bit by bit with Sonnet or Sonnet-quality models was a very different experience. The big win was that I could study the LLM traces live (the tool calls and file edits it proposes step by step) and see where edits were being made. If I noticed a feature handler getting added to when it clearly belonged in a plugin file, I could intervene immediately and ask, \"Why is that thing over there in ? Why is it not inside the plugin file instead?\" That kind of interactive, traceable work is where the short-horizon models shine. Examples from my own testing, with all the usual caveats: Opus-4.5 (Anthropic), GPT-5.2 (OpenAI), and GLM-4.7 (z.ai) have been solid for the long-horizon, get-it-moving-fast mode. Minimax M 2.1 (OpenCode Zen) feels closer to the short-horizon mode for me. Composer-1 (Cursor) also feels closer to that style. I suspect GPT-4o and GPT-5.1 (both OpenAI) might land there too, but I have not really test-driven them. The practical takeaway is that I now switch modes on purpose. When I need speed and initial momentum, I reach for long-horizon autonomy. When I need control, I choose a short-horizon model so I can babysit the work, watch the traces, and intercept it when it tries to do something clever in the wrong place. The harness lesson (Cypress beat model hopping) One more lesson from that period: I did a bunch of model hopping, trying to find something that would fix a particular class of behavioral breakage. The most frustrating failures were not subtle logic bugs. They were basic syntax errors introduced during tool-call patching, unclosed brackets, unclosed parentheses, that kind of thing. When that happens, you do not get a slightly-wrong feature, you get a page that fails to load. Debugging it manually is fine the first time, and infuriating on the seventh. The thing that actually moved the needle was listening to my colleague Anand Murthy and instantiating Cypress tests. A simple automated page reload catches those failures immediately. It shifts the pain earlier, gives the agent a verification loop it can run on demand, and turns \"agentic coding\" into something I can trust. Here is the dumbest possible example, taken straight from the Cypress suite in canvas-chat. It is not fancy, and that is the point. It catches \"the page does not load\" failures quickly. Beyond model choice, a great agentic ha",
    "tags": [
      "llm",
      "autonomy",
      "supervision",
      "personality",
      "verbosity",
      "harness",
      "refactoring",
      "workflow",
      "testing",
      "ergonomics"
    ],
    "pub_date": "2026-01-25",
    "type": "blog"
  },
  {
    "id": "blog-how-to-build-self-improving-coding-agents-part-3",
    "url": "/blog/2026/1/19/how-to-build-self-improving-coding-agents-part-3/",
    "title": "How to build self-improving coding agents - Part 3",
    "summary": "In this blog post, I share how to combine repo memory and reusable skills to create self-improving coding agents. I walk through a maturity model, explain when to update AGENTS.md versus creating a skill, and highlight the importance of metacognition in systematizing your workflows. I also discuss how agents are evolving beyond coding tools into general-purpose teammates. Curious how you can make your coding agents smarter and more helpful over time?",
    "body": "In part 1, I covered as repo memory. In part 2, I covered skills as reusable playbooks. This post is about turning those two ideas into something you can run as a practice. The maturity model Once you have both repo memory and skills, you can think about how the practice evolves over time. Stage 0: Ad hoc prompting You keep re-explaining the same things in chat. It works, but it does not compound. Stage 1: Repo-local memory You add repository-specific guardrails and a code map. This is where shines. Stage 2: Global personal skills Once a workflow repeats across repos, you promote it into a global skill on your machine. If you want a concrete bootstrap set, here is what I would install globally: - [](https://github.com/ericmjl/skills/blob/main/skills/skill-creator/): lowers the activation energy for making new skills. - an installer and updater for skills, for example [](https://github.com/numman-ali/openskills): makes distribution and updates less annoying. - [](https://github.com/ericmjl/skills/tree/main/skills/agents-md-improver): keeps the repo map current without you thinking about it. Stage 3: Shared skills If a workflow repeats across a team, it belongs in a shared location with a clear install path. I do not think you should start here. Start repo-local, then promote only when you feel the pain twice. Promotion decisions come from paying attention to what the agent actually does in practice. Watch traces, then distill constraints If you work with agents long enough, you start to notice the model\u2019s default moves. When I see an agent repeatedly: - taking an overcomplicated path - missing a file I know is relevant - applying a global refactor when a surgical fix is needed I treat that as a signal. Then I decide what kind of fix it is. If it is a repo invariant, a navigation hint, or a local norm, it belongs in . That is the always-on context for how work should happen in this repo. If it is a repeatable procedure with a clear output contract, it belongs in a skill. Sometimes the procedure is repo-specific. In that case I keep it as a repo-local skill. If I feel the pain twice in another repo, I promote it into a global skill. This is how you get operational learning without pretending the model is learning. Underneath, a lot of this comes down to writing instructions in a way that can be executed. Markdown is becoming executable One reason this whole approach works is that the agent can execute what you write. When an LLM can execute tool calls, Markdown becomes an executable language. Skills fit this pattern. A is just a structured instruction sheet, but it is also runnable in the sense that the agent can turn it into searches, file reads, edits, and command execution. The other trick is that skills are loaded on demand. The agent reads a short description first, then loads the full instructions only when it needs them. You can write a precise plan in plain language, and the agent can turn it into: - searches - file reads - surgical edits - test runs This is not magic. It still depends on linguistic precision. But the ergonomics shift. You can describe a workflow at the level you actually think about it, then let the agent do the clerical work. This is also why I like the runbook analogy, even with the caveats. When to update vs create a skill Skills tell an agent how to do something. tells an agent how this repo works, and what rules it must follow while doing anything at all. Here is how I decide. Update when the instruction is specific to the repo: - navigation help: where things live, what files matter, what to ignore - local norms: build commands, test commands, environment rules, style constraints - guardrails: what not to do in this repo Create a skill when the workflow is reusable, or when you want a named, on-demand playbook: - a multi-step procedure you want to invoke repeatedly - a workflow that spans repos or products - a task with a strict output contract (release announcements, status updates, summaries) If I am unsure, I start repo-local. If I feel the pain twice in another repo, I promote it into a global skill. The meta skill is metacognition The most valuable \u201cskill\u201d, however, is not a file format. It is the habit of watching yourself work. I try to ask: what am I doing repeatedly that should be systematized? If the answer is \u201cI keep re-explaining how this repo is organized\u201d, that goes into . If the answer is \u201cI keep asking for the same kind of summary, debug sequence, or release note format\u201d, that becomes a skill. Once you start doing this, you build a compounding loop. The agent handles more of the repeated work, and you spend more time on judgment and design. If this all sounds like more than coding, that is because it is. Where this seems to be going I buy Simon Willison\u2019s framing that these tools are general agents disguised as developer tools (Claude Cowork post). Even if you start with coding, the moment an agent can run terminal commands and manipulate files, the surface area ",
    "tags": [
      "agents",
      "ai",
      "workflows",
      "productivity",
      "skills"
    ],
    "pub_date": "2026-01-19",
    "type": "blog"
  },
  {
    "id": "blog-how-to-build-self-improving-coding-agents-part-2",
    "url": "/blog/2026/1/18/how-to-build-self-improving-coding-agents-part-2/",
    "title": "How to build self-improving coding agents - Part 2",
    "summary": "In this blog post, I dive into the concept of 'skills' for coding agents\u2014reusable playbooks that streamline repetitive tasks and make workflows explicit. I share real examples, from debugging to release announcements, and discuss how skills evolve through iteration and feedback. I also touch on the challenges of distributing and updating skills compared to MCP servers. Curious about how these skills can make your coding agents smarter and more efficient?",
    "body": "In part 1, I focused on repo memory with . In this post, I am switching to the other lever: skills. Skills are prompt compression Skills are the other half of the system. When a task repeats, I do not want to keep re-explaining the workflow. I want a playbook I can invoke. What a skill is A skill is a folder with a file. The is the prompt. The bundled scripts and assets are the tool layer. A good skill makes three things explicit: - when to use it - what steps to take - what good output looks like If you want the spec, see Agent Skills. Skills are best formed around jobs to be done: concrete, repeatable workflows rather than abstract capabilities. Think \"debug a GitHub Actions failure\" or \"draft a release announcement,\" not \"know about CI\" or \"write good prose.\" When the job is clear, the skill has a natural boundary and a clear trigger. When it is vague, the skill is hard to invoke and hard to improve. A wrong framing is \"skills for tools.\" Skills get invoked in the loop of trying to accomplish a job, not in the context of trying to use a tool. The tool is a means; the job is why you reach for it. If you design a skill around a tool, you end up with something the agent has to remember to use. If you design it around a job, the agent reaches for it when the job shows up. Examples A GitHub debugging skill is the obvious starting point. CI failures are repetitive and usually want the same sequence: identify failing jobs, pull logs, inspect diffs, reproduce locally, then patch. A second example is a release announcement skill. The motivation here was not abstract. I was spending a good half hour each release just trying to compose the announcement, and I did not want to do that anymore. The output contract was also specific. I wanted release announcements that are copy-pasteable into Microsoft Teams, with emojis, but otherwise minimal formatting because Teams formatting is inconsistent. A third example is more technical. At work I had a session with a coding agent to train an ML model inside a script. After that session, I had it write a report on what it learned and what changed. Then I turned that report writing into a skill. The report format was familiar to everyone on the team: Abstract, Introduction, Methods, Results, Discussion. The content came from real artifacts: stdout logs, metrics, code, config files, git diffs, and the agent\u2019s own session history. A fourth example is about tacit domain expertise. A teammate of mine created a skill that encoded her implicit knowledge from years of debugging chromatography traces. The point was not that the agent suddenly became a scientist. The point was that her debugging procedure became explicit and reusable. Skill creation and iteration I now like skills because they are easy to iterate on. I used to be more skeptical, and I still think MCP servers have a cleaner distribution story, but my opinion has shifted as I have used skills more in real workflows (Exploring Skills vs MCP Servers). For the release announcements, I fed my coding agent a few examples of what \u201cgood\u201d looked like. I was using Anthropic\u2019s [](https://github.com/ericmjl/skills/blob/main/skills/skill-creator/) skill at the time, and those examples became part of the skill itself, stored as assets that the agent could reuse. This is a huge energy barrier reducer. It is much easier to iterate on a Markdown-based skill than it is to start from scratch with \u201cwrite me a Python script that does X\u201d. You can still add scripts inside a skill when you need determinism, but the interface is the Markdown. The other half is the feedback loop. When I edit the generated release announcement, I feed the revised version back to the agent and tell it to update the skill with the new example. That way the skill evolves as my taste evolves. This is also a way to share. A skill is reviewable. I can open a PR and let collaborators comment on both the output and the process that produced it. In the chromatography example, using [](https://github.com/ericmjl/skills/blob/main/skills/skill-creator/) to generate the first draft mattered for another reason too. English is not my teammate\u2019s first language. The structure makes it much easier to get from \u201cI know what I do\u201d to \u201chere is the procedure an agent can follow\u201d. Distribution and updates This is where skills feel less mature than MCP servers. An MCP server has a clean distribution story. You can it, configure auth once, and you get a centrally versioned bundle of prompts and tools. Updating is a normal package update. Skills still involve moving folders between machines and repos, and remembering where each harness expects skills to live. I originally ended up writing a [](https://github.com/ericmjl/skills/tree/main/skills/skill-installer) skill. It is the same move as [](https://github.com/ericmjl/skills/blob/main/skills/skill-creator/), but for distribution and updates. When I say \u201cinstall this skill\u201d or \u201cupdate this skill from this URL\u201d, the agent needs to ask two ke",
    "tags": [
      "agents",
      "ai",
      "skills",
      "mcp",
      "workflows"
    ],
    "pub_date": "2026-01-18",
    "type": "blog"
  },
  {
    "id": "blog-how-to-build-self-improving-coding-agents-part-1",
    "url": "/blog/2026/1/17/how-to-build-self-improving-coding-agents-part-1/",
    "title": "How to build self-improving coding agents - Part 1",
    "summary": "In this blog post, I share my approach to making coding agents truly self-improving by focusing on operational feedback, not just model updates. I explain how using an AGENTS.md file as repository memory and developing reusable skills can help agents learn from mistakes and reduce repetitive guidance. My goal is to create an environment where agents get better each week without constant babysitting. Curious how these strategies can make your coding agents more effective?",
    "body": "I want my coding agents to get better every week. Not in the abstract \u201cthe models are improving\u201d sense. I mean it in the operational sense: if an agent makes a mistake, or takes a path I would not take, I want that feedback to stick. If I have to repeat the same preference every session, I am not using an agent. I am babysitting a very fast intern. The trick is that the model weights are not changing mid-week. So if you want \u201cself-improvement\u201d, you need to change the environment the agent works inside. I have found two levers that compound: - as repository memory - skills as reusable playbooks This post is a longer \u201csource of truth\u201d version. My intent is to later break it into smaller blog entries, and also rework it into chapters for my data science bootstrap notes. Where improvement comes from The UX I am after is simple: I stop repeating myself. I stop doing the same end-of-day cleanup, writing the same reminders, re-explaining where files live. The agent starts each session closer to how I want it to work. If the model weights are not changing mid-week, improvement has to come from the environment you wrap around the agent. For me that environment has two pieces: - durable repository memory () - reusable playbooks (skills) Once you have those two, you can treat \u201cagent improvement\u201d like runbooks plus postmortems. The analogy is imperfect, because this is not documentation for humans. The loop is the same though: write down the repeatable steps, then write down what surprised you and what you will do differently next time. The difference is that natural language can turn into tool calls. When you write things down precisely, the agent can execute them. I usually start with , because it cuts down exploration immediately. as repository memory If you have not run into the convention before, see agents.md. To be effective, needs to do two things for the agent. First, it needs to make the agent fast at navigating the repo so it can get to the right files with minimal wandering. A code map is a straightforward way to do that. Second, it needs to encode the local ways of working in this repo so the agent stops repeating the same mistakes. That is where corrections and norms live. This is the loop I want: - I observe a mismatch. - I tell the agent what must be true. - The agent writes the correction into (or a repo-local skill). - The agent reads it next time. Fast navigation to the right files In the ideal state, the agent gets to the right files quickly. A code map is the simplest way I know to make that happen. It does not have to be perfect. It can be slightly stale and still be useful. I have seen this pay off in a very practical way. In my codebase, having a map of the repo let the agent one-shot an obscure spot where events were emitted for node rendering. Without a map, the agent previously needed 5 to 6 searches, just to find the right neighborhood of the code. The difference is small in absolute time, something like 40 seconds versus 2 seconds. But it changes the feel of the collaboration. The agent spends less time wandering, and I spend less time steering. Close the loop when the map is stale There is one extra move that makes this feel self-correcting: When the agent notices that the code map looks stale, it should update the code map. This is a subtle point. The map is not a static artifact. It is part of a feedback loop. When the agent\u2019s exploration discovers a mismatch between the map and reality, that discovery should flow back into . You can encode this as an explicit instruction inside . You can also refresh on a schedule, like weekly, but the on-demand update is the part that makes the loop feel alive. Corrections that become durable norms The second job of is to hold repo-specific corrections to agents behaviour. These are the things you find yourself saying out loud. Two examples from my own work: - Run Python in the context. Use . - Do not cheat by modifying the tests to make them pass. I say the first one because the agent will often try to quickly check something. In a -managed project, that fails if you do not have a global Python. I say the second one because changing tests to make them pass destroys the point of having tests. Once these rules are written down, the agent stops making you restate them. This is the simplest way I know to reduce repeated friction. A starter prompt for generating I have found it useful to bootstrap with a one-time deep dive. Here is a prompt I use as a starting point. It is intentionally repo-specific. If you want, you can go further and add a cadence rule like \u201crefresh weekly\u201d, but I would keep it lightweight. The goal is compounding value, not bureaucracy. Once exists, skills are the second lever. Coming next Part 2 is about skills as reusable playbooks. It covers what a skill is, several examples from coding and scientific work, and why I ended up writing a skill to deal with the current distribution story. How to build self-improving coding agents - Par",
    "tags": [
      "agents",
      "ai",
      "workflows",
      "productivity",
      "software"
    ],
    "pub_date": "2026-01-17",
    "type": "blog"
  },
  {
    "id": "blog-how-i-fixed-a-browser-selection-bug-with-sequence-alignment-algorithms",
    "url": "/blog/2026/1/6/how-i-fixed-a-browser-selection-bug-with-sequence-alignment-algorithms/",
    "title": "How I fixed a browser selection bug with sequence alignment algorithms",
    "summary": "In this blog post, I share how a tricky text highlighting bug in my canvas-chat project led me to use a classic bioinformatics algorithm, Smith-Waterman, to solve messy browser selection issues\u2014especially with KaTeX-rendered math. Instead of struggling with normalization, I reframed the problem as sequence alignment, which proved robust and effective. Curious how an algorithm from DNA analysis can fix web UI bugs?",
    "body": "I ran into a frustrating bug this week in canvas-chat, my experimental canvas-based chat interface I built at the end of last year. The bug seemed simple on the surface: when users selected text from a rendered markdown table and clicked to highlight it, the highlighting would sometimes stop partway through, or highlight the wrong characters entirely. What started as a \"quick fix\" turned into a journey through several failed approaches before I remembered an algorithm from my undergraduate bioinformatics days. Sometimes the best solution to a problem comes from an unexpected domain. The problem: Browser selections are messy Canvas-chat has a feature where you can select text from an AI response, and the app creates a \"highlight\" node that links back to the source. When you click on the highlight, the corresponding text in the source gets wrapped in a tag. This worked fine for simple paragraphs. But when I tried it on tables containing KaTeX-rendered math, things went wrong: What I expected to highlight: 66.00 (0.18\u00b10.58) What actually got highlighted: 66.00 ( 0.18\u00b10.58) The highlighting was off by more than a few characters, and would stop before the end of my selection. In some cases, it would highlight completely wrong sections. Digging into the root cause The problem came from how KaTeX renders math and how browsers handle text selection. KaTeX renders math with multiple text representations: When you select text that spans across KaTeX-rendered content, gives you something like this: Notice the duplicated and the random newlines? The browser included text from both the MathML (for accessibility) and the visual spans. Add in tabs between table cells and inconsistent spacing around operators, and you have a string that looks nothing like the clean HTML text content. First attempt: Normalization layers My initial approach was to normalize both strings (the user's selection and the HTML text) before matching: 1. Collapse all whitespace to single spaces 2. Remove KaTeX duplication patterns (like \u2192 ) 3. Normalize spacing around operators Then find the match in the normalized strings, and map the positions back to the original. This is where things got complicated. I needed to track: - Which positions in the normalized string corresponded to which positions in the original - How to reverse the mapping after finding a match - How to handle characters that got removed entirely during normalization The code became a tangled mess of position arrays and off-by-one bugs. Here's a simplified version of what it looked like: The position mapping kept breaking. I'd fix one case only to break another. I was trying to maintain a bijection between two strings that had been transformed through multiple non-invertible operations. It wasn't going to work. The insight: This is a sequence alignment problem After banging my head against the normalization approach for a while, I took a step back. What was I actually trying to do? I had two strings: 1. The user's selection (messy, with artifacts) 2. The HTML text content (clean) I needed to find where the user's selection \"matched\" in the HTML text, tolerating: - Insertions (extra whitespace, duplicated characters in the selection) - Deletions (characters present in HTML but not in selection) - Mismatches (different whitespace characters) This is exactly what sequence alignment algorithms are designed for. In bioinformatics, we use these algorithms to compare DNA or protein sequences that may have evolved with insertions, deletions, and mutations. The classic algorithm for finding the best local alignment between two sequences is Smith-Waterman. I learned Smith-Waterman as an undergraduate, probably around 2008. I never thought I'd use it for web development. The solution: Align the beginning and end I didn't need to align the entire selection - I just needed to find where it started and ended in the HTML text. So I: 1. Take the first ~20 characters of the user's selection and align them to find the start position 2. Take the last ~20 characters, reverse both strings, align to find the end position Here's the core alignment function: The key insight is that Smith-Waterman's local alignment naturally handles all the messiness. Extra newlines in the selection? They're just gaps. Duplicated numbers? They align to the same position. Different whitespace characters? They all match each other. The result The new approach passes all the test cases that the normalization approach failed: Test: Simple word - Target: - Query: - Result: world (positions 6-11) Test: KaTeX duplication - Target: - Query: - Result: 66.00 (0.18 \u00b1 0.18\u00b10.58) (positions 0-25) Test: Cross-block selection - Target: - Query: - Result: The Heading Some paragraph (positions 0-25) The lesson: Know your algorithms I didn't invent anything new here. Smith-Waterman has been around since 1981. I just recognized that my web development problem was, at its core, a sequence alignment problem. This is why I think it's valuable t",
    "tags": [
      "javascript",
      "bioinformatics",
      "katex",
      "canvas",
      "algorithms",
      "bugfix",
      "highlighting",
      "selection",
      "web development",
      "ui"
    ],
    "pub_date": "2026-01-06",
    "type": "blog"
  },
  {
    "id": "blog-canvas-chat-a-visual-interface-for-thinking-with-llms",
    "url": "/blog/2025/12/31/canvas-chat-a-visual-interface-for-thinking-with-llms/",
    "title": "Canvas Chat: A Visual Interface for Thinking with LLMs",
    "summary": "I built Canvas Chat, a visual interface for nonlinear LLM conversations, over Christmas break using OpenCode and Claude Opus 4.5. The tool solves a specific job: thinking through complex problems where exploration branches in multiple directions. Features include branching conversations, highlight-and-branch, multi-select merge for synthesis, matrix evaluation, and integrated web search. It's open source and runs on Modal.",
    "body": "I've been mulling over this idea since last year January: A visual, nonlinear interface for LLM conversations\u2014something like an infinite canvas where you could branch, merge, and see the shape of your thinking. It stayed in the \"someday\" pile because the implementation cost felt too high for a speculative side project; I wasn't skilled in browser technologies or anything UI-related. Then came the Christmas break ultralearning exercise I documented in my recent blog post about building with OpenCode and Claude Opus 4.5. Pressure-testing Opus 4.5 made me realize it was finally feasible to spend a day trying to make this work. I pushed Canvas Chat from idea to working prototype in about 24 hours of actual building time, and then another 24 hrs to get it up on Modal and add in many, many refinements, each of which may have taken me multiple weeks. The final result is this: But before I explain what I built, let me explain why I wanted it in the first place. The job to be done Clayton Christensen's Jobs to Be Done framework asks: what job is the customer hiring this product to do? For Canvas Chat, the job isn't \"chat with an LLM\"\u2014ChatGPT already does that fine. The job is: think through a complex problem where the exploration is nonlinear. Here's the struggling moment. You're deep in a conversation with Claude or GPT, and you want to try a different framing of your question. But if you do, you'll lose the current thread. Or an LLM gives you a list of ten ideas, one catches your eye, and you want to drill into it\u2014but the conversation keeps scrolling and you lose the overview. Or you've been exploring a problem across three separate chat sessions and now you need to synthesize, but you can't see them together. Linear chat actively works against this kind of thinking. It forces linear structure onto nonlinear exploration. You end up managing context in your head, copy-pasting between windows, losing track of which threads went where. Canvas Chat exists to solve that. When your thinking branches in multiple directions, it keeps all the threads visible and connected so you don't lose context and can synthesize across them. How it works Canvas Chat is an infinite canvas where conversations are nodes in a directed graph. You type a message, it appears as a node. The LLM's response appears as another node, connected by an edge. So far, standard. But then: Branch from any node. Click reply on any message, and your new message connects to that point, not the end of the conversation. The response branches off visually. Try two different prompts from the same starting point and see both branches side by side. Highlight and branch. Select text within a node, and a tooltip appears. Type a follow-up question, and Canvas Chat creates a highlight node (showing the excerpt with a blockquote) plus your question, plus the LLM response. The original node stays intact. This works especially well when an LLM gives a list of ideas and you want to drill into one without losing the overview. Multi-select for merge context. Cmd-click multiple nodes, then type. The new message connects to all selected nodes, and the LLM sees the full ancestry of every selected node. I use this to synthesize: select two branches that went in different directions, ask \"What do these approaches have in common?\" The context includes everything that led to both. Context flows through the graph When you send a message, Canvas Chat walks the DAG backward from your selected node(s), collecting all ancestors. It sorts them by creation time and sends them to the LLM as conversation history. If you've selected multiple nodes (a merge), the context is the union of all their ancestors, deduplicated. The practical effect: the LLM always knows how you arrived at the current question, even if the path is nonlinear. Branch from a discussion about protein folding dynamics, ask a follow-up about computational costs, and the context includes the protein folding discussion. No manual copy-paste. Matrix evaluation This feature came out of a specific struggling moment: evaluating many options against many criteria and losing track of which combinations I'd thought through. Select one or more nodes as context, type . Canvas Chat parses out the list items and shows a confirmation modal where you can remove items or swap rows/columns. Click create, and a matrix node appears. Each cell has a \"+\" button. Click it and the LLM fills that cell, seeing the matrix context you provided, the row item, the column item, and the full DAG history from the source nodes. \"Fill All\" processes every empty cell sequentially. Click any filled cell to see the full text. \"Pin to Canvas\" extracts that evaluation into a standalone node, which you can then branch from. Say you're comparing business ideas against criteria, one cell says \"strong market fit with enterprise customers,\" you want to dig into that\u2014pin and branch. Web search and deep research Canvas Chat integrates Exa's APIs for two slash comman",
    "tags": [
      "ai",
      "llm",
      "opencode",
      "claude",
      "visualization",
      "tools",
      "productivity"
    ],
    "pub_date": "2025-12-31",
    "type": "blog"
  },
  {
    "id": "blog-you-can-just-make-stuff-with-opencode-and-claude-opus-4-5",
    "url": "/blog/2025/12/28/you-can-just-make-stuff-with-opencode-and-claude-opus-4-5/",
    "title": "You Can Just Make Stuff with OpenCode and Claude Opus 4.5",
    "summary": "In this blog post, I share how using OpenCode and Claude Opus 4.5 has transformed my approach from writing code to simply building\u2014directing AI to create what I envision. I discuss how these tools handle everything from infrastructure to greenfield apps, and how reasoning traces have become more important than code review. I also reflect on unlearning old habits and embracing new possibilities as AI models improve. Curious how this shift could change your own workflow?",
    "body": "Tommy Tang asked me about my opinions on OpenCode, so here's what I've learned after spending significant time with OpenCode and Claude Opus 4.5. I don't code anymore, I build This is the punchline, so let me start with it. I've shifted from writing code to directing its creation. The change happened gradually, then all at once. I used to think about syntax, edge cases, and implementation details. Now I think about what I want to exist, describe it clearly, and watch it materialize. Genesis 1:3 describes this pattern at a cosmic scale: \"And God said, 'Let there be light,' and there was light.\" Working with Claude Opus 4.5 through OpenCode feels like a microcosm of that creative act. Eric said, \"Let there be a feature,\" and there was the feature, in code. I'm not claiming divinity here, just noting that the creative pattern of speaking things into existence has become surprisingly literal in my daily work. The tools: OpenCode and Claude Opus 4.5 Like Theo Brown from t3.gg, I've settled on Claude Opus 4.5 as my primary model for coding tasks. It just knows what to do. I've stopped trying to micro-manage the model's actions because it handles most tasks autonomously and correctly. When I ask for a refactor, it refactors. When I describe a feature, it implements it. The gap between intention and execution has shrunk to almost nothing. Other models require more hand-holding. Opus 4.5 seems to have internalized enough software engineering patterns that I can trust it to make reasonable architectural decisions without constant course corrections. I can literally ask it to \"do the docs, keep things up-to-date, and also give me a document that has an overview of code organization and architecture.\" It just goes to town autonomously. No step-by-step prompting, no breaking the task into smaller pieces. I describe the outcome I want and it figures out the path. The tooling layer matters too. OpenCode orchestrates the AI coding in a way that feels natural. The tools it calls are always logical, the reasoning traces are transparent, and the execution flow makes sense. It shows a running list of modified files, giving me context about what's changing without running constantly. Context compaction lets me stay in one long-running session without hitting token limits. I've thrown out the old playbook of \"switch sessions when you approach the context window.\" Now I only switch when I want to do something entirely different. My setup: OpenCode with auto-updating, GitHub Copilot Pro as the LLM provider (routing to Opus 4.5), running inside a tmux session for persistence. Each repo gets an AGENTS.md file where I encode my preferences and patterns - the model's training data for my specific context. Opus 4.5 actually respects what's in there, unlike some other models that seem to ignore custom instructions. Ten days of deliberate practice I decided to pressure-test the \"I build\" claim over the holidays. Ten days, December 19-28, using OpenCode as my primary development interface. The goal: see how much I could actually ship. The answer surprised me. Across six repositories, I pushed over 150 commits spanning infrastructure work, documentation, greenfield apps, and maintenance. Here's what emerged: A ski trip coordination website (59 commits). My family was heading to New Hampshire for a week. Normally I'd have used a shared Google Doc for the itinerary. Instead, I built a full website with recipe modals, restaurant links with Apple and Google Maps integration, a photo album with lightbox navigation, automatic thumbnail generation, and a hero video background. I updated it live during the trip - adding photos, adjusting the grocery list, swapping menu items. The implementation cost would have been absurd for a week-long trip before. Now the jazz and snazz was well worth the effort - my family actually enjoyed using it. A teaching clock app for my kids (2 commits, but a complete app). An analog clock trainer plus a jigsaw puzzle game with difficulty levels and themes. Pure JavaScript and CSS - exactly the kind of project my decade-old \"no JavaScript\" rule would have blocked. The model wrote it; I directed. pyjanitor infrastructure (40 commits). Currency symbol support for international formats. Automated patch releases on every merge. Test isolation fixes. And a major expansion of AGENTS.md into what I now think of as the repository's \"agent constitution\" - a document that tells AI assistants how to work within this specific codebase. A new conda-forge package for janitor-rs. The model handled the unfamiliar territory of Rust packaging and conda-forge recipe formats. I was the novice here; it was the guide. This role reversal keeps happening - when I set up PostHog analytics or migrated to GA4 on my website, the model walked me through each step, explained what I was doing and why, and waited for confirmation before proceeding. The expert-novice relationship flips depending on who knows more about the task at hand. A custom tmux s",
    "tags": [
      "ai",
      "opencode",
      "claude",
      "automation",
      "workflow",
      "llm",
      "reasoning",
      "development",
      "review",
      "tools"
    ],
    "pub_date": "2025-12-28",
    "type": "blog"
  },
  {
    "id": "blog-how-i-themed-my-tmux-with-opencode-and-claude",
    "url": "/blog/2025/12/27/how-i-themed-my-tmux-with-opencode-and-claude/",
    "title": "How I Themed My tmux with OpenCode + Claude (And When to Switch Models)",
    "summary": "I pair-programmed a tmux status bar theme with OpenCode and Claude, discovering along the way when to switch between Sonnet and Opus models. The real insight: AI enables creative expression by bridging the gap between aesthetic vision and technical implementation, letting me work like a designer even though I'm not one.",
    "body": "I had a beautiful tmux status bar on my old laptop. Nord colors, powerline arrows, clean and minimal. The kind that makes you feel like a proper terminal power user. When I got a new machine back in April, I was too lazy to set up tmux properly. The sensible thing would have been to spend five minutes copying over my old config. Instead, eight months later, I finally spent an hour pair-programming the whole thing from scratch with OpenCode and Claude. Why? Honestly, I wanted to try out a new tool. The irony isn't lost on me. The Setup OpenCode is a CLI tool that lets you interact with Claude directly from your terminal. Perfect for this kind of task: I'm already in the terminal configuring tmux, so having my AI pair programmer right there keeps the feedback loop tight. Describe what I want. See the change. Describe what's wrong, with precision. Iterate. No context switching to a browser. That tight loop is what let me stay in the creative headspace. I could say things like \"I want the arrows to overlap like in this screenshot\" or \"the colors feel too muted, try the frost blue from Nord\" without knowing the exact syntax. Claude translated my aesthetic intent into working config. The other superpower: model switching. OpenCode lets you flip between any models you have API keys for. For this session, I toggled between Claude Sonnet (fast, good for quick iterations) and Claude Opus (slower, but sharper for complex debugging). This turned out to be crucial. Starting with Research First, I asked Sonnet to search online for tmux status bar customization. It pulled resources from the official tmux wiki and various tutorials, giving me a foundation: , , , color options, the basics. Armed with that, we dove in. First attempt with a custom theme Claude created a custom dark theme inspired by Catppuccin colors. Worked immediately: Clean. Functional. Pretty. But I wanted more: those beautiful powerline arrows flowing between segments. That's when things got interesting. The Powerline Saga Claude suggested , a Go-based powerline prompt generator. We installed it via Homebrew (not pip, since I keep my system Python-free): Updated the tmux config to call powerline-go for the status bar. Reloaded. And... disaster. Instead of beautiful arrows, raw escape codes: The terminal was spitting out ANSI codes instead of interpreting them. We tried various fixes, but powerline-go simply wasn't designed for tmux status bars; it's meant for shell prompts. Back to square one. Trying the tmux-powerline Plugin Next attempt: the actual plugin via TPM (Tmux Plugin Manager): Added the plugin, pressed to install, and... the status bar exploded with information. IP addresses, weather, load averages, hostname. Way too much. I asked Claude to simplify and switch to Nord colors. We created a custom theme at , updated the config, reloaded tmux. Nothing changed. The theme wasn't loading. Killed the server entirely. Restarted. Still the old crowded theme. This is where Sonnet started struggling. Same fixes over and over: reload the config, check the theme path, restart tmux. Loop after loop of suggestions that weren't working. The model switch from Sonnet to Opus I noticed Sonnet spinning its wheels. Same suggestions, same non-results. Time to switch. The difference was immediate. Instead of repeating failed approaches, Opus stepped back and proposed something different entirely: ditch the plugin and go native. Tmux's built-in formatting is powerful enough to create powerline-style status bars without any plugins. We just needed the right Unicode characters and color transitions. This stuck with me: Sonnet is fantastic for speed and quick iterations, but when you're stuck in a loop, Opus brings the lateral thinking to break out. Going native as the winning approach Fresh start. Clean native tmux config. The key insight was understanding how powerline arrows actually work: the arrow character's foreground color matches the background of the segment it's coming from, and its background matches what it's going into. Here's the final status-left (session name with powerline arrow): The window formats, with arrows on both sides so they flow into neighboring elements: And the right side (battery, date, time) using left-pointing arrows and a smooth Nord color gradient: The Final Result After all that iteration, here's what my tmux status bar looks like: system-config 1 opencode \ud83d\udd0b 100% Dec 23 06:05 Session name in frost blue on the left. Active window in cyan. Right side flows through battery (green), date (blue), and time (cyan). All connected by powerline arrows with smooth color transitions. (I asked Claude to recreate the status bar in HTML so I wouldn't have to screenshot it for the blog.) What I Took Away There's a growing conversation about AI-assisted programming: the tight feedback loops, model selection strategies, iterative workflows. I've written about some of these patterns myself. But this session crystallized something different. I can expres",
    "tags": [
      "ai",
      "opencode",
      "claude",
      "tmux",
      "terminal",
      "creativity",
      "workflow",
      "pair-programming"
    ],
    "pub_date": "2025-12-27",
    "type": "blog"
  },
  {
    "id": "blog-two-years-of-weekly-blogging-and-what-2025-taught-me",
    "url": "/blog/2025/12/25/two-years-of-weekly-blogging-and-what-2025-taught-me/",
    "title": "Two years of weekly blogging and what 2025 taught me",
    "summary": "Reflecting on my second year of weekly blogging, I published 50 posts in 2025, bringing my two-year total past 100. This year was dominated by coding agents and AI-assisted programming, with extensive writing on AGENTS.md, autonomous agents, and productive patterns for working with AI. I also explored Bayesian methods for biological applications, got excited about Marimo and Modal, and wrote about data science leadership and career development. Two years of consistent writing has reinforced that writing clarifies thinking, consistency compounds, and the best posts come from problems you're actively solving.",
    "body": "Last year, I challenged myself to write one blog post per week, and I hit 53 posts by the end of 2024. This year, I doubled down on that commitment and wrote 50 posts in 2025. Including this one, it's 51, bringing me to 104 blog posts over two years. The year of coding agents Looking at my 2025 posts, one theme dominates: coding agents. I wrote extensively about how to work with AI coding assistants, from teaching them with AGENTS.md files to letting them work autonomously. This reflected a shift in how I work day-to-day. Some highlights from this theme: - How to teach your coding agent with AGENTS.md - Safe ways to let your coding agent work autonomously - Productive Patterns for Agent-Assisted Programming - How I Replaced 307 Lines of Agent Code with 4 Lines The shift from \"AI as a tool\" to \"AI as a collaborator\" captures how my practice evolved this year. I've gone from cautiously experimenting with Cursor to having established patterns for multi-repository agent workflows. Bayesian methods and biological applications My work continued to inform my writing, with several posts on applying Bayesian statistics to real lab problems. The R2D2 prior posts were particularly satisfying to write because I felt equipped with new theoretical knowledge that was directly applicable, and I appreciated the mathematical aesthetics behind the approach: - Bayesian Superiority Estimation with R2D2 Priors: A Practical Guide for Protein Screening - Stop guessing at priors: R2D2's automated approach to Bayesian modeling - From data chaos to statistical clarity: A laboratory transformation story I also explored the challenges of working with lab data, including why preclinical experiments make ML challenging and how to communicate effectively with lab scientists. Tools I got excited about Every year brings new tools that change how I work. In 2025, two stood out. Marimo is a reactive notebook tool that I wrote about with enthusiasm, and followed up with practical guidance on using coding agents to write Marimo notebooks. The reactive execution model aligns well with how I think about data exploration. Modal is cloud computing that actually feels Pythonic. My \"Wow, Modal!\" post captured the delight of finding infrastructure that doesn't fight against my workflow. Data science leadership and career I continued writing about the human side of data science work, including standardizing ways of working, communicating with lab scientists, and navigating the biotech industry's ups and downs. The year ended with The selfish reason to do your best work, which synthesized lessons from a challenging year in biotech. Looking ahead to 2026 After two years of writing almost weekly on whatever is on my mind, I am adjusting my goals. Next year, my attention shifts towards (a) learning the fundamentals of quantum computing through an ultralearning project, (b) writing more on data science leadership and career development to encourage colleagues navigating similar paths, and (c) building out at least 10 experimental things with AI. I am also dropping the goal of \"one blog post per week\" to four per month, which brings me to a goal of 48 for 2026. I am giving myself space to rest and strategically plan out writing going into 2026. Merry Christmas and a happy new year to all my readers! Blog posts by theme Biology & Chemistry - What makes an agent? (2025-01-04) - Why data from preclinical biotech lab experiments make machine learning challenging (2025-01-19) - Reliable biological data requires physical quantities, not statistical artifacts (2025-02-23) - A blueprint for data-driven molecule engineering (2025-03-06) - Bayesian Superiority Estimation with R2D2 Priors: A Practical Guide for Protein Screening (2025-04-03) - From data chaos to statistical clarity: A laboratory transformation story (2025-04-05) - Good practices for AI-assisted development from a live protein calculator demo (2025-04-19) - Build your own tools! (2025-06-27) - Reflections on the SciPy 2025 Conference (2025-07-14) - How to use xarray for unified laboratory data storage (2025-07-15) - How to communicate with lab scientists (when you're the data person) (2025-08-24) - How data scientists can master life sciences and software skills for biotech using ultralearning (2025-10-01) - What does it take to build a statistics agent? (2025-12-02) Career Advice - Writing at the speed of thought (2025-01-13) - Why you should take part in the SciPy sprints! (2025-03-17) - Why I'm excited for SciPy 2025! (2025-05-08) - Reflections on the SciPy 2025 Conference (2025-07-14) - Data scientists aren't becoming obsolete in the LLM era (2025-08-15) - How to use AI to accelerate your career in 2025 (2025-09-01) - How data scientists can master life sciences and software skills for biotech using ultralearning (2025-10-01) - The selfish reason to do your best work (2025-12-17) Data Science Practice & Leadership - A practical guide to securing secrets in data science projects (2025-01-10) - Why da",
    "tags": [
      "blogging",
      "retrospective",
      "coding agents",
      "llms",
      "bayesian",
      "biotech",
      "career",
      "writing",
      "marimo",
      "modal",
      "data science"
    ],
    "pub_date": "2025-12-25",
    "type": "blog"
  },
  {
    "id": "blog-the-selfish-reason-to-do-your-best-work",
    "url": "/blog/2025/12/17/the-selfish-reason-to-do-your-best-work/",
    "title": "The selfish reason to do your best work",
    "summary": "In this post, I share a philosophy on career growth and resilience. Instead of doing the bare minimum or succumbing to frustration, I encourage you to view your work as a personal investment. By doing your best work for yourself, you build professional instincts, a strong reputation, and the character to handle both success and failure. Whether you're navigating a tough job market or rebounding from a mistake, the wealth you accumulate in skills and reputation is yours to keep forever. Ready to invest in yourself?",
    "body": "I\u2019ve been thinking a lot about career lately. This has been a pretty lean year for biotech; we've seen ups and downs at Moderna and across the industry. So, I want to offer a word of encouragement and a philosophy on work that I hope can be useful for you, regardless of where you are in your journey. It starts with a reframing of why we work. Do your best work for yourself I know there is a lot of sentiment going around the internet right now about \"acting your wage\"\u2014limiting your effort to exactly what you are paid for\u2014or doing the bare minimum. The logic goes: Why should I care about doing my best for my job if my company doesn't care for me? I get where that sentiment comes from. But I want to redirect your attention a little bit. *You do not have to do your best work for your company. You should do your best work for yourself at the company you\u2019re at. It just so happens that the company will benefit, but you should treat your effort as an investment in your own professional instincts and habits. Sooner or later, you may not be at that company. You actually don't really have to care about the entity itself. If you don't care about the people who work at your workplace, no one is compelling you to. (Though, if you do happen to like your colleagues\u2014which is true for me where I work\u2014then that\u2019s all good.) But even if you can\u2019t find much to be inspired by, do your best work anyway. Why? Because you are building the person you will be in five or ten years. President Obama once gave this advice to young interns: \"Don't ask for the plum assignments. Just knock out everything you're doing.\" I guarantee you someone will notice. Even if no one at your current company notices, if you build a track record of quality, people outside will notice. Your reputation precedes you. It is the one thing you accumulate over time that serves as a form of wealth that can never be taken away from you. Only you can lose it. To borrow a phrase from Jocko Willink, this is \"Extreme Ownership\"\u2014taking total responsibility for your world. Yes, circumstances happen, but if you guard your reputation well, it is yours to keep. Think about it: Who knows where you will be ten years down the road? If you are a software engineer or data scientist now, in five years you might be a Director. You\u2019re going to be calling the shots. If you don't take the time now* to practice making decisions, witnessing judgment calls, and battle-testing your engineering foresight, will you be ready? I had a former teammate who worked under me at Moderna, Arkadij Kummer. He\u2019s now the CTO of a startup\u2014a title I haven't even held. I saw him put in the effort to develop the strategic thinking patterns that helped him get the skills he needed to lead a tech organization. He sought out opportunities to practice making judgment calls and owning the outcomes. You have to get those reps in early, or you won\u2019t be ready for what happens later. So, if you are working at a company you want to leave: do not give up on investing in yourself. Fortune favors the prepared. Resilience is also an investment Building this \"career wealth\" isn't just about technical execution. It's also about how you handle failure. And here is the second word of encouragement I want to offer: Everyone will make a blunder at some point. Part of growing up\u2014and part of investing in your own character\u2014is owning up to those mistakes, being proactive about remedying them, and graciously accepting help. I have been there. I once wrote a very scathing internal blog post about my leadership at a previous company. Looking back, I sometimes think of it as the darkest weekend of my career. I was a guy who had just finished school, entered the workforce, and within three years decided I knew better than people who had done their jobs for twenty-odd years. I'm not saying it's impossible that I was right, but it was pretty presumptuous. I lashed out at other groups that I thought were incompetent, effectively attacking my own team. The leadership team responded with extreme grace. They knew the problems I outlined were real, but they also saw a junior person who hadn't picked his battles wisely. We have limited energy and limited ability to focus. There are only so many battles we can handle simultaneously. I chose a bad battle to fight. That experience prompted deep reflection. I decided to lean back into that first philosophy: I was going to go back and do a good job. Not necessarily because I was feeling proud of the company at that moment, but because I recognized that if I ever wanted to be the type of person with the authority to change things, I needed to be the best version of myself first. I needed to learn how to handle authority and how to elevate the people around me. If you are in a situation where you\u2019ve made a mistake, the best thing you can do for your reputation is to own it. Propose an action to rectify it. Move on. Most mistakes are not unforgivable. There is a classic business story about Tom Wats",
    "tags": [
      "career",
      "growth",
      "work",
      "philosophy",
      "resilience",
      "reputation",
      "leadership",
      "mistakes",
      "success",
      "advice"
    ],
    "pub_date": "2025-12-17",
    "type": "blog"
  },
  {
    "id": "blog-productive-patterns-for-agent-assisted-programming",
    "url": "/blog/2025/12/10/productive-patterns-for-agent-assisted-programming/",
    "title": "Productive Patterns for Agent-Assisted Programming",
    "summary": "In this blog post, I share the patterns that have made my experience with coding agents much more productive, from planning with AI and writing docs/tests first, to using AGENTS.md as a knowledge base and leveraging command line tools. I also discuss pacing your agent, letting it write temporary tools, and developing your own productivity boosters. Want to know how these strategies can make your agent-assisted programming smoother and more effective?",
    "body": "I've been using coding agents for a while now, and I've learned a few patterns that make the experience much more productive. The thing is, a lot of these \"productive patterns\" aren't being shared enough\u2014they're more like folk knowledge that you can only really pick up by watching someone else do their work live. I decided to write this blog post to kickstart conversations about the matter. Here's what works for me. Build a detailed plan with AI Before jumping into implementation, spend time building a detailed plan with your AI assistant. Iterate 2-3 times over the plan, checking every detail. You want the ability to see in your head what the code might look like\u2014just a \"fat finger sketch\" of the implementation. The plan should include: - Details on implementation - How to test (this is the most important part) - Documentation plan Do docs and tests first Humans usually adhere to test-driven development if you're a software engineer, or exploration-driven software builds if you're more of a data scientist. Because of the sequential nature of generative AI, it's advantageous to instruct AI to do the docs and tests first before the implementation. This is a complex conditional probability problem. If the tests and docs are written first, the implementation has to satisfy those constraints, which leads to better code. The test plan should include instructions on how to run tests using command line tools. Don't assume the AI knows your project's specific testing setup. Use AGENTS.md as your repo's AI university AGENTS.md is a great place to store the specific instructions that you need for the repo. For example, AI will tend to write as a shell command, but if I'm running a pixi project, it's better to always run instead. Treat AGENTS.md as the AI's university of your particular repo; it's where you encode all the project-specific knowledge that the AI needs to work effectively. Control the pace of the agent Know the default behavior of your agent; it may be over-eager to do lots of things. You can pace the coding agent by asking it to \"slow down, walk me through the changes one at a time, starting with the most important ones first.\" This helps you maintain control and review changes as they happen, rather than being overwhelmed by a massive diff. Leverage local and command line tools You can use local and command line tools to your advantage! Here are some examples: Firstly, the GitHub CLI () can be used to: - Store plans on GitHub as issues first (a matter of taste\u2014you can avoid cluttering up your local filesystem) - Pull GitHub Actions logs - Call out to the GitHub API for other general tasks Environment management: - ensures you're always running within the correct Python environment - lets me check that marimo notebooks are syntactically valid - lets me run notebooks as scripts to check outputs - lets me export marimo notebooks as markdown Linting and quality: - runs on every edit of markdown files so you never have markdown linting issues - Get AI to \"commit relevant files and fix issues raised by pre-commit hooks\" Let agents use CLI tools and read outputs directly so that you don't have to switch between windows copying and pasting things manually. Let agents write temporary tools Coding agents can write their own temporary tools inside files. Encourage coding agents to do that to test that what it wrote works on-the-fly. This is a great way to validate code before integrating it into your main codebase. You can even experiment with self-improving agents: if it detects you correcting its action, it should auto-update AGENTS.md with what is the correct thing to do. I haven't fully battle-tested this yet, but you can write an \"AI constitution\" at the top of AGENTS.md that instructs the agent to learn from corrections by remembering them inside AGENTS.md. Develop your own tools Isabel Zimmerman mentioned this in her keynote talk: develop your own tools. Here are some examples of my own: - Personal MCP productivity server: gives me prompts that I can take from project to project, so I don't have to keep copying/pasting them - Shell aliases: lets me run - LlamaBot git hooks: auto-writes commit messages for me These custom tools compound over time and make your workflow significantly more efficient. These patterns have made my agent-assisted programming much more productive. Treat the AI as a collaborator that needs clear instructions, proper context, and the right tools to work effectively. Start with a good plan, control the pace, and build tools that make the whole process smoother. What patterns have you discovered? I'd love to hear what works for you\u2014let's make this folk knowledge more accessible to everyone.",
    "tags": [
      "productivity",
      "documentation",
      "testing",
      "planning",
      "automation",
      "workflow",
      "agents",
      "ai",
      "tools",
      "development"
    ],
    "pub_date": "2025-12-10",
    "type": "blog"
  },
  {
    "id": "blog-what-does-it-take-to-build-a-statistics-agent",
    "url": "/blog/2025/12/2/what-does-it-take-to-build-a-statistics-agent/",
    "title": "What does it take to build a statistics agent?",
    "summary": "In this blog post, I share my journey building a domain-specific statistics agent to help researchers design better experiments, inspired by the challenges of limited access to statisticians in pharma and biotech. I discuss the pitfalls of \"folk statistics,\" the importance of prompt engineering, and the lessons learned through iterative testing and refinement. Curious how an AI agent can elevate experimental design and what it takes to make it truly helpful?",
    "body": "Within research organizations at most pharma and biotech companies, professionally-trained statisticians are often staffed at extremely low ratios relative to the number of lab scientists. By rough Fermi estimation, I'd hazard a guess that ratios anywhere from 1:10 to 1:100 are plausible, meaning most researchers have limited access to statistical expertise when they need it most, during experiment design. This statistician shortage creates a critical bottleneck in experimental design, power calculations, and biostatistical consultation\u2014areas where proper statistical guidance can prevent costly mistakes and improve research reproducibility. This creates a costly problem. When statisticians aren't available, researchers fall back to what I call \"folk statistics\" - the kind you learn by immersion in a lab, or from 1-2 graduate lectures hidden within broader \"laboratory methods\" or \"computational methods\" classes. I know this because I practiced folk statistics myself in the life sciences, blindly following rules like \"just do n=3\" or \"just use the t-test with your count data\" without understanding the statistical reasoning behind these choices. The consequences are documented in stark numbers. Amgen scientists attempted to reproduce 53 landmark preclinical papers and failed in 47 cases (89%)\u2014even after contacting original authors and exchanging reagents. Bayer's internal validation found only 20-25% of studies \"completely in line\" with original publications. These studies consistently identified poor experimental design and inadequate statistical analysis as major contributors. Freedman et al. (2015) estimated $28 billion annually spent on irreproducible preclinical research in the United States alone. Breakdown of causes of preclinical irreproducibility from Freedman et al. (2015). Study design accounts for 27.6% of irreproducibility. At the individual experiment level, this translates to teams throwing out hard-won experimental data that can cost anywhere from thousands to hundreds of thousands of dollars to collect per round, wasting up to millions of dollars downstream by basing decisions on poorly-collected data, and missing opportunities to set up machine learners with high quality laboratory data that could shortcut the amount of laboratory experimentation needed. I took one semester of graduate-level biostatistics, then a decade of self-study in Bayesian statistics, followed by professional work where accurate estimation was critical\u2014whether estimating half-life of a molecule, binding affinity of an antibody, or other performance properties. Through this journey, I no longer trust folk statistics. Folk statistics relies on faulty assumptions\u2014like \"n=3 is all you'll really need,\" \"use the t-test for count data,\" or \"calculate the SEM and don't show the SD\"\u2014which influence bad decision-making when people don't know better. Once you see how these assumptions break down and lead to wrong conclusions, you can't unsee it. Quantities like half-life, binding affinity, and other performance properties need to be accurately estimated through proper experimental design and statistically-informed mechanistic modeling. Statisticians are expensive, but they're also 100% critical for generating high quality, high fidelity data. Their role at the experiment design phase is usually that of a consultant, asking probing questions to ensure experiments are designed with good controls, confounders are accounted for, and the right statistical models are chosen. The question is: can we scale this expertise? Not to replace statisticians, but to level up the organizational statistical practice before researchers check in with a professionally-trained stats person. If lab scientists can think through their experimental designs more rigorously beforehand - understanding power calculations, considering confounders, planning proper controls - then the conversations they have with statisticians can be elevated. Instead of starting from scratch, they can engage in more sophisticated discussions about design trade-offs, model selection, and advanced statistical considerations. In turn, this amplifies the value of the statistician's time and improves outcomes for everyone. I was inspired by Dr. Emi Tanaka's slides on extracting elements of statistical experiment design using LLMs, which showed how we can extract structured information like response variables, treatments, experimental units, design types, replicate structure, and controls. I decided to take a stab at building something that could do more than just extract information\u2014something that could actually consult on experiment design. And so was born: an AI-powered statistics agent for experiment design consultation. Here's how I designed and evaluated this domain-specific AI agent. As a preface, I initially explored the ReAct pattern but switched to PocketFlow, a minimalist graph-based framework that replaced 307 lines of agent orchestration code with just 4 lines. This graph",
    "tags": [
      "statistics",
      "biotech",
      "reproducibility",
      "experiments",
      "research",
      "automation",
      "ai",
      "data",
      "open source",
      "bayesian"
    ],
    "pub_date": "2025-12-02",
    "type": "blog"
  },
  {
    "id": "blog-how-to-reference-code-across-repositories-with-coding-agents",
    "url": "/blog/2025/11/17/how-to-reference-code-across-repositories-with-coding-agents/",
    "title": "How to Reference Code Across Repositories with Coding Agents",
    "summary": "In this blog post, I share how coding agents like Cursor, GitHub Copilot, and Claude Code can access files across your entire file system\u2014not just your current workspace. By simply referencing explicit file paths, you can pull code, documentation, or configs from any repository without complex workspace setups or copying files. I explain practical workflows and tips for cross-repository access, making coding and writing more seamless. Curious how this can simplify your development process and boost productivity?",
    "body": "I used to assume that coding agents like Cursor, GitHub Copilot, and Claude Code only work within a single workspace. This mental model led me to workarounds like copying files, creating complex multi-root workspace configurations, or constantly switching between projects. But coding agents can already read and write files from anywhere on your file system, not just the current workspace. The limitation wasn't in the tools; it was in my awareness of what they can do. You don't need to add folders to workspaces, create multi-root workspaces, or jump through configuration hoops. If you know where a repository lives on your disk, you can reference it directly. How to reference code from other repositories The key is being explicit about file paths. Modern AI coding assistants like Cursor, GitHub Copilot, and Claude Code can access your entire file system, not just the current workspace. You just need to tell them where to look. I do most of my writing in an Obsidian vault, which isn't a Git repository; it's just a folder on disk. Sometimes I need to reference code from my LlamaBot repository, or other code repositories in which I am doing development. Instead of copying files or creating complex workspace configurations, I just tell the agent to read directly from the other directory. When I need the agent to understand something from LlamaBot, I can say \"read the implementation from \" and it works immediately. The key is being explicit with the path. You can also search by filename within a directory: \"find the notebook named in \". The agent reads the file directly from disk, no workspace configuration needed. You don't need to document paths anywhere; just reference them directly when you need them. That said, if you have commonly accessed paths, documenting them in can be helpful for quick reference. I used this method while writing my blog post \"How I Replaced 307 Lines of Agent Code with 4 Lines\". I was drafting the post in my Obsidian vault, but the actual code examples lived in a Marimo notebook within the LlamaBot repository. Rather than copying code snippets or switching workspaces, I had the agent read directly from to pull in the exact implementation details I needed. This let me write about the code while staying in my writing environment, with the agent able to reference the actual source files to ensure accuracy. File system access for AI coding assistants enables this Coding agents that have file system access can perform read and write operations anywhere they have permission. Tools like Cursor, GitHub Copilot, and Claude Code aren't restricted to the current workspace directory. This works because agents have access to shell tools, the most generic, text-based interface to computers. Shell commands produce text output that agents can read and understand, and they can execute commands anywhere on your system. This means: - You can reference code from any repository on your machine - You can pull in documentation from other projects - You can compare implementations across different codebases - You can reference configuration files from related projects - You can modify files across multiple repositories when needed The only requirement is that you know the path and can tell the agent where to look. Common scenarios Blogging: When writing blog posts about code, reference implementation details from your repositories. The agent can read the actual code to ensure accuracy, pulling in exact examples without copying files or switching workspaces. Architecture decisions: Compare how similar problems are solved across different projects. The agent can read multiple implementations and help you understand trade-offs. Code reuse: Before copying code, have the agent check if similar functionality exists elsewhere. It can read files from other repos to find existing solutions. Dependency understanding: When working with a library you maintain, reference the library's source code directly. The agent can read implementation details to help you use it correctly. Cross-repository updates: Update related files across multiple repositories simultaneously. For example, update documentation in one repo while modifying the implementation in another, or sync configuration changes across related projects. Step-by-step workflow for cross-repository code access The key trick is being explicit with paths or explicit instructions about how to get to files. Here's how to do it: For repositories you already have cloned locally: 1. Reference the absolute path directly when asking the agent: \"read \" 2. Or search by filename within a directory: \"find the notebook named in \" 3. The agent reads the file immediately, no workspace configuration needed For repositories you don't have locally: 1. Tell the agent exactly how to get to the file: \"clone the repo into a temporary directory, then find the file at relative path \" 2. You can also specify a specific commit, branch, or tag: \"clone the repo at commit into a temporary directo",
    "tags": [
      "automation",
      "coding",
      "agents",
      "workflows",
      "productivity",
      "ai",
      "filesystem",
      "reference",
      "workspaces",
      "shell"
    ],
    "pub_date": "2025-11-17",
    "type": "blog"
  },
  {
    "id": "blog-how-i-replaced-307-lines-of-agent-code-with-4-lines",
    "url": "/blog/2025/11/16/how-i-replaced-307-lines-of-agent-code-with-4-lines/",
    "title": "How I Replaced 307 Lines of Agent Code with 4 Lines",
    "summary": "In this blog post, I share how I discovered PocketFlow, a minimalist framework for building LLM-powered programs using graph-based flows instead of complex loops. By rethinking my approach, I replaced 307 lines of agent orchestration code with just 4 lines, making my agents more modular, clear, and easy to visualize. I walk through practical examples, show how to build and visualize agent architectures, and reflect on the benefits of graph-based thinking for LLM applications. Curious how this shift can simplify your own AI projects?",
    "body": "I recently discovered PocketFlow, a framework for building LLM-enabled programs created by Zachary Huang. The entire framework is tiny\u2014only 100 lines of code. What caught my attention is that PocketFlow takes a fundamentally different approach to LLM-powered programs, including Anthropic's workflows and agents, by structuring them as graphs. As someone who used graphs in my thesis work, taught tutorials on applied graph theory, and builds my own agent frameworks, my curiosity was piqued. I wanted to see two things: whether I could learn enough of the framework to build something useful, and whether LlamaBot's abstractions could complement PocketFlow's approach. To explore this, I fired up a Marimo notebook. (You can fire it up too by running: ) Understanding the Core - Nodes and Flows I started by building what I consider a \"Hello World\" program: a text topic extractor and question generator. This let me familiarize myself with PocketFlow's two core abstractions: and . A is a unit of execution structured like this: There's one more concept to introduce: . In PocketFlow, is like a big workspace that all s can read and write from. Think of it as a kitchen island where chefs and cooks can grab ingredients and leave finished dishes. In computing terms, it's global state that programs can access. In practice, it's simply a dictionary that lives in memory, which any node can manipulate. For example, program might be a key in there, implemented as a list. The design within a node is intentional. In theory, you could do everything in one step\u2014there are no hooks that inject stuff between, say, and . In practice, doing everything in one step muddies the program and makes it harder to reason about. I'll show you why later in this post. Here's what each step is designed to do: - * takes stuff from the dictionary, does any preprocessing, and passes it to . This could include grabbing stuff from memory, interpolating it into a prompt, and returning it for execution with the LLM. - is where the bulk of heavy computation happens. We put API calls to LLM providers (Ollama, OpenAI, Anthropic, etc.) here. What gets returned is passed to the method. - handles any post-processing. It receives , (the result of ), and (result of ). The pattern I've settled on is archiving results in \u2014for example, storing execution results in memory. What gets returned by should be a string indicating which downstream path to follow. If nothing specific is needed, it returns . A is declared with a starting and follows the program until completion. With this abstraction, multiple LLM-powered abstractions and design patterns can be designed: (Image from the PocketFlow official documentation.) Example 1 - Topic Extractor and Question Generator Here's how I built the two-step/node topic extractor + question generator. First, I declared the nodes: Then, I declared the graph: The magic happens in this line: This tells the flow that once the node emits , it should proceed to the node. The syntax is compact and looks exactly like an edge specification between two nodes. At this point, I deeply appreciate the clarity this approach forces upfront. When thinking about the flow as a graph, I'm forced to think about each node as a function that accepts inputs from shared state and returns a decision about what to do next. That decision can be deterministic (as above) or data-dependent (as we'll see below). Since GenAI can be viewed through the lens of automation, we should earn the privilege to use it. Automation requires a well-established process to be most effective. Framing a process in the language of graphs, inputs, and outputs\u2014defining the process as a graph with carefully specified inputs and outputs, just like writing a computer program\u2014is the clearest path to making automation work. Running the Flow looks like this: After running, we can inspect the dictionary to see our results: One thing missing from PocketFlow is the ability to visualize the graph directly. Since the codebase was new to me, I sent a Cursor agent in the background to research and propose a solution. It came back with this PR. Impressive! The Mermaid diagram for this workflow is: graph LR N1[\"ExtractTopics\"] N2[\"GenerateQuestions\"] N1 --> N2 style N1 fill:#e1f5ff,stroke:#01579b,stroke-width:2px; style N2 fill:#e1f5ff,stroke:#01579b,stroke-width:2px; Example 2 - Building an Agent Now, what if we want to build an agent? I'm going to work backwards here. My \"hello world\" test for agentic systems is making them tell me today's date. This works because an LLM will always hallucinate a date on its own, and that hallucination may or may not be correct. An agent that works properly should call a tool to get the actual date. The agent's graph should look like this: graph LR N1[\"Decide\"] N2[\"TodayDate\"] N3[\"RespondToUser\"] N1 -->|\"todaydate\"| N2 N2 -->|\"decide\"| N1 N1 -->|\"respondtouser\"| N3 style N1 fill:#e1f5ff,stroke:#01579b,stroke-width:2px; style N2 fill:#e1f5ff,stroke:#01579b,stroke-wid",
    "tags": [
      "llm",
      "graphs",
      "agents",
      "automation",
      "pocketflow",
      "llamabot",
      "python",
      "workflows",
      "abstractions",
      "state"
    ],
    "pub_date": "2025-11-16",
    "type": "blog"
  },
  {
    "id": "blog-safe-ways-to-let-your-coding-agent-work-autonomously",
    "url": "/blog/2025/11/8/safe-ways-to-let-your-coding-agent-work-autonomously/",
    "title": "Safe ways to let your coding agent work autonomously",
    "summary": "In this blog post, I share practical strategies for letting coding agents work autonomously while minimizing risks, like setting intelligent boundaries for command approvals, using plan mode, and writing prescriptive prompts. I also discuss real-world lessons learned from agent mishaps and offer tips for managing multiple agents safely. Curious about how to empower your coding agents without losing control?",
    "body": "Coding agents promise to unlock significant productivity gains by working autonomously in the background\u2014gathering context, running tests, searching documentation, and making progress on tasks without constant human intervention. The more autonomous they become, the more value they deliver. Yet this autonomy creates a fundamental tension: we need agents to act independently to realize their potential, but we must prevent them from taking irreversible actions we don't want. This tension became painfully clear when I asked Comet, an agentic browser, \"how to archive repo\" in the same casual way I'd ask Google. The agent interpreted this as a direct command and archived my LlamaBot repository. What I wanted was information; what I got was an unintended action with real consequences. The problem isn't unique to Comet. Any coding agent with sufficient autonomy can make destructive changes: deleting files, force-pushing to main, committing broken code, or modifying critical configurations. We need safeguards that allow agents to work freely on safe operations while blocking potentially harmful actions. The solution lies in configuring your development environment with intelligent boundaries\u2014auto-approving read-only commands while requiring explicit approval for anything that modifies state. Auto-approve safe command line commands The foundation of autonomous coding agent operation is allowing certain command line commands to run without manual approval. Commands like /, /, , and similar read-only or context-gathering operations enable LLM agents to autonomously understand codebases and test suites. For CLI tools that interact with external services, I also auto-approve , which allows the agent to gather context from GitHub pull requests while working in the background. The critical rule: only auto-accept commands that are non-destructive. Never auto-approve , , , or other filesystem, git, or state-modifying changes. This creates a safe boundary where agents can explore and learn, but cannot make irreversible changes without your explicit approval. Here's my mental model for categorizing commands: Safe to auto-approve: - Read operations: , , , , , - Code analysis: (read-only test runs), , (without ) - Context gathering: , , , , - Package managers (read-only): , , - Documentation build: Never auto-approve: - File system mutations: , , , , - Git writes: , , , - Package installs: The edge cases are where it gets interesting. I auto-approve because test runs are read-only, but I require approval for any command that modifies files, even if it's technically reversible. The key distinction is whether a command changes state: and are safe because they're pure reads, while and modify repository state and require explicit approval. is a bit of a gray area, but I am ok with auto-approving it since it's technically reversible, and because coding agents are often much faster than I could be at selectively adding files to the staging area. Enable automatic web search For Cursor and Claude Code, automatic web search without approval requests is another powerful capability. I have web search auto-approved on my machine, which allows agents to look up documentation, error messages, and solutions independently. This is particularly valuable when agents encounter unfamiliar error messages or need to check current API documentation that may have changed since the model's training cutoff. However, I monitor outputs for prompt poisoning, since internet-based prompt poisoning is a known attack vector for AI systems. The risk is that malicious content from web searches could influence the agent's behavior in subsequent actions. I've found this risk manageable for coding tasks, but I'm more cautious with agents that have broader system access or handle sensitive data. Know your emergency stop shortcuts Every coding agent platform provides keyboard shortcuts to cancel actions in progress. These are essential when you notice an agent looping, going down an unproductive path, or making changes you don't want: - Cursor: - VSCode + GitHub Copilot: - Claude Code: If you're monitoring the agent's activity, these shortcuts let you intervene immediately when something goes wrong. Correct agent behavior in real-time When you catch an agent doing something undesirable, stop it immediately, then redirect it. I instruct agents to record corrections in and continue with the updated guidance. An example prompt: This approach creates a persistent record of preferences that improves future agent behavior. The file becomes a living document of your development standards and preferences, which agents can reference in future sessions. I've implemented this pattern in my personal productivity MCP server, which provides a standardized way to store and retrieve these preferences across different agent platforms. Write prescriptive prompts for complex tasks I created the personal productivity MCP server to help me take my favourite prompts from system to system.",
    "tags": [
      "automation",
      "productivity",
      "coding",
      "agents",
      "safety",
      "workflow",
      "development",
      "prompting",
      "command line",
      "ai"
    ],
    "pub_date": "2025-11-08",
    "type": "blog"
  },
  {
    "id": "blog-use-coding-agents-to-write-marimo-notebooks",
    "url": "/blog/2025/10/28/use-coding-agents-to-write-marimo-notebooks/",
    "title": "Use coding agents to write Marimo notebooks",
    "summary": "In this blog post, I share how combining AI coding assistants with Marimo notebooks can supercharge your Python development and data science workflows. I walk through handy features like the flag for live updates, the command for code quality, and even advanced options like MCP and built-in AI editing. Curious how you can automate and speed up your notebook workflow while keeping your code clean?",
    "body": "If you're like me, you might find coding with AI assistants somewhat addictive. And if you're like me, you might also like to write code in Marimo notebooks, the modern alternative to Jupyter that offers better reproducibility and cleaner Python development. Turns out there's a way to put these two together for automated Python development and data science workflows, creating a powerful combination for rapid prototyping and iterative coding. Marimo's Flag A few months ago, at SciPy 2025, my friend Trevor Manz showed me a cool neat trick for writing Marimo notebooks. Apart from launching a Marimo notebook in sandbox mode, you add a flag: When edits are made to the source file , they will now be reflected in the browser as well. This was my reaction: If you ever meet Trevor in person, he can confirm that reaction of mine. Ensure code quality with So now, AI coding assistants can write your Marimo notebooks for you... but it's not always going to be correct first time, right? After all, the latest features of Marimo are not going to be part of the large language model training sets. Turns out, Marimo also ships with a command that you can ask coding agents to call on: And that will print to stdout any issues that Marimo finds that break its execution model, such as variables that are repeated variables or invalid cells. You can instruct coding agents to always run by adding the following prompt (or analogous) into : This will virtually guarantee correctly-written, AI-generated notebooks. All that's left for us as users is to check the correctness of the analysis that was done. Real-world use Now, AI coding assistants (like Cursor, GitHub Copilot, or Claude Code) can write and edit large chunks of Marimo notebook cells for you, check what they wrote, and fix any syntactic issues that show up. And by checking that the cells are syntactically valid. Now you can speed-run those routine and yet highly mundane data manipulation code-writing activities while making yourself an espresso drink. This aligns perfectly with my philosophy on optimizing for productivity in data science workflows. I've used this mode to speed-run first versions of probabilistic models in PyMC, create explainer notebooks for hard concepts, make notebooks that process data, and many, many more things that you'd usually be able to do within a coding notebook system. The key thing that makes this work is feedback given (via the command line) that the coding agent can use for self-correction. Advanced functionality using MCP and built-in AI features It doesn't stop there, though. There's a new flag that makes a notebook an MCP server that coding agents can connect to; read more about it here. Marimo also has built-in AI editing capabilities itself as well. Check out the functionality here, as well as Vincent Warmerdam's short video on using coding agents from within Marimo. He's got my vote for best facial/eyebrow expressions from a coding YouTuber! Addendum After sharing this post on LinkedIn, S\u00e9verin H. shared a couple of additional use cases worth highlighting: One use case I would also recommend is getting the coding assistant to run queries for you, especially when it is to debug a existing query. You can ask [it] to check for corner cases (and especially dig into the data to understand the corner cases). (LinkedIn comment) The flag is indeed very interesting use case. Also to note they created a Claude.md to get you started that you can directly curl: Some more reference from their blog: https://marimo.io/blog/claude-code (LinkedIn comment) Thanks for the suggestions, S\u00e9verin!",
    "tags": [
      "marimo",
      "python",
      "ai",
      "notebooks",
      "automation",
      "productivity",
      "workflow",
      "coding",
      "development",
      "data science"
    ],
    "pub_date": "2025-10-28",
    "type": "blog"
  },
  {
    "id": "blog-exploring-skills-vs-mcp-servers",
    "url": "/blog/2025/10/20/exploring-skills-vs-mcp-servers/",
    "title": "Exploring Skills vs MCP Servers",
    "summary": "In this blog post, I share my first impressions of Anthropic's skills repository, comparing its token-efficient, customizable approach to the more standardized MCP server model. I break down the strengths and trade-offs of each, from creative workflows to technical utilities, and raise open questions about distribution and cross-vendor support. Curious about which approach might fit your workflow best?",
    "body": "I spent time digging through Anthropic's skills repository. These are my first impressions, organized for clarity and future reference. What the Anthropic Skills repository offers - Creative & design workflows: (generative art with p5.js), (beautiful PNG/PDF outputs guided by design philosophies), (pre-set or on-the-fly themes), and (animated GIFs tuned for Slack). These are turnkey \u201ctaste plus tooling\u201d bundles that let the model produce high-quality visuals with consistent aesthetics. - Document skills for real formats: cover , , , and with serious capabilities: layout/templates, tracked changes and comments, text/table extraction, merges/splits, charting, formulas, and formatting preservation. This feels like a pragmatic spec+runtime for working with binary formats\u2014lean instructions up front, heavy lifting when needed. - Development & technical utilities: (compose complex Claude HTML artifacts using React/Tailwind/shadcn), (Playwright-driven UI testing), and (guidance for creating high-quality MCP servers). These reduce boilerplate for the \u201cbuild and test\u201d loop. - Enterprise & communication: (apply Anthropic\u2019s official brand colors and typography) and (status reports, newsletters, FAQs). These encode editorial and brand guardrails so outputs stay on-message. - Meta skills and templates: and show how to structure your own skills: a folder per skill with a (YAML front matter for and , plus instructions/examples/guidelines), optional scripts, and assets. This is the pattern to replicate. If you want the source for these examples, it\u2019s viewable in the repo. Start here: . How skills are loaded and used - Minimal prompt footprint: A skill's short description is passed up front. The larger is only read when the model decides it needs more detail. - On-demand details: The model can iterate (ReAct loop) to fetch instructions and then execute scripts or read additional files. This access pattern keeps the initial token budget small and defers detail until it\u2019s actually needed. Contrast with MCP servers - MCP call shape: Tool names and descriptions are typically sent on every call. That keeps tools globally discoverable but increases token overhead. - Skills call shape: A tiny descriptor up front; details fetched lazily. Lower baseline token cost. - Distribution model: - MCP: Centrally hostable (e.g. web server) or vendable (e.g., a Python package). Easy to version, release, and update for many users at once. - Skills: Feel local-first. You can drag-and-drop into a Claude workspace. Easy to customize, but harder to standardize and propagate updates across a team. Given current industry patterns, MCP servers are the widely accepted way to expose functionality to LLMs across tools and vendors. Skills are Anthropic-specific at the moment. Token efficiency (and why it\u2019s emphasized) Anthropic\u2019s materials lean into token efficiency. The cost of LLM calls adds up, and repeatedly sending long tool descriptions can be expensive. Skills reduce baseline tokens: spend a handful of tokens to register intent, read detail only when needed, then execute. That\u2019s the economic story. Practical trade-offs - Standardization vs customization: - MCP servers: Strong for shared, versioned, and centrally updated capabilities. - Skills: Great for rapid, local customization without infrastructure. - Discovery vs cost: - MCP: High discoverability; the model always sees the tools. Higher token floor. - Skills: Low token floor; details fetched when needed. Requires the model to choose to read more. Open questions I\u2019m tracking - How will teams distribute and update skills at scale without a central registry or packaging story? - Will skills gain cross-vendor support, or remain Anthropic-only? - What\u2019s the best practice to map a complex skill into smaller, composable units without losing clarity? Early take IMO, skills are a clear attempt to lower token costs and streamline task-specific workflows with minimal upfront context. MCP servers remain the well-understood, cross-ecosystem pattern for exposing capabilities. If your goal is a shareable, versioned interface for many users, MCP is still the safer default. If you need quick, local customization inside Claude with a lean prompt footprint, skills are compelling. But this field has been evolving at breawkneck speed anyways, so expect changes.",
    "tags": [
      "anthropic",
      "skills",
      "token",
      "efficiency",
      "llm",
      "automation",
      "customization",
      "workflows",
      "development",
      "mcp"
    ],
    "pub_date": "2025-10-20",
    "type": "blog"
  },
  {
    "id": "blog-how-to-expose-any-documentation-to-any-llm-agent",
    "url": "/blog/2025/10/19/how-to-expose-any-documentation-to-any-llm-agent/",
    "title": "How to expose any documentation to any LLM agent",
    "summary": "In this blog post, I share what I learned building LlamaBot: the real challenge in AI-assisted development is keeping AI agents up-to-date with evolving documentation. I explain how the Model Context Protocol (MCP) lets LLMs access dynamic, queryable knowledge bases, solving the obsolescence problem and enabling smarter, context-aware AI assistants. Curious how you can make your documentation instantly accessible to any AI agent?",
    "body": "Like cars that lose value as soon as they roll off the lot, LLMs become outdated as soon as their training sets are fixed. Software documentation evolves constantly\u2014new features, API changes, bug fixes, and best practices emerge daily. Yet AI agents are stuck with whatever knowledge was captured in their training data, creating a fundamental mismatch between what they know and what developers actually need in real-time. Building LlamaBot taught me something unexpected: the hardest part of AI-assisted development isn't writing better prompts or designing cleaner abstractions. It's equipping AI agents with up-to-date information in a stable, standardized fashion. Most developers know the frustration of context-switching between code and documentation. You're deep in a coding session, need to check how a specific function works, and suddenly you're hunting through static documentation files. AI agents face this same problem, but with an added layer of complexity\u2014they need structured, queryable access to documentation that can be searched semantically. I discovered that web searches by coding agents were less reliable than manually adding context, but manual approaches don't scale. The solution emerged through the Model Context Protocol (MCP), a standard that enables LLMs to interact with external tools and data sources. In LlamaBot v0.13.10, I introduced a documentation MCP server that automatically equips AI agents with current information. This enables AI agents to access organizational knowledge, process documentation, and domain expertise in structured ways. The obsolescence problem in AI-assisted development The core issue more than mere documentation access, it's about obsolescence. LLMs are trained on data that becomes outdated the moment it's fixed in their training sets. Meanwhile, software documentation evolves constantly. New features are added, APIs change, bugs are fixed, and best practices emerge. Yet AI agents remain frozen in time, working with knowledge that may be months or years out of date. Consider a typical data science workflow: you're building an AI pipeline and need to understand how LlamaBot's StructuredBot handles data validation. The AI agent might reference documentation from six months ago, missing critical updates or new features that could solve your problem more elegantly. This creates a fundamental mismatch between what the agent knows and what's actually available. The deeper problem is that AI agents need structured, queryable access to documentation that can be searched semantically and updated automatically. They need to understand not just what functions exist, but how they relate to each other, what patterns they follow, and how they fit into broader workflows. Static documentation simply cannot provide this level of contextual understanding, particularly in data science environments where teams maintain scattered knowledge across wikis, Slack threads, and onboarding documents. Building a semantic documentation layer LlamaBot's MCP server demonstrates how to give AI agents structured access to its documentation by creating a dynamic, queryable knowledge base that agents can search semantically. The implementation centers around a single tool: This interface sits in front of a data pipeline that builds a vector database for the documentation. The server fetches the latest documentation from GitHub, extracts Python module docstrings from source code, and constructs a LanceDB vector database optimized for semantic search. The database is built during CI/CD and packaged directly with the wheel distribution, giving users instant access without setup while staying current with each release. This approach works with any AI agent system through the MCP protocol, providing a standardized way to keep AI agents current with documentation. The architecture behind semantic documentation The MCP server combines several technologies to create a robust documentation system. FastMCP handles the protocol implementation, enabling seamless communication between AI agents and the documentation database. LanceDB powers the semantic search capabilities, leveraging LlamaBot's existing class with hybrid search and reranking for optimal results. The system uses the checked-out documentation from the repository during the CI/CD build process, ensuring the packaged database contains current information. The build script first attempts to fetch docs from GitHub, but falls back to the local directory when available, making it work seamlessly in both CI/CD and development environments. The build process runs the script during CI/CD, which creates the LanceDB database and copies it to for packaging. I believe this architecture represents a fundamental shift in how we think about documentation for AI systems. Instead of treating documentation as static reference material, we're creating dynamic, queryable knowledge bases that AI agents can interact with directly. The core pattern to replicate The LlamaB",
    "tags": [
      "llm",
      "documentation",
      "ai",
      "mcp",
      "workflow",
      "context",
      "search",
      "knowledge",
      "development",
      "automation"
    ],
    "pub_date": "2025-10-19",
    "type": "blog"
  },
  {
    "id": "blog-a-practical-comparison-of-dspy-and-llamabot-for-structured-llm-applications",
    "url": "/blog/2025/10/18/a-practical-comparison-of-dspy-and-llamabot-for-structured-llm-applications/",
    "title": "A practical comparison of DSPy and LlamaBot for structured LLM applications",
    "summary": "In this blog post, I share my hands-on comparison of DSPy and LlamaBot for building structured LLM applications, using a real-world expense extraction example. I explore how each framework handles schema design, type safety, and prompt optimization, highlighting their strengths and trade-offs. Curious which approach might best fit your next LLM project?",
    "body": "When Omar Khattabe presented DSPy 3.0 at PyData Boston Cambridge last week, I finally had the chance to dig into a framework that's been generating significant buzz in the LLM development community. As someone who's built structured LLM applications with LlamaBot, I was particularly curious about DSPy's core claim: that signatures represent the only abstraction you need for LLM-powered programs. The presentation focused on two key concepts: signatures as a new LLM abstraction and prompt optimization techniques. But what caught my attention was the practical similarity between DSPy's approach and what I've been doing with LlamaBot's StructuredBot. This led me to build a direct comparison using a real-world example from my personal expense tracking application. The structured LLM challenge Most developers working with LLMs face the same fundamental problem: how do you reliably extract structured data from unstructured inputs? Whether you're processing receipts, parsing documents, or analyzing text, you need consistent, typed outputs that integrate cleanly with your existing systems. Traditional approaches rely heavily on natural language prompts, which are fragile, hard to maintain, and difficult to optimize. DSPy proposes a different path through its signature abstraction, claiming this eliminates the need for verbose prompt engineering. A real-world comparison: Receipt processing To test DSPy's claims, I built a practical comparison using an expense extraction system I developed for personal use. This application processes receipts in various formats (PNG, PDF, JPG, WEBP) and automatically extracts structured expense data into Notion \u2014 essentially a lightweight alternative to enterprise expense management systems. The challenge here is typical of structured LLM applications: converting unstructured visual and textual data into consistent, typed outputs that integrate with existing workflows. Let's see how both frameworks handle this task. LlamaBot's StructuredBot approach LlamaBot uses Pydantic models to define structured outputs, leveraging Python's type system for validation and documentation. The approach emphasizes explicit data modeling with detailed field descriptions: DSPy's signature approach DSPy takes a different approach with its signature abstraction, which defines both inputs and outputs in a single class. The framework emphasizes simplicity and automatic prompt optimization: Comparing the approaches Both frameworks successfully extracted structured data from receipt images, but they take fundamentally different approaches to the problem. LlamaBot's StructuredBot leverages Python's existing type system through Pydantic models. This approach provides several advantages: automatic validation, IDE support, and integration with existing Python data processing pipelines. The explicit type definitions make the data contract clear and enforceable. DSPy's signatures offer a more streamlined interface that combines input and output definitions in a single class. The framework's strength lies in its automatic prompt optimization capabilities, which can improve performance over time without manual intervention. Key differences in practice The most noticeable difference is verbosity. LlamaBot requires more explicit type definitions and imports, while DSPy's signature approach is more concise. However, this conciseness may come at the cost of some type safety and IDE support that Pydantic provides. Both frameworks use LiteLLM for model routing, making it easy to switch between different LLM providers. The model configuration syntax is identical, which suggests a common underlying architecture. The schema-first principle Regardless of which framework you choose, structured LLM applications require careful upfront schema design. The bulk of development time goes into defining your data model, not writing prompts. This schema-first approach is what makes these frameworks powerful\u2014they force you to think clearly about your data requirements before implementation. Looking ahead: DSPy's broader vision DSPy's claim that signatures are the only abstraction needed for LLM applications is ambitious but not entirely accurate. The framework includes additional abstractions like modules and optimizers that handle more complex scenarios. Signatures represent the core abstraction for simple input-output transformations, but building production LLM applications often requires more sophisticated orchestration. I'm planning to explore DSPy's more advanced features as I rebuild LlamaBot's agent abstractions. The goal is to understand how to construct autonomous LLM agent frameworks rather than individual agents\u2014a challenge that requires thinking beyond simple input-output mappings. Being unfamiliar with DSPy's documentation initially, I found it challenging to follow, but thanks to fellow PyData Boston Cambridge organizer Nash Sabti's guidance, I was able to make it happen and build this comparison. The structured LLM landsca",
    "tags": [
      "llm",
      "dspy",
      "llamabot",
      "python",
      "frameworks",
      "extraction",
      "schema",
      "prompting",
      "expenses",
      "automation"
    ],
    "pub_date": "2025-10-18",
    "type": "blog"
  },
  {
    "id": "blog-how-to-use-coding-agents-effectively",
    "url": "/blog/2025/10/14/how-to-use-coding-agents-effectively/",
    "title": "How to Use Coding Agents Effectively",
    "summary": "In this blog post, I share hard-earned lessons from using AI coding agents on real projects. I discuss why effective agent use goes beyond good prompts, highlighting the importance of systematic workflows, external memory, and fast iteration. I cover practical patterns for planning, testing, refactoring, and documentation, plus tips for integrating agents into your development process. Curious how these strategies can help you get the most out of coding agents?",
    "body": "This past week, I went on a building spree, a part of my ongoing ultralearning practice, and built multiple projects using AI coding assistants. After many months of working with AI coding assistants on real projects, I've learned that effective agent usage requires more than just good prompts. You need systematic workflows, external memory systems, and a willingness to let the agent fail fast so you can discover architectural boundaries. These are the patterns that make coding agents productive. Starting Out Effective agent usage starts with establishing a disciplined workflow that covers the complete development lifecycle. This isn't just about fancy prompts; we're talking about creating a repeatable process that works from start to finish. The Complete Lifecycle flowchart TD A[Plan] --> B[Write Tests] B --> C[Implement Code] C --> D[Run Tests] D --> E{Tests Pass?} E -->|No| F[Fix Issues] F --> D E -->|Yes| G[Document] G --> H[Run Full Test Suite] H --> I{All Tests Pass?} I -->|No| F I -->|Yes| J[Complete] style A fill:#e1f5fe style B fill:#f3e5f5 style C fill:#e8f5e8 style D fill:#fff3e0 style G fill:#f1f8e9 style J fill:#e8f5e8 Here's the systematic workflow that works best with coding agents: 1. Plan first, then execute. Break your work into planning and execution phases, just like you would if writing code yourself. Have the agent write planning documents it can follow. This separation matters because planning and execution often use different parts of the model, and sometimes \"dumber\" models execute plans better than expensive ones. 2. Write tests before implementation. This is where TDD becomes crucial with agents. Write tests first, then implement, then test the code. When tests pass, document. This workflow becomes more important with AI assistants because they're working with small context windows compared to your entire codebase. You must have the AI write tests for everything it generates. 3. Implement with clear feedback loops. The proper TDD flow is: tests are always written first, executed and failed (because the implementation is lacking), then implemented, and executed again\u2014ideally succeeding on the first try. This is super important for highest effectiveness with coding agents. The AI needs the clear feedback loop of failing tests to understand what to implement. 4. Document as you go. When tests pass, document the implementation. This creates a complete record of what was built and why. 5. Loop back to tests until everything is fixed. This is the critical step that many people miss. Don't stop at the first passing test\u2014run the full test suite, check edge cases, and iterate until all tests pass consistently. The agent should keep running tests and fixing issues until the entire system is stable. Learn your tool's shortcuts and modes. In Cursor, for example, you can open a new agent window with Cmd+E, and use Shift+Tab to toggle to plan mode (yellow colored). These modes work different parts of the model\u2014planning models are better at analyzing code and planning than executing, while execution models are cheaper and sometimes more reliable at following plans. In VS Code with GitHub Copilot, you can define custom modes. You can even get Agent Mode to write a Planning Mode for you as a way to bootstrap Plan Mode. This gives you specialized interfaces for different types of work. Most of us software builders like to do the building part, not the verification part. TDD with agents lets you delegate the tedious verification work while keeping the fun building part for humans, as long as you review the tests the agent writes. This is another place where agents excel at taking over work we'd rather not do ourselves. Without this discipline, you'll find yourself debugging issues that could have been caught earlier. The complete lifecycle ensures that every piece of code is tested, documented, and verified before moving on. Finally, break work into chunks you can maintain concentration for during review. This takes practice getting used to an LLM's outputs, but it's important for effectiveness. Start with smaller scopes and gradually increase as you get comfortable with the agent's output patterns. The goal is to find the sweet spot where you can maintain focus while the agent does meaningful work. Building Momentum When starting a new project, don't try to get everything right the first time. Instead, speed-run your project twice, perhaps even thrice, in quick iteration mode. Just accept and vibe-code your way to the point where it gets hard for the LLM to do what you're asking. On each speed-run, you'll likely find yourself cornered architecturally. Step back and diagnose what's going wrong. Then speed-run the process once more to see if you can corner yourself another way. On your third try, you'll have made enough mistakes to clarify the mental model of the problem. I recently built a dataset versioning package called Kirin this way. It took three iterations over about a week to get the archite",
    "tags": [
      "workflow",
      "tdd",
      "automation",
      "agents",
      "refactoring",
      "documentation",
      "planning",
      "memory",
      "iteration",
      "shortcuts"
    ],
    "pub_date": "2025-10-14",
    "type": "blog"
  },
  {
    "id": "blog-how-to-use-multiple-github-accounts-on-the-same-computer",
    "url": "/blog/2025/10/10/how-to-use-multiple-github-accounts-on-the-same-computer/",
    "title": "How to use multiple GitHub accounts on the same computer",
    "summary": "In this blog post, I share how I solved the challenge of using multiple GitHub accounts on the same computer by configuring separate SSH keys and updating SSH and Git settings. I walk through step-by-step instructions, troubleshooting tips, and ways to automate account switching for different repositories. If you've ever struggled with Git pushing to the wrong account or want a smoother workflow for personal and volunteer projects, this guide is for you. Curious how to make Git always use the right account without hassle?",
    "body": "How to use multiple GitHub accounts on the same computer I recently ran into a frustrating situation where I couldn't push to a repository even though I had the right permissions. The problem? I was trying to use two different GitHub accounts on the same computer, and Git was getting confused about which account to use. If you're in a similar situation - maybe you have a personal account and also contribute to a non-profit or open source project with a separate account - this guide will help you set everything up correctly. The core problem Here's what was happening to me: I had switched my GitHub CLI to my other account using , but when I tried to push, Git was still authenticating with my personal account's SSH key. The issue is that only changes which account the GitHub CLI uses for API operations. It doesn't affect which SSH key Git uses for push/pull operations. Git and SSH operate independently from the tool. What you'll need Two GitHub accounts (I'll call them and in this guide), terminal access, admin permissions on your repositories, and about 10-15 minutes. Step 1: Create separate SSH keys for each account First, we need distinct SSH keys for each account. If you don't already have separate keys, create them: When prompted for a passphrase, you can either set one or leave it empty (though a passphrase is more secure). Step 2: Add the SSH keys to your SSH agent You can verify both keys are loaded: Step 3: Add the public keys to GitHub For each account, you need to add its corresponding public key: Then: 1. Log into GitHub as your volunteer account 2. Go to Settings \u2192 SSH and GPG keys \u2192 New SSH key 3. Paste the public key there Repeat this process for your personal account with . Step 4: Configure SSH to use different keys for different \"hosts\" Edit or create : The line creates a local alias that only exists in your SSH config. When Git tries to connect to , SSH will actually connect to but use the specified SSH key. The line tells SSH to only use the key you specified and not try other keys from your SSH agent. Step 5: Update your repository's remote URL For any repository belonging to your volunteer account, you need to update the remote URL to use the SSH alias: Notice the change: instead of . This is necessary because the hostname in the URL is what triggers SSH to look up the configuration in your file. When Git sees , SSH matches it to the entry and uses the correct key. Step 6: Test the connection Before pushing, verify SSH is authenticating correctly: You should see: If it says your personal account name instead, something's wrong with your SSH config. Now try pushing: Troubleshooting common issues Issue 1: SSH still authenticates with the wrong account If shows your personal account name instead of your volunteer account, the problem is usually that SSH is trying multiple keys and GitHub is accepting the first one that works. Make sure you have in your for the host. This forces SSH to only use the specified key. Issue 2: \"Could not resolve hostname github.com-volunteer\" This usually means Git has a custom SSH command configured that's bypassing your SSH config file. Check: If this returns something with , that's your problem. The flag tells SSH to ignore all config files. Remove it: Issue 3: Config changes don't seem to apply If you have conditional Git configs (using directives), they might be overriding your settings. Check: This shows you exactly which config file is setting the SSH command. You may need to edit that file directly. For example, I had a file that was automatically loaded for repos in certain directories, and it had a problematic setting that needed to be fixed. Issue 4: \"Repository not found\" error This means SSH is connecting and authenticating, but as the wrong account. Double-check: 1. Run and verify it shows the correct account name 2. Verify the account has access to the repository on GitHub 3. Check that your remote URL uses the correct alias: Optional: Set up conditional Git configs If you keep repositories for your volunteer work in a specific directory (like ), you can automatically apply settings to all repos in that directory. Add this to your : Then create : This automatically sets your volunteer account's email for commits in that directory. The should be set to plain so it uses your properly. How this all works together When you run : 1. Git reads the remote URL: 2. Git asks SSH to connect to 3. SSH looks in and finds the entry 4. SSH sees it should actually connect to but use the key 5. SSH connects to GitHub with the correct key 6. GitHub authenticates you as your volunteer account 7. Push succeeds Each repository uses the correct account automatically based on its remote URL, so you never have to manually specify which key to use. Wrapping up Managing multiple GitHub accounts on the same computer isn't intuitive, but once you understand that Git uses SSH keys (not settings), the solution becomes clear. The SSH config host alias pattern is the standard way",
    "tags": [
      "github",
      "ssh",
      "git",
      "accounts",
      "configuration",
      "authentication",
      "troubleshooting",
      "setup",
      "remotes",
      "workflow"
    ],
    "pub_date": "2025-10-10",
    "type": "blog"
  },
  {
    "id": "blog-how-to-teach-your-coding-agent-with-agentsmd",
    "url": "/blog/2025/10/4/how-to-teach-your-coding-agent-with-agentsmd/",
    "title": "How to teach your coding agent with AGENTS.md",
    "summary": "In this blog post, I share how using AGENTS.md\u2014a new open standard for AI coding agents\u2014lets you teach your LLM assistant project-specific preferences that persist across sessions. I cover practical tips like enforcing markdown standards, specifying test styles, and introducing new tools, all by updating AGENTS.md. This approach turns your agent into a trainable teammate, not just a forgetful bot. Want to know how to make your coding agent smarter and more aligned with your workflow?",
    "body": "Let me start with the most valuable thing I learned this week: if there's anything you want your LLM coding agent to remember for future sessions, just tell it to \"Please update AGENTS.md with...\" and then specify what you want it to remember. That's it. That's the meta-tip that changes everything. What is AGENTS.md anyway AGENTS.md is an emerging open standard that's been adopted by over 20,000 repositories on GitHub. Think of it as a README for your AI coding agents\u2014a predictable location where you provide context, instructions, and preferences that your agent needs to work effectively on your project. You might think of this as similar to ChatGPT's memory feature, but there's a crucial difference: AGENTS.md is explicitly curated by you. You decide exactly what the agent remembers and how it applies that knowledge. I prefer this approach because it means I have control over what the agent knows, rather than the agent autonomously deciding what to remember about me and my preferences. It's transparent, version-controlled, and intentional. The format emerged from collaborative efforts across OpenAI, Google (Jules), Cursor, Factory, and other major players in the AI development space. It's just standard Markdown, which means it's accessible, portable, and fits naturally into any project structure. While your README.md is optimized for humans\u2014covering project introductions, contribution guidelines, and quick starts\u2014AGENTS.md serves as machine-readable instructions for your coding agents. Setup commands, testing workflows, coding style preferences, and project-specific conventions all live here. Training an employee, not programming a bot I was inspired by NetworkChuck's approach to building Terry, his N8n automation agent. The philosophy framing he uses is both brilliant and yet practical: you're not programming a bot, you're training an employee. In Terry's case, Chuck teaches the agent by continuously updating its system prompt with new instructions and context. The same principle applies perfectly to AGENTS.md in coding environments. Here's what makes this powerful: AGENTS.md gets sent with every LLM API call in Cursor, Claude Code, GitHub Copilot, and other modern coding tools. This means you can standardize on AGENTS.md, and as you progress through your project, you effectively teach the LLM what to do by instructing it to update this file with your preferences and learnings. The beauty is that these instructions persist across sessions. Your agent doesn't forget; it gets smarter as your project evolves. Concrete tip 1: Enforce markdown standards automatically One of my first uses for AGENTS.md was ensuring consistent markdown formatting. I asked my coding agent to update AGENTS.md with instructions to always run markdownlint on any markdown files it creates or edits. Here's what I added to the file: The effect is immediate. Now, anytime my agent writes or edits a markdown file, it automatically runs markdownlint and fixes issues. I don't have to remember to ask for this. The agent just knows it's part of the workflow. Concrete tip 2: Specify your testing style I prefer writing tests as style functions rather than unittest-style classes. Most LLMs default to the unittest approach because it's more prevalent in their training data. So I instructed my agent to add this to AGENTS.md: Now when I ask for tests, I consistently get style functions. The agent is steered toward my preferred approach without me having to specify it in every request. Concrete tip 3: Stop writing throwaway test scripts Here's a pattern I noticed with Cursor: when the agent wants to test something, it loves to write little throwaway scripts. You know the type\u2014 or that do some ad hoc verification and then just sit there cluttering your project. The problem is these scripts aren't real tests\u2014yet they're also tests. They don't run with your test suite. They don't provide ongoing regression protection. They're just... there. I taught my agent to write proper tests instead: Now when the agent needs to verify something works, it writes an actual test that becomes part of the project. These tests continue to provide value by catching regressions, documenting expected behavior, and running in CI. The shift is subtle but powerful: instead of creating technical debt in the form of random scripts, you're building up a proper test suite. Concrete tip 4: Teach your agent about new tooling I recently adopted Pixi as my main package manager. The problem? Most LLMs aren't familiar with Pixi commands yet. They kept trying to run directly when I only have Python available through Pixi. The solution was to teach the agent: This works for any new tooling. If you've adopted or or any other modern Python tools that aren't well-represented in LLM training data, you can explicitly teach your agent how to use them through AGENTS.md. The same principle applies to any domain-specific tools or workflows unique to your project. For example, if you're working wit",
    "tags": [
      "llm",
      "agents",
      "coding",
      "automation",
      "markdown",
      "testing",
      "package",
      "memory",
      "workflow",
      "scripts"
    ],
    "pub_date": "2025-10-04",
    "type": "blog"
  },
  {
    "id": "blog-how-data-scientists-can-master-life-sciences-and-software-skills-for-biotech-using-ultralearning",
    "url": "/blog/2025/10/1/how-data-scientists-can-master-life-sciences-and-software-skills-for-biotech-using-ultralearning/",
    "title": "How data scientists can master life sciences and software skills for biotech using ultralearning",
    "summary": "In this blog post, I share how effective biotech data scientists master both life sciences and software skills by applying Scott Young's ultralearning principles. Drawing from my own experience, I explain how to strategically bridge knowledge gaps, focus on real-world projects, and alternate deep dives between domains for continuous growth. Want to know which ultralearning strategies can help you level up your biotech data science career?",
    "body": "After 8 years working in biotech and 6 years of graduate training before that, I've observed something about the most effective data scientists in biotech: they aren't just T- or \u03c0-shaped -- posessing breadth in skill while being deep in 1 or 2 specialties. They're continuously learning new skills to bridge their knowledge gaps. There are two common knowledge gaps that I've observed. On one side, there's the vast world of life sciences: molecular biology, cell biology, genetics, immunology, neuroscience, analytical chemistry, organic chemistry, biochemistry. On the other, there's software development, the kind of skills that let you build reliable, maintainable tools that actually work in production. The challenge is that these two domains are fundamentally different in how you learn them. And here's the thing: you can't just \"take courses\" in those domains and call it done. The life sciences alone are too vast. You need a strategy for continuous, rapid learning in both domains over your entire career. When I interview data scientists for biotech roles, I assess five key areas: people skills, communication skills, scientific knowledge, coding skills, and modeling skills. The two domains I'm talking about here \u2014 life sciences and software development \u2014 map directly to scientific knowledge and coding skills. These aren't just nice-to-haves; they're essential for effectiveness in biotech data science. That's where \"ultralearning\" comes in. It's Scott Young's framework for aggressive, self-directed learning, and I know it works because I've lived it. I started as a bench scientist but taught myself computing, software development, and machine learning over the years. Now I want to show you how data scientists in biotech can do the same\u2014whether you're learning domain knowledge or software skills. How do you strategically build depth in both life sciences and software over time? I'm going to walk through the 9 principles of ultralearning that Scott Young outlines and show you how they map to learning both domains for biotech data science. I've reordered them in a way that builds momentum, starting with what matters most. The 9 principles Principle 3: Directness - learn by doing the real thing Starting with principle 3, here's what directness means: you learn most viscerally in the actual context where you'll apply the skill. And I'm putting this first because it's where most people go wrong. Most people read textbooks and take courses. This isn't bad in and of itself, but if you assume that covering the material means you have learned it, you are wrong. Without a context to apply it, the knowledge doesn't stick. You need a real project where you actually use what you're learning. If you're already working in biotech, you have a huge advantage: you already have real projects with real stakes. These projects naturally focus your learning because you have a job to be done! This is why I put directness first: it leverages the learning environment you already have. For learning life sciences, this means treating your current project as your learning laboratory. You're analyzing RNA-seq data? Learn the biology behind the genes you're seeing, then immediately apply that knowledge to interpret your results and suggest follow-up experiments. You're working with metabolomics data? Learn the metabolic pathways, then use that understanding to identify which metabolites are actually biologically meaningful versus technical artifacts. You're building models for drug discovery? Learn the specific organic chemistry that you're working with, then apply it to explain why your model predicts certain compounds will work and others won't, and use that reasoning to guide your next round of experiments. Your current project is your learning laboratory. Treat the scientific knowledge gaps you encounter as targets for deep learning. And here's something I've noticed: when you write internal documentation or reports, that's actually retrieval practice for the science you're learning. More on retrieval later, but the point is your work gives you built-in learning opportunities if you use them intentionally. The same applies to learning software. Your pipeline is getting slow? Learn performance optimization and profiling, then immediately apply those techniques to identify bottlenecks and speed up your actual pipeline. Your code is getting hard to maintain? Learn design patterns, then refactor your existing codebase using those patterns to make it more modular and testable. You need to deploy something? Learn containerization and orchestration, then use those skills to get your tool running in production and accessible to your team. Your work projects provide the constraints and requirements that make software concepts meaningful. When you document your code or write design docs, you're forced to articulate the architectural decisions you're making\u2014and that's when you really learn them. Principle 4: Drill - isolate and attack weak points Dr",
    "tags": [
      "biotech",
      "ultralearning",
      "datascience",
      "lifesciences",
      "software",
      "learning",
      "career",
      "skills",
      "modeling",
      "feedback"
    ],
    "pub_date": "2025-10-01",
    "type": "blog"
  },
  {
    "id": "blog-the-data-science-bootstrap-notes-a-major-upgrade-for-2025",
    "url": "/blog/2025/9/2/the-data-science-bootstrap-notes-a-major-upgrade-for-2025/",
    "title": "The Data Science Bootstrap Notes: A major upgrade for 2025",
    "summary": "In this blog post, I share how I've completely revamped The Data Science Bootstrap Notes for 2025, reflecting major changes in Python tooling and best practices. I discuss moving from conda to pixi and uv, automating project setup with pyds-cli, integrating AI thoughtfully, and embracing CI/CD for reproducible workflows. I also highlight the core philosophies that guide my approach and explain what outdated advice I've removed. Curious how these changes can help you build scalable, modern data science projects?",
    "body": "After 8 years since the first edition, I've completely overhauled The Data Science Bootstrap Notes to reflect the dramatic changes in the Python data science ecosystem. What started as a collection of Obsidian notes has evolved into a comprehensive, modern guide that addresses the tools and practices that actually matter in 2025. From Obsidian notes to a proper book The most visible change is the format itself. The original version existed as a navigable knowledge base in Obsidian, mimicking Andy Matuschak's online notes. While that format was intellectually interesting to make, I found that after years of experimentation, simple was indeed better than cool. The new version uses MkDocs to create a clean, linear book format that's easier to navigate and more accessible to readers. But the real transformation goes far deeper than just the presentation layer. The tooling revolution: conda + pip \u2192 pixi + uv The biggest shift in my recommendations centers around environment management. In 2017, conda was the obvious choice for Python data science environments. Today, that's no longer the case. Enter pixi: the environment management multi-tool I've completely replaced conda with , a modern environment manager written in Rust that solves many of the fundamental problems that plagued the conda ecosystem. The key advantages are: Automatic Lock Files: Pixi automatically generates and maintains lock files () every time you modify your environment. This solves the critical \"it works on my machine\" problem that conda users faced when environments would drift over time. Feature-Based Environments: Instead of creating separate environments for each purpose, pixi lets you define reusable \"features\" that can be combined into different environments. You can have , , , and features that combine into purpose-built environments like , , or . Task Automation: Pixi enables you to replace Makefiles with tasks defined in . Commands like or standardize common operations across your team. uv: the Python tool manager Complementing pixi is , an extremely fast Python package installer and resolver written in Rust. UV handles global tool installation by automatically creating isolated environments for each tool, giving you the convenience of global tools without the mess of a global Python installation. This means you can run tools like or my own without worrying about dependency conflicts. The command even lets you run tools without installing them first. Modern project scaffolding The new edition introduces , my opinionated tooling for data scientists that scaffolds new projects using cookiecutter and pixi. Instead of manually setting up project structures, you can now run: This creates a complete project structure with proper environment management, testing setup, documentation configuration, and CI/CD pipelines already configured. AI integration beyond the hype Generative AI has fundamentally changed how I think about data science workflows. The new edition includes a comprehensive chapter on working with AI tools that goes beyond simple code generation to address: The speed of thought: AI tools help bridge the gap between how fast we can think and how fast we can type. There's fascinating research showing humans process information at $10^9$ bits/second but think at only 10 bits/second - AI helps bridge this massive gap. The right kind of lazy: I distinguish between being \"Bill Gates lazy\" (finding efficient ways to work) and being intellectually lazy (blindly trusting AI outputs). You must maintain intellectual responsibility. Effective patterns: I share specific strategies for structuring AI interactions, from starting with the big picture to rapid iteration and verification. This includes the \"fat finger sketch\" approach where you outline what you want before asking AI to fill in details. Beyond code: AI tools are particularly valuable for documentation acceleration, code review assistance, and learning new libraries or techniques. The key insight is that AI should amplify our capabilities, not replace our judgment. We need to develop a mindset that embraces these tools while maintaining intellectual rigor. CI/CD and automation The new edition heavily emphasizes GitHub Actions for continuous integration and deployment. Instead of manual processes, you now have trigger-able bots that can: - Run tests automatically on every commit - Build and deploy documentation - Validate code quality with pre-commit hooks - Deploy applications to various environments This automation eliminates the drudgery that often accompanies data science projects and ensures consistency across team members. I've even applied this philosophy to the book itself; the entire publishing process is automated through GitHub Actions that build and deploy the website, while simultaneously updating the Leanpub version with every commit. Philosophical foundations While the tools have changed dramatically, the core philosophies remain the same but are now more clearly a",
    "tags": [
      "python",
      "pixi",
      "uv",
      "mkdocs",
      "automation",
      "ai",
      "scaffolding",
      "integration",
      "tooling",
      "workflows"
    ],
    "pub_date": "2025-09-02",
    "type": "blog"
  },
  {
    "id": "blog-how-to-use-ai-to-accelerate-your-career-in-2025",
    "url": "/blog/2025/9/1/how-to-use-ai-to-accelerate-your-career-in-2025/",
    "title": "How to use AI to accelerate your career in 2025",
    "summary": "In this blog post, I share 10 practical ways I've used AI and large language models to save time and boost my effectiveness at work\u2014beyond just coding and emails. From crafting tailored presentations and prepping for negotiations to automating tedious forms and practicing tough conversations, these strategies help you focus on what really matters. Want to know how AI can help you work smarter, not harder, beyond 2025?",
    "body": "Everyone knows LLMs can help with coding and drafting emails. But there are less obvious ways to hack your career with AI that can save you hours and make you more effective at work. Here are 10 strategies I've tested, with sample prompts you can steal: Draft presentations that actually land Most people start with slides. Start with your audience instead. First, research who you're presenting to. If you know specific attendees, have ChatGPT or Claude build dossiers from their public profiles - LinkedIn, company bios, recent interviews. Then ask the LLM what these people care about most. Next, have it craft your core message and angle based on those audience insights. Finally, get it to describe in words how each slide should look before you build anything - making sure to feed in both your audience research and your refined message. This approach works because you're designing for actual humans, not abstract concepts. Pro tip: Or just skip the manual work entirely and use Gamma.ai. Sample prompts: I'm presenting to . Here are their LinkedIn profiles: . What do they care about most professionally right now? My presentation topic is . My audience cares about . Help me craft a compelling angle that will resonate with them. Generate slide-by-slide instructions for a -slide presentation on . My audience is and they care about . My core message is . For each slide, tell me: the title (which should be the main point of that slide), what elements to include, and how to lay them out. The title style should be . Research your negotiation counterparts like a detective Context is everything in negotiations. Feed your LLM everything you know about the other party - their backgrounds, the situation they're in, potential constraints they're facing. Describe your own circumstances, goals, and BATNA (Best Alternative to a Negotiated Agreement). Then iterate with the LLM on potential objections and counter-strategies. The more specific information you provide, the better it gets at uncovering blind spots you hadn't considered. Sample prompts: I'm negotiating with . Here's what I know about them: . Here are my goals: . My BATNA is: . What objections might they raise? Given this context: , what leverage points might I have that I'm not seeing? If I propose , how might they respond based on ? Help me prepare counter-responses. Transform content between formats effortlessly You wrote a technical document that needs to become a blog post. Or you have a blog post that needs to become a slide deck. Instead of starting from scratch, let the LLM remix your existing content into the right format. The key is being specific about your target audience. A technical document transformed for executives needs different language and emphasis than one transformed for peer engineers. Without clear audience context, the LLM can't make effective choices about tone, depth, and focus. This works for any content transformation - meeting notes to executive summaries, brainstorming sessions to project proposals, quarterly reviews to team updates. Just remember: same content, different audience, different approach. Sample prompts: Transform this technical document into a blog post for : Turn these meeting notes into an executive summary for : Convert this brainstorming session into a structured project proposal for : Fill out administrative forms without the dread OKRs, performance reviews, expense reports - we all have forms that feel like bureaucratic hurdles. Here's the hack: don't write directly into the form. Instead, do a brain dump by talking through your accomplishments and goals. Transcribe this (voice memos work great), then paste the form questions plus your transcript into ChatGPT. Have it fill out the form for you, then copy-paste back. What used to take half a day now takes 30 minutes. Check out the Dia browser, which lets you insert LLM-generated text directly into web forms. Sample prompts: Here are my form questions: . Here's my brain dump of accomplishments: . Fill out the form professionally. Help me write OKRs based on this verbal dump of my goals: Turn this expense description into proper business justification: Ghost-write in your own voice Including this blog post, I start by verbally dumping my ideas into a Markdown file in Obsidian, usually via voice transcription. Then I have Claude ghostwrite using my tone - my verbal dump contains my natural writing patterns, plus I feed it samples of my previous blog posts. The key is multiple editing rounds. I push hard on the LLM during edits, which is how I make the content truly mine. I don't publish anything until it's been through at least two rounds of refinement. Sample prompts: Here's my verbal brain dump: . Here are samples of my writing style: . Ghostwrite my brain dump as a blog post in my voice. This draft doesn't sound like me yet. Make it more . Here's what my natural voice sounds like: Polish this draft but keep my conversational tone and specific phrases: Prepare manager up",
    "tags": [
      "productivity",
      "negotiation",
      "presentations",
      "llm",
      "automation",
      "communication",
      "competencies",
      "ghostwriting",
      "updates",
      "ai"
    ],
    "pub_date": "2025-09-01",
    "type": "blog"
  },
  {
    "id": "blog-how-to-communicate-with-lab-scientists-when-youre-the-data-person",
    "url": "/blog/2025/8/24/how-to-communicate-with-lab-scientists-when-youre-the-data-person/",
    "title": "How to communicate with lab scientists (when you're the data person)",
    "summary": "In this blog post, I share practical strategies for data scientists and statisticians to communicate more effectively with lab scientists in biotech. Instead of overwhelming collaborators with methods, I explain how to focus on decision-making, translate complex analyses into actionable probabilities, and build trust through clarity. I also offer tips for structuring meetings and anticipating common questions. Want to know how to make your insights drive real decisions in the lab?",
    "body": "Imagine this scenario: A data scientist explains a hierarchical Bayesian model for 45 minutes. Beautiful math. Elegant handling of batch effects. The lab scientists are polite but glazed over. Finally, someone interrupts: \"Sorry, but should we move this compound forward or not?\" The data scientist hadn't even calculated that probability. Sound familiar? If you're a statistician or data scientist in biotech, you've probably been there. You've spent hours on sophisticated analyses, crafted beautiful slides about your methods, and watched your audience's eyes glaze over while you explained mixed-effects models. Meanwhile, they just needed to know if they should spend $200K on the next experiment. Here's the thing: Lab scientists aren't struggling to understand your statistics because they're not smart enough. They're brilliant experts who've spent years mastering protein folding, cell signaling, or synthetic chemistry. They're just juggling their own complex problems and need you to translate your analysis into something they can act on. Today, I'm going to show you exactly how to do that. Here's what we're covering: 1. Your communication budget is finite \u2014 spend it wisely 2. Know what mode they're in before you open your laptop 3. Use the three-layer translation model 4. Decode what they're really asking 5. Build trust through clarity, not complexity 6. Master the decision-first meeting structure 7. Ask yourself what they'll ask you 1. Your communication budget is finite \u2014 spend it wisely Every interaction has a finite \"communication budget\" \u2014 limited attention, time, and cognitive load. Most data scientists spend this budget like tourists with foreign currency, not realizing the exchange rate. Think about your last presentation. Where did you spend your time? \ud83d\udeab The typical (failed) allocation: - 60% on methodology and statistical details - 30% on results (tables, coefficients, credible intervals) - 10% on \"what this means\" (usually rushed at the end) - 0% on \"what you should do next\" I get it. We're trained to show our work. We think rigor equals value. We assume that if we explain our methods thoroughly enough, scientists will understand what to do. But here's what actually works: \u2705 The allocation that drives decisions: - 10% on methods (just enough for credibility) - 20% on results (simplified, visual, contextual) - 40% on implications for their specific decisions - 30% on uncertainty and what it means for their next steps But context matters. A curious scientist with time might genuinely want 30% methods\u2014they're building mental models for future decisions. Someone facing a go/no-go decision tomorrow? They need 70% decision implications, minimal methods. The key is adopting BLUF (Bottom-Line Up-Front)). Structure your presentation by working backwards from the decision to be made. Try this: Start with the decision and recommendation, then work backwards to the evidence that supports it. Lead with \"Based on our analysis, I recommend we proceed with Compound A because there's an 87% probability it meets our potency threshold.\" This tells them immediately what they need to know, then you can spend the remaining time explaining why. Here's what happens when you don't use BLUF: A data scientist spent an entire program review meeting walking through their elegant approach to handling missing data. Really sophisticated stuff. Multiple imputation with careful consideration of the missing-at-random assumption. Twenty minutes in, the program lead interrupted: \"This is interesting, but we need to decide today whether to advance this molecule. Does it meet our potency threshold or not?\" They hadn't even calculated that probability. They'd spent their entire communication budget on something that wasn't even the program lead's concern that day. With BLUF, they would have started: \"Based on our analysis, I recommend we advance this molecule. There's an 82% probability it meets our potency threshold, even accounting for the missing data. Here's how I handled the missing data to arrive at this conclusion...\" 2. Know what mode they're in before you open your laptop Lab scientists operate in three distinct modes, and each requires a completely different communication approach. Decision Mode (Most of the time) They're under time pressure for go/no-go decisions. Maybe it's a pipeline review tomorrow. Maybe they need to order materials today. Maybe the synthesis team is literally waiting for their answer. Signs you'll hear: - \"What's the bottom line?\" - \"Should we proceed?\" - \"Just tell me if it worked\" What they need: Probability of success and a clear recommendation. That's it. Learning Mode (When they have bandwidth) They're genuinely curious about your methods. Maybe they're trying to understand why this analysis differs from last time. Maybe they're building intuition for future experiments. Signs you'll hear: - \"How does that work?\" - \"Why did you choose that approach?\" - \"Can you explain the intuition behind this?\" What",
    "tags": [
      "biotech",
      "communication",
      "decisions",
      "statistics",
      "translation",
      "collaboration",
      "trust",
      "meetings",
      "probability",
      "stakeholders"
    ],
    "pub_date": "2025-08-24",
    "type": "blog"
  },
  {
    "id": "blog-wicked-python-trickery-dynamically-patch-a-python-functions-source-code-at-runtime",
    "url": "/blog/2025/8/23/wicked-python-trickery-dynamically-patch-a-python-functions-source-code-at-runtime/",
    "title": "Wicked Python trickery - dynamically patch a Python function's source code at runtime",
    "summary": "In this blog post, I share how I discovered a powerful Python trick: dynamically changing a function's source code at runtime using the compile and exec functions. This technique enabled me to build more flexible AI bots, like ToolBot, that can generate and execute code with access to the current environment. While this opens up exciting possibilities for LLM-powered agents and generative UIs, it also raises serious security concerns. Curious how this hack can supercharge your AI projects\u2014and what risks you should watch out for?",
    "body": "So today, I learned a very dangerous and yet fascinating trick. It's possible to dynamically change a Python function's source code at runtime. What this does is open a world of possibilities in building AI bots! How this actually works Every function has a attribute. For example, for this function: looks like this: If I were to execute , it would return a . Now, let's say that, for some reason that I shall not speculate, I decided that I wanted to instead do multiplication by 2. I can create new source code: I can do the following three magical steps to swap it in. Firstly, compile the code into bytecode: The three arguments to are: 1. The code to compile (), 2. The filename in which the code is compiled (), and 3. The mode in which compilation happens (in this case, mode). On the third point, the docstring of explains what the three modes are: The mode must be 'exec' to compile a module, 'single' to compile a single (interactive) statement, or 'eval' to compile an expression. The object now is a \"code object\": I can then execute the compiled code to make it imported into a particular namespace. Here, the three arguments passed to are: 1. The code we want to execute (), and in this case, by \"executing\" it after being compiled in mode, we are really just simulating an into our namespace. 2. The globals (), which in this case are passed in as an empty dictionary. These are the global variables that are available to the function at runtime. 3. is the \"namespace\" in which we want the function to be present; namespaces in Python are just dictionary mappings from function/object name to the function/object itself. Finally, I can replace my existing function with the compiled function inserted into the namespace: But really, the real lesson here is not that one can monkeypatch over an existing Python function's source code at runtime, but that you can actually compile the string of a Python function definition and give it access to a namespace's variables, including that of the current global namespace. When would you ever want to do this? At first glance, never really! This is a bit of hackery that lives on the fringes of Python-land, and is basically a party trick. But as it turns out, I actually had a real motivation for wanting to do this. Within LlamaBot, I've always had as a first-pass implementation of what I think an LLM agent should look like, having studied LLM agent implementations in other libraries. However, I've never been fully satisfied with 's implementation. The core issue was that it mixed too many concerns together - function execution, function call determination, and user response generation all lived in the same loop. Here's what looked like at a high level: While this worked, it wasn't great at separating concerns. I had function execution mixed in with function call determination mixed in with responding to a user. The bigger limitation was with code execution tools. My original implementation isolated generated code in a Docker container sandbox, which was secure but meant the code couldn't access variables from my current Python runtime. This severely limited what kinds of useful tasks the bot could perform with my existing data and variables. I realized that if I could: 1. Use an LLM to generate Python functions that referenced existing variables in my runtime, 2. Compile those functions on-the-fly within the same Python environment, and 3. Execute them with access to my current namespace, I could build something much more powerful. This led me to create within LlamaBot. ToolBot focuses on tool selection instead of execution takes a different approach - it focuses purely on tool selection rather than execution. Here's the key structure: The key insight: just selects a tool to be executed, but does not execute it. Instead, it returns the tools to be called to the external environment, giving you full control over execution. The magic happens with writeandexecutecode One of the most powerful tools that can be chosen is . Here's the core implementation: python # Function with NO parameters - use empty dict {} def analyzedepartments(): '''Analyze department performance.''' import pandas as pd import numpy as np result = fakedf.groupby('department')['salary'].mean() return result # Function WITH parameters - pass matching keywordargs def filteremployees(minage, department): '''Filter employees by criteria.''' import pandas as pd filtered = fakedf[(fakedf['age'] >= minage) & (fakedf['department'] == department)] return filtered This extensive docstring gets passed as part of the JSON schema and effectively serves as instructions to the LLM on when and how to use this tool. I stripped out logging and error handling to simplify what's shown here, but the actual codebase has more robustness built in. Notice how , and more specifically , gains explicit access to the dictionary when a user passes it in. This approach allows us to ensure that function execution takes place within the proper namesp",
    "tags": [
      "python",
      "runtime",
      "llm",
      "security",
      "namespace",
      "compilation",
      "execution",
      "functions",
      "toolbot",
      "monkeypatching"
    ],
    "pub_date": "2025-08-23",
    "type": "blog"
  },
  {
    "id": "blog-data-scientists-arent-becoming-obsolete-in-the-llm-era",
    "url": "/blog/2025/8/15/data-scientists-arent-becoming-obsolete-in-the-llm-era/",
    "title": "Data scientists aren't becoming obsolete in the LLM era",
    "summary": "In my latest post, I share how large language models are changing the data science landscape\u2014not by replacing us, but by making us more effective and opening up new opportunities to build custom AI solutions. I discuss why our skills in measurement and evaluation are more valuable than ever. Curious how data scientists can thrive in the LLM era?",
    "body": "I keep hearing the same question: \"Are data scientists becoming obsolete now that LLMs can code?\" The anxiety is understandable. When you watch Claude or ChatGPT write Python scripts, build models, and even debug code, it's natural to wonder where that leaves us. But here's what I've found after spending months integrating LLMs into my own workflow: they're not replacing us. They're fundamentally reshaping what it means to be a data scientist. To ponder this question properly, I examine it from two angles. How are LLMs enhancing our existing work? The first angle is using LLMs as tools for data scientists. This means finding ways to incorporate them into our day-to-day work as consumers of LLM-powered applications. I've experienced the productivity-enhancing benefits firsthand. GitHub Copilot and Cursor have dramatically accelerated my coding. Research agents like Elicit.org help me navigate literature in ways that would have taken hours before. I use transcription tools to type faster than I can touch type by hand, getting my thoughts out of my brain closer to the speed at which I'm actually thinking. I rely on AI for cleaning up messy thoughts and as a thinking tool to help me draw out what I'm really trying to articulate. Having lived with these tools for months now, I think being proficient with AI-assisted coding is table stakes. Just as spreadsheets changed what we expected from accountants, AI assistance is now a baseline expectation. But there's a crucial skill here that goes beyond just using the tools: knowing how to use AI to verify information and catch the inevitable errors these systems make. More importantly, this is just the beginning. How are we building custom LLM solutions? The second angle is more profound: data scientists becoming part of the team that builds custom LLM agent workflows to accelerate others' work. Here's what this looks like in practice: You get hands-dirty with business workflows. You co-create with business partners to build new tools and ways of working that remove boring work from their plates. You build technical prototypes that prove out value, then partner with engineers for custom app builds where appropriate. The scientist skill becomes crucial here: experimentation. You're figuring out whether a thing is actually working by measuring performance of LLM-based workflows and tying it back to business value. This is fundamentally different from being an app developer, a machine learning engineer, or a business analyst doing reporting and dashboards. Those aren't really data science roles. The scientist in data science lies in hypothesizing, defining metrics and estimates, then testing and measuring them. What does the 'scientist' in 'data scientist' mean in the LLM era? Taking Hamel Husain and Shreya Shankar's course on LLM evaluation crystallized this for me. I'm much more convinced that the role of a data scientist is to measure, evaluate, and design metrics. It's going back to the science. Think about the parallel here. In discovery science, data scientists work with laboratory scientists and statisticians to hypothesize about relationships between molecular structure and biological activity, then together define what estimate we need to measure the performance of biological or chemical systems. They build machine learning models to predict those estimands from sequence and structure, test the hypotheses, and measure whether they hold. The estimands matter because they connect to whether a drug works or a process is optimized. With LLM applications automating business processes, it's analogous but the stakes are operational performance. You hypothesize that a particular LLM workflow will improve efficiency or accuracy. You define evaluation metrics\u2014the equivalent of the assays you measure in lab science. You design experiments to test whether your hypothesis about the LLM's impact is correct. You build automation around measurement to continuously validate whether your hypotheses about improved workflows are actually playing out. In both contexts, you hypothesize, define, test, and measure. That's what a scientist does! In what I'd describe as a meta move, data scientists should absolutely be experimenting with LLMs to create LLM-based tooling for their own work. We're uniquely positioned to understand both the technical possibilities and the measurement challenges these systems present. Why this matters more than ever This role differs fundamentally from what people might think we should become. We're not primarily app developers (that should be for software developers), even if we might ship and app or two out of necessity. We're not machine learning engineers building complex production pipelines (though we should be able to ship components that get stitched together on platforms). We're not business analysts doing reporting and dashboards, even if we do build visualizations to help with communication. Rather, we're scientists who hypothesize, define metrics,",
    "tags": [
      "productivity",
      "workflows",
      "evaluation",
      "metrics",
      "business",
      "science",
      "models",
      "ai",
      "tools",
      "measurement"
    ],
    "pub_date": "2025-08-15",
    "type": "blog"
  },
  {
    "id": "blog-stop-guessing-at-priors-r2d2s-automated-approach-to-bayesian-modeling",
    "url": "/blog/2025/8/6/stop-guessing-at-priors-r2d2s-automated-approach-to-bayesian-modeling/",
    "title": "Stop guessing at priors: R2D2's automated approach to Bayesian modeling",
    "summary": "In this blog post, I share my journey exploring the R2D2 framework for Bayesian modeling, which lets you intuitively control model fit by placing a prior on R\u00b2 instead of individual coefficients. I walk through its elegant extensions to generalized linear and multilevel models, showing how it automatically allocates explained variance and prevents overfitting. Curious how this approach can simplify your modeling and highlight the most important factors in your data?",
    "body": "When I first encountered the R2D2 (R\u00b2-induced Dirichlet Decomposition) framework (Zhang et al., 2020), I was struck by its intuitive approach to Bayesian regularization. Instead of placing priors on individual regression coefficients and hoping for the best, R2D2 lets you directly specify your beliefs about how much variance the model should explain. But what really fascinated me was how the framework elegantly extends from simple linear regression to complex multilevel models through a series of principled modifications. This post documents my journey understanding the progression from the basic R2D2 shrinkage prior to its sophisticated multilevel variant (R2D2M2), with stops along the way to explore generalized linear models. What emerged was a beautiful mathematical architecture where each extension builds naturally on the previous. The foundation: R2D2 shrinkage prior The journey begins with the elegant insight that motivated the original R2D2 framework: why not place a prior directly on the coefficient of determination (R\u00b2) rather than fumbling with individual coefficient priors? The challenge with individual coefficient priors isn't just knowing where to center them, but defining appropriate variance parameters - it's remarkably difficult to know a priori how much variability each coefficient should have. The core mathematical insight For any model, R\u00b2 represents the proportion of output variance that can be explained: Rearranging this relationship shows us what W actually represents: This reveals that W is the total explained variance (on the data scale), which equals the signal-to-noise ratio multiplied by the noise variance. Let's define the signal-to-noise ratio as: So that: This gives us: - \u03c4\u00b2 = 1: Signal equals noise (R\u00b2 = 0.5) - \u03c4\u00b2 = 4: Signal is 4 times stronger than noise (R\u00b2 = 0.8) - \u03c4\u00b2 = 0.25: Noise is 4 times stronger than signal (R\u00b2 = 0.2) The R2D2 framework starts by placing a Beta prior on R\u00b2: Zhang et al. show that when R\u00b2 has a Beta(a,b) prior, the induced prior density for \u03c4\u00b2 = R\u00b2/(1-R\u00b2) follows a Beta Prime distribution BP(a,b), giving us intuitive control over model fit through the familiar Beta hyperparameters. Allocating explained variance: the Dirichlet decomposition But here's where R2D2 gets clever. Instead of requiring the modeler to manually specify variance parameters for each predictor's prior, it uses a Dirichlet decomposition to automatically allocate the total explained variance W across predictors: This means answers the question: \"What fraction of the total explained variance does predictor j get?\" Example: If \u03c4\u00b2 = 4 (signal is 4 times stronger than noise) and \u03c3\u00b2 = 2, then W = 8, and if \u03c6 = [0.5, 0.3, 0.2], then: - Predictor 1: \u03bb\u2081 = 0.5 \u00d7 8 = 4 (gets 50% of explained variance) - Predictor 2: \u03bb\u2082 = 0.3 \u00d7 8 = 2.4 (gets 30% of explained variance) - Predictor 3: \u03bb\u2083 = 0.2 \u00d7 8 = 1.6 (gets 20% of explained variance) As Zhang et al. describe, this creates adaptive behavior where \"the heavy tail reduces the bias in estimation of large coefficients, while the high concentration around zero shrinks the irrelevant coefficients heavily to zero, thus reducing the noise\" - factors that explain a lot of the output variance get allocated more of the total explained variance (larger \u03bb\u2c7c values), while factors that don't explain much output variance get allocated less explained variance (smaller \u03bb\u2c7c values). The R2D2 model Bringing these pieces together - the R\u00b2 prior, the Dirichlet variance allocation, and the coefficient distributions - we get the R2D2 model: The beauty of this approach lies in the competitive nature of the Dirichlet allocation: all predictors compete for the total explained variance W. If one predictor becomes more important (higher \u03c6\u2c7c), others must become less important. This creates natural sparsity and prevents overfitting. The signal-to-noise ratio \u03c4\u00b2 provides intuitive control over model complexity, while W gives us the actual variance scale for coefficient priors. First extension: R2D2 for generalized linear models The first major challenge came when extending R2D2 to non-Gaussian outcomes. Yanchenko et al. (2021) tackled this problem by developing clever approximation methods that preserve the intuitive R\u00b2 interpretation. The beautiful relationship that made everything work cleanly suddenly becomes complex when dealing with Poisson counts, binary outcomes, or other GLM families. The challenge: no more simple \u03c3\u00b2 In GLMs, the \"noise\" isn't a simple \u03c3\u00b2 anymore. Instead, we have: - Poisson: Variance equals the mean () - Binomial: Variance depends on probability () - Gaussian: Still simple () This breaks our clean R\u00b2 = W/(W+\u03c3\u00b2) relationship because now both the signal and noise are functions of the linear predictor \u03b7. The elegant solution: linear approximation The GLM extension uses a brilliant linear approximation approach. As Yanchenko et al. describe, \"applying a first-order Taylor series approximation of \u03bc(\u03b7) and \u03c3\u00b2(\u03b7) around \u03b2\u2080\" allows them to handle the GLM comp",
    "tags": [
      "bayesian",
      "variance",
      "r2d2",
      "dirichlet",
      "multilevel",
      "glm",
      "regularization",
      "priors",
      "inference",
      "pymc"
    ],
    "pub_date": "2025-08-06",
    "type": "blog"
  },
  {
    "id": "blog-from-nerd-sniped-to-shipped-using-ai-as-a-thinking-tool",
    "url": "/blog/2025/7/21/from-nerd-sniped-to-shipped-using-ai-as-a-thinking-tool/",
    "title": "From nerd-sniped to shipped using AI as a thinking tool",
    "summary": "In this blog post, I share how months of hands-on struggle and learning paved the way for me to ship a complex graph-based memory feature for Llamabot in just two days\u2014with AI as my design partner. I explain why you have to \"earn your automation\" and how AI can amplify, not replace, your critical thinking. Curious how pairing deep preparation with AI can supercharge your workflow and lead to breakthroughs?",
    "body": "What if I told you I shipped a complex feature rewrite in just two days using AI as a design partner? Before you roll your eyes at another \"AI did everything for me\" story, here's the catch: those two days were only possible because I spent months doing the hard work of earning that automation. Fresh off being thoroughly nerd-sniped by Joe Cheng (Posit PBC's CTO) at SciPy 2025, I found myself on a plane with a mission: finally implement robust graph-based memory for my Llamabot project. What happened next taught me everything about the difference between delegating thinking to AI versus using AI to amplify your thinking. The key insight? You have to earn your automation first. First, I had to struggle (and that was the point) The timeline is crucial to understanding why this approach worked. For four months, I'd been mulling over how graph-based memory for LLM applications could work. Then Joe read my Llamabot code (which at the time didn't have graph-based memory), we chatted, and I got completely nerd-sniped. Over the next few days, I decided I had to make graph memory happen, so I finally built a working prototype during my week in Seattle for work. (All on my personal laptop, keeping work and personal projects separate.) What made this experience transformative was having Joe look at my code. Here was someone I'd never met taking such a thorough look at my design choices - I was deeply impressed by how careful and thoughtful he was. That validation convinced me: it was time to do this right. But here's what mattered most: my prototype was fragile. Things were very intertwined with one another. Because everything was so coupled, I was naturally feeling the difficulty in making any changes. This hands-on struggle was teaching me exactly what needed to be separated and how to think about the architecture. This struggle wasn't wasted time - it was earning my automation. Why struggling first was essential This connects directly to an earlier blog post I wrote about earning your automation. I wouldn't have been able to critique AI the way I did if I hadn't first developed taste through hands-on struggle. That initial prototype work - building something fragile but functional by hand - gave me the judgment needed to meaningfully critique AI's suggestions. Without that foundation, I would have been delegating critical thinking to AI instead of using it as a thinking partner. The prototype taught me what worked, what didn't, and most importantly, what the real problems were that needed solving. When AI later proposed architectural changes, I could evaluate them against my lived experience of the pain points. This preparation set the stage for what happened next on that plane ride home. Then I unleashed AI as a design partner At SEA-TAC airport with three hours until boarding, I decided this was it. Time to compress all my implementation work into a focused sprint. But instead of jumping straight into coding, I started with what felt like a radical approach: a pure design phase. (Now, to be clear, it's not exactly radical - lots of people have said you should write requirements first. But most vibe coders don't actually follow this practice.) I asked AI to critique my existing prototype and propose a new architecture. What followed was intense iteration on a design document right there in the airport. I did try to continue on the plane, but JetBlue's spotty Wi-Fi made that unproductive. Most of the design thinking and iteration happened during those airport hours - no code written yet, just pure design thinking. AI proposed an interface with chat memory at the high level, with separate graph memory and list memory structures underneath. It included a visualization module (originally tailored for graphs) and a node selector module for intelligent node selection. The design doc grew to at least 400-500 lines of markdown. The beauty of this approach? I could look at the prospective code in markdown blocks and play through scenarios in my head. How would someone use this API? How would the internals work? By asking very specific \"how\" questions, I could probe deeper and make sure I truly understood and agreed with every design choice. The major breakthrough came when I scrutinized the design and asked: why do we have two chat memory implementations, one for linear memory and one for graph memory? The natural follow-up hit me: lists are just linear graphs, so why do I need two separate structures? I can just have one that defaults to a linear graph, and then use an LLM for intelligent node selection in the threaded case. So I generalized everything to use NetworkX graphs underneath, with intelligent node selection for threaded memory. This single insight simplified the entire architecture. This is exactly what I mean about earning your automation - I could inject my own opinions into the design because I understood the problem space. We were iterating on a design doc, not just generating code. The real power: AI as a ",
    "tags": [
      "automation",
      "ai",
      "memory",
      "design",
      "coding",
      "testing",
      "architecture",
      "prototyping",
      "review",
      "pairing"
    ],
    "pub_date": "2025-07-21",
    "type": "blog"
  },
  {
    "id": "blog-how-to-use-xarray-for-unified-laboratory-data-storage",
    "url": "/blog/2025/7/15/how-to-use-xarray-for-unified-laboratory-data-storage/",
    "title": "How to use xarray for unified laboratory data storage",
    "summary": "In this blog post, I share how using xarray can transform laboratory and machine learning data management by unifying everything\u2014measurements, features, model outputs, and splits\u2014into a single, coordinate-aligned dataset. This approach eliminates the hassle of index-matching across multiple files, reduces errors, and makes your workflow more reproducible and cloud-ready. Curious how this unified structure can simplify your experimental data analysis and save you time? Read on to find out!",
    "body": "What if your laboratory and machine learning related data could be managed within a single data structure? From raw experimental measurements to computed features to model outputs, everything coordinate-aligned and ready for analysis. I've been thinking about this problem across different experimental contexts. We generate measurement data, then computed features, then model outputs, then train/test splits. Each piece typically lives in its own file, its own format, with its own indexing scheme. The cognitive overhead of keeping track of which sample corresponds to which row in which CSV is exhausting. Let me illustrate this with a microRNA expression study as a concrete example. Here's an approach that could solve this: store everything in a unified xarray Dataset where sample identifiers are the shared coordinate system. Your experimental measurements, computed features, statistical estimates, and data splits all aligned by the same IDs. No more integer indices. No more file juggling. Just clean, coordinated data that scales to the cloud. What's wrong with traditional laboratory data management? Picture this: you're three months into a microRNA expression study. You've got the following files: - expression measurements in , - ML features in , - model outputs in , and - train/test splits scattered across and . Each file has its own indexing scheme - some use row numbers, others use identifiers, and you're constantly writing index-matching code just to keep everything aligned. The cognitive overhead is brutal. Which microRNA corresponds to row 47 in the features file? Did you remember to filter out the same samples from both your training data and your metadata? When you subset your data for analysis, do all your indices still match? I've lost count of how many times I've seen analysis pipelines break because someone forgot to apply the same filtering to all their data files. It's not just inefficient - it's error-prone and exhausting. How does xarray solve this? Xarray changes the game by making coordinates the foundation of your data structure. Instead of managing separate files with separate indexing schemes, you create one unified dataset where every piece of data knows exactly which microRNA it belongs to. The beauty lies in the coordinate system. Each data point is labeled with meaningful coordinates: not just row numbers, but actual experimental factors like microRNA ID, treatment condition, time point, and replicate. When you slice your data, everything stays aligned automatically. This is transformative! When everything shares the same coordinate system, you can slice across any dimension and everything stays connected. Want features for specific microRNAs? The model results for those same microRNAs come along automatically. What does unified data storage look like? Let me walk you through how this works in practice. We start with a coordinate system that captures the experimental design: Then we progressively add data that aligns with these coordinates: The magic happens when you realize that every piece of data is automatically aligned by the shared coordinate system. Need to analyze expression patterns for microRNAs in your training set? It's just coordinate selection: Everything stays connected automatically. No manual bookkeeping required. How do we build this step by step? The approach is straightforward - progressive data accumulation. You don't need to have everything figured out upfront. Start with your core experimental data, then add layers as your analysis develops. Stage 1: Laboratory measurements Your foundation is the experimental data with meaningful coordinates: You should note here how the coordinates basically mirror the experimental design. Stage 2: Bayesian estimation Add effect estimates that align with your experimental coordinates: The beauty is that your Bayesian effects model estimates align perfectly with your experimental design coordinates. Each experimental factor gets its own effect estimate with uncertainty, organized by the same coordinate system as your raw data. Stage 3: ML features Features slot right into the same coordinate system: Stage 4: Train/test splits Even data splits become part of the unified structure: Progressive build = reduced cognitive load The beauty of this approach is that you can build it incrementally. Start with your core experimental data, then add statistical results, then ML features, then splits. Each stage builds on the previous coordinate system, so everything stays aligned automatically. What are the practical benefits? No more index juggling Remember the nightmare of keeping track of which microRNA corresponds to which row in which file? That's gone. Every piece of data knows its own coordinates. Bulletproof data consistency When you slice your data, everything stays aligned automatically. No more worrying about applying the same filtering to all your files. Cloud-native scaling Store everything in Zarr format and your unified datase",
    "tags": [
      "xarray",
      "bioinformatics",
      "reproducibility",
      "cloud",
      "workflow",
      "alignment",
      "features",
      "laboratory",
      "datasets",
      "scaling"
    ],
    "pub_date": "2025-07-15",
    "type": "blog"
  },
  {
    "id": "blog-reflections-on-the-scipy-2025-conference",
    "url": "/blog/2025/7/14/reflections-on-the-scipy-2025-conference/",
    "title": "Reflections on the SciPy 2025 Conference",
    "summary": "In this blog post, I reflect on my 10th year at the SciPy Conference, sharing highlights from teaching tutorials, attending inspiring talks, recording informal podcast conversations, and contributing to open source projects. I discuss the power of community, the evolution of scientific notebooks, and the importance of financial aid in making SciPy accessible. Curious about the behind-the-scenes moments and lessons learned from a decade at SciPy?",
    "body": "This year marks my 10th year of being involved with the Scientific Python Conference, and it has been an absolute blast! What started as curiosity about the intersection of science and software has grown into a decade of learning, teaching, and contributing to this incredible community. Conference Activities Summary This year's SciPy was particularly active for me. I taught two tutorials: \"Building with LLMs Made Simple\" (a new one) and \"Network Analysis Made Simple\" (my longtime favorite). After the tutorials, I attended several inspiring talks, including an especially motivating presentation on XArray in biology that prompted me to create a Marimo notebook demonstrating XArray's applications in biological data analysis. One of my favorite conference activities this year was recording conversations with fellow attendees. In lieu of my Insta360 camera, I brought my DJI mic everywhere and captured numerous insightful discussions, creating an informal podcast collection of SciPy conversations. Finally, during the sprints, I felt more tapped out than usual but still managed to contribute to Llamabot development with others and work on the XArray biology materials I had envisioned. Tutorials Building with LLMs Made Simple This was my first time teaching this tutorial, and I was thrilled to use Marimo notebooks throughout the entire session. The tutorial covered three main areas: simple LLM interactions, structured generation, and RAG (Retrieval-Augmented Generation). You can find the tutorial materials at: https://github.com/ericmjl/building-with-llms-made-simple The structured generation section was particularly powerful. I emphasized that structured generation is fundamentally about automating form-filling using natural language. Having free text input and getting a filled-out Pydantic model output is incredibly valuable for productivity. One participant mentioned the concept of automating \"the dangerous, the dull, and the dirty\" - which perfectly captures how LLMs can handle routine tasks. For RAG, I clarified that RAG doesn't necessarily equal vector databases - it's about information retrieval through various means including keyword search. I demonstrated custom chunking strategies for standard operating procedures, showing how simple solutions (like appending source references) often work better than complex hierarchical structures. The tutorial concluded with brief demos on evaluation and agents. I shared my experience testing different models (Gemma, Llama 3, Llama 4) for docstring generation, emphasizing the importance of experimentation and model selection. For agents, I stressed starting with simpler structured generation approaches before building complex autonomous systems. Thanks to Modal's generous credit allocation from their DevRel Charles, I was able to deploy an Ollama endpoint in the cloud, making the tutorial accessible to all participants. Network Analysis Made Simple This marked either my ninth or tenth time teaching this tutorial at SciPy - my longtime favorite. This year I made the significant transition from Jupyter to Marimo notebooks, which was an experiment that generally worked well despite some setup challenges. You can find the tutorial materials at: https://github.com/ericmjl/Network-Analysis-Made-Simple The tutorial faced some technical hurdles for installation with the Network Analysis Made Simple package being published on my own PyPI server, plus some participants weren't familiar with Marimo. Fortunately, Erik Welch from NVIDIA was present to help assist participants. By the end of the conference talk days, I was able to resolve the issue by changing the notebooks to draw from the Network Analysis Made Simple source directly instead of my own PyPI server, which solved most of the installation problems. What I loved most was the audience engagement. We didn't cover as much content as usual because participants asked so many thoughtful questions, especially during the visualization section. This interaction made the session incredibly valuable, as people were clearly learning and developing new ideas for their own work. The Marimo experiment succeeded in shifting the learning environment with minimal overhead. For future iterations, I'm considering eliminating the separate NAMS package and making the entire notebook self-contained with answers included at the bottom. Overarching thoughts on the tutorials Both tutorials were conducted entirely within Marimo notebooks, which convinced quite a few participants to switch over to Marimo. They saw the power of fully reactive notebooks and the ability to seamlessly share analysis from one person to another - something that's much more cumbersome with traditional Jupyter notebooks. Both tutorials will also be available on YouTube! There was a technical glitch with the Building with LLMs Made Simple tutorial recording, so I'm planning to re-record the full tutorial this coming Saturday - including content we didn't get to cover during",
    "tags": [
      "scipy",
      "python",
      "conference",
      "marimo",
      "tutorials",
      "llms",
      "xarray",
      "community",
      "networking",
      "career"
    ],
    "pub_date": "2025-07-14",
    "type": "blog"
  },
  {
    "id": "blog-earn-the-privilege-to-use-automation",
    "url": "/blog/2025/7/13/earn-the-privilege-to-use-automation/",
    "title": "Earn the privilege to use automation",
    "summary": "In this blog post, I reflect on the challenges of integrating AI into education and the workplace, sharing lessons from educators who found that unrestricted AI access can undermine true learning and assessment. I discuss why it's crucial to earn the privilege to use automation by first mastering foundational skills and demonstrating the ability to verify AI outputs. How can we ensure that AI enhances, rather than replaces, our critical thinking and problem-solving abilities?",
    "body": "AI in education was supposed to be transformative. We imagined students with AI tutors available 24/7, personalized learning at scale, and democratized access to high-quality education. The promise was intoxicating: every student could have their own Socrates, guiding them through complex concepts with infinite patience. Then reality hit. When AI integration fails spectacularly Lorena Barba, a respected engineering professor at George Washington University, shared her experience at SciPy 2025 of deciding to fully embrace AI in her computational engineering course. She built a custom AI tool with her technical partners, complete with document upload capabilities, retrieval augmented generation, and safety moderation features. She gave her students what seemed like the perfect educational AI assistant. The results were devastating. Her course evaluations plummeted from 4.8/5 to 2.3/5. Students stopped attending class. They stopped doing homework with any rigor. Some copied entire assignment questions, including instructions like \"your code here,\" and expected complete answers they could submit without understanding. The most damning feedback? Students told her: \"I would have learned better if AI were not present.\" What went wrong? Lorena had given her students unbridled access to AI without ensuring they had the foundational skills to use it effectively. Students developed what she called an \"illusion of competence\"\u2014they overestimated their knowledge because AI made everything feel easy. They missed the deep processing necessary for long-term memory formation. After 20 years of successful teaching, Lorena experienced what she called a \"frustrating, humbling failure.\" She's now considering returning to oral examinations to preserve assessment authenticity. The assessment validity crisis Lorena's experience reveals a fundamental problem: AI has broken traditional assessment methods. If students can get AI to do their work, how do we evaluate their actual understanding? How do we conduct meaningful assessments in both educational and workplace settings? This question hits close to home for me. As a team lead, I constantly assess whether candidates are ready for the job and whether my teammates are performing at expected levels. If I'm only looking at work outputs\u2014the final code, the completed analysis, the polished presentation\u2014that's an inadequate assessment method. AI has made it trivially easy to produce impressive-looking outputs while learning nothing. I need to understand how people think through problems, not just whether they can deliver results. This challenge sparked intense conversations with educators at SciPy 2025. Daniel Chen (University of British Columbia) and Ryan Cooper (University of Connecticut) each brought unique perspectives on adapting our assessment methods to this new reality. Assessing the process, not just the product Daniel Chen had to fundamentally shift his approach. He moved up Bloom's taxonomy for assessment, focusing on questioning and synthesis rather than factual regurgitation. His key insight: when students ask questions, it reveals their level of understanding. Insightful questions indicate pursuit of mastery. Surface-level \"how do I get this done\" questions reveal a lack of deep engagement. Daniel proposed assessing students through their AI chat transcripts. Instead of only evaluating final products, we could examine both process and outcome. This approach reveals how students think through problems, potentially restoring validity to our assessments. Ryan Cooper had already started implementing this idea, collecting chat transcripts to understand student thinking patterns. He also experimented with having students generate their own exam questions\u2014leveraging the fact that creation sits at the highest level of Bloom's taxonomy. Ryan gave students access to a curated AI system conditioned with course context, generating on-the-fly assessment questions. While innovative, he encountered challenges with rubric-based grading when AI suggested grades without clear criteria. Why this matters in the workplace These educational assessment challenges directly mirror my daily reality as a team lead. AI assistance allows me to work solo and move incredibly fast\u2014I love that turbocharged feeling. But this speed creates a dangerous blind spot that affects both my personal development and my team's growth. Here's my dilemma: if I don't slow down to demonstrate my thinking process, we lose opportunities to train junior team members. More concerning, if I can't see how my team members approach problems\u2014only their final outputs\u2014I can't effectively assess their capabilities or guide their development. When team members use GitHub Copilot or similar tools, I need visibility into their thought processes, not just their code. Are they asking insightful questions? Do they understand the trade-offs they're making? Can they spot when the AI suggests something problematic? Without access to their rea",
    "tags": [
      "education",
      "assessment",
      "automation",
      "ai",
      "learning",
      "workplace",
      "skills",
      "process",
      "outcomes",
      "privilege"
    ],
    "pub_date": "2025-07-13",
    "type": "blog"
  },
  {
    "id": "blog-the-job-your-docs-need-to-do",
    "url": "/blog/2025/7/7/the-job-your-docs-need-to-do/",
    "title": "The job your docs need to do",
    "summary": "In this blog post, I explore how combining the Diataxis documentation framework with Clayton Christensen's jobs-to-be-done theory can transform the way we write docs. By focusing on the specific outcomes readers want to achieve, we can make our documentation more useful and competitive\u2014not just against other docs, but against all the ways people solve their problems. What happens when you treat your documentation as a product designed to help users get real jobs done?",
    "body": "What is the job that your docs need to do? Two threads have been running through my mind recently, and I keep finding connections between them that I can't shake. The first is Diataxis - a structured framework for documentation that divides all docs into four distinct types: tutorials, how-to guides, reference, and explanation. The second is Clayton Christensen's jobs theory, which asks a deceptively simple question: what is the job that your customer needs to get done? Side note: I've been heavily inspired by Clayton Christensen's books recently, and have audiobooked my way through Innovator's Dilemma/Solution/DNA, as well as Competing Against Luck. All good books, 100% recommended if you're interested in understanding how innovation actually works. Here's the key insight: your documentation isn't competing with other documentation. It's competing with every other way someone could accomplish their job. The competition you didn't know you had When someone opens your internal documentation, they're looking for more than information. They're trying to accomplish something specific, and they're evaluating whether your docs are the right tool for that job. For internal company documentation\u2014whether it's for internally built software, processes, or systems\u2014the competition is different but equally real. Your how-to guide competes with asking a colleague, digging through Slack history, or reverse-engineering from existing code. Your reference docs compete with reading the source code directly, checking configuration files, or experimenting in a staging environment. | Diataxis Doc Type | Jobs to be Done (JTBD) | Alternative Product Categories That Could Be Hired | | --------------------- | ------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------ | | How-to Guides | \"Show me how to achieve a specific outcome.\" | Asking a colleague, Slack/Teams search, reverse-engineering from existing code, trial and error in staging | | Reference | \"Give me exact technical information I can look up quickly.\" | Reading source code, checking config files, database schemas, API endpoint testing, environment variables | | Explanation | \"Help me understand how/why it works.\" | Architecture diagrams, code comments, git history, team knowledge sharing sessions, design documents | | Tutorials | \"Help me learn by doing, in a safe structured way.\" | Pair programming, shadowing a colleague, sandbox environments, local development setup walkthroughs | Once you see this competition, it reframes how you think about documentation structure. The opportunity hiding in plain sight Here's what's fascinating about internal documentation: the competition is actually pretty terrible. Think about it: - Asking a colleague interrupts their work and creates context switching for both of you - Digging through Slack history is time-consuming and often incomplete - Reverse-engineering from existing code is slow and error-prone - Trial and error in staging environments wastes time and resources This means that even moderately good internal documentation has a much lower bar to clear than external documentation. Your internal how-to guide doesn't need to compete with polished YouTube tutorials - it just needs to be better than interrupting Sarah from accounting or spending 30 minutes searching through #engineering-general. This is actually a huge opportunity. While external documentation faces fierce competition from Stack Overflow's crowdsourced answers and professionally produced tutorials, internal documentation often competes with... nothing systematic at all. The result? Even basic improvements to internal documentation can have outsized impact on team productivity. When your reference docs are faster than reading source code, people will use them. When your how-to guides are clearer than tribal knowledge, they become the default choice. Understanding the competition Most documentation is written from the perspective of the product being documented. It's organized around features, capabilities, and technical architecture. But when you flip the perspective to focus on jobs-to-be-done, you can structure information more effectively around what readers actually need to accomplish. A job-focused approach in practice Let me show you what this looks like in practice. Say you're writing a how-to guide for deploying your company's internal microservice. The traditional approach focuses on what information to include. The job-focused approach starts with the specific outcome: \"Help me get this service deployed so I can test my feature and merge my PR.\" That job-focused lens shifts how you structure the guide: - You lead with the most common use case and a working example first, then dive into edge cases - You include troubleshooting steps for the most common failure modes - You assume they're in a hurry and want to get back to their main project Every decisi",
    "tags": [
      "documentation",
      "diataxis",
      "innovation",
      "tutorials",
      "guides",
      "reference",
      "ai",
      "strategy",
      "product",
      "jobs theory"
    ],
    "pub_date": "2025-07-07",
    "type": "blog"
  },
  {
    "id": "blog-one-hour-and-eight-minutes-building-a-receipt-scanner-with-the-weirdest-tech-stack-imaginable",
    "url": "/blog/2025/7/1/one-hour-and-eight-minutes-building-a-receipt-scanner-with-the-weirdest-tech-stack-imaginable/",
    "title": "One hour and eight minutes: Building a receipt scanner with the weirdest tech stack imaginable",
    "summary": "In this blog post, I share how I built a fully functional receipt scanning and expense tracking app in just over an hour using an unconventional tech stack\u2014FastAPI, HTMX, Notion, and my own LlamaBot AI. I describe how Claude Code enabled a focused, terminal-based workflow that kept me in the zone and made rapid prototyping possible. Curious how combining unusual tools can unlock new possibilities and boost your productivity? Read on to find out!",
    "body": "After bouncing between Cursor and GitHub Copilot for the past couple of years, I kept hearing about Claude Code. People's experiences were really piquing my curiosity, so I decided to give it a shot. What happened next completely changed how I think about rapid prototyping. I built a fully functional receipt scanning and expense tracking app in exactly one hour and eight minutes. But here's the kicker\u2014I used a technology stack so unconventional that most developers would probably laugh at me. And it worked beautifully. Let me tell you what I learned about the immersive power of terminal-based development and why weird tech combinations might be the secret to lightning-fast tool building. The problem I wanted to solve At work, I noticed SAP Concur can automatically extract fields from uploaded receipts. I thought, \"What if I could replicate that at home?\" I wanted to track my expenses without paying for QuickBooks, using Notion as my database instead. Most developers would reach for the standard stack: React frontend, PostgreSQL backend, maybe throw in some Express.js. That's the sensible approach. But I'm not building production software for thousands of users. I'm a data scientist experimenting with tools for myself. So I decided to get weird with it. The stack that shouldn't work but does Here's what Claude Code helped me build with: - FastAPI for the backend (this part makes sense) - HTMX for the frontend instead of React (getting unusual) - Vanilla HTML/CSS with minimal JavaScript (now we're talking) - LlamaBot for AI interactions (I made it, so I know it works) - Notion as the database (yes, you read that right) If I were to describe this stack to a seasoned developer, they'd probably be surprised, then laugh out loud, and then go \"what?\" But when I described it to Claude Code and specified that I wanted everything in a single file that I could run with , Claude Code got creative. It generated a beautiful single-file application with PEP 723 metadata at the top. The code was clean and well-structured. It took a few iterations of AI-generated code writing followed by testing, but it was always generally headed in the right direction. And this is the result: The development experience that changed everything Here's what blew my mind about using Claude Code: the immersive experience. I spent the entire development session in just two terminal tabs. One tab running Claude Code, another tab with my server running with auto-reload. That's it. No switching between file explorers, no hunting through directory structures, no context switching between different applications. I was in what I can only describe as \"vibe-ish coding\" mode\u2014not quite the vibe coding that Simon Willison describes, but close. I'd type a request to Claude Code, see the changes instantly in my browser, then iterate. The feedback loop was immediate and distraction-free. This terminal-focused workflow kept me in the zone in a way that traditional IDEs never have. Without all the little icons, bells, and whistles that can distract you in an IDE, I could maintain focus on the actual problem I was solving instead of fighting with tools. What got built in 68 minutes By the time my terminal session ended, I had a fully functional application that could: - Upload single or multiple receipt images - Extract expense data using LlamaBot's AI capabilities - Allow manual editing of fields that the AI got wrong (inside Notion) - Handle enumerated types for expense categories - Automatically populate a Notion database with extracted data The AI integration was seamless. I provided my OpenAI API key, and through LlamaBot I was able to hit the OpenAI API while Claude Code handled all the integration complexity. When I needed to add file upload functionality, I pasted some Notion API documentation as context, and Claude Code implemented it correctly. The end result? I can now drag and drop receipts into a web interface, hit submit, and watch the data appear automatically in my Notion expense tracker. Exactly what I wanted. Pushing language models to their limits Here's the thing that really fascinated me about this experiment: I deliberately chose this weird tech stack to test Claude Code's boundaries. Think about it\u2014if I had gone with React, Node.js, and PostgreSQL, that would be easy for any language model. Those patterns show up constantly in training data. But I wanted to push to the edges. What happens when you combine technologies that people don't usually think about together? HTMX with FastAPI? Notion as a database backend? A single-file Python app doing receipt processing with AI? This is uncharted territory for most language models. There aren't thousands of tutorials showing how to integrate LlamaBot with HTMX forms, or how to structure FastAPI routes that return HTML fragments for dynamic updates. Yet Claude Code handled it beautifully. It figured out how to make these disparate pieces work together, even when the combination got weird. Why this",
    "tags": [
      "prototyping",
      "ai",
      "claude",
      "terminal",
      "fastapi",
      "htmx",
      "notion",
      "llamabot",
      "experimentation",
      "expenses"
    ],
    "pub_date": "2025-07-01",
    "type": "blog"
  },
  {
    "id": "blog-build-your-own-tools",
    "url": "/blog/2025/6/27/build-your-own-tools/",
    "title": "Build your own tools!",
    "summary": "In this blog post, I share my journey from dreading slide creation to building DeckBot, a tool that automates the process, and reflect on the power of building your own tools as a data scientist. I discuss how creating custom solutions has empowered me and my teams, scaled our impact, and fostered a culture of innovation. I also highlight the importance of organizational support and the joy of learning through building. Curious how building your own tools can transform your work and mindset? Read on to find out!",
    "body": "On 25 June 2025, I delivered a talk at Data-Driven Pharma, an event organized by Ilya Captain and the namesake Data-Driven Pharma organization. In the run-up to the talk, I had been reflecting on two points: 1. I hate making slides, and 2. I really love building tools. To that end, I decided... well, I'm not going to bother with making slides. And I'll build a tool that makes slides for me instead. Hence [DeckBot], which currently lives in a marimo notebook, was born. I started off by telling the crowd how much I hated making slides: In an age of LLMs and plain .txt, I understand why I have such a disdain for powerpoint: you can't easily automate their creation, there's too much that can be hidden behind a bullet point, and it's just an all-round ineffective media for lasting crystal clear communication. By contrast, Markdown slides are better. -- Original post link here And how even Andrej Karpathy laments the absence of an LLM-enabled tool for building slides: Making slides manually feels especially painful now that you know Cursor for slides should exist but doesn\u2019t.&mdash; Andrej Karpathy (@karpathy) June 6, 2025 Also, my informal poll of the audience revealed that approximately 2/3 of the crowd also hated making slides. Not surprising! So I decided to take that as a nerdsnipe and actually make DeckBot. After showing the audience (live!) how I can make rando slides for completely nondescript topics, such as, \"Why eating well is so important\" or \"pros and cons of buying a thing\", I then proceeded with the real exciting challenge of this talk: to get an LLM to generate my entire slide deck for the actual topic I wanted to talk about, from which I would present. And that topic was, well, \"Build your own tools!\". I then proceeded to copy/paste in the first draft of this blog post into the notebook, and 1 minute later, I had my slides, from which I presented live. Below is a writeup of what I actually presented, including a written description of some of the interactions. My main message to everybody today is this: If you're a data scientist, computational biologist, or software developer, you should learn how to build your own tools. Building your own tools is a liberating endeavor. It injects joy back into your day-to-day work. People were made to be creative creators. Build your own tools. A flashback from my grad school days Do you know what this diagram is? The audience came in clutch, many people knew what this was -- it's a Circos plot. Some may have seen it with arcs rather than dots around the edges, but the concept remains the same: prioritize ordering nodes and then draw in the edges. I wanted to learn how to make a graph visualization like this. But the only tool I saw out there was written in a different language (Perl), had no Python bindings, and was way too complicated for me\u2014a beginner programmer in 2014\u2014to learn. So I decided to leverage two other tools that I knew at the time, Python and matplotlib, to make my own Python package, both to learn software development and to understand the principles of rational network visualization. The precursor to nxviz, , was born in 2015. One year later, I knew enough to make all sorts of network visualizations! Like this, the matrix plot: Or this, a geo plot: Or this, an arc plot: Or this, another circos plot: Or this beautiful thing, a hive plot: What's the unifying thread behind all of those plots? As it turns out, the thing I learned while building my own graph visualization tool was that *rational and beautiful graph visualization starts with knowing how to order nodes in a graph, and then drawing in the edges. I would have never learned that had I not attempted to reinvent the wheel (or, perhaps, Circos plots)! Additionally, being able to build my own Python package was superbly empowering, especially as a graduate student! I could build my own tools, archive them in the public domain, and never have to solve the same problem again. This echoed Simon Willison's approach to software development: I realized that one of the best things about open source software is that you can solve a problem once and then you can slap an open source license on that solution and you will never have to solve that problem ever again, no matter who's employing you in the future. It's a sneaky way of solving a problem permanently. -- Original post link by Simon Willison here If I didn't know how to build my own tools, I'd have been stuck, and I'd never have learned anything new. Fast-forward to 2018 at Novartis My colleague Brant Peterson showed me the R package , and I thought, \"Why can't Pythonistas have nice things?\" Then, I remembered Gandhi's admonition \"Be the change you wish to see in the world.\" And so, was born. Your dataframe manipulation and processing code can now be more expressive than native pandas: By being the change I wanted to see, Pythonistas now have one more nice thing available to them. And of course, I just had to inject this in: that was all in 2",
    "tags": [
      "automation",
      "python",
      "open source",
      "llm",
      "tooling",
      "scaling",
      "impact",
      "software",
      "customization",
      "development"
    ],
    "pub_date": "2025-06-27",
    "type": "blog"
  },
  {
    "id": "blog-rethinking-llm-interfaces-from-chatbots-to-contextual-applications",
    "url": "/blog/2025/6/14/rethinking-llm-interfaces-from-chatbots-to-contextual-applications/",
    "title": "Rethinking LLM interfaces, from chatbots to contextual applications",
    "summary": "In this blog post, I share why I believe the future of LLM applications lies beyond chat interfaces. Drawing on insights from colleagues, thought leaders, and my own experience building DeckBot, I argue that embedding AI into structured workflows\u2014like TurboTax\u2014creates more effective and delightful user experiences. Instead of relying on open-ended chat, we should inject LLMs at key moments within well-defined processes. Curious how this shift could transform the way we build and use AI-powered tools?",
    "body": "Chat interfaces were a great starting point for interacting with large language models, but they're not the endgame. My thesis is that we should build LLM applications as contextual tools embedded in structured workflows, not as open-ended chat interfaces. This insight came from three converging threads that fundamentally changed how I think about building LLM-powered applications. The first thread came from a conversation with my colleague Michelle Faits, who articulated that apps powered by generative AI really need to end up looking less like chat interfaces and more like TurboTax -- where there's a well-defined process that needs to happen, and instead of users filling out forms manually, we ask an AI to help with the form-filling process. The second thread was a YouTube video titled \"AI UX Design: ChatGPT interfaces are already obsolete\" by Alan Pike from Vancouver. In it, he talks about shifting from chatbot to context-native interfaces, a change that's both subtle and dramatic. It's subtle because there's little visible change, but dramatic because the way you interact with the interface changes fundamentally. You're no longer stuck with the drudge work of filling out yet another form, but are instead presented with an AI-powered interface capable of understanding what your next action is likely to be and anticipating it just in time. The third thread is Clayton Christensen's \"jobs to be done\" theory. What I've been noticing is that there are too many ChatGPT copycat clones, and those chat clones don't really help me accomplish the job that I'm trying to do. It takes a different type of interface to make that happen. These threads converge on a simple truth What connects TurboTax's structured approach, Pike's context-native interfaces, and jobs-to-be-done theory is this: the most effective LLM applications will embed AI capabilities directly into well-defined workflows rather than forcing users to articulate their needs through chat. This means moving from \"tell the AI what you want\" to \"let the AI assist you as you work through a process you already understand.\" The TurboTax moment Michelle's insight about TurboTax really stuck with me. TurboTax works because it represents a well-defined business process with pretty routine steps that we need to walk through, but some of the steps do require judgment calls. Do you fill out this section or not? And what do you fill in? You need to determine that from context, so there's a little bit of agency for LLM bots inside there. But for the most part, it's just form filling. This is a powerful analogy for LLM apps, one that gets to the heart of any app build. The question becomes: how do you go about building a user interface that works like this? When we build chat interfaces, we put a lot of onus on the LLM to make smart decisions on behalf of us. But what if chat wasn't the primary way of interacting? What if we had well-defined business workflows supported by custom apps that just require us to fill out forms in a delightful way? The obsolescence of chat interfaces Alan Pike's perspective really crystallized something I'd been feeling. In his talk, he showed how we're moving from text-based interfaces that are powerful but confounding to 90% of people, toward context-native interfaces that inject AI capabilities right where you need them. Think about it: we've already started seeing hints of tools pushing chat to the side. ChatGPT has Canvas mode now, where if you ask it to co-author a document, it sticks the chat up in the corner and lets you focus on the work you're doing. But this is still just the beginning. Pike showed examples of right-click contextual actions, natural language search that understands intent rather than requiring exact phrases, and date pickers where you can just say \"next Thursday at 11\" instead of clicking through calendar grids. These represent a fundamental shift in how we think about human-computer interaction. I thought the talk was quite good, and I'm embedding it below to share. Jobs to be done theory meets LLM apps Clayton Christensen's jobs-to-be-done framework is perfect for thinking about LLM applications. When I look at most LLM interfaces, we've become hooked on chat -- but they don't necessarily always help me accomplish the specific job I'm trying to do. Generic chat interfaces put the burden on me to figure out how to express my needs and on the LLM to figure out what I actually want. What if we could do better? I think what we're going to see is an evolution of LLM-powered apps from being text and chat driven to being deeply embedded within applications, making it possible to flow through business processes in a way that's much smoother and more delightful than what was possible before. It's not really about agentic capabilities, which are nice, but the winners will be the interfaces that inject LLMs in just the right places -- in the boring work! Building DeckBot demonstrates this approach Let me show you what this",
    "tags": [
      "llm",
      "ai",
      "workflow",
      "interfaces",
      "automation",
      "contextual",
      "ux",
      "apps",
      "business",
      "augmentation"
    ],
    "pub_date": "2025-06-14",
    "type": "blog"
  },
  {
    "id": "blog-principles-for-using-ai-autodidactically",
    "url": "/blog/2025/6/7/principles-for-using-ai-autodidactically/",
    "title": "Principles for using AI autodidactically",
    "summary": "In this blog post, I share insights from my interviews with researchers and digital professionals on how to use AI, especially large language models, as a tool for active learning rather than passive consumption. I discuss strategies like creating personalized syllabi, applying critical thinking, and using AI for feedback, emphasizing that true learning requires effort and agency. Want to know the key trick to making AI your learning partner instead of your crutch?",
    "body": "We need to move beyond passive consumption Imagine having a personal tutor who's absorbed millions of books, papers, and discussions across every field of human knowledge. That's essentially what Large Language Models (LLMs) offer us today. As David Duvenaud aptly describes them, LLMs are a \"galaxy brain\" of knowledge waiting to be tapped. But having access to information isn't the same as learning from it. The difference lies in how we engage with these AI tools - passively consuming their outputs versus actively using them to expand our understanding. Through my interviews with researchers and digital professionals, I've discovered patterns in how the most effective learners use AI autodidactically - teaching themselves with AI as their assistant, not their replacement. Lessons from autodidactic AI users at work I have conducted many interviews at work about how folks in Moderna's Research and Digital organizations use AI. While the discussions are insanely specific to work and sometimes touch on IP that I cannot reveal, there are principles and patterns in what I observe the best folks do when using AI in their day-to-day work to learn new stuff. Generate a personalized syllabus for learning They recognize that any kind of learning involves effort and hard work, and that the pain of the process is a non-negotiable to make anything stick. So instead of using AI to do stuff for them, they start by using AI to provide a tailored syllabus that allows them to progressively move up the knowledge ladder with increasing effort. This is what I would call \"scaffolding a personalized syllabus\". Their prompts here often include a bit of their current role, their prior training, their own objectives for learning, and what they know from prior experience about how they learn best. On the basis of the syllabus, iterate and follow up. Apply one's ability to think critically to LLM outputs They recognize that questions are a great way to learn, so they will continuously question and LLM to draw out answers. The act of generating a question as a human is part of the effort needed. They apply the skill of critical thinking to the answers generated by an LLM, asking questions such as, \"if this is true...\" or \"is this coherent with...\". They do not blindly accept the output of an LLM! Apart from self-coherence with what they know, they verify by cross-checking reputable sources on the internet -- scholarly literature, expert writing, etc. At a meta-level, if they find an angle that demands explanation, knowing that sometimes an LLM can be blinded by conversation history, they will explicitly prompt an LLM on contrary points, using prompts that start with, \"but I remember that...\" or \"this sounds suspicious, could it be that...\" Also, in the absence of another human, they use LLMs to provide initial critique about what they have produced (e.g. in writing form). They use LLMs in the same way jazz musicians riff off one another. What's the core trick? At its core, the main \"trick\" to using an LLM autodidactically is to avoid delegating critical thinking to the LLM and instead applying the full force of one's agency. We need to leverage the galaxy brain of knowledge from its training set (and, where applicable, internet search capabilities) and apply individual effort by critically thinking through LLM outputs. Essentially, every skill we were taught to hone in literature class in high school, debate club in junior college, science philosophy class in undergrad, and scientific journal clubs during graduate training! AI has brought the philosophical points of human agency into sharp relief. Like any tool, LLMs can be used to increase your agency or diminish it. It's a double-edged sword. Use it for the former!",
    "tags": [
      "llms",
      "autodidactic",
      "ai",
      "learning",
      "agency",
      "syllabus",
      "education",
      "critical",
      "digital",
      "knowledge"
    ],
    "pub_date": "2025-06-07",
    "type": "blog"
  },
  {
    "id": "blog-the-invisible-polish-of-automatic-model-routing",
    "url": "/blog/2025/5/25/the-invisible-polish-of-automatic-model-routing/",
    "title": "The invisible polish of automatic model routing",
    "summary": "In this blog post, I explore Cursor's latest update featuring automatic model routing, which eliminates the need for manual model selection. This change reduces the cognitive tax of micro-decisions, allowing me to focus more on coding. Drawing parallels to Apple's design philosophy, I discuss how removing unnecessary interfaces can enhance user experience. This shift in AI tool design suggests a future where systems handle complexity, simplifying user interactions. What other invisible frictions in AI tools could be automated away to improve our workflow?",
    "body": "I've been using Cursor's latest updates, and while the surface-level improvements are nice\u2014better edge rounding, refined colors, thoughtful layering\u2014there's one change that's got me genuinely excited: automatic model routing. No more model picker. No more stopping mid-thought to decide between OpenAI's models, Claude, or whatever other model might be appropriate for my current task. Cursor just figures it out and routes my request to the right model automatically. Why model pickers are UI bugs I remember reading somewhere (probably on Twitter, let's be honest) that model pickers are fundamentally a UI bug. The argument was simple: users shouldn't need to understand the technical differences between models to get their work done. They should just describe what they want, and the system should handle the rest. At the time, I nodded along but didn't fully appreciate how right this was until I experienced Cursor's implementation. Before this change, I was making micro-decisions about model selection multiple times per day. Should I use the faster model for this simple refactoring? Do I need the more capable model for this complex architectural question? Each decision was small, maybe taking 2-3 seconds, but they added up. The cognitive tax of micro-decisions These tiny decisions represent what I think of as cognitive tax: small mental overhead that accumulates throughout the day. Each model selection forced a brief context switch: I had to step out of my coding flow, evaluate the complexity of my request, weigh speed versus capability, and make a choice. The individual cost was negligible. The cumulative cost was not. By the end of the day, I'd made dozens of these micro-decisions, each one pulling a small amount of mental energy away from the actual problem I was trying to solve. Cursor's automatic routing eliminates this entirely. I describe what I want, hit enter, and trust that the right model will handle it. The decision-making burden shifts from me to the system, where it belongs. Parallels to Apple's design This reminds me of something I read about Apple's design philosophy during the Jony Ive era. The idea more than making things look beautiful, it was about removing friction at every possible level, even in places users might not consciously notice. Think about the original iPhone's lack of a keyboard. Everyone said it was crazy, that people needed physical keys. But Apple understood that the mental model of \"keyboard for typing\" was actually limiting. By removing the physical keyboard, they freed up space for context-sensitive interfaces that could adapt to what you were actually trying to do. Cursor's automatic model routing feels like the same kind of thinking. Instead of optimizing the model picker interface, they eliminated the need for it entirely. The best interface is often no interface at all. The broader principle What makes this interesting isn't just that it saves me a few seconds per day. It's that it represents a shift in how we think about AI tool design. Instead of exposing the complexity of the underlying system to users, we can build intelligence into the routing layer itself. This has implications beyond just model selection. How many other micro-decisions are we forcing users to make that could be automated away? How many interface elements exist because we haven't figured out how to make them unnecessary? I suspect we'll see more of this pattern as AI tools mature. The first generation of AI interfaces were necessarily explicit: users needed to understand models, parameters, and context windows because the tools couldn't make those decisions reliably. But as the underlying systems get smarter, the interfaces can get simpler. The invisible improvements The best improvements are often the ones you don't notice consciously but feel in your workflow. Cursor's automatic model routing is exactly this kind of enhancement. I don't think about it while I'm coding, but I feel its absence when I use other tools that still require manual model selection. This is the kind of polish that compounds. Each eliminated micro-decision, each removed point of friction, each automated choice creates space for deeper focus on the work that actually matters. It's not revolutionary on its own, but it's part of building tools that feel like extensions of thought rather than obstacles to it. The question for other AI tool builders is: what other invisible friction exists in your interfaces? What decisions are you forcing users to make that your system could handle automatically? The model picker was just the beginning.",
    "tags": [
      "cursor",
      "routing",
      "ui",
      "cognitive",
      "ai",
      "automation",
      "apple",
      "workflow",
      "design",
      "improvement"
    ],
    "pub_date": "2025-05-25",
    "type": "blog"
  },
  {
    "id": "blog-supercharge-your-coding-agents-with-vscode-workspaces",
    "url": "/blog/2025/5/24/supercharge-your-coding-agents-with-vscode-workspaces/",
    "title": "Supercharge your coding agents with VSCode workspaces",
    "summary": "In this blog post, I share how using Workspaces transformed my workflow while building out my tutorial repositories for SciPy 2025. By adding multiple repositories to a single workspace, I eliminated constant window switching and enabled my coding agent to access context across all repos simultaneously. This setup allows for seamless coordination between library code and tutorial examples. I provide a step-by-step guide to setting up a multi-repo workspace and offer tips for maximizing its benefits. Curious about how this can streamline your coding process and enhance your productivity?",
    "body": "I was building out my LLM tutorial repository for SciPy 2025 and found myself constantly switching between windows\u2014improving the LlamaBot library in one window, then flipping to my tutorial repo in another to update examples that used the new features. Every time I added a new method or changed an API in LlamaBot, I had to remember to update the corresponding tutorial examples. The constant context switching was slowing me down and making it easy to miss places where the tutorial needed updates. Then I discovered something that changed how I code across repos: Workspaces! They aren't just convenient for organizing multiple repositories, they're also game-changers for coding agents in Cursor. When you add multiple repositories to the same workspace, your coding agent magically gains context across all your repos simultaneously. No more window switching, no more explaining relationships between codebases. Instead, your coding assistants can access code in multiple repositories at once. Here's how to set this up and why it matters. Setting up your first multi-repo workspace Step 1: Create the workspace Open a blank Cursor/VSCode window and immediately save it as a workspace file (File \u2192 Save Workspace As). I recommend saving it outside any repository; I keep mine as a sibling directory to my repos, like . Then, go to File \u2192 Add Folder to Workspace to add your first repository (my main tutorial project ), and repeat for your second repo (the companion library I was improving, ). You'll see both folders appear in the Explorer sidebar. Step 2: Watch the magic happen Here's where it gets interesting. Fire up Cursor's AI or GitHub Copilot and give it a specific prompt that references files across both repos. Try something like: \"Look at @llamabot/llamabot/bot/simplebot.py and edit @building-with-llms-made-simple/notebooks/03advancedbot.py to update the StructuredBot example for document summarization.\" (If you're using VSCode instead of Cursor, just replace the @ symbols with #.) Your agent can now see both codebases simultaneously. It understands how your LlamaBot library works and can create coherent examples in your tutorial repo, suggesting coordinated changes across repos while maintaining consistency between your library code and tutorial examples. Step 3: Reopening your workspace Next time you open Cursor or VSCode, you'll see your workspace listed on the welcome screen under \"Recent\". Click it to instantly load all your repositories with the same folder structure and settings. Quick tips that make this even better Keep workspaces outside repositories: I always save workspace files as siblings to my repo directories, never inside them. This prevents workspace files from accidentally getting committed and keeps things clean when you're working across multiple projects. Quick tip on scale: You can add as many repositories as you need. I've had workspaces with multiple model experiment repos, shared data utilities, and production pipelines, and Cursor's agent could reference files across all of them. When you use in Cursor, it considers every file in every repository. Fair warning though\u2014I recently worked across 5 repos at work and my head was spinning even with LLM help. Sometimes less is more. Be prescriptive with file references: Prompting across repos works best when you can pinpoint exactly which file to reference or edit. In Cursor, use syntax, while in VSCode it's . This helps the agent focus on the specific files you care about rather than wandering through your entire workspace. Use this pattern strategically: This approach shines when your current project depends on functionality that was developed beforehand in other repositories. Think data science projects that depend on internal tools built by other teams, or tutorial repositories that need to stay consistent with the underlying library they're demonstrating. When you have models or analyses that depend on utilities, libraries, or frameworks developed separately, workspaces let your coding agent understand both the dependency and the dependent code simultaneously. For single-repo exploratory work, stick to regular folders. That's it. Next time you're coordinating changes across multiple data science repositories, set up a workspace and let your coding agent see the full picture.",
    "tags": [
      "vscode",
      "workspaces",
      "coding",
      "agents",
      "llamabot",
      "tutorial",
      "context",
      "ai",
      "organization",
      "efficient"
    ],
    "pub_date": "2025-05-24",
    "type": "blog"
  },
  {
    "id": "blog-why-im-excited-for-scipy-2025",
    "url": "/blog/2025/5/8/why-im-excited-for-scipy-2025/",
    "title": "Why I'm excited for SciPy 2025!",
    "summary": "In this blog post, I share my excitement for SciPy 2025 in Tacoma, Seattle. The Pacific Northwest's beauty, the vibrant community of tool builders, and the chance to give back through financial aid make it special. I'm thrilled to teach tutorials on LLMs and network analysis, and I can't wait for the code sprints, where collaboration and learning thrive. If you're considering attending, the opportunity to meet tool makers and explore new libraries is unmatched. Will you join us in this unique experience?",
    "body": "SciPy 2025 is just around the corner, and it's one of my favorite conferences of the year. I wanted to share why I'm looking forward to going to year's event in Tacoma, Seattle. Back to the Pacific Northwest SciPy returns to Seattle this year, specifically Tacoma. As someone whose hometown is Victoria, BC, the Pacific Northwest holds a special place in my heart. The PNW in summer is absolutely gorgeous, and beyond the conference itself, the area offers incredible food options and beautiful scenery. Seattle has some of the best Thai food you'll find anywhere\u2014definitely worth exploring between sessions! Perfect for recharging during the conference. Also, I've got some of my favorite colleagues out there, and like last year, I hope to take the chance to visit them and build bridges across sites. The community of tool builders What makes SciPy special is the community. I'm a tool builder at heart, and there's nothing quite like mingling with the folks who create the scientific computing tools that power the Python ecosystem. These are the people whose libraries we import daily, and getting to network with them is invaluable. Giving back through financial aid I've been one of the organizers running the financial aid program since around 2015\u2014almost a decade now! This work is particularly meaningful to me because I was once a beneficiary of financial aid while in graduate school. Attending SciPy truly enabled my career, and I'm passionate about providing that same opportunity to others. My tutorials: LLMs and Network Analysis This year, I'm excited to be teaching not just one but two tutorials: Building with LLMs Made Simple (Day 1) - This new tutorial distills what I've learned over the past few years working with large language models both at Moderna and in my personal projects. We'll cover fundamental LLM interactions, extracting structured outputs from text, and if time permits, dive into retrieval-augmented generation (RAG). Rather than building ambitious systems, we'll focus on teaching practical fundamentals and exchanging ideas. Network Analysis Made Simple (Day 2) - I've taught this tutorial many times, sometimes solo and sometimes with collaborators like Mridul Seth. It focuses on thinking with graphs and applied network science. What I love about this subject is how versatile graph applications can be\u2014from simpler use cases where graphs serve as powerful data models to more complex implementations like graph neural networks. Code sprints: where the magic happens Finally, I'm looking forward to the code sprints\u2014two full days dedicated to contributing to the open-source tools that power our work. Every year I attend the sprints because they're simply fun! It's a great opportunity to give back to projects I use regularly and learn from other contributors. I've also written before why, if you're attending, code sprints are a worthwhile thing to attend -- check out the post here. If you're on the fence about attending SciPy 2025, I can't recommend it enough. Meeting the tool makers, learning about new libraries, and being part of this vibrant community is an experience unlike any other conference. Hope to see you there!",
    "tags": [
      "scipy",
      "tacoma",
      "seattle",
      "community",
      "tutorials",
      "llms",
      "networks",
      "sprints",
      "python",
      "open source"
    ],
    "pub_date": "2025-05-08",
    "type": "blog"
  },
  {
    "id": "blog-wow-modal",
    "url": "/blog/2025/4/26/wow-modal/",
    "title": "Wow, Modal!",
    "summary": "In this blog post, I share my experiences with Modal, a platform that simplifies cloud infrastructure. I discuss deploying web apps and hosting Ollama with low latency using Modal's cost-efficient, serverless solutions. I also explore creating a 'serverless workstation' with persistent storage and flexible resource allocation. Modal's intuitive volume management, unified abstractions, and easy debugging make it a standout choice for developers. Curious about how Modal can transform your cloud projects?",
    "body": "I've finally got back to playing with Modal, and I'm thoroughly impressed. Stuff you can do with Modal are a dime a dozen, and their docs are so good and chock full of real-life examples that I won't reiterate them here. I instead wanted to highlight some of the things I've tried that were useful to me. Deploy entire web apps on Modal My recent experiments with Modal are to use it as a PaaS, where I deploy whole apps onto Modal backed by a SQLite database on a Modal Volume. This turns out to be an uber cost-efficient way to deploy stuff -- fully serverless with no constitutively-running server. My previous stack was Docker + a self-hosted Dokku instance on DigitalOcean, and while I was able to get it running after a few tries, it was hard for me to nail down the patterns because of the sheer number of manual steps involved. With Modal, the simplicity of configuration-as-Python (for someone who is familiar with Python) coupled with very simple CI/CD configuration makes this super easy for solo builders to quickly ship deployments on every commit. To show an example, I hosted a quick PyPI server with Basic Auth on Modal through this git repository. If you study the repository, you'll also see the endpoints created for registering a user (using an authentication token that should only be known by administrators). Moreover, the CI/CD pipelines allow me to re-deploy the app fresh on each commit to . While I didn't necessarily put the highest degree of security on it, the deployed PyPI server can get the job done if one just needs to quickly stand up one for internal packages. Host Ollama with low latency One of the most exciting use cases I've found is deploying Ollama on Modal with remarkably low latency. After some experimentation with this repository, I was able to get a super responsive setup by using a Modal volume to host the model weights. The architecture is straightforward but powerful: 1. Create a Modal function dedicated to downloading and pre-caching LLMs on a Modal volume 2. Run an Ollama web endpoint on Modal that accesses these cached models 3. Benefit from Modal's GPU infrastructure without the overhead of spinning up your own instances What makes this particularly compelling is the cost efficiency. Rather than maintaining a dedicated GPU instance 24/7, Modal lets you pay only for what you use while still keeping your models readily available thanks to the persistent volume storage. Ollama's container also loads very quickly from the persistent volume, giving me at most seconds of wait time for larger models like Mistral Small 3.1 when I send an API call over the wire. To test this API, I did two experiments: (a) I connected to it using OpenWebUI (running locally, though in principle, this can be run on Modal as well), and (b) through LlamaBot. In both cases, all I needed to do was to provide (without a trailing !) as the API endpoint to hit, and it worked seamlessly: The entire setup requires surprisingly little code\u2014just define your volumes, create functions for model management, and set up the web endpoint. Modal handles all the complex infrastructure orchestration behind the scenes, letting you focus on actually using the models rather than maintaining them. My first contribution to the Modal examples gallery was this exact one! You can find it here. Spin up a \"serverless workstation\" I also thought of asking the question: can we create a serverless cloud-based workstation? If this term confuses you, let me explain. The heart of a workstation is nothing more than a persistent hard disk that stores user-level configuration inside their home directory. RAM, CPU, and GPU are fungible and swappable; what makes a \"workstation\" feel at home is the level of customization you can put into it to make it feel like your own machine. My gut told me that we should be able to do this as well on Modal, and the answer is yes! My implementation uses a Debian-based container with VSCode, Git, and both and pre-installed. The magic happens with three Modal volumes that persist between sessions: one for VSCode settings, another for server data, and a third for my code repositories. VSCode runs in \"tunnel\" mode, which means I can securely access my development environment from any device with a web browser. I can clone repositories to the persistent directory, install all my favorite extensions, and use Pixi to manage Python environments for different projects. To keep costs low, I configured the server to automatically time out after 5 minutes of inactivity. The flexibility to adjust RAM, CPU, and even GPU allocations based on what I'm working on has been incredibly useful\u2014lightweight coding sessions can use minimal resources, while data processing tasks can scale up as needed. I wouldn't generally recommend doing this because you already have access to interactive GPU compute via Modal functions, so in principle, it's easy for you to primarily work out of your laptop and burst to the cloud through modal Function",
    "tags": [
      "modal",
      "serverless",
      "pypi",
      "ollama",
      "gpu",
      "vscode",
      "storage",
      "cloud",
      "deployment",
      "cicd"
    ],
    "pub_date": "2025-04-26",
    "type": "blog"
  },
  {
    "id": "blog-good-practices-for-ai-assisted-development-from-a-live-protein-calculator-demo",
    "url": "/blog/2025/4/19/good-practices-for-ai-assisted-development-from-a-live-protein-calculator-demo/",
    "title": "Good practices for AI-assisted development from a live protein calculator demo",
    "summary": "In this blog post, I share my experience from a live coding demo at BioIT World 2025, where I built a protein mass spectrometry calculator tool. I emphasize the importance of standardization in data science, starting with a design document, and using AI assistance for rapid development. Despite a live demo hiccup, I showcased the tool's capabilities and highlighted key lessons in AI collaboration, such as the value of context and interactive communication. How can AI tools enhance your development process while maintaining human oversight and creativity?",
    "body": "In a previous blog post, I explored how standardization helps data science teams fly and actually unleashes creativity rather than constraining it. Here, I'd like to share my experience from a live coding demonstration I gave at BioIT World 2025 as part of the AI in Software Engineering track. What I built During my presentation, I built a protein mass spectrometry calculator tool live in front of the audience. This command-line tool mirrors the delivery model that I helped co-design at work, where we typically deliver our solutions as command-line tools. For this demonstration, I specifically chose to build a tool that simulates proteolytic digests and calculates mass spectra. The goal was to create a tool that could: - Accept a protein sequence input in various file formats - Simulate proteolytic digestion - Calculate what the mass spectra would look like - Display results as a plot or PNG file I wanted this tool to have three interfaces: 1. A Python library for notebook interaction 2. A command-line interface using the Python library 3. A web frontend backed by an API (though I didn't implement this during the demo) Starting with structure I began by scaffolding a new project repository using my personal open-source tool called [](). This automatically creates a standardized project structure - something I emphasized is critical for collaborative data science work. \"Standardization is really important for a data science team to fly,\" I explained. \"We've standardized our ways of working such that every project looks pretty much exactly the same.\" The key design principle here is consistency in interface while maintaining flexibility in implementation. As I noted during the presentation, \"the project looks the same, but underneath the hood, what methods you use, what plotting packages you like, what algorithms you choose, they don't matter. You do what you need.\" This standardization means that when someone else needs to jump in and help with a project, they can do so without wasting time figuring out how things are organized. The familiar structure lets them focus immediately on the substance of the work rather than deciphering a new system. Far from constraining creativity, this standardization enables it by removing unnecessary cognitive overhead. Data scientists can concentrate on solving problems instead of reinventing project structures and interfaces for each new initiative. Design first, code second Instead of diving straight into \"vibe coding\" (coding without clear direction), I started with a design document. I dictated my requirements to the AI assistant: - The functionality I needed - The three interfaces I wanted - The overall architecture The AI generated a comprehensive design document that included: - Project overview - Functional requirements - Architecture design - Key modules (parsers, digestion simulation, mass spec calculation, visualization) - Usage patterns As I reviewed this document, I noted areas where I lacked expertise: \"I don't have enough expertise to evaluate this. Got to talk with a biologist or mass spec expert.\" This self-awareness is crucial when working with AI tools to ensure that you don't end up building something with a fundamental logical error. Implementing with AI assistance After reviewing the design, I switched to using the AI agent mode to implement the library and command-line interface. The agent created: - Parsers for protein sequences - A digestion module with support for various enzymes - Mass spec calculation functionality - Visualization capabilities - A command-line interface What would have taken me about three days of work was accomplished in minutes. However, I emphasized that this wasn't the end of the process - I would normally spend at least an hour or two reviewing the code to ensure it was correct and identify knowledge gaps. The final product When I tried to demonstrate the tool's functionality, I ran into some issues - a common occurrence in live demos! In my case, it was a package dependency problem. As I joked to the audience, \"It is these moments like these that make me go, alright, I'm going to pull out the cooking show style thing and switch over to the pre-baked, already-working thing that I have prepared for everybody.\" This \"cooking show mode\" saved the demo, allowing me to showcase the working version I had prepared beforehand: - Calculating protein mass - Performing trypsin digestion - Outputting mass/charge values - Generating a visualization of the mass spectrum To the audience's applause, I concluded with the lessons learned. Key lessons learned Throughout this exercise, I noted several important principles for effective AI collaboration. Context is key - the more contextual information you provide to AI, the better its output. As Michael Smallegen had mentioned in his preceding lightning talk, trying to vibe code from scratch with minimal context would inevitably result in \"spaghetti\" code that's difficult to maintain. Even as da",
    "tags": [
      "standardization",
      "ai",
      "coding",
      "protein",
      "mass",
      "tools",
      "project",
      "collaboration",
      "development"
    ],
    "pub_date": "2025-04-19",
    "type": "blog"
  },
  {
    "id": "blog-wow-marimo",
    "url": "/blog/2025/4/8/wow-marimo/",
    "title": "Wow, Marimo!",
    "summary": "In this blog post, I share my experience with Marimo notebooks, highlighting their fully reactive nature and self-contained environments. I discuss how to run Marimo without installation using , and the benefits of AI-assisted coding. I also cover exporting notebooks to Markdown and deploying them as Modal apps. While Marimo's keybindings differ from Jupyter, its reactive execution and UI builder offer unique advantages. Curious about how Marimo can transform your coding workflow?",
    "body": "I have been using Marimo notebooks recently, and I'm thoroughly impressed. There are many benefits to using . The biggest are fully reactive notebooks, such that if you change a cell and execute it, all cells that depend on it will automatically re-execute, and self-contained notebook environments, so you never have to create a scratch environment to run a notebook. If you haven't started test-driving Marimo to see whether it works for you, I think it's time to start experimenting! Run Marimo... without ever installing it Thanks to , we can run without ever needing to explicitly install it. This is a major upgrade from installing it with / and having to remember where on my PATH environment variable is. No more asking the question, \"Did I install it in my base conda environment? Or was it installed in another env?\" To do this, with installed on your : The notebook will get created automatically if it doesn't exist. What I love about this is that I get to use the latest all the time. And I don't have to think about what version of I'm carrying around in my local machine. is providing the equivalent of a serverless API for CLI tools. Ensure notebooks carry their own environment Marimo notebooks can be self-contained, with Python dependencies fully-specified in-file with PEP723-compatible in-line script metadata. The way to ensure that this is done is by running exactly the command above, with being the key. Additionally, if you add packages to the self-contained notebook via UI, they automagically get added into the in-line script metadata. The resulting file looks something like this: Also, you can specify alternate sources in-line, with this example coming from my Network Analysis Made Simple repository: Install local package in editable mode You can interact with your local package in editable mode with Marimo notebooks. To do so you can add the package using the tab in the UI: It will be installed in editable mode, and it will be added to the in-line script metadata: Enable AI assistance with Marimo notebooks At this point, for anyone experienced enough in code writing, writing code with AI assistance is pretty much table stakes. (If you are just starting out, I would still encourage you to seek out a human mentor to teach you good patterns for writing reliable code!) Marimo has support for AI-assisted coding, and though it has some rough spots, I think it's worth taking a look at. To enable AI assistance, you'll need an API key for one of the major API providers (OpenAI, Anthropic, Google), and you can enable code completion using GitHub Copilot, Codeium, or Ollama (custom). That said, having gotten used to Cursor's more interactive style of coding assistance, I found Marimo's implementation of AI assistance to be a tad constraining. I can't do multi-cell edits, for example, and the connection to GitHub Copilot (for inline assistance) often shows an error connecting. My workaround for now has been to write a bunch of cells in Marimo, and then switch over to Cursor to directly edit the file (e.g. to condense it to be less verbose, or correct inconsistencies I might have accumulated). Run a marimo notebook directly from the web This is so, so, so, so powerful and convenient! Simply point to an existing URL that constitutes the notebook, and will clone it and run it within a sandboxed environment for you. There is also an option there to run it within a Dockerized container of its own too, so it'll be entirely fenced off from your system. Export Marimo notebooks as Markdown, Jupyter notebook-style When I wrote my blog post on the use of the R2D2 prior and Bayesian probability of superiority calculation, I used alongside to write the post with prose alongside code. One thing I wanted to do was to export it as a Markdown file with cell execution outputs, but Marimo's markdown exports don't carry the capability to do so natively. Pre-requisites: 1. Make sure that your notebooks contain as part of its PEP723-style dependency declaration, and 2. Make sure you have installed on your system. Firstly, export Marimo notebook while including outputs to a Jupyter notebook: Secondly, export the Jupyter notebook to Markdown: And in this way, your notebook will be exportable to Markdown with outputs from the notebook cells. It took me a little while to figure this out, but now that I did, I'm glad I have it as an option, as I can now write entire notebooks in Marimo notebooks and have them exportable to my blog or eBooks. Serve a marimo notebook as a Modal app As it turns out, with Modal's ability to serve up any arbitrary web server, we can deploy Marimo notebooks to Modal easily. Given a Marimo notebook that looks like this: And a Modal deployment script that looks like this: One can iterate quickly on the notebook in Marimo, and then once you're ready, check that the deployment works (for fast iteration): And to do the final deployment (best done via GitHub CI/CD): This gives you an outlet to deploy Marimo apps in a c",
    "tags": [
      "marimo",
      "reactive notebooks",
      "uv",
      "deployment",
      "serverless",
      "data science",
      "modal"
    ],
    "pub_date": "2025-04-08",
    "type": "blog"
  },
  {
    "id": "blog-from-data-chaos-to-statistical-clarity-a-laboratory-transformation-story",
    "url": "/blog/2025/4/5/from-data-chaos-to-statistical-clarity-a-laboratory-transformation-story/",
    "title": "From data chaos to statistical clarity: A laboratory transformation story",
    "summary": "A high-throughput screening lab transforms their data analysis workflow by applying statistical thinking from the start. By combining robust estimation with R2D2 priors, they eliminate tedious manual data cleaning, automatically handle outliers, decompose sources of variation, and objectively measure both statistical estimation model and laboratory quality performance. This story demonstrates how thoughtful experimental design paired with principled statistical methods can dramatically improve both efficiency and scientific quality. How might statistical thinking transform your experimental workflow?",
    "body": "The Breaking Point Dr. Chen's team was drowning in spreadsheets. Again. \"I don't understand why these position effects keep showing up,\" muttered Li, the senior scientist, as she manually flagged another set of outlier wells on their latest assay. \"We're spending more time cleaning data than running experiments.\" Their high-throughput screening lab had followed the same workflow for years: carefully plan the biological aspects, run plate-based assays, then spend days wrestling with the resulting data before any actual analysis could begin. \"Remember when that vendor rep insisted we put all our controls in column 1?\" remarked Jamal, gesturing to their plate layout diagram. \"Said their software couldn't handle distributed controls?\" The team nodded grimly. That single decision had cascaded into countless hours of manual data correction as edge effects and position-dependent variations routinely skewed their results. Their analysis scripts had evolved into a jungle of if-else statements attempting to address every new edge case. When they approached the research digital team about automating their analysis pipeline, the response was disheartening but predictable. \"This can't be automated reliably,\" Maya, the team's embedded data scientist from the research digital team, explained. \"There are too many subjective decisions buried in your process. We'd just be encoding your workarounds, not solving the actual problems.\" The Turning Point The breakthrough came during a seminar on Bayesian statistics. Dr. Chen realized their problem wasn't poor analysis\u2014it was addressing experimental design flaws after data collection instead of preventing them from the start. \"What if we completely reimagined our workflow?\" he proposed at the next team meeting. \"Rather than retrofitting solutions to poorly designed experiments, we could address these problems before they occur.\" The team was skeptical. Their current approach, despite its flaws, was familiar. But the hours spent manually curating data had become unsustainable, and they knew something had to change. Dr. Chen decided to reach out to Maya again. \"I know you said our current workflow can't be reliably automated,\" he explained, \"but we're ready to rebuild from the ground up. Would you consider working more closely with us?\" Maya hesitated. \"What's different this time?\" \"We're not trying to automate our existing process anymore,\" Dr. Chen explained. \"We want to redesign our experiments with statistical principles from the start.\" Maya's expression changed. As a data scientist, she'd spent years trying to salvage analyses from poorly designed experiments. The opportunity to shape the experimental design itself was rare. \"That changes everything,\" she said, nodding slowly. \"If you're willing to rethink your plate layouts and measurement protocols based on statistical considerations... I'm in.\" Maya agreed, intrigued by the opportunity to apply statistical principles from the beginning rather than patching an existing system. Rebuilding From First Principles Their transformation began with mapping the sources of variation in their experiments: First, they tackled the position effects problem. Against the vendor's recommendations, they designed a distributed control system, placing reference samples strategically throughout each plate rather than restricting them to dedicated columns. \"But the software can't handle this layout,\" protested Li, recalling the vendor's warning. \"Then we'll write our own analysis scripts,\" Dr. Chen countered. \"One that treats position effects as systematically measurable phenomena rather than nuisance variables.\" They ran several plates with only control samples to create a detailed map of well position biases. The results were striking. \"Look at this heatmap,\" Maya pointed out during their analysis review. \"It's not just the column 1 effect we've been fighting. The entire plate edge shows elevated values compared to the center wells.\" \"The edge wells are consistently showing higher values,\" Dr. Chen observed, tracing the dark purple regions along the perimeter of the plate. \"And it's not uniform\u2014some corners are worse than others.\" This visualization became the foundation for a hierarchical Bayesian model that could account for position-specific variation while maximizing plate space for test samples. \"Now we can quantify and correct for these position effects systematically,\" Maya explained. \"Rather than manually flagging outliers, we're modeling the actual physical phenomena causing the variation.\" Next, they addressed their outlier problem. Instead of manually flagging extreme values\u2014a process fraught with subjectivity\u2014they needed a more principled approach. \"I've been reading about robust statistical methods,\" Jamal shared during a team meeting. \"What if we use t-distributed rather than Gaussian residuals in our model?\" He pulled up a figure from a Bayesian statistics handbook. \"Look at this comparison between standard Gaussian regression an",
    "tags": [
      "automation",
      "bayesian estimation",
      "robust estimation",
      "r2d2",
      "experiment design"
    ],
    "pub_date": "2025-04-05",
    "type": "blog"
  },
  {
    "id": "blog-bayesian-superiority-estimation-with-r2d2-priors-a-practical-guide-for-protein-screening",
    "url": "/blog/2025/4/3/bayesian-superiority-estimation-with-r2d2-priors-a-practical-guide-for-protein-screening/",
    "title": "Bayesian Superiority Estimation with R2D2 Priors: A Practical Guide for Protein Screening",
    "summary": "In this blog post, I explore how to tackle experimental noise and candidate ranking in protein screening using Bayesian methods. By employing R2D2 priors, we can decompose variance into interpretable components, helping us understand the true biological signal versus experimental artifacts. Additionally, Bayesian superiority calculation allows us to quantify the probability that one protein outperforms another, providing a more robust comparison than traditional methods. These techniques are not only applicable to protein screening but also to drug discovery, materials science, and more. Are you ready to enhance your experimental insights with Bayesian logic?",
    "body": "Recently, I've been thinking and writing a lot about statistics. It's because good statistical practice is both under-rated, under-taught, and under-valued amongst machine learning practitioners and laboratory scientists, and yet it underpins the ability of machine learning practitioners in life sciences to build high performing machine learning models that accelerate decisions in the lab. It also enables laboratory scientists to design experiments that yield interpretable, and actionable results. My prior experience doing the full spectrum of laboratory and computational science is one of the reasons why it pains me to see potentially good data go to waste due to poor experimental design and statistical analysis. Without good statistical practice underlying the data generating process -- and by that I mean good experimental design, and explicit quantification of uncertainty -- all ML models become equal: equally bad. In this blog post, I want to show you how to use Bayesian methods to tackle two critical questions: 1. Is our experimental setup actually measuring the effect we care about (vs. experimental noise)? 2. Which candidates are truly superior to others? As always, I go back to my favorite example: screening proteins. But as you read, note the generalities: they aren't specific to protein screening at all! Note: The original notebook can be found here. If there are discrepancies between my blog post and the original marimo notebook, I note that the original notebook is correct. To run it, download it, and then execute: When screening hundreds of molecules, proteins, or interventions, two critical questions often arise: 1. Is our experimental setup actually measuring the effect we care about (vs. experimental noise)? 2. Which candidates are truly superior to others? In this tutorial, we'll tackle both challenges using a practical example: a protein screening experiment with fluorescence readouts. We'll show how R2D2 priors help interpret variance decomposition, and how Bayesian superiority calculation enables robust ranking of candidates. Both techniques generalize to drug discovery, material science, or any domain requiring rigorous comparison of multiple alternatives. The Protein Screening Example We'll use a dataset with fluorescence measurements for over 100 proteins across multiple experiments and replicates, with an experimental design that includes: - A control protein present in all experiments and replicates - \"Crossover\" proteins measured across all experiments - Unique test proteins in each experiment This design is common in high-throughput screening where measuring all proteins in all conditions is impractical. We'll implement our analysis using PyMC, a powerful probabilistic programming framework for Bayesian modeling. Generating Synthetic Data To demonstrate our approach, we'll generate synthetic data that mimics a realistic protein screening experiment with: - 3 experiments with 2 replicates each - A control protein and crossover proteins present in all experiments - Test proteins unique to each experiment - Systematic experiment effects (batch effects) - Replicate-level variation - Measurement noise .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Experiment Replicate Protein Fluorescence 0 Exp1 Rep1 Control 1087.476281 1 Exp1 Rep1 Crossover0 1054.176224 2 Exp1 Rep1 Crossover1 955.647739 3 Exp1 Rep1 Crossover2 1091.751188 4 Exp1 Rep1 Crossover3 1189.344109 ... ... ... ... ... 205 Exp3 Rep2 Protein85 1765.149428 206 Exp3 Rep2 Protein86 1595.422298 207 Exp3 Rep2 Protein87 1889.585595 208 Exp3 Rep2 Protein88 1394.395041 209 Exp3 Rep2 Protein89 1411.831297 210 rows \u00d7 4 columns In the synthetic dataset we've created, we simulated: - 3 experiments with 2 replicates each - A control protein and 4 crossover proteins present in all experiments - 100 other proteins distributed across experiments - Multiplicative experiment effects (mean=1, sd=0.3) - Multiplicative replicate effects (mean=1, sd=0.1) - Multiplicative measurement noise (mean=1, sd=0.05) This structure simulates a typical screening setup where batch effects between experiments are stronger than replicate variation, and both contribute significantly to the observed fluorescence values. The setting mirrors real experimental challenges where we need to separate biological signal from technical noise. Examining the Raw Data Before modeling, let's visualize the control and crossover proteins to understand experimental variation: Notice the dramatic shift in fluorescence values across experiments. Experiment 3 shows substantially higher fluorescence readings (around 1500-2000 units) compared to Experiments 1 and 2 (mostly below 1300 units). This pattern illustrates a common challenge in high-throughput screening: significant batch effects between experiments that can mask the true biological signal. Without accounting for these expe",
    "tags": [
      "bayesian",
      "r2d2",
      "variance modelling",
      "fluorescence",
      "experimental design",
      "probability of superiority",
      "probabilistic modelling",
      "data science",
      "statistics"
    ],
    "pub_date": "2025-04-03",
    "type": "blog"
  },
  {
    "id": "blog-how-to-standardize-data-science-ways-of-working-to-unlock-your-teams-creativity",
    "url": "/blog/2025/4/2/how-to-standardize-data-science-ways-of-working-to-unlock-your-teams-creativity/",
    "title": "How to standardize Data Science ways of working to unlock your team's creativity",
    "summary": "In this blog post, I share insights from my talk at BioIT World about leading one of Moderna's data science teams. I discuss our mission to make science run at the speed of thought and how we standardize workflows across the data scientists to enhance creativity. Key points include designing delivery models, making best practices easy, and balancing standards with innovation. I also touch on AI-assisted coding and our open-source infrastructure. Our approach aims to liberate scientists for their best work. Curious about how we achieve this balance and what it means for the future of data science at Moderna?",
    "body": "On 2 April 2025, I had the opportunity to speak at BioIT World about my experiences building data science teams and the associated tooling at Moderna. Unlike most speakers, I ditched the slides and went for a more interactive approach \u2013 that's just my style. Why subject everyone to 25 minutes of me blabbering when we could have an engaging discussion instead? For those who couldn't attend, I wanted to share the key insights from my talk on how we've made our data science team fly at Moderna. These lessons come from my experience as a data science team lead, where I manage a team of six (including myself), serving Moderna's 600-person research organization, and where we share tools with other data science teams totaling 12, serving a wide remit across Moderna\u2019s ~6000-person organization. The Mission First, a bit about us: my home team's mission is to \"make science run at the speed of thought and to quantify the unquantified.\" I joined Moderna in summer 2021 \u2013 too late to profit from the pandemic, so I'm really there for the science, not the money! Part of making science run at the speed of thought is to make sure we data scientists have the tools and practices at hand that enable us to work at the speed of thought. That is where standardization comes in. Talk Overview In my talk, I focused on three key aspects of standardizing data science workflows: 1. How to design delivery models that serve stakeholder, customer, and/or collaborator needs 2. Ways to make best practices the path of least resistance 3. Strategies for implementing standards on the path of least resistance I shared concrete examples from my experience at Moderna, but also encouraged the audience to discuss these topics with their neighbors so we could learn from the collective wisdom in the room. My goal was for everyone to leave with practical ideas they could implement right away\u2014regardless of their team or org size. I will admit, the room was relatively quiet at first (I think most people came not expecting my talk delivery format), but gradually the room warmed up. 1. Standardized Delivery Models The first thing to address is the question: what exactly does your data science team deliver? And I don't mean \"insights\" \u2013 that's not a specific enough answer. I'm talking about concrete work products. Something tangible that people can interact with or mull over. Before choosing delivery formats, it's essential to understand the \"jobs to be done\" (from Clayton Christensen's Innovator's Solution) \u2013 what are stakeholders actually trying to accomplish with your outputs? Delivery models should be tailored to what your company actually needs, not just what's trendy. At Moderna, we deliver two things as a data science team: - Python packages: Reusable components that encapsulate data science work that other computational and data scientists can reuse. - Compute tasks: CLI tools that serve as a wrapper for computational workflows, which are deployed on the cloud and accessible via web UI, RESTful API, and Python client. These serve both technical and non-technical users. We made this decision early on to avoid building things like dashboards, which in my opinion are where data science projects go to die. Dashboards should be built by the people who actually want to see the data. I've also been in the position of promising a UI and suffering the maintenance costs later. It was not a happy time. So we cut out building any kind of user interface from our work products, preferring to leave this to professional front-end engineers. Yet at the same time, there is a strong \"engineering\" component to our work, because we want to reduce the friction that occurs from handover. I've heard stories in finance of data science and quant teams completing their prototype in a notebook within a month, but the ML engineering team needing 8 months to productionize it because the deployment language was different (Java, not Python), the runtime environment was different, and the engineering team was not intimately familiar with the problem domain, and so they were delayed on the appropriate suite of tests to write. So at Moderna, every data scientist must understand that our return on investment is only realized if we package our work as software, and we avoid these problems associated with handoff by ensuring that we have the tools to make deployment easy. Moreover, I don't care if you know how to build a fancy Transformer ML model in a Jupyter notebook. If your work cannot be operationalized hands-free without your involvement, you have not delivered a return on investment on your time. Software is how we scale labour! The benefits? Consistency, reusability, and very clear paths from exploration to production. Expectations with stakeholders are crystal clear, yet what we deliver is flexible enough to work with a variety of collaborators. 2. Making Best Practices the Path of Least Resistance Consistent organization and automation reduces cognitive load and frees up menta",
    "tags": [
      "data science",
      "leadership",
      "workflows",
      "best practices",
      "software",
      "change management",
      "innovation",
      "ai",
      "open source"
    ],
    "pub_date": "2025-04-02",
    "type": "blog"
  },
  {
    "id": "blog-why-you-should-take-part-in-the-scipy-sprints",
    "url": "/blog/2025/3/17/why-you-should-take-part-in-the-scipy-sprints/",
    "title": "Why you should take part in the SciPy sprints!",
    "summary": "In this blog post, I share my transformative experience participating in the SciPy sprints, where I made my first open source contribution to Matplotlib. Through this journey, I gained confidence in git, improved my software skills, and learned the importance of effective communication with maintainers. Sprints offer invaluable networking opportunities and skill development, especially for students and data scientists. By contributing, you can make a lasting impact on projects you care about. Are you ready to join a sprint and potentially change the trajectory of your career? Read on to find out more!",
    "body": "If you're a newcomer to the scientific Python open source community, hear me out: I would like to share with you why participating in sprints can be incredibly valuable, beneficial, and possibly even transformative for you! My first open source contribution was with the Matplotlib project at the SciPy 2015 sprints. I was assigned what seemed like a simple task: replace all PyLab imports in the examples gallery with proper import statements. Instead of this: The examples would now have this: And I would make sure that each example that I changed ran correctly. Manually. By hand. One-by-one. Painstaking done for posterity. What started as a straightforward assignment turned into 70 pull requests over three months. But here's the thing - I learned so much during that process! First off, I got so many reps with that by the end, I finally had the confidence to do -based software development on my own. When people say repetitions are the key to mastery, this rang absolutely true for me. My software skills improved dramatically. I witnessed firsthand how CI/CD was used to automatically check code, which I later applied to my own open source projects. I also observed how people collaborate both in-person and remotely. This became invaluable when I later worked at Novartis, where half my time was spent working remotely with teams in Basel while I was in Cambridge. I was used to asynchronous communication. And because there was always the risk of being blocked on one thing and not knowing how to proceed for a few days, I got used to juggling multiple projects in parallel so that I would always have something moving forward. Perhaps most importantly, I got to meet Matplotlib maintainers! Through those interactions, I learned how to communicate with them - understanding what information they needed whenever I reported an issue and how to provide minimal reproducible examples. The face time with them and our consistent interactions through those pull requests built trust, making it easier to approach maintainers when I needed help. And a side effect is that as my time in the open source world grew, I could build that same trust with other open source project maintainers. This way of working with maintainers and understanding their perspective has helped me tremendously throughout my career. It's also been a great networking opportunity and an excellent way to learn new skills - especially for data scientists who can benefit significantly from stronger software development skills. For students especially (as I was back in 2015), sprints provide an invaluable opportunity to connect with the people developing the tools that you may rely on. You get to become more familiar with Python packages you might use in your day job while simultaneously giving back to the codebases you care about. If you're looking to make a permanent impact, contributing to software is a great way to do it. My impact on Matplotlib was helping people understand they shouldn't use PyLab by eliminating PyLab examples, reducing support questions for the developers. Here's my encouragement to you: book an extra day or two at your hotel, delay your return flight, and join the sprints. Network, write some code, and have fun. It could be an experience that absolutely changes the arc of your career trajectory!",
    "tags": [
      "open source",
      "python",
      "sprints",
      "matplotlib",
      "git",
      "networking",
      "career",
      "community",
      "skills",
      "development"
    ],
    "pub_date": "2025-03-17",
    "type": "blog"
  },
  {
    "id": "blog-the-art-of-finesse-as-a-data-scientist",
    "url": "/blog/2025/3/16/the-art-of-finesse-as-a-data-scientist/",
    "title": "The art of finesse as a data scientist",
    "summary": "Finesse in data science is the subtle skill that distinguishes exceptional practitioners from the merely competent. It involves recognizing when you're stuck in technical rabbit holes, creating tangible progress markers for stakeholders, working backwards from meaningful milestones, adapting with purpose when approaches aren't working, creatively overcoming technical roadblocks, and cultivating a network for timely assistance. These skills help data scientists navigate complex challenges while consistently delivering value, balancing persistence with adaptability, technical depth with clear communication, and planning with flexibility. How might developing your finesse transform your effectiveness as a data scientist?",
    "body": "I was reading Wes Kao's newsletter recently when she brought up this concept of \"finesse.\" This idea immediately clicked for me. In data science, finesse distinguishes exceptional practitioners from the merely competent. It's the subtle skill that allows data scientists to navigate complex challenges while consistently delivering value. What finesse looks like in practice Finesse in data science manifests in several interconnected ways. At its core, it's about making smart decisions about where to focus your efforts, communicating progress effectively, and maintaining momentum even when faced with obstacles. Let me walk through how this plays out in real work situations. Recognizing and escaping rabbit holes Every data scientist has experienced it\u2014what should have been a quick debugging task somehow consumes your entire week. You find yourself four days deep in a technical rabbit hole with nothing tangible to show for it. Finesse means developing the self-awareness to recognize when you're stuck and the judgment to adjust course. Instead of persisting with diminishing returns, ask yourself: \"Is this the best use of my time right now? What alternative approaches might get me to a useful result faster?\" For example, when working on prioritizing protein mutations with a computation taking far longer than expected, finesse guides you to develop a parallel approach. Rather than just waiting, you tell collaborators: \"While my comprehensive analysis runs in the background, here's an interim solution to review. Can you identify any concerns with these initial results?\" This strategic pivot keeps the project moving forward while still pursuing the optimal solution. You've created a path to progress rather than allowing a technical challenge to become a complete roadblock. Creating tangible progress markers The most visible demonstration of finesse is consistently showing progress through concrete evidence. Your stakeholders need to see the work advancing through tangible artifacts: - Visualizations that highlight initial insights - Written summaries documenting your current understanding - Regularly updated documentation of your approach - Working prototypes, even if they represent simplified solutions Your collaborators aren't simply curious\u2014they're accountable to others and depend on your work to meet their own commitments. They need evidence of progress they can share with their stakeholders. Consider your manager specifically. They directly justify the resources allocated to your work. As one of the most expensive investments in the company, you need to demonstrate continuous value creation. Without visible progress markers, this becomes nearly impossible, putting both your manager and your project at risk. Working backwards from meaningful milestones A powerful technique for maintaining momentum is identifying meaningful intermediate milestones and working backwards from them. Ask yourself: \"What halfway point would give my stakeholders confidence we're on the right track?\" This approach forces you to break complex problems into demonstrable chunks. Each milestone provides an opportunity to gather feedback, validate assumptions, and adjust course if necessary. Rather than waiting until you have the perfect model, share preliminary results that show promise. Validate your data processing pipeline before completing the full analysis. These intermediate deliverables build credibility and trust with your team while ensuring you're headed in the right direction. Adapting with purpose and justification Finesse requires purposeful adaptability. When evidence suggests your current approach isn't optimal, make calculated adjustments while keeping your ultimate objective in focus. These adaptations must be defensible and based on what you've learned. When changing course, clearly articulate: 1. What you've discovered that prompted this change 2. Why the new direction is more promising 3. How it still connects to your ultimate goal This balance between persistence and flexibility demonstrates maturity and judgment. You're neither rigidly sticking to a failing approach nor capriciously jumping between methods without justification. Overcoming technical roadblocks creatively Technical limitations test your finesse perhaps more than anything else. When your stack is outdated or your environment creates blockers, finding creative workarounds becomes essential. Apply finesse by temporarily using alternative tools, simplifying your approach to work within current constraints, or creating mock-ups of what would be possible once technical issues are resolved. The goal is to ensure technical challenges don't halt all progress, even if you can't completely solve them immediately. This creative problem-solving shows initiative and demonstrates your commitment to delivering value despite obstacles. It builds confidence with stakeholders who see your ability to navigate complex technical environments. Cultivating a network for timely ",
    "tags": [
      "data science",
      "finesse",
      "productivity",
      "career",
      "communication",
      "leadership",
      "strategy",
      "problem solving",
      "technical skill",
      "professional development",
      "stakeholder management",
      "project management",
      "collaboration",
      "adaptability",
      "work effectiveness"
    ],
    "pub_date": "2025-03-16",
    "type": "blog"
  },
  {
    "id": "blog-a-blueprint-for-data-driven-molecule-engineering",
    "url": "/blog/2025/3/6/a-blueprint-for-data-driven-molecule-engineering/",
    "title": "A blueprint for data-driven molecule engineering",
    "summary": "In this blog post, I explore how cross-functional teams in biotech can accelerate molecule discovery using a strategic playbook. Through the story of a fictitious biotech, Catalyst Therapeutics, I highlight the importance of robust experimental design, integrating data science with human intuition, and balancing computational methods with practical insights. The team's journey reveals how better experiments lead to better models and ultimately, better molecules. Are you ready to discover how these principles can transform your biotech projects?",
    "body": "Recently, I've been writing about my thoughts on data science in the biotech world, especially for those who are molecule hunters. You can find some of those posts below: - Reliable biological data requires physical quantities, not statistical artifacts - Why data from preclinical biotech lab experiments make machine learning challenging - A modest proposal for data catalogues at biotechs - The Human Dimension to Clean, Distributable, and Documented Data Science Code - Keys to effective collaborative data science This post kicks off a series on how cross-functional molecule design teams can achieve operational speed and efficiency. Through the lens of a fictitious startup, Catalyst Therapeutics (any similarities to real companies are coincidental), I'll share insights drawn from patterns and challenges that I have seen and heard. This series represents a strategic playbook I've developed to accelerate molecule discovery to what I call \"the speed of thought.\" Here's my goal: I want to illustrate that none of this has to be fancy. It just has to be a stable enough crank to be turned into a flywheel. Throughout this series, I'll address the crucial aspects every biotech data science team must master: data capture and engineering, with statistical modeling at its core, supercharged with machine learning and proper experimental design. Catalyst's mission\u2014developing novel protein binders for an oncology target that has eluded conventional methods\u2014serves as the perfect vehicle to demonstrate the principles in this playbook. If your interest is piqued, read on, and I hope you enjoy it :). The Catalyst Therapeutics story: designing a novel protein binder When I first walked into their lab, I met the core team who would become the heroes of this story: Maya, the protein engineer with a decade of wet lab experience and a healthy skepticism about computational methods. \"Models are nice,\" she often said, \"but proteins don't read papers.\" Dev, the junior scientist fresh from a top-tier PhD program, eager to apply cutting-edge techniques but still learning the gap between academic benchmarks and real-world biology. Sophie, the pragmatic data scientist with stints at three different biotechs\u2014one acquired, one failed, and one struggling with scaling challenges. As she told me, \"I've seen enough ways this can go wrong across different contexts. I'm here to avoid repeating those mistakes.\" This team was about to embark on a campaign to develop a novel protein scaffold that could bind to their target with high specificity and favorable biophysical properties. What made their approach work\u2014when so many similar efforts fail\u2014was their integrated approach to experimental design from day one. First Meeting: Setting the Foundation I sat in on their kickoff meeting, where Sophie immediately steered the conversation away from fancy algorithms. \"Before we talk about deep learning or any computational methods,\" she said, \"let's map out what we're measuring and how we'll account for variables that have nothing to do with our protein sequences.\" Maya nodded enthusiastically. \"In my last role, we spent six months optimizing the wrong property because our assay was actually measuring something else entirely.\" They spent the entire first day diagramming their experimental workflow on a whiteboard, identifying every potential confounder: \"If we use different plates on different days, we need to track that,\" Sophie noted, creating a column in her spreadsheet. \"The position in the plate matters too,\" Dev added. \"Edge wells behave differently due to evaporation.\" \"And don't forget who's running the experiment,\" Maya said. \"Different hands, different results.\" What struck me was how Sophie approached this not as a statistics problem but as a practical data collection issue. She wasn't trying to impress anyone with complex experimental designs\u2014she was making sure their foundation was solid. \"Every variable we don't account for now becomes noise that we can't remove later,\" she explained. \"And noise puts a hard ceiling on what our models can achieve.\" The experimental design takes shape Over the next week, the team developed an experimental plan: - Every plate would include control proteins in the same positions\u2014a practical compromise since truly randomized placement would be statistically ideal but experimentally infeasible - Each experimental run would include samples from previous runs to detect day-to-day drift - They included standardized control samples that both Maya and Dev would process independently once a week to quantify operator effects, also taking advantage of these controls to measure well position effects - They created detailed metadata sheets that captured everything from reagent lot numbers to ambient temperature \"Slow is smooth, and smooth is fast,\" Maya said when Dev worried about the extra controls. \"Taking time to set this up properly now will save us months of troubleshooting later.\" When I asked Dev if this felt like overk",
    "tags": [
      "data science",
      "biotech",
      "molecule discovery",
      "experiment design",
      "machine learning",
      "protein engineering"
    ],
    "pub_date": "2025-03-06",
    "type": "blog"
  },
  {
    "id": "blog-how-to-fix-pypi-upload-errors-related-to-license-metadata",
    "url": "/blog/2025/3/1/how-to-fix-pypi-upload-errors-related-to-license-metadata/",
    "title": "How to fix PyPI upload errors related to license metadata",
    "summary": "Encountering a PyPI upload error related to license metadata? The solution is straightforward - switch from setuptools to Hatchling as your build backend. In this post, I walk through how to fix the \"license-file introduced in metadata version 2.4\" error by updating your pyproject.toml configuration. Along the way, I learned some new things, including the fact that modern build backends like Hatchling provide better support for PEP 621 metadata features compared to older tools like setuptools.",
    "body": "Background I use a GitHub Actions workflow to automate the release process for my Python packages. The workflow was designed to: 1. Bump the version number 2. Automatically generate release notes using LlamaBot 3. Build the package using , a blazing-fast Python package installer and resolver 4. Upload to PyPI 5. Create a GitHub release I chose for my CI/CD pipelines because it's significantly faster than pip and provides more reliable dependency resolution. However, even with 's modern tooling, I encountered this error: The problem The error occurred because I was using setuptools as my build backend, which has less mature support for newer PEP 621 metadata features like the field with a specification. This led me down a path of discovery about modern Python packaging tools. Even though I was using for package installation, the underlying build system still needed modernization. The solution: Switch to Hatchling Pre-requisites - A Python package with a file - GitHub Actions workflow for package publishing Steps 1. Update your build system specification from setuptools to Hatchling: 2. Keep your existing license specification: That's it! The error should be resolved because Hatchling has better support for modern packaging metadata. Explanation The Python packaging ecosystem has been evolving rapidly. Newer tools like Hatchling are built from the ground up to support modern packaging standards and provide better defaults. The error I encountered was just one symptom of using an older tool (setuptools) that's still catching up with newer metadata specifications. References - Why Hatch? - Python Packaging User Guide - Python Discourse Discussion on Build Backends",
    "tags": [
      "python",
      "packaging",
      "pypi",
      "hatchling",
      "setuptools",
      "build-backend",
      "metadata",
      "license",
      "github actions",
      "workflow",
      "automation",
      "deployment",
      "error handling"
    ],
    "pub_date": "2025-03-01",
    "type": "blog"
  },
  {
    "id": "blog-reliable-biological-data-requires-physical-quantities-not-statistical-artifacts",
    "url": "/blog/2025/2/23/reliable-biological-data-requires-physical-quantities-not-statistical-artifacts/",
    "title": "Reliable biological data requires physical quantities, not statistical artifacts",
    "summary": "When building machine learning models in biology, we often encounter data that's been heavily processed with statistical transformations like p-values and normalizations. This essay argues that this practice fundamentally undermines our ability to build reliable models and maintain interpretable datasets. Through a real-world example of protein binding experiments, it demonstrates why collecting physical quantities (like binding strength in nM) with proper replicates is vastly superior to statistical artifacts, and how Bayesian estimation can help us properly handle experimental variation while maintaining physical units. Are you tired of wrestling with hard-to-interpret biological data and ready to build more reliable experimental pipelines?",
    "body": "In my work with building machine learning models in the life sciences, I've noticed a recurring pattern that needs addressing. We might try to use sequence to predict a quantity that is normalized against a control (that might then be normalized again against some baseline), and we might be asked to make our model predict a statistical quantity that is an artifact of our experimental noise and has nothing to do with sequence. I argue that this is fundamentally flawed. Let me explain by highlighting a prototypical example. A prototypical example A team is looking at developing novel non-antibody binders using modern protein design tools (such as ProteinMPNN and RFDiffusion). It being a new startup, at the behest of company management, the laboratory scientists design the experiment such that only controls are replicated across plates, while test proteins were not -- all to save on cost and iteration time. As such, we only have a single value readout per test protein. To make matters worse, this being a novel non-antibody binder, we have no non-antibody binders to act as positive controls, and can only look at a positive control antibody instead. To get around this problem, the data scientist, who was not involved in the experiment design but was involved in analysis of the data, decides to pool the negative controls, estimate a distribution for the control, and calculate a one-sided p-value of the test protein against the pooled control distributions. He does the same for the positive controls. The more extreme the p-value of the test protein against the negative controls, the better the protein is. The two numbers that get recorded for data archival purposes is something like this: | Sequence | Neg control p-value | Pos control p-value | | ---------------------- | ------------------- | ------------------- | | MKLLTVFLGLLLLWPGAQS... | 0.003 | 0.891 | | DVQLVESGGGLVQPGGSL... | 0.008 | 0.756 | | EVQLLESGGGLVKPGGSL... | 0.001 | 0.945 | | QVQLQESGPGLVKPSQTL... | 0.015 | 0.682 | | MALWMRLLPLLALLALWG... | 0.042 | 0.523 | Single replicate binding measurements were deemed not sufficiently reliable to record as archival data, and as such, but the massaged version of that data (in the form of p-values) gave 1-3 hits, so the team felt confident in the decision-making process and went ahead with this. The data scientists decided to train a machine learning model to predict two p-values, and use those p-values to prioritize new non-antibody designs. Whoa, hold your horses here... (If you're looking at the state of affairs I just described and are puzzled, confused, and perhaps even incredulous at why this is even happening, just know that you're not alone -- it's why I'm writing this down. Bear with me!) Let's think carefully about the long-term state of this data. It being the company crown jewels, we need to make sure it has the following qualities: - It's easily interpretable: as few pieces of experiment-specific background knowledge are necessary to understand - It is comparable with other datasets with as few caveats as possible. - It is easy to document and justify the design choices in the data that are present. I would like to argue that the team collectively failed the data scientist and circumstances forced him into poor choices. What's wrong with p-values as archival data? A LOT! p-values are a statistical artifact, a property of the noise of your measurement system. If you store a p-value (as the estimated quantity) next to a sequence in a table, the next unsuspecting data scientist and member of leadership looking at the table will think that sequence can be used to predict p-value, and that is completely illogical! Here's why: The p-value depends on the noise of the measurement system in the negative controls and positive controls. That noise level can evolve over time as laboratory experimenters get better and better at ferreting out sources of noise in their system. If the noise of the system gets progressively reduced, the controls will have tighter bounds. In turn, your non-control measurements may appear to have a smaller and smaller p-value, thereby giving the illusion that you are gaining more and more hits, when in fact all you've done is just made your control distribution tighter. Moreover, storing of p-values as archival data fails the test of easy interpretability and comparability. Proper data archival requires three key pieces of context: 1. The exact experimental protocol used (which can change over time) 2. The statistical analysis method used to process the raw data 3. The identity of controls used for comparison Without versioning both protocols and analysis code, and without explicitly storing control identities, data becomes increasingly difficult to interpret over time. Even small changes in laboratory technique or analysis methods can dramatically affect p-values, making historical comparisons meaningless. Some might argue that p-values are the standard in biological research and have ",
    "tags": [
      "machine learning",
      "biology",
      "measurement",
      "data science",
      "bayesian",
      "statistics",
      "metrology",
      "reproducibility",
      "biophysics",
      "protein design",
      "protein engineering",
      "uncertainty",
      "experimental design"
    ],
    "pub_date": "2025-02-23",
    "type": "blog"
  },
  {
    "id": "blog-let-me-ship-you-the-python-you-need",
    "url": "/blog/2025/2/17/let-me-ship-you-the-python-you-need/",
    "title": "Let me ship you the Python you need",
    "summary": "In this blog post, I explore how modern Python tooling is flipping the script on the age-old \"which Python should I use?\" question. Through my experience with , , and , I show how we're moving away from the traditional headache of environment setup and toward a world where tools automatically ship you the exact Python you need. No more environment setup puzzles \u2013 just specify your Python version and get straight to work. It's a liberating shift that's changing how I approach one-off Python work, and I think it's pretty exciting!",
    "body": "Recently, I watched Peter Wang's PyBay talk, The Five Demons of Python Packaging That Fuel Our Persistent Nightmare, and have been reflecting on a quote from Peter's talk: ...I don't care you'll get a Python we'll get you the Python you need what's the thing you want to do It's the difference between first worrying about \"what Python do you have\", instead of worrying about \"what Python do you need\". , and in particular , really adopts this paradigm. When I install a tool using or run a tool using , I can specify the exact Python interpreter version that I'd like: This is incredibly liberating as a user who is looking to install globally-available tools. It's come to the point that for one-off work, I now do one of the following. coupled with / and PEP723 now helps invert the process of getting started. There's no more finagling with first creating a Python environment and then getting to work. Instead, we just get to work and install packages on an as-needed basis, done on-the-fly. and will ship you the Python that you need to do what you want to do. That's pretty rad.",
    "tags": [
      "packaging",
      "uv",
      "tools",
      "marimo",
      "juv",
      "environments"
    ],
    "pub_date": "2025-02-17",
    "type": "blog"
  },
  {
    "id": "blog-lightening-the-llamabot",
    "url": "/blog/2025/2/7/lightening-the-llamabot/",
    "title": "Lightening the LlamaBot",
    "summary": "In this blog post, I share my journey of tackling dependency bloat in LlamaBot. What began as a simple LLM bot framework had grown into a monolithic system with an extensive dependency chain, leading to massive installation sizes. By mapping dependencies, refactoring the code, and organizing optional dependencies, I managed to reduce the container size significantly. This exercise taught me the importance of regular codebase maintenance and focusing on core functionalities. Now, LlamaBot is leaner and more efficient. Curious about the strategies I used to achieve this transformation?",
    "body": "In my recent work with LlamaBot, I faced a challenge that many developers might recognize: dependency bloat. What started as a simple bot framework had grown into a monolithic beast that pulled in everything but the kitchen sink. Let me share how I tackled this issue and what I learned along the way. The weight of dependencies When I looked at LlamaBot's dependency chain, it was extensive: Panel, Bokeh, LanceDB, ChromaDB, Tantivy, Astor, PyperClip, PromptToolkit - the list went on. Being honest with myself, I had gotten lazy with dependencies. The consequence was that every LlamaBot installation involved downloading seven gigabytes worth of packages. 7 GB!! All to just use either the git hooks () CLI, or just to use . This is what the dependencies looked like before, on commit (which you can also view here): The real kicker came from dependencies like ChromaDB, which pulled in sentence-transformers, which then required PyTorch. On Linux systems, this cascade would drag in all the CUDA packages too. It was overkill for what most users needed. Breaking down the monolith I started by mapping out where each dependency was actually used. As an example: - CLI components: pyzotero, Rich, GitPython, nbformat - RAG functionality: ChromaDB and related packages - Agent features: Docker, python-docker, markdownify, dotgosearch For context, I developed a few prototype CLI applications within LlamaBot to test drive the user experience of some tools I had in mind. Most of them, it turns out, I don't use, and at some future date, I may just deprecate them. Others, however, like the git commit writer, are now fully integrated into my workflow that I can't do without it. An interesting realization hit me: the RAG (Retrieval-Augmented Generation) corner of LlamaBot wasn't getting much use. While cosine similarity RAG had its moment in the spotlight, it wasn't as revolutionary as I had thought it would be -- it turns out to still be useful for small context window models, but with many models hitting 128K to 2M token context lengths, and with other advances in RAG (e.g. GraphRAG), it may become less and less useful. But it still lives around just in case it's handy, as there are use cases for small LMs. Finally, the Agent features were new and experimental, so it didn't make sense to ship them as part of the core dependencies. The refactoring strategy For the core of LlamaBot, I decided to focus on and - the core components that delivered the most value. After all, I find myself using the most. The strategy? Move non-essential imports outside of and into try-except blocks. Yes, this meant more try-except blocks throughout the codebase, but the trade-off was worth it. Here's an example of what this looks like: Cursor absolutely helped here; I was able to eliminate a lot of repetitive error message typing through Cursor's tab-completion. In the CLI, I went a step further. Instead of importing optional dependencies at the module level, I moved imports into specific functions. This meant managing imports at runtime rather than import time. Here's an example from the my experimental Zotero chat CLI tool: New dependency specification With these changes, I could change my dependency specification to look like this: The new dependency specification is much more organized! Measuring success The results were dramatic. I created two Dockerfiles, one for the full suite of optional dependencies, and one without. Here's what the two Dockerfiles looked like: To build the containers, I used the following commands: And to view the container sizes: The numbers don't lie -- we have an approximately 20X smaller container size by removing unnecessary dependencies. I'm all for anything that lets me squeeze out 80% of performance with 20% of effort! Securing the changes I would consider these changes to be somewhat brittle; it relies on a developer's knowledge of the codebase to know that something should be considered core v.s. non-core. To secure these changes for the future, I added safeguards: 1. PR tests that verify SimpleBot and StructuredBot functionality 2. A bare package test that runs against Gemma 2B The PR tests look like this: And that infamous : Essentially, I wanted to guarantee that this script's behaviour worked whenever we installed the package without optional dependencies. That's what motivated this test. Looking forward Even before this refactor, I've found myself gravitating more toward than . Its abstractions build naturally on 's patterns, making it more versatile for my needs. The refactoring exercise taught me valuable lessons about dependency management and the importance of regular codebase maintenance. Sometimes, less really is more - especially when \"more\" means downloading half the Python ecosystem. Now, I have a leaner, more focused tool that does exactly what it needs to do, without the bloat. That's a win in my book!",
    "tags": [
      "refactoring",
      "llamabot",
      "optimization",
      "docker",
      "python",
      "cli",
      "packages",
      "performance",
      "engineering"
    ],
    "pub_date": "2025-02-07",
    "type": "blog"
  },
  {
    "id": "blog-pydata-bostoncambridge-talk-moderna-what-makes-an-agent",
    "url": "/blog/2025/1/31/pydata-bostoncambridge-talk-moderna-what-makes-an-agent/",
    "title": "PyData Boston/Cambridge Talk @ Moderna: What makes an agent?",
    "summary": "In this blog post, I explore the concept of 'what makes an agent' by discussing various implementations of LlamaBot, a Python package for LLM exploration. I dissect the differences between SimpleBots, StructuredBots, and AgentBots, highlighting their capabilities and limitations in terms of agency and decision-making. Through audience discussions and examples, I aimed to provoke thought on the definition and design of agents, and together, we had an engaging discussion. Can we truly define an agent, or is it like the Turing Test, a concept that evolves with our understanding and technological advancements?",
    "body": "This is a written and edited version of a talk I gave at PyData Boston/Cambridge on 29 Jan 2025, hosted at Moderna's new office buildings for the first time ever. This talk will likely leave us with more questions than answers about \"what makes an agent.\" But I hope it also leaves us with more focused questions and frameworks for thinking about \"what makes an agent\". It is based loosely on my blog post. The original notebook that I presented is available within the LlamaBot repo. Let's start with LLM bots This is a helpful framing to anchor our discussion of agents. LlamaBot is a Python package that I made to pedagogically explore the landscape of LLMs as the field evolves. Intentionally designed to lag 1-2 steps behind innovations and incorporate only what turns out to be robust. LlamaBot implements a variety of LLM bots, the simplest being the . makes clear the basic anatomy of an LLM API call: 1\ufe0f\u20e3 A system prompt: maintained over all calls. 2\ufe0f\u20e3 A model name: specifying which model to chat with. 3\ufe0f\u20e3 A user's prompt: your input into the LLM's internals. 4\ufe0f\u20e3 The response: returned by autoregressive next-token prediction. In , that looks like this: And the response looks like this: Together with the audience, we evaluate that response together. Is this an agent? Audience commentary: - Not an agent: didn't take action apart from introspection of weights. - Not an agent: no external APIs called. - Yes: agent doesn't mean it has to have a tool, anything that gives usable information is an agent. - Yes (to an extent): Follows up with a question/prompt back to the human. - No: agent should make decisions. This LLM call did not make any decisions. Decision-making outsourced to user. My thoughts are as follows: s are heavily reliant on what's in the training data; they are just calling the LLM directly, with no memory of historical chat information. Given the range of regular human experiences in North America, I would venture to guess that it is rare for us to go outside of any base LLM's training data, unless we hit very specialized and niche topics not covered on the internet. (Try thinking of some!) It's hard to argue that this would have the properties of agency, which we would associate with an agent. Agents that give us structure? Let's try a different view of agents. What if we said, \"Agents give you information structured the way you wanted it?\" Let's start with that as a working definition. LlamaBot implements structured generation. To use it, you need to provide a Pydantic model, effectively a form that you want the bot to fill, and pair it with a . s have the same basic requirements as s, which are a system prompt and model name, but also need to have a Pydantic model passed in. When called, it will return an instance of that Pydantic model. Its response looks like this: Audience discussion: Is this an agent? I specifically asked, where does this fall short of an agent's definition? - No: still like RAG. Doesn't read the situation, is this a good time to buy? Still a simple answer. - No: it would independently decide what to look at, where to look. - Yes: Based on the working definition, yes, and the nature of decision-making means the LLM is doing rapid-fire retrieval from memorized data. Recommended arrival time is an added recommendation that was not part of the original query intent. - No: Is the definition even correct? Why isn't a SQL query an agent, under that definition? - No: within this context, the latent query intent might be to buy tix, but this bot doesn't help me do it. In summary, this bot still misses the crucial elements of (a) autonomy/agency, and (b) intent guessing. Structured generation gets us midway It affords us more control over what we are asking an LLM to generate. The response is constrained. But it seems like it may still lack a crucial element of an \"agent\". I'm glad the audience shred up this definition! \ud83e\udd17 What if we said, \"An agent needed to interact externally\"? Here is my attempt, within LlamaBot, to try to nail down what an agent actually is. This flight agent-like bot helps me plan out trips, using one tool, the tool. Its response looks like this: Looks like this could be an agent, but let's see what the audience said. Audience discussion: is this really an agent? I asked the audience to disregard the fact that I called this an . Just because I said so doesn't make it so! Where are the holes in this definition of an agent? - Getting closer: agent will do research, in this example, it is the use of a search tool, and processing the text to give a response; the task could have been clearer for end user. - Yes: it has introduced a new error mode. A new way to be wrong, need someone to blame for this. - New component here: this bot has agency, decides \"do I call this function or not\". - Principal-Agent problem: agent does something that may not be what the Principal wanted it to do. The audience also had more questions raised: - Q: Is part of the definition of an agent t",
    "tags": [
      "large language models",
      "python",
      "llamabot",
      "pydantic",
      "structuredbot",
      "agentbot",
      "talks",
      "meetups"
    ],
    "pub_date": "2025-01-31",
    "type": "blog"
  },
  {
    "id": "blog-why-data-from-preclinical-biotech-lab-experiments-make-machine-learning-challenging",
    "url": "/blog/2025/1/19/why-data-from-preclinical-biotech-lab-experiments-make-machine-learning-challenging/",
    "title": "Why data from preclinical biotech lab experiments make machine learning challenging",
    "summary": "In this blog post, I explore the challenges biotech teams face when integrating public datasets with internal data for machine learning. Despite initial excitement, issues like data compatibility, missing variables, domain shifts, and biological complexity often arise. I suggest a shift from a machine learning perspective to a decision support approach, advocating for separate models and a decision fusion layer that incorporates human expertise. This method respects the complexity of biological systems and aids in effective decision-making. How can we better navigate these challenges to accelerate biotech discoveries?",
    "body": "Biotech teams keep running into the same wall. It starts innocently enough with a promising discovery: a public dataset that appears to perfectly align with their research objectives, to identify molecules with a particular function. The excitement builds quickly. A data scientist, who is matrixed into the team, envisions combining it with internal data (about to be generated) to build better models and accelerate their research. But what begins with enthusiasm inevitably leads to hard lessons about the unique challenges of preclinical biotech data. Let's explore this prototypical story in more detail. The initial promise: finding public data Midway through the project, an apparently perfect dataset appears. It's cleanly organized with binary classifications (active/inactive) and enough examples to train a solid classifier -- 1000+ molecules are included, a luxury number in preclinical laboratory experiments. Initial models look promising. Validation metrics are strong. Cross-validation results encourage further development. Everyone aligns behind the opportunity, and both project and department leadership provide enthusiastic support! The laboratory team is ready to generate data that, in all likelihood, could be combined with the existing public data. Together with the data scientist, the team designs the experiment and gets ready to execute on it. However, as the team dives deeper into the experiment design, complexities emerge that challenge their initial optimism. When reality crashes in: fundamental challenges When planning out the measurement experiment, fundamental challenges with data compatibility emerge. These aren't just technical hurdles - they're systemic issues that any data scientist supporting preclinical work needs to overcome. Challenge 1: reconciling binary and continuous measurements The first roadblock appears immediately. The lab team can generate continuous-valued activity measurements. But these aren't binary, as they were in the public data! How do you reconcile binary classifications with continuous measurements? What seems like a straightforward thresholding problem quickly spirals into deeper questions. What threshold did the original researchers use? Their methods mention a cutoff, but it turns out their dataset is an amalgamation of many other datasets, each with their own binary classification values. Different labs define \"active\" behavior differently, and in this case, these definitions were poorly documented; even when they are documented, the experimental conditions behind those definitions may differ dramatically from the new team's setup. We are now in a logical deadlock. Challenge 2: hunting invisible variables Next, certain experimental factors were not recorded in the public dataset, making them difficult to compare against the internal data. These include temperature, pH levels, incubation times, and buffer composition for dissolving the molecule -- potentially important process information that was not captured. Do these matter? They may play a role, but without the data recorded, it's impossible to know. Much of this information remains forever lost, buried in lab notebooks that will never see the light of day. (This, in case you weren't aware, represents a fundamental problem in biotech data collection!) Challenge 3: navigating the domain shift maze The most insidious challenge lurks in domain shifts between experiments. The team is working with HEK293 cells. The public dataset, however, used HEK293T cells. The similar names suggest compatibility, but reality proves far more complex: dramatic differences can exist between these cell lines! For those who didn't know, HEK293 and HEK293T cells are both derived from human embryonic kidney cells, but HEK293T cells contain an additional gene expressing the SV40 Large T antigen. This key difference makes HEK293T cells capable of episomal replication of plasmids containing the SV40 origin, resulting in higher transfection efficiency (of the external DNA added, it'll take up more) and protein expression levels (overall, more protein produced) compared to standard HEK293 cells. Bridging this domain shift demands careful validation through \"bridging experiments.\" Only then can teams confidently build hybrid models combining both datasets. But practical constraints often fight against these technical requirements. Funding for the project may be running out. Or there may be an important deadline that leads the team to conclude that it isn't high enough of a priority to do so. Challenge 4: wrestling with biological complexity All these challenges stem from a fundamental truth: biological systems are inherently complex. Teams aren't working with static, controlled systems. They're dealing with living organisms that change over time. These systems interact in non-linear ways. Their behavior often defies expectations. Teams are forced to make critical assumptions, like stability of their cell lines over time. A card-carrying biol",
    "tags": [
      "biotech",
      "datasets",
      "machine learning",
      "research",
      "data fusion",
      "decision support systems",
      "data science"
    ],
    "pub_date": "2025-01-19",
    "type": "blog"
  },
  {
    "id": "blog-writing-at-the-speed-of-thought",
    "url": "/blog/2025/1/13/writing-at-the-speed-of-thought/",
    "title": "Writing at the speed of thought",
    "summary": "When typing became physically demanding, I discovered that dictation tools could do more than just help me write \u2013 they could fundamentally change how I capture and develop ideas. Using Better Dictation and VoicePal, combined with AI assistance, I found a way to write that matches the natural flow of thought. This isn't just about accessibility or working around limitations; it's about finding a better way to translate the nonlinear, rapid-fire nature of our thoughts into written words. I share my approach to preserving authentic voice while using AI tools, and why sometimes constraints push us toward unexpected improvements in how we work.",
    "body": "In recent months, I've been dealing with wrist pain that made typing increasingly difficult. What started as a limitation, though, led me to discover something unexpected: a better way to write that not only solved my physical constraints but revolutionized how I capture my thoughts. The challenge: When typing becomes your enemy The frustration of having ideas trapped in your head while your body won't cooperate is real. Typing had become tedious and painful, creating a genuine barrier between my thoughts and their expression. But beyond the physical limitations, I realized typing itself was always a bottleneck \u2013 my fingers simply couldn't keep up with my brain. Finding freedom in voice The solution came through dictation. While macOS's native dictation works, it left me with a lot of manual cleanup work. Through a class by Jason Liu, I discovered Better Dictation, and I also found VoicePal for my phone through Ali Abdaal's YouTube channel. Both tools are game-changers \u2013 they can handle spelled-out words intelligently, returning the correct spelling of technical terms or names without showing the letter-by-letter dictation. With either tool, I finally had a system that could truly keep up with my thought process while minimizing the need for manual editing. What makes this approach powerful isn't just about avoiding physical pain \u2013 it's about capturing what I call \"nonlinear ideas.\" When I'm typing, if I go down one train of thought and something else bubbles up, I face a frustrating choice: either lose my original thread while trying to capture the new idea, or lose the new idea while trying to finish my current thought. Speaking allows me to quickly address new thoughts and return to my original path without losing either. My current writing stack Here's what works for me: 1. I use VoicePal for initial dictation, which automatically cleans up filler words and makes the raw content more readable. 2. This cleaned-up text goes into Claude (Anthropic's AI) for initial editing while preserving my voice and style. 3. Final edits happen in Cursor, where I can make minor tweaks if needed. The AI question: Keeping it authentic The common concern about AI writing tools isn't quite that they strip away personality \u2013 it's that they tend to generate generic, templated-sounding text. I've found two key elements that help me avoid this. First, through in-context learning \u2013 showing the AI examples of my writing style \u2013 I can keep the tone and flow that I've developed through years of debate, public speaking, and blogging. This foundation of finding my voice through traditional means, without AI assistance, has been crucial. But style is just the surface. The real key to authentic writing with AI is in the editing process. When I critically review and edit AI-generated drafts, I'm not just proofreading \u2013 I'm actively shaping the text to match the true substance of what's in my head. This is where ownership really happens. The AI helps get ideas on the page, but through editing, I ensure the final text genuinely reflects what I'm trying to say. So yes, you've been reading AI-generated text \u2013 but not for the reasons you might think. I'm not trying to be another content creator churning out quick posts for monetization. I'm using AI because I have real physical constraints now, and I need to find efficient ways to keep writing while managing my wrist pain. Sometimes constraints lead us to better solutions. The freedom to think What started as a solution to physical pain has become something much more valuable: a way to write at the speed of thought. It's not just about accessibility \u2013 it's about removing the barriers between our ideas and their expression. For me, that's absolutely huge. Being able to capture my thoughts painlessly and naturally has significantly improved not just my writing, but my quality of life. Embracing new ways to create Sometimes our greatest innovations come from working around limitations. While I started this journey looking for a way to write through pain, I've ended up discovering a method that's actually superior to my old way of working. The combination of dictation and AI assistance hasn't just helped me continue writing \u2013 it's helped me write better, think more clearly, and express ideas more naturally than ever before. If you're facing similar challenges, whether they're physical limitations or just the frustration of thoughts moving faster than your fingers, I encourage you to experiment with these tools. The technology exists now to remove the barriers between your thoughts and their expression. All that's left is to find the combination that works for you and to let your ideas flow freely.",
    "tags": [
      "dictation",
      "accessibility",
      "productivity",
      "artificial intelligence",
      "writing workflow",
      "voicepal",
      "creativity"
    ],
    "pub_date": "2025-01-13",
    "type": "blog"
  },
  {
    "id": "blog-a-practical-guide-to-securing-secrets-in-data-science-projects",
    "url": "/blog/2025/1/10/a-practical-guide-to-securing-secrets-in-data-science-projects/",
    "title": "A practical guide to securing secrets in data science projects",
    "summary": "In this post, I share a practical approach to managing secrets in data science workflows, learned from personal experience with both successes and mistakes. I cover essential tools like and files for local development, strategies for secure secret handling in Jupyter notebooks, and crucial version control practices including pre-commit hooks to catch accidental API key commits. I also discuss team collaboration approaches for secret sharing, platform-specific secrets management features, and what to do when secrets accidentally get committed to repositories. While tools like AWS Secrets Manager exist for enterprise needs, I focus on practical, accessible methods that create robust security through layered defenses, following proven software development principles that apply equally well to data science work.",
    "body": "I've made my share of mistakes with secrets management - from accidentally committing API keys in Jupyter notebooks to storing passwords in plain text. Today, I want to share what I've learned about keeping secrets secure in our data science workflows. Let's face it: as data scientists, we deal with a lot of sensitive credentials. API keys for data vendors, database passwords, cloud service tokens - the list goes on. Here's my practical approach to handling these securely. Start local: the power of environment variables The foundation of my secret management starts with environment variables. I've found that using combined with files is a game-changer for local development. Here's the how and why. First, install [](https://direnv.net/) in your shell. This tool automatically loads environment variables when you enter a directory and unloads them when you leave. This is way more secure than setting globals in your file, where any program in your user space could potentially access them. In my projects, I create a file with permissions set to 400 (meaning only I can read it): Inside this file, I store my secrets: Now, here's an important technical detail: if your Jupyter notebooks are launched in an environment where is already active, the environment variables will be automatically available through . You won't need any additional setup: However, if your notebooks launch without support (which is common in many setups), you can fall back to : I generally include in my projects anyway - it's a lightweight safeguard that ensures my code works across different environments, whether is available or not. Version control: the first line of defense One crucial habit I've developed is to immediately add to my file. This prevents me from accidentally committing secrets to version control. Here's what I add to : Preventing commits with pre-commit hooks While helps prevent committing specific files, I've learned that having an automated check for accidental API key commits is invaluable. I use a pre-commit hook that scans for potential API keys in my code before they ever make it into a commit. Here's the pre-commit hook I use (you can find it here on GitHub): This hook has saved me numerous times. Based on the configuration, you can catch keys like: - Generic 32+ character keys - OpenAI API keys - GitHub personal access tokens - Slack tokens - And any other patterns you configure it to catch Hot tip: use a local LLM to help you generate the pattern. You can copy/paste the key a few times in a plain text editor, flip around 4-5 characters in there, and then ask it for the regex pattern. Much faster than trying to decipher it yourself! Here's the prompt: Once you have this set up, the pre-commit hook will automatically scan your code before each commit. If it finds anything that looks like an API key, it'll stop the commit process and point out exactly where the potential exposure is. This gives you a chance to remove any secrets before they ever make it into version control - which has saved me countless times from accidentally exposing sensitive information. I've found that combining this with creates a double-layered defense against accidentally exposing secrets. The prevents committing known secret-containing files, while the pre-commit hook catches any secrets that might slip into your actual code. Sharing secrets in collaborative projects First, let me address something critical: if you absolutely must share a secret with a teammate (which should be rare), never do it through chat, email, or externally hosted services. I've learned to use self-destroying notes through open source tools like Cryptgeon, which you can deploy on your internal network. Here's the thing - NEVER trust externally hosted secret-sharing services. You have no idea what they're doing with your data behind the scenes! While I've built my own tool (pigeon notes, which I'll open source later this year), I actually recommend you don't trust my hosted version either. The safest approach is to deploy your own instance of an open source tool like Cryptgeon on your internal infrastructure. This way, you have full control and visibility over how your sensitive data is handled. Now, for managing secrets in your development workflow, I've found that almost every major vendor platform that involves code execution provides some form of secrets management. For example: - GitHub provides Secrets features for CI/CD pipelines - Workspace platforms like Domino or Nebari have their own secrets management systems - Cloud providers like AWS offer services like Secrets Manager to interact with their services - Most compute platforms come with built-in ways to handle secrets securely The key is to check your platform's documentation and invest time in understanding its secrets management capabilities. Instead of hard-coding secrets directly in your notebooks or scripts (which is both insecure and a common mistake), leverage these built-in features - they're usually ",
    "tags": [
      "cybersecurity",
      "pre-commit hooks",
      "jupyter",
      "secrets management",
      "best practices",
      "data science"
    ],
    "pub_date": "2025-01-10",
    "type": "blog"
  },
  {
    "id": "blog-what-makes-an-agent",
    "url": "/blog/2025/1/4/what-makes-an-agent/",
    "title": "What makes an agent?",
    "summary": "In this blog post, I explore what defines an LLM agent, highlighting its goal-oriented non-determinism, decision-making flow control, and natural language interfaces. I also discuss when to use agents, emphasizing the importance of variable scope inputs and constrained actions. By examining industry perspectives from Anthropic and Google, I also explore how agents can effectively handle diverse inputs while maintaining defined action boundaries. Real-world examples, like a bill calculation bot and a literature research assistant, illustrate these principles. How can these insights transform your approach to designing agent applications?",
    "body": "In a previous blog post, I explored how LlamaBot's agent features can simplify complex task automation. Here, I'd like to explore what exactly makes an agent. My working definition In my view, an LLM agent is a system that demonstrates the appearance of making goal-directed decisions autonomously. It operates within a scoped set of tasks it can perform, without requiring pre-specification of the order in which these tasks are executed. Three key attributes set it apart: 1. Goal-oriented non-determinism: The agent makes decisions dynamically rather than following a pre-defined sequence of actions. 2. Decision-making flow control: Decisions adapt based on context, resembling a \"thinking\" process. 3. Natural language interfaces: Inputs and outputs are in plain language, making interactions feel intuitive. A familiar way to understand these attributes is to think about a flight booking agent you might call on the phone. They handle diverse requests (\"I want to fly to Tokyo next month\" or \"I need the cheapest flight to London this weekend\"), make decisions based on context (checking different airlines, considering your preferences), and communicate naturally. The parallel helps clarify what we mean by agent-like behavior - though with one key distinction. While a human agent exercises genuine autonomy in their decision-making, our LLM agents create the appearance of autonomy and flexibility through careful engineering of their design and tools. These attributes combine to give agents their apparent autonomy and their flexibility in handling diverse inputs. With these principles in mind, I designed LlamaBot's AgentBot to make creating and deploying agents simple, flexible, and powerful. A key insight: The guiding principle Through my experience with website scraping and other projects, I've developed what I believe is a crucial principle for when to use agents: You really don't want to use agents until you have variable scope input paired with a constrained scope of actions that could be taken. This principle really emerged from my observations of what makes for successful applications of agents. In fact, I've noticed that an agent's effectiveness - that is, the probability it chooses the correct action - appears to be highly dependent on two key factors: 1. The number of options available to it (the more constrained, the better) 2. The vagueness of the incoming instructions (the clearer, the better) This relationship between input clarity and action constraints isn't just theoretical. I've also observed that an agent's ability to clarify these actions when needed plays a significant role in its success. When an agent can effectively navigate between understanding variable inputs and selecting from constrained actions, it performs at its best. Industry perspectives on agents While my observations come from hands-on experience building and deploying agents, it's worth examining how others in the field think about agents. These diverse perspectives help contextualize the practical principles I've discovered and offer additional frameworks for thinking about agent design. Anthropic's architectural distinction Anthropic's perspective on agents particularly resonates with my thinking about variable scope inputs and constrained actions. They make an important architectural distinction in how they categorize agent systems: 1. Workflows: Systems where LLMs and tools are orchestrated through predefined code paths 2. Agents: Systems where LLMs dynamically direct their own processes and tool usage, maintaining control over how they accomplish tasks What's particularly compelling about their approach is how they emphasize finding the simplest solution possible. They explicitly note that agent systems often trade latency and cost for better task performance - a tradeoff that needs careful consideration. This aligns perfectly with my observations: simple scales, while complexity doesn't. Building a Rube Goldberg machine is a recipe for long-term disaster. Their framework suggests using workflows for predictability and consistency in well-defined tasks, while reserving agents for situations where flexibility and model-driven decision-making are needed at scale. This maps well to my principle about variable scope inputs - workflows for fixed patterns, agents for variable ones. Google's comprehensive framework Google's definition, outlined in their whitepaper, takes a more expansive view while still complementing these ideas. They define a generative AI agent as an application that attempts to achieve goals by observing and acting upon the world using available tools. Their framework emphasizes several key characteristics that help clarify when to use agents: - Autonomy: Agents can act independently of human intervention when provided with proper goals or objectives - Proactive Planning: They can reason about next steps even without explicit instruction sets - Tool Integration: They emphasize how agents extend beyond standalone m",
    "tags": [
      "llms",
      "automation",
      "workflows",
      "tools",
      "agents"
    ],
    "pub_date": "2025-01-04",
    "type": "blog"
  },
  {
    "id": "blog-what-i-learned-blogging-every-week-for-one-year",
    "url": "/blog/2024/12/31/what-i-learned-blogging-every-week-for-one-year/",
    "title": "What I learned blogging every week for one year",
    "summary": "In this blog post, I reflect on my year-long challenge of writing a blog post every week, surpassing my goal with 53 posts. This journey taught me the power of consistency, improved my ability to communicate complex ideas, and helped me develop AI-assisted tools to streamline my workflow. I also explored the intersection of life sciences and computation, aiming to accelerate scientific discovery. How did these experiences shape my approach to integrating AI into creative processes and what insights can you gain from my journey?",
    "body": "A challenge I set for myself this year was to write a blog post on average once every week for the year, regardless of what's going on. This meant writing 52 blog posts in total. As of December 19, I've hit 53, meaning that I've accomplished my goals! What I learned Here's what I've learned so far. Consistency is king! One of the biggest lessons I learned from this year-long blogging journey is that consistency truly is king. By committing to writing weekly, I noticed my sense of what to write about became increasingly refined and sharpened over time. The regular practice helped me: - Develop a better radar for interesting technical topics worth sharing - Improve my ability to explain complex concepts clearly - Build a more systematic approach to documenting my learning journey - Create a sustainable writing habit that feels natural rather than forced Build AI-assistance tools and use them Throughout this journey, I found myself building quite a bit of tooling to support my blogging workflow, most of it done with AI assistance. Some notable examples include: Blog Banner Generation: I developed a system that uses DALL-E to automatically generate watercolor-style banner images for each post, maintaining a consistent visual identity while saving time on design. Social Media Integration: I built tools that use LLMs to help craft: - Twitter posts that effectively summarize and promote new blog entries - Substack newsletters that engage my subscriber base - LinkedIn updates that resonate with my professional network Content Enhancement: I created automated systems for: - Generating relevant tags for better post categorization - Creating compelling summaries that capture the essence of each post - Suggesting related content links to improve site navigation This tooling journey has been particularly valuable as it helped me: - Further hone my prompting practices with LLMs - Develop better intuition for designing programs that incorporate LLMs as integral components - Create a more efficient and consistent blogging workflow - Experiment with different approaches to human-AI collaboration The experience of building these tools has not only made my blogging more efficient but has also sharpened my intuition for how to effectively integrate AI into creative workflows while maintaining human oversight and editorial control. Wordcloud And here is the obligatory wordcloud of the blog posts: Reflections One year of blogging once a week, with regular posts on LinkedIn, Twitter, Substack, and Bluesky, have left my colleagues jokingly calling me a content creator. Jokes aside, this year has seen a significant focus on LLMs in my writing, as evidenced by the numerous posts in the LLMs & Data Science Tooling section. This reflects the broader excitement and rapid developments in the LLM space. At the same time, my core interests continue to lie at the intersection of life sciences and computation - you can see this in the many posts about protein language models, biological applications of deep learning, and data science in biotech organizations. My professional goal is to make discovery science run at the speed of thought and quantify the unquantified. This means leveraging advances in AI and computation to accelerate scientific discovery, while developing tools that help scientists measure and understand previously intangible aspects of biology. After all, the most transformative breakthroughs happen when we remove the friction from scientific exploration and shine light on unexplored territory. Blog Posts by Theme To help you navigate through these posts more easily, I've organized them by major themes below. You'll notice some posts appear under multiple themes - that's because many topics naturally overlap, reflecting the interconnected nature of modern data science work. Whether you're interested in LLMs, leadership, tooling, or biology, you'll find relevant content in these curated lists. Biology & Chemistry - Success Factors for Data Science Teams in Biotech (2024-02-07) - Dashboard-ready data is often machine learning-ready data (2024-02-18) - Mixtral-8x7b-Instruct works on an old GTX1080! (2024-03-10) - Data Science in the Biotech Research Organization (2024-05-05) - Paper Review: Design of highly functional genome editors by modeling the universe of CRISPR-Cas sequences (2024-05-12) - How to control PyMOL from Jupyter notebooks (2024-05-16) - Multi-modality Deep Learning (2024-05-27) - The Neural Von Mises Mixture Model (2024-06-08) - A survey of how to use protein language models for protein design: Part 1 (2024-07-26) - A survey of how to use protein language models for protein design: Part 2 (2024-08-02) - A survey of how to use protein language models for protein design: Part 3 (2024-08-09) - A modest proposal for data catalogues at biotechs (2024-11-22) Career Advice - GitHub Actions secrets need to be explicitly declared (2024-01-11) - How to keep sharp with technical skills as a data science team lead (2024-02",
    "tags": [
      "blogging",
      "consistency",
      "ai",
      "content",
      "llms",
      "data",
      "biotech",
      "career",
      "writing",
      "discovery"
    ],
    "pub_date": "2024-12-31",
    "type": "blog"
  },
  {
    "id": "blog-accurately-extract-text-from-research-literature-pdfs-with-nougat-ocr-and-docling",
    "url": "/blog/2024/12/20/accurately-extract-text-from-research-literature-pdfs-with-nougat-ocr-and-docling/",
    "title": "Accurately extract text from research literature PDFs with Nougat-OCR and Docling",
    "summary": "In this blog post, I explore the challenges of extracting structured text from PDFs, especially when dealing with equations, tables, and figures. I discuss two tools, Nougat-OCR by Facebook Research and Docling by IBM, which I found effective for this task. Nougat-OCR excels at handling equations and tables, while Docling excels on extracting figures. By combining these tools, we can develop a workflow that captures all critical components of a PDF. Want to know how to retain valuable knowledge from complex PDFs?",
    "body": "Parsing published literature into plain text is a task that seems deceptively simple. In reality, PDFs can be notoriously difficult to work with, especially when they include elements like equations, tables, and figures. If you're working with large language models (LLMs) or just trying to extract data for analysis, the standard text extraction tools often leave significant amounts of useful context behind. Recently, I explored two tools Nougat-OCR by Facebook Research and Docling by IBM to address this problem more effectively. The problem: What vanilla tools miss Standard methods for extracting text from PDFs often work well for plain paragraphs but stumble when it comes to three critical areas: 1. Equations: PDFs store equations as images or special font renderings, making it challenging to extract them accurately as structured text. 2. Tables: Extracting tables often results in misaligned columns or garbled data, losing the relationships between rows and columns. 3. Figures: Figures and diagrams frequently get ignored or reduced to low-quality placeholders, stripping away valuable visual context. Given these challenges, I wanted to find tools that could improve text extraction for equations, tables, and figures. Here\u2019s what I found. Nougat-OCR: Accurate equation and table extraction Nougat-OCR is a tool developed by Facebook Research that focuses on converting scientific PDFs into structured text, including support for equations and tables. Its installation and usage are straightforward. Installation I used the package manager to set up Nougat-OCR: Once installed, the command becomes available on your system PATH. Usage To extract text from a PDF, run the following command: This processes the PDF and redirects the extracted text into a Markdown file. Key strengths Nougat-OCR handles equations and tables impressively well. For example, consider this equation from the paper A curve similarity approach to parallelism testing in bioassay: It also processes tables cleanly. Here\u2019s an example table: This table is extracted with alignment preserved, making it ideal for further analysis. However, Nougat-OCR does not perform well with figures. Docling: Extracting figures accurately For figures, I turned to Docling by IBM. While Nougat-OCR shines at text-based elements like equations and tables, Docling focuses on images and visual components. Installation Like Nougat-OCR, Docling can be installed with : Usage To extract images from a PDF, run: Docling processes the PDF and outputs images as base64-encoded strings embedded in a Markdown file. For example: Key strengths Docling faithfully extracts all figures and encodes them as reusable base64 strings, which can then be passed to a multimodal LLM for description or analysis. Limitations Docling does not handle equations well. For example, the earlier equation: was rendered incorrectly as: Combining Nougat-OCR and Docling: A complete workflow To get the best of both tools, I used a multi-step workflow: Extract text-based elements (equations, tables) with Nougat-OCR: Extract figures with Docling: Process figures with a multimodal LLM like LlamaBot: This approach ensures that you capture all critical components of a PDF\u2014 equations, tables, and figures\u2014 with minimal loss of context. Next steps: Deploying on Modal for scalable preprocessing Both Nougat-OCR and Docling benefit significantly from GPU acceleration, especially when processing large volumes of PDFs. To make this workflow more scalable and accessible, my next step is to deploy these tools on Modal, a serverless platform that supports GPU-based processing. By deploying Nougat and Docling as APIs on Modal, I can: - Preprocess PDFs on-demand: Use simple API calls to trigger text and figure extraction. - Leverage GPUs for performance: Accelerate processing for large or complex PDFs. - Integrate with existing workflows: Seamlessly use these APIs in multimodal LLM pipelines or downstream analysis. This deployment will allow me to scale preprocessing tasks effortlessly and unlock the full potential of structured PDF data. Conclusion Parsing PDFs into structured plain text is more than just a convenience; it's a necessity when working with LLMs or conducting scientific analysis. By combining Nougat-OCR for text-based elements and Docling for visual content, you can extract high-quality data from published literature. To make this solution scalable, deploying these tools on Modal with GPU support will ensure rapid, on-demand preprocessing through simple API calls. This workflow allows you to retain equations, tables, and figures, ensuring that no valuable knowledge is left behind. As tools like Nougat-OCR and Docling continue to improve, so too will our ability to make sense of complex, multimodal content.",
    "tags": [
      "docling",
      "nougat",
      "llms",
      "document parsing",
      "gpu"
    ],
    "pub_date": "2024-12-20",
    "type": "blog"
  },
  {
    "id": "blog-how-to-thrive-and-not-just-survive-during-organizational-change",
    "url": "/blog/2024/12/17/how-to-thrive-and-not-just-survive-during-organizational-change/",
    "title": "How to thrive, and not just survive, during organizational change",
    "summary": "In this blog post, I explore how to navigate and thrive during organizational changes. I share personal insights and practical strategies, such as focusing on meaningful relationships with colleagues, consistently delivering great work, and proactively building your career path. I also emphasize the importance of staying present and cultivating a 'career committee' of trusted advisors. Change is inevitable in any organization, but how we respond can transform these shifts into growth opportunities. Curious about how to build your own resilience in changing times?",
    "body": "Organizational change can feel overwhelming, but with the right mindset and strategies, you can thrive\u2014even in times of uncertainty. Whether it\u2019s a restructuring, a leadership shift, or new priorities, the way you navigate change will shape your career trajectory. Having recently gone through a year of multiple larger-scale changes, I've had some thoughts that I wanted to share. Here are some practical insights to help you adapt and grow. Focus on what matters: your colleagues We do our best work for the colleagues we care about. When things feel chaotic, lean into this mindset: block out the noise, and focus on the relationships that matter. Here\u2019s the reality: companies are bound by rules and regulations that often prevent them from fully prioritizing your well-being. These can be things like accounting timelines and investor pressure, things that are entirely out of your control. Your colleagues, on the other hand, can be your greatest allies. By investing in these relationships, you\u2019ll create a network of support that will carry you through tough times. Keep shipping great work Amid change, the best way to stand out is by continuing to deliver. Roll up your sleeves and keep shipping awesome work. Consistently delight your customers, whether internal or external. Here\u2019s why: people notice. When you show that you\u2019re steady and reliable, even in uncertain times, you\u2019ll earn respect and trust. And that kind of reputation can open doors\u2014both within your company and beyond. I think we have something to learn from President Obama: Build your own optionality Right from day one of your job, don\u2019t wait for your manager to carve a path for you. Be proactive: - Build relationships across teams and departments. - Stay visible by sharing your work and offering to help others. - Keep nurturing your external network\u2014opportunities often come from unexpected places. By taking charge of your own opportunities, you\u2019ll have more options to pivot when changes arise. Stay grounded in the present \u201cDo not worry about tomorrow, for tomorrow will worry about itself.\u201d If this quote resonates with you, you already know its source\u2014and if not, a quick search will uncover its wisdom. The message is simple: don\u2019t get paralyzed by what-ifs. Focus on what you can control today. By staying present and calm, you\u2019ll be better equipped to handle whatever comes next. Think long-term: cultivate your \"career committee\" One of the best ways to navigate change is to have a trusted group of advisors guiding you. Think of it like a PhD committee\u2014but for your career: - Identify 3\u20134 people whose opinions you trust. - These could be mentors, peers, or leaders who have a clear view of the organization. - Check in with them periodically to get advice, stay aligned, and anticipate changes before they happen. Having this \u201ccareer committee\u201d will help you make more informed decisions and ensure you stay on track, even as the organization evolves. Conclusion Organizations are made in the image of their leaders. Which means there is no right or wrong organizational structure ever, there's just the one one that works for the leader in charge. When leadership changes, organizations will change. This is a fact of work life, and the earlier that we are accustomed to this fact, the better. Change is inevitable, but how you respond to it is what sets you apart. By focusing on relationships, delivering great work, and staying proactive about your career, you can turn organizational shifts into opportunities for growth!",
    "tags": [
      "professional growth",
      "leadership",
      "relationships",
      "networking",
      "organizational change",
      "professional development"
    ],
    "pub_date": "2024-12-17",
    "type": "blog"
  },
  {
    "id": "blog-5-retrieval-strategies-to-boost-your-rag-systems-performance",
    "url": "/blog/2024/12/16/5-retrieval-strategies-to-boost-your-rag-systems-performance/",
    "title": "5 retrieval strategies to boost your RAG system's performance",
    "summary": "In this blog post, I provide an overview of retrieval methods for Retrieval-Augmented Generation (RAG), exploring various methods like human-curated, exact keyword search, fuzzy keyword search, vector similarity search, and knowledge graph-based retrieval. Each method is dissected to reveal its unique strengths and ideal use cases, providing insights into how they can enhance RAG systems' performance. Curious about how these strategies can be combined for even more robust results?",
    "body": "import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@11/dist/mermaid.esm.min.mjs'; mermaid.initialize({ startOnLoad: true }); Retrieval-Augmented Generation (RAG) is gaining traction as a powerful paradigm for large language models (LLMs). At its core, RAG combines two key processes: retrieval, which fetches relevant information from an external source, and generation, where the model uses that information to produce a response. This allows systems to provide more accurate, context-aware answers beyond what the model was trained on. In Jason Liu's blog post, RAG is more than just embedding search, he makes a compelling argument: Retrieval in RAG systems goes far beyond vector similarity searches. Expanding on this idea, we'll explore a basic ontology of retrieval methods, breaking down their features, use cases, and practical examples. Methods of retrieval Retrieval methods in RAG can be broadly categorized based on their level of automation and how they interact with data. Below is a table summarizing the key approaches: | Method Name | Automation | Example | |---------------------------------|------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------| | Human-Curated | Manual | Humans copy/paste known context into ChatGPT\u2019s chat box, which is then used to generate a response. Lack of automation makes this non-scalable | | Exact Keyword Search | Automated | Use a bot to extract or enhance a query with keywords, which are in turn used to retrieve texts with exact keyword matching. | | Fuzzy Keyword Search | Automated | Using keywords, perform fuzzy text search of each term to identify which texts to return. | | Vector Similarity Search | Automated | Using the full-length query, or enhanced versions of the query, identify either the top K text chunks with embedding vector similarity or those above a threshold. | | Knowledge Graph-based Retrieval | Automated | Using information on a knowledge graph, retrieve text chunks based on graph structure, statistics, or information. | Each method has unique strengths and weaknesses, and understanding these nuances is key to designing an effective RAG system. Deep dive into retrieval methods Human-Curated Human-curated retrieval involves manually gathering the necessary context for a query. The user selects specific information from a source, such as copying a definition or relevant text, and supplies it directly to the system. Example: A user researching quantum mechanics copies a specific definition of \"quantum entanglement\" into ChatGPT before asking follow-up questions. The context is manually curated and acts as the retrieval mechanism. graph TD; User[User Curates Context] --> ChatGPT[ChatGPT]; ChatGPT --> Response[Generate Response Based on Context]; LlamaBot's documentation writer uses this in the form of pointers to source code: When to Use: This method excels in scenarios where the user has domain knowledge and can curate precise context manually. However, it doesn't scale well for larger datasets or automated systems. Exact Keyword Search Exact keyword search relies on identifying specific words or phrases in the query and using them to retrieve matching documents. It is a straightforward method that works well for structured data and predefined terminologies. graph TD; Query[User Query] --> Keywords[Extract Keywords]; Keywords --> Database[Search Database]; Database --> Results[Exact Matches Returned]; Example: A chatbot receives the query \"Python error handling.\" The system extracts the keywords \"Python\" and \"error handling\" to retrieve matching documentation. Within LlamaBot, you can stuff texts into a lightweight BM25DocStore and perform keyword-based retrieval in memory: When to Use: This is effective for structured, well-tagged datasets where precision is key. However, it can miss relevant results if keywords are absent or phrased differently. Fuzzy Keyword Search Fuzzy keyword search extends exact search by allowing for minor variations, typos, or alternate phrasings in the query. It's particularly useful for dealing with noisy or unstructured data. graph TD; Query[User Query] --> FuzzyLogic[Apply Fuzzy Matching]; FuzzyLogic --> Database[Search Database]; Database --> Results[Fuzzy Matches Returned]; Example: A user searches for \"data cleaning\" in a system, and fuzzy search identifies relevant results like \"data preprocessing\" or \"cleaning datasets.\" When to Use: Useful for noisy or unstructured datasets. Fuzzy searches are tolerant to typos and alternate phrasings but may retrieve less precise results. Vector Similarity Search Vector similarity search uses dense embeddings to compare the semantic meaning of a query with chunks of text. This method is particularly popular in modern systems because it captures semantic relationships beyond exact keyword matches, making it highly effective for natural language understanding. Unlike k",
    "tags": [
      "retrieval augmented generation",
      "keyword search",
      "fuzzy search",
      "vector search",
      "knowledge graph",
      "large language models"
    ],
    "pub_date": "2024-12-16",
    "type": "blog"
  },
  {
    "id": "blog-how-llamabots-new-agent-features-simplify-complex-task-automation",
    "url": "/blog/2024/12/15/how-llamabots-new-agent-features-simplify-complex-task-automation/",
    "title": "How LlamaBot's new agent features simplify complex task automation",
    "summary": "In this blog post, I explore the innovative features of LlamaBot's new AgentBot, designed to simplify complex task automation. These agents operate with goal-oriented non-determinism, decision-making flow control, and natural language interfaces, making them powerful yet user-friendly. I also provide real-world examples, including a detailed walkthrough of a stock market analysis. Curious about how these agents can streamline your workflows and enhance the flexibility of your LLM applications?",
    "body": "A Pythonic way to build agents The concept of \"agents\" has been swirling in the AI space for a while now, sometimes polarizing opinion. Some see agents as the next logical step in automating complex tasks, while others dismiss them as over-hyped, glorified \"just programming.\" I believe the truth lies somewhere in between\u2014agents are neither magical nor trivial. Instead, they're tools that, when well-designed, bring immense utility to the table. With LlamaBot's new agent capabilities, I aimed to create an approachable yet powerful implementation for building agents. This post dives into the core ideas that underpin these new features, how they simplify agent construction, and why I think they strike a balance between complexity and usability. What makes an agent? Let's start with the fundamentals. What exactly is an agent? My working definition is as follows: An LLM agent is a system that demonstrates the appearance of making goal-directed decisions autonomously. It operates within a scoped set of tasks it can perform, without requiring pre-specification of the order in which these tasks are executed. Three attributes set it apart: 1. Goal-oriented non-determinism: The agent makes decisions dynamically rather than following a pre-defined sequence of actions. 2. Decision-making flow control: Decisions adapt based on context, resembling a \"thinking\" process. 3. Natural language interfaces: Inputs and outputs are in plain language, making interactions feel intuitive. These attributes combine to give agents their apparent autonomy. With these principles in mind, I designed LlamaBot's to make creating and deploying agents simple, flexible, and powerful. The heart of LlamaBot's new AgentBot Decision-making with tools At its core, the orchestrates tools\u2014Python functions annotated with the decorator. Given a user prompt, the bot selects which tools to use and in what order, allowing for dynamic workflows. This decision-making process is powered by an LLM and is wrapped in a structured system prompt that guides the agent toward making the right choices. For example, consider the simple task of splitting a restaurant bill: Here, the bot dynamically decides which tool to use based on the input. For a prompt like \"My dinner was 2300 without tips. Calculate my total with an 18% tip and split the bill between 4 people,\" the bot intelligently selects both and , chaining them together to provide a complete answer: The total bill with an 18% tip is 2714.00, and when split between 4 people, each person should pay 678.50. Real-world example: Stock market analysis LlamaBot's new makes it easy to perform multi-step tasks like stock market analysis. Here's an example where I created an agent to: 1. Scrape stock prices, 2. Calculate percentiles, 3. Detect outliers, and 4. Summarize statistical insights. With this setup, you can prompt the bot with natural language, like: \"Analyze the last 100 days of MRNA stock prices\", and receive detailed, actionable insights in plain English. Code implementation Here's how the was constructed for this task: For the prompt \"Analyze the last 100 days of MRNA stock prices\", the bot responds: Key features of the new AgentBot (a) Caching intermediate results:The agent caches results like numerical arrays, reducing redundant computation and saving time and cost. Results are stored in a memory dictionary indexed by their SHA-256 hash keys, ensuring efficient reuse of previously computed outcomes and cost savings on output token usage. (b) Error handling as a first-class citizen: Errors are handled gracefully, guiding users without derailing workflows. The tool, which is built-in to the agent, allows the agent to identify issues explicitly, providing developers with actionable feedback. (c) Dynamic tool selection: The agent adapts its actions based on context and task complexity. It leverages a decision-making model that determines the most relevant tools to call and uses cached results dynamically where appropriate. (d) High-level APIs for streamlined usage: The API design ensures readability and ease of use. By combining Pydantic models for structured inputs and outputs with well-annotated Python functions, developers can build powerful agents with minimal boilerplate. Conclusion: Why agents matter LlamaBot's new agent capabilities offer developers a practical toolset for managing complex workflows with ease. Rather than reinventing the wheel, focuses on integrating thoughtful design and simplicity, helping users harness the strengths of existing frameworks while tailoring solutions to specific needs. To explore these features further, check out the LlamaBot documentation. Let's build something amazing together!",
    "tags": [
      "agents",
      "llamabot",
      "automation",
      "interface",
      "python",
      "analysis",
      "workflow"
    ],
    "pub_date": "2024-12-15",
    "type": "blog"
  },
  {
    "id": "blog-a-modest-proposal-for-data-catalogues-at-biotechs",
    "url": "/blog/2024/11/22/a-modest-proposal-for-data-catalogues-at-biotechs/",
    "title": "A modest proposal for data catalogues at biotechs",
    "summary": "Building data platforms at biotechs often fails because we ask scientists to change their workflow and manually catalog data. This leads to poor adoption, wasted engineering effort, and continued data accessibility problems. Instead of building new systems, I propose automatically capturing data sharing patterns that already exist. This approach: - Reduces implementation costs by 60-80% compared to traditional platforms - Requires zero change in scientist behavior - Creates an automatically-maintained data catalog - Enables rapid data discovery through social connections - Can be implemented incrementally, showing value within 3-6 months",
    "body": "The fatal flaw Having now worked at two companies (Moderna and Novartis) and discussed the matter with friends across the industry, I have noticed a fatal flaw in data platform strategy that I believe makes adoption very challenging from the beginning. Here's the flaw: asking people to invest additional time in submitting data, managing permissions to it, cataloguing it, and discovering it on a newly built system instead of leveraging the existing ways of working that people already have. The build-out invariably looks like this: a data platform team builds new software that sits in the cloud. The product team, being product-driven, starts asking for \"use cases\". The identified customers, i.e. the bench scientists, aren't exactly thrilled, even if they understand the importance. There's an eye of skepticism. Your PhD-trained scientists, who may or may not necessarily be computationally-savvy, give you that look that communicates one or more of the following questions: - Storage: Now where do I actually store my data? Do I still get to keep it in SharePoint/OneDrive/Dropbox? Or is it now on your platform, in AWS/S3, where I don't get to have control over it? - Permissions: Where do I manage permissions? On your new data platform? Why do I need to learn how to do that, when I can just use SharePoint/OneDrive/Dropbox? Do I really have to learn AWS? - Cataloguing: Sure, I get the idea of \"discoverability\" and \"reuse\" and \"FAIR\" principles, but what benefit does it give me today? If someone else needs to ask me for data, won't it just increase my burden as an access gatekeeper? - Discovering: Yeah, okay, so I can go to your platform, or I can just ask my social network of people at the company, right? Are you saying you're building a search engine for data across the company? The team looks at those questions, but being digital folks with an incentive to build a product, they don't exactly have a satisfactory answer for each question. The team instead sets success criteria that doesn't exactly resonate with the customers: data are migrated from existing system to shiny new data platform, and computational scientists can access it. The team then does a lift and shift of some bulk RNAseq data from three experiments that were run before. Metrics are calculated: \"We moved hundreds of gigabytes of data off of on-prem storage into the cloud, saving the company $X amount over three years!\" The data platform team calls this a success and moves onto the next use case. No new RNAseq data are uploaded to the shiny new data platform because, well, the sequencing core was just overwhelmed with other work and didn't want the additional burden of cataloging and tracing where data came from, and just wanted to have automation that piped RNAseq data back to their collaborators for downstream analyses. So herein lies a massive problem with this approach: How are you going to get buy-in for data cataloguing if the data catalogue you build doesn't actually benefit your customer? Your \"buy-in\" is going to be half-baked. Focusing on the laboratory scientist as the customer, but asking them to do additional work, is laying the groundwork for building a product that allows individuals to fail upwards. And that's just a waste of money. My modest proposal My proposal starts with this: your data platform should start as the exhaust pipe of data sharing interactions throughout the company. The files that are actively being shared are probably the ones that need storage, cataloguing, and permissions the most. These are also the ones with the highest amount of current institutional knowledge present. Backing up, cataloguing, and maintaining a record of data production and consumption, should be done in the background with zero intervention from anyone. My proposal continues with a different definition of the customer: Start with your computational & data scientists! They are the ones who are wrangling, well, data. Build tooling for them that makes it easier for people to access files directly on the platforms that are already being used, so they never need to ask collaborators to send files over. Make it easy for them to write code like this: Then, use that same toolset to ensure that those files are being version controlled in a backup location on the cloud. Within that toolset, inject the code necessary to log the fact that: - Amy Landry, the computational scientist, asked Brendan Lee, the laboratory scientist, for access to - , - with SHA256 hash (for versioning purposes) - on date , Here, the source of would have the following signature, with an emphasis on the decorator: Along the way, within the data access tooling, can trigger an automatic backup of that file to a centralized buckets in the cloud, and create or update a catalogue entry based on the hash of the file and the filename, with known access permissions accurately logged. Your data catalogue now has automatic entries being generated with no effort on anybody's part whatsoever",
    "tags": [
      "strategy",
      "cloud adoption",
      "data catalog",
      "data discovery",
      "data scientist",
      "biotech",
      "data governance",
      "social graph"
    ],
    "pub_date": "2024-11-22",
    "type": "blog"
  },
  {
    "id": "blog-deploying-ollama-on-modal",
    "url": "/blog/2024/11/14/deploying-ollama-on-modal/",
    "title": "Deploying Ollama on Modal",
    "summary": "In this blog post, I share my journey of deploying Ollama to Modal, enhancing my understanding of Modal's capabilities. I detail the script used, the setup of the Modal app, and the deployment process, which includes ensuring the Ollama service is ready and operational. I also implement an OpenAI-compatible endpoint that makes it easy to use the deployment with existing tools and libraries. This exploration not only expanded my technical skills but also created a practical solution for using open-source models in production. Curious about how this deployment could streamline your projects?",
    "body": "I recently learned how to deploy Ollama to Modal! I mostly copied code from another source but modified it just enough that I think I have upgraded my mental model of Modal and want to leave notes. My motivation here was to gain access to open source models that are larger than can fit comfortably on my 16GB M1 MacBook Air. Credits In this case, I feel obliged to give credit where credit is due: - The Modal Blog has a lot of great resources. - The original code by Irfan Sharif was great for my learning journey. The script If you're here just for the code, then you'll want to check out my modal-deployments repository! I have also embedded the code below for reference: python MODEL = os.environ.get(\"MODEL\", \"gemma2:27b\") DEFAULTMODELS = [\"gemma2:27b\"] python def pull() -> None: subprocess.run([\"systemctl\", \"daemon-reload\"]) subprocess.run([\"systemctl\", \"enable\", \"ollama\"]) subprocess.run([\"systemctl\", \"start\", \"ollama\"]) waitforollama() subprocess.run([\"ollama\", \"pull\", MODEL], stdout=subprocess.PIPE) python def waitforollama(timeout: int = 30, interval: int = 2) -> None: \"\"\"Wait for Ollama service to be ready. :param timeout: Maximum time to wait in seconds :param interval: Time between checks in seconds \"\"\" import httpx from loguru import logger starttime = time.time() while True: try: response = httpx.get(\"http://localhost:11434/api/version\") if response.statuscode == 200: logger.info(\"Ollama service is ready\") return except httpx.ConnectError: if time.time() - starttime > timeout: raise TimeoutError(\"Ollama service failed to start\") logger.info( f\"Waiting for Ollama service... ({int(time.time() - starttime)}s)\" ) time.sleep(interval) python image = ( modal.Image.debianslim() .aptinstall(\"curl\", \"systemctl\") .runcommands( \"curl -L https://ollama.com/download/ollama-linux-amd64.tgz -o ollama-linux-amd64.tgz\", \"tar -C /usr -xzf ollama-linux-amd64.tgz\", \"useradd -r -s /bin/false -U -m -d /usr/share/ollama ollama\", \"usermod -a -G ollama $(whoami)\", ) .copylocalfile(\"ollama.service\", \"/etc/systemd/system/ollama.service\") .pipinstall(\"ollama\", \"httpx\", \"loguru\") .runfunction(pull) ) python class ChatMessage(BaseModel): role: str content: str class ChatCompletionRequest(BaseModel): model: Optional[str] = Field(default=MODEL) messages: List[ChatMessage] stream: bool = Field(default=False) python @api.post(\"/v1/chat/completions\") async def v1chatcompletions(request: ChatCompletionRequest) -> Any: python @app.cls( gpu=modal.gpu.A10G(count=1), containeridletimeout=10, ) class Ollama: bash modal deploy endpoint.py python import llamabot as lmb bot = lmb.SimpleBot( modelname=\"openai/gemma2:27b\", apibase=\"https://.modal.run/v1\", system_prompt=\"You are a helpful assistant.\", ) response = bot(\"Hello!\") ``` This compatibility makes it easy to experiment with different open-source models while maintaining compatibility with existing tools and workflows.",
    "tags": [
      "modal",
      "deployment",
      "open source",
      "api",
      "cloud",
      "gpu",
      "software",
      "models",
      "ollama",
      "large language models"
    ],
    "pub_date": "2024-11-14",
    "type": "blog"
  },
  {
    "id": "blog-disposable-environments-for-ad-hoc-analyses",
    "url": "/blog/2024/11/8/disposable-environments-for-ad-hoc-analyses/",
    "title": "Disposable environments for ad-hoc analyses",
    "summary": "In this blog post, I explore the innovative 'juv' package, which simplifies Python environment management for Jupyter notebooks by embedding dependencies directly within the notebook file. This approach eliminates the need for separate environment files, making notebooks easily shareable and reducing setup complexity. I also discuss integrating 'juv' with 'pyds-cli' to streamline ad-hoc data analyses within organizations, enhancing reproducibility and reducing environment conflicts. Curious about how this could change your data science workflow?",
    "body": "I recently learned about , a package by my SciPy friend Trevor Manz. It builds upon , which, alongside , are the hottest things in Python (and more broadly, Data Science) tooling since officially replaced as the de facto way to install Python packages. supports PEP723, which is a Python enhancement proposal (PEP) that specifies a metadata format that can be embedded in single-file Python scripts to assist launchers, IDEs and other external tools which may need to interact with such scripts. Made more concretely for the purposes of this post, this enables us to specify the Python version that is supposed to be used to execute a script, and, more importantly, the dependencies that are needed! It basically looks like this, as the first cell in your notebook, say, The following cells are the notebook code that you write in a normal fashion. One can then use to run a notebook server with that auto-created Python environment: And will auto-install the dependencies: You'll get this notebook server, in which you can run the code, just like in regular JupyterLab. Compared to writing out a full , , or file, this reduces the burden of distributing code + environments to a single file, your notebook. In terms of mental overload, for an ad-hoc analysis conducted within a single notebook, this is much better than needing to work within, say, a environment that is maintained to support multiple notebooks. In the words of the folks: In other words, marimo makes it possible to create and share standalone notebooks, without shipping files alongside them. This provides two key benefits: 1. Notebooks that carry their own dependencies are easy to share \u2014 just send the file! 2. Isolating a notebook from other installed packages prevents obscure bugs arising from environment pollution, while also hardening reproducibility. To paraphrase 's README (which is accurate as of 5 Nov 2024), this is the equivalent of having a fully-specified but also disposable environment, in which the notebook itself specifies its own environment. (I'll note that the notebook also supports PEP723 as well!) Incorporating this into pyds-cli As a prototype of how this would work within company-internal tooling, I wrapped inside as part of a new and set of commands. does what my friend Logan Thomas gave as early feedback for 's when I put together an initial project structure scaffold -- he wanted to see a \"minimal initialization\". Until PEP723 became a reality, I didn't have a good story for that. Now we do! To use to initialize a repo that contains only ad-hoc notebooks, we do: This will trigger a form that asks you for some information: With the being the name of the folder, you can see that it is initialized with a directory and a contains a section for configuring with default packages that should be specified in every new notebook created using commands: Using , we can use the command, which basically creates a new notebook in the directory with the name that you specify, and additional packages on top of the default packages: Actually running it as an example: We get a new notebook that I can preview: As you can see, we have the default packages but also , , and installed. Underneath the hood, is doing most of the magic of adding dependencies, but simply composes in the ability to read from . In a world where company-specific tooling were built, such tooling might also include automatically configuring keyrings for authentication on every call. All in all, I very much like the idea of configurable and disposable (but nonetheless configurable) environments. helps support this idea! It essentially takes what would do and apply it to Jupyter notebooks, auto-creating the necessary kernel behind-the-scenes. My wrapping of within merely has to add in opinionated configuration; the rest of it is a near one-to-one mapping. I like where the Python tooling ecosystem is headed, the future is exciting!",
    "tags": [
      "python",
      "tooling",
      "data science",
      "notebook",
      "reproducibility",
      "juv",
      "uv",
      "environment management",
      "scripts",
      "analysis"
    ],
    "pub_date": "2024-11-08",
    "type": "blog"
  },
  {
    "id": "blog-introducing-new-local-llamabot-logging-features",
    "url": "/blog/2024/11/2/introducing-new-local-llamabot-logging-features/",
    "title": "Introducing new (local) LlamaBot logging features",
    "summary": "In this blog post, I share the latest updates to LlamaBot, including automatic logging of LLM interactions, version-controlled prompt logging, and a new web-based UI for visualizing these logs. These features enhance prompt analysis and make prompt engineering more intuitive and data-driven. Additionally, logs can now be exported in OpenAI fine-tuning format for easier sharing and integration. If you're keen on refining your LLM interactions and prompt crafting, these tools might be just what you need. Curious to see how these new features can streamline your workflow?",
    "body": "Recently, I've added some exciting new features to LlamaBot that focus on keeping track of every interaction and making it easy to understand the history of prompt evolution. Here's a look at what's new: Automatic logging of LLM interactions One of the biggest improvements is that every LLM interaction is now automatically logged to a local SQLite database. Whether you're using LlamaBot to generate text, answer questions, or anything else, these calls are captured without needing manual intervention. This means you have a complete history of your LLM interactions for later review or analysis\u2014 all stored locally, making it easy to access without additional cloud dependencies. Version-controlled prompt logging Another key addition is automatic version logging of prompts. Whenever you make changes to a prompt, LlamaBot now tracks the version history. This lets you trace back and understand how prompts evolved over time, providing valuable insights into prompt iterations and improvements. Each version is logged with a unique identifier, making it simple to reference past versions for reuse or comparison. Web-based UI for prompt and message visualization I've also added a new command, , that starts a web-based UI for exploring logged interactions. This interface allows you to load and view detailed logs, including prompts, responses, and their versions, all stored in the SQLite database. The interface features an interactive log table where you can filter interactions by function name, timestamp, or model version, giving you a powerful way to sift through data efficiently. You can visualize changes in prompts using version history tracking, which displays diffs between different versions, making it clear how each prompt evolved. Additionally, the UI enables you to inspect how different parameters, such as model type or temperature, affected the responses, helping you refine your approach systematically. (These were shown in the screenshots above!) To launch it, you do: This visualization tool is designed to be an useful tool for anyone wanting to dive deep into prompt analysis, refine their approaches, and make prompt engineering both intuitive and data-driven. By providing rich visual comparisons, filtering options, and an easy-to-use interface, this tool empowers you to iterate faster and more effectively. Exporting logs in OpenAI format To make working with your logged interactions even more flexible, you can now export the logs in OpenAI format directly from the web-based UI. This feature is particularly useful if you want to share the logs with collaborators, integrate them into other workflows, or simply keep an external backup. The OpenAI export captures all the key details of your LLM interactions, providing a structured, human-readable representation. Support for additional formats, such as those used for fine-tuning Llama, is planned for future updates, making it even easier to use your data in different contexts, such as when you want to fine-tuning an LLM to produce a cheaper LLM that performs well on your specific task. Every LLM call is valuable, don't let them go to waste! Conclusion If you're interested in digging deeper into your LLM interactions or improving your prompt crafting through detailed iteration tracking, I think you'll find these new features super useful. Feel free to give them a try and let me know your thoughts! P.S. As always, all of these were coded up using AI assistance in Cursor! And with GitHub Copilot Edit's release, we finally have competition in the space of AI-assisted code generation -- it's an exciting time to be a coder!",
    "tags": [
      "llamabot",
      "logging",
      "software development",
      "llm",
      "large language models",
      "web development",
      "version control"
    ],
    "pub_date": "2024-11-02",
    "type": "blog"
  },
  {
    "id": "blog-the-human-dimension-to-clean-distributable-and-documented-data-science-code",
    "url": "/blog/2024/10/25/the-human-dimension-to-clean-distributable-and-documented-data-science-code/",
    "title": "The Human Dimension to Clean, Distributable, and Documented Data Science Code",
    "summary": "This blog post, which is my pyOpenSci Fall Training keynote, explores the importance of creating clean, distributable, and well-documented data science code, emphasizing the human dimension of coding practices. In it, I discuss key concepts such as readability, cognitive load, and the toolmaker's mindset, and provide practical insights on how to make code more accessible and impactful for both the creator and other users. I also touch on the role of AI in coding and documentation.",
    "body": "Note: This blog post is my keynote talk for the pyOpenSci Fall Festival keynote. In lieu of making slides and presenting that way, I decided to write a blog post to (a) leave attendees with an artifact that can be referenced, (b) provide pyOpenSci with content that can be hosted on their blog, (c) leave myself an artifact for my own blog, and (d) do it in a way that is more fleshed out than a presentation deck. Introduction Since 2016 (8 years now!), I've been advocating for data scientists to apply basic software development practices in their work. This means making sure that one's work is well-documented -- and thus easily understandable, working in a way that is portable across machines -- and thus easily accessible, making sure that one's code is modular -- and thus easy to reuse, and ensuring that one's code is well-tested -- and thus reliable. (Table 1) In this post, we'll dive deeper into why it is crucial for making your work impactful to consider the psychology of people who read, install, and use your work. We'll explore the \"why\" behind the \"what\" that will be covered in the upcoming pyOpenSci training course. This training course will be teaching you a lot of valuable skills - the \"what to do\" - and I'd like to help reinforce the \"why\" behind all of these. By understanding the reasoning and motivation behind these practices, you'll be better equipped to apply them effectively and adapt them to your specific needs in data science projects. | Practice | Benefit | |--------------------------|-----------------------| | Well-documented | Easily understandable | | Portable across machines | Easily accessible | | Modular code | Easy to reuse | | Well-tested code | Reliable | Table 1: Key practices and their benefits in data science code development. Key concept 1: readability and cognitive load When we write code, it's easy to forget that we're not just communicating with machines, but with other humans as well - including our future selves. This principle is deeply ingrained in Python's philosophy, as expressed in \"The Zen of Python\" which states, \"Readability counts.\" Python's emphasis on readability over performance underscores the importance of human-friendly code. Readability is crucial because it directly impacts the cognitive load placed on anyone trying to understand or use your code. The importance of readability Readable code offers several significant benefits. First and foremost, it enables faster comprehension. When code is clean and well-structured, readers can quickly grasp its purpose and functionality, allowing them to understand the logic and flow more efficiently. This leads to the second advantage: easier maintenance. Code that is easy to read is also easier to debug, update, and extend, saving valuable time and resources in the long run. Furthermore, readable code greatly improves collaboration! When team members can easily understand each other's code, it fosters better teamwork and enables them to build upon each other's work more effectively. This synergy can lead to more innovative solutions and faster project completion. Lastly, clear and readable code reduces the likelihood of errors. When code is easy to interpret, there's less chance of misunderstanding its intent or functionality. This clarity minimizes the risk of introducing bugs during modifications or when integrating with other parts of the system, ultimately leading to more robust and reliable software. A cautionary tale from my graduate school days During my graduate studies, I developed a complex data analysis project for identifying putative reassortant influenza viruses from the Influenza Research Database (IRD). While the algorithm was scientifically sound and potentially valuable, its implementation left much to be desired from a software engineering perspective. The project consisted of more than 10 separate Python scripts that had to be executed in a specific order. There was little to no documentation, and the code itself was not written with readability or maintainability in mind. You can find the original code repository here to see an example of what not to do. The project had several significant drawbacks: - Lack of modularity: The scripts were tightly coupled, making it difficult to reuse or modify individual components. - Poor documentation: Without clear documentation, understanding the purpose and workflow of each script was challenging. - Difficult deployment: The complex execution order and tight coupling to a GridEngine HPC cluster made it nearly impossible for others to deploy or use the tool effectively. The consequences of this poor code quality became apparent when the IRD team expressed interest in incorporating my algorithm into their database. I remember being engaged with them in discussion, and was excited to see my work potentially being used by researchers around the world. However, the barrier to deployment was immense due to the scrappy implementation and lack of documentation. While",
    "tags": [
      "data science",
      "coding",
      "documentation",
      "readability",
      "distribution",
      "best practices",
      "cognition",
      "tool making"
    ],
    "pub_date": "2024-10-25",
    "type": "blog"
  },
  {
    "id": "blog-cursor-did-a-one-shot-rewrite-of-a-panel-app-i-built",
    "url": "/blog/2024/10/20/cursor-did-a-one-shot-rewrite-of-a-panel-app-i-built/",
    "title": "Cursor did a one-shot rewrite of a Panel app I built",
    "summary": "In this blog post, I share my experience using Cursor to transform a sluggish Panel app into a sleek HTMX + FastAPI application. I detail how AI-assisted coding not only sped up the process but also enhanced my understanding of web development. Despite my limited skills in this area, the AI tools helped me prototype faster and learn valuable lessons. I also reflect on the importance of critically assessing AI-generated code to improve one's coding instincts. Curious to see how it turned out and what insights I gained?",
    "body": "Today, I used Cursor to convert a Panel app into HTMX + FastAPI and it was pretty amazing sight to see it in action. Blogbot: My home-built tool for writing social media posts I built blogbot because I'm too lazy to write social media posts. I'd rather it be a one-click affair. So I did what any good software developer does when they need to automate a task: build a tool to automate it. Was it worth the time? I'll let XKCD speak for me: In any case, the UI I built was in Panel, and it looks like this: As you can tell, with one click of a button, I can grab the contents of my blog and use it to synthesize LinkedIn posts, Twitter posts, Substack (newsletter) posts, summaries, tags, and banner images. That said, the Panel app always felt a little sluggish relative to other FastAPI + HTMX apps that I'd built before. So the thought came to mind: what if I rewrote it? One-shot rewrite So I did this: I pointed Claude 3.5 sonnet to the notebook in which I implemented the Panel app and gave it the following instruction: ok, so I think I would like to convert app.ipynb into an HTMX + FastAPI app. Can you help me make this happen? For context, this is what the app looks like. (screenshot pasted, which is what I had above) Claude went to town! Within one prompt, I had all 5 necessary files created on the filesystem. Here's a screenshot of the Composer's state when I ran it: I stood up the FastAPI web server, and voila: And this is what it looks like with content rendered: Finally, this is what it looks like with banner images: Reflections In the month that I've subscribed to Cursor, I've gradually gotten used to how to use it as a tool to 10X my ability to produce prototypes. (I'm being careful with my words here -- I wouldn't go so far as to claim a 10X productivity boost, but I am much faster at standing up an MVP.) This was yet another example of how using AI-assisted coding got me where I wanted in a very short amount of time. I remember my reaction when I first saw the UI, and it was one of massive surprise! This being the third full-stack app that I've built using AI-assisted coding, I'm starting to see a pattern of usage that I think is a productive path forward. For someone who is in my shoes -- skilled programmer but not necessarily in one sub-genre of programming (web development), AI-assistance is a great way to advance a prototype forward. But being relatively unskilled in web development, there are many patterns of coding that I'm unaware of, unlike in regular Python programming for data science. For example, what is a better pattern for interfacing with a database -- through raw SQL or through an ORM? If I'm using HTMX with Jinja templates, how much data processing logic should be shoved into the Jinja template and how much should be handled by HTMX instead? These are questions for which I have no strong patterns to anchor my review of the code, and I risk AI-generated code biting me back later in the future. But on the flip side, by reviewing AI-generated code, and doing personal introspection about what is difficult to handle in that code, I'm also sharpening my own sense of taste. It's this last point that I want to provide an admonishment on: if you omit the step where you critically assess AI-generated code, you pass up the chance to skill up! And again on the positive side, LLMs are producing code that does follow more or less sane practices left on the internet by other coders. By critically assessing AI-generated code and applying principles that I've learned writing programs before, I can, nonetheless, build and refine my judgment calls on what to accept and what to reject.",
    "tags": [
      "coding",
      "fastapi",
      "htmx",
      "automation",
      "design",
      "prototype",
      "development",
      "patterns",
      "assistance",
      "productivity"
    ],
    "pub_date": "2024-10-20",
    "type": "blog"
  },
  {
    "id": "blog-keys-to-effective-collaborative-data-science",
    "url": "/blog/2024/10/18/keys-to-effective-collaborative-data-science/",
    "title": "Keys to effective collaborative data science",
    "summary": "In this blog post, I discuss enhancing collaboration between data scientists and non-computational colleagues to ensure impactful work. I emphasize the importance of standardizing code initialization, involving non-coders in code reviews, empathetic communication across roles, and ensuring work is runnable on any computer. These practices aim to bridge gaps, reduce cognitive load, and foster mutual appreciation of challenges faced in data science projects. How can these strategies be implemented in your team?",
    "body": "Note: This is a blog post version of a guest lecture that I gave at Duke University on 16 October 2024 to a class taught by Brian Lerner. I've been working as a data scientist in the life sciences space for ~7 years now; ~10 years if you count my graduate school days. Over the years, I've observed how the absence of standardized practices can cause our work to fade away once we leave a company. I've also witnessed the friction that can arise between laboratory teams\u2014the data producers\u2014 and computational teams\u2014the data consumers\u2014 often stemming from a lack of empathy for each other's challenges. Today, I want to share some thoughts on how we can improve our collaborative efforts between data scientists and non-computational colleagues and ensure our work has a lasting impact. My exhortations for today are simple: 1. Bring structure to how you work individually. 2. Bring empathy to how you work collaboratively. 3. Bring clarity to how you work! Towards this end, I have four ideas to share: 1. For quantitative-types, standardize how you initialize your code repositories. 2. Introduce code reviews, especially involving non-coders where possible. 3. Communicate across roles with empathy. 4. Ensure your work is runnable on any computer. Let's dive in to each of these ideas, and how they relate to the three exhortations. Standardize how you initialize your code repositories Standardizing the way you initialize code repositories for new projects is more important than you might think. It reduces the cognitive load required to find what you need, making it easier for someone else to onboard onto the project in the future. It also helps you regain contextual knowledge when you return to a project after some time. Moreover, adding automation for standardized needs can streamline your workflow significantly. How can you do this? Start by using template repositories, such as those that your instructor Brian might have provided you. And if you're not already using one, feel free to design one collaboratively with your teammates! To put these ideas into action, I've built tools like that help you get started with a standardized yet flexible project structure. For instance, in , all projects are structured with a view towards becoming a Python package that can be distributed, and include things like a source directory, tests, a pyproject.toml configuration file, a notebooks directory, and a docs directory. But you don't have to start using all of these directories right from the get-go! Rather, you can start within the directory and leverage other tooling later. At where I work, we also add additional automation, such as installing the computational environment using standardized tools like Pixi or Conda/Mamba. We also install automatic code checkers, which reduce the cognitive load during setup and help put lightweight guardrails in place. Every place where we can add automation and reduce cognitive load during setup is another way to increase the odds that someone else can pick up your project and run with it. Introduce code reviews between coders and non-coders Introducing code reviews is a practical way to foster knowledge sharing between coders and non-coders. The dialogue that ensues during a code review increases the odds that implicit knowledge is made explicit. Including non-coders in the code review process elevates appreciation for the behind-the-scenes complexity that coders deal with. Non-coders, please reciprocate! Where possible, involve coders early in the manual data collection process to foster mutual appreciation for the challenges each of us faces. How should you approach code reviews? For the very experienced, asynchronous reviews may suffice. For newcomers, doing this in person can be a great socializing experience. Leave comments and questions about anything that's unclear. Non-coders should feel empowered to ask questions that might seem obvious; this is crucial for drawing out implicit knowledge. Examples of questions to ask during code reviews: - Are there lines of code that might be incorrectly implementing what's intended? - Are there lines where it's unclear why they exist or what they're used for? - Are there hard-coded file paths that prevent the code from being executable on someone else's system? - Is there unnecessary code duplication? - Are convenience functions used appropriately? - Is the code organized so that newcomers can easily find what they're looking for? - Are the names of files, variables, functions, and classes descriptive without being overly verbose? - Is the code following sane patterns? - What is the code intended to do, and is it documented? (Even this question can help draw out implicit knowledge!) Communicate across roles with empathy From what I've gathered, some of you are quantitatively inclined \u2014 doing data analysis and acting as data consumers\u2014 while others are more qualitative, being involved in unstructured data collection process as data producers. Hopefully, bo",
    "tags": [
      "data science",
      "science",
      "collaboration",
      "standardization",
      "empathy",
      "automation",
      "reproducibility",
      "reviews",
      "coding",
      "research"
    ],
    "pub_date": "2024-10-18",
    "type": "blog"
  },
  {
    "id": "blog-what-brings-you-joy-at-work",
    "url": "/blog/2024/10/9/what-brings-you-joy-at-work/",
    "title": "What brings you joy at work?",
    "summary": "In this blog post, I explore what brings me joy at work, highlighting my passion for setting high standards, mentoring, building tools, and engaging in meaningful projects at Moderna. I discuss the satisfaction derived from creating efficient solutions and the thrill of solving complex problems. Conversely, I touch on aspects of my job that are less joyful, like operational tasks and unnecessary complexity, which I strive to minimize through automation and effective processes. What brings you joy at work, and what removes joy from work for you?",
    "body": "In a recent 1:1 conversation with my manager Wade Davis, the question of, \"What brings you joy at work?\" came up. I thought I would pen down everything that came to mind. Things I enjoy doing, in no particular order Setting the standard for how we work: When I first joined Moderna, it was because I saw an opportunity to do what I couldn't do when I was at NIBR: to set a standard, a high minimum bar, for how we work and ship stuff into the hands of the folks we serve. Back in 2021, Moderna was small enough that this was a possibility. With Andrew Giessel and Dave Johnson's support, we were able to set up a standardized repository structure (and evolve it gradually over the years as our environment evolved), define two deployment endpoints (Python packages, and Dockerized CLI applications that run on the web), and setting a standard for the sub-team I lead on how we interview for core competencies. Sharing and mentoring others in sane patterns of work: For those who adopt and embrace the standard ways of working, I enjoy seeing their eyes light up when they realize that years of pain induced by a lack of structure gets cured by introducing structure in their lives. (Yes, I was a Montessori kid, and yes, expensive as it might be, I insist on sending my kids to Montessori school, it's worth it!) Building tools: On the build vs. buy spectrum, I fall very hard on \"build\" unless it's something I genuinely don't understand. I'd rather we simplify business processes and build custom tooling around them than buy tools and introduce complex business processes around them. It brings me joy when things I build help others reclaim the joy of work. Executing on the mission to make science run at the speed of thought and quantify the previously unquantified: The data science work that my teammates and I engage in should result in high power tools that help shortcut experimentation, standardize the way quantification happens, automates repetitive work, and helps us reclaim the joy of work! (Yes, this is a theme!) Bringing people together purposefully: Things like the docathon that we run at work bring me joy when I can marshall resources to purposefully bring folks together. Gathering purposelessly doesn't bring me joy; social events take a drain on my energy. I much prefer coming together to work on a problem and socializing that way. The thrill of solving challenging puzzles en route to solving scientific problems: In making the building blocks of some of our deep neural network models, I've encountered the thrill of (a) figuring out how to express a deep learning operation in elegant boolean logic, (b) finding a simple mathematical approximation after attacking a problem from multiple angles, and (c) figuring out a complex deployment and making it work. When, one day, I figured out, that codon to amino acid translation boils down to a single dot product in tensor land, that was a glorious day! Building tools that have extremely high ROI: When something I build has a very high return on investment, that brings me a ton of joy! If my investment of a few hours at work helps save dozens to hours of others' time, that brings me satisfaction. As I've learned over the years, sometimes, it isn't the fancy and complex solution to a problem that has the highest ROI -- sometimes, building a simple thing is all that's needed. Deep 1:1 conversations with colleagues: I would take these over big social events any day. (Especially the ones that are noisy and have alcohol.) In these 1:1s, it doesn't matter if we're talking about technical matters or philosophical ones, it just needs to be deep and non-superficial. I like podcast-style 1:1s too. I remember talking with a Literature professor over our kids' play date, and when I could ask deep, complex, and probing questions, that was super energizing! Pair coding: Pair coding is a supremely fun activity for me, and it provides me with an opportunity to engage in technical mentorship of others, even those outside of my home team. It remains my favorite mode of socialization as well! Working with tools that provide flexible standardization: As much as this might sound like an oxymoron, it's not. High-power computing tools are insanely flexible, and on top of those, we can build sane patterns of standardization without introducing excessive (or even any) abstractions. Those tools are the ones I enjoy working with the most: slightly opinionated, but mostly gets out of the way. Conversely, tools that restrict my flexibility to work are the tools I would rather we never see. Things that don't bring me joy Since I've written about what brings me joy at work, I thought I would also touch on what does not bring me joy. Some of it is a necessary part of my job, things that my colleague Frank Pickard would say are \"the stuff we get paid to do.\" For those things, I try to automate as much as possible, but it's not always possible. Here's a partial listing of the stuff that doesn't bring me joy. Writi",
    "tags": [
      "joy",
      "standardization",
      "mentorship",
      "building",
      "speed",
      "gatherings",
      "puzzles",
      "investment",
      "conversations",
      "standardization"
    ],
    "pub_date": "2024-10-09",
    "type": "blog"
  },
  {
    "id": "blog-building-pigeon-secure-notes-in-under-15-minutes-of-coding",
    "url": "/blog/2024/10/6/building-pigeon-secure-notes-in-under-15-minutes-of-coding/",
    "title": "Building Pigeon Secure Notes in under 15 minutes of coding",
    "summary": "In this post, I detail my experience building Pigeon Secure Notes, a secure note-sharing app, in under 15 minutes of coding time. I highlight the use of AI-assisted coding tools like Cursor and Claude Sonnet, discuss the challenges faced during development and deployment, and reflect on the benefits and limitations of AI-assisted programming for experienced developers. The post also touches on the importance of practical wisdom in software development and the thrill of building at \"the speed of thought.\"",
    "body": "15 minutes of work This past week, I gave myself a challenge: to replicate Cryptgeon's core functionality as fast as I could using AI-assisted coding. I did it! And I put a demo up at https://pigeon.ericmjl.com. Turns out, I only needed 15 minutes of coding to make it happen. This is how it went. 7 minutes to local development version First off, I opened up Cursor, took a screenshot of Cryptgeon, and prompted Claude Sonnet 3.5 with the screenshot and the following prompt in the Composer UI: I would like to replicate the website in the attached screenshot. It is an app that lets people create encrypted notes that are only viewable X number of times. Use HTMX for the front-end, FastAPI for the back-end, and SQLite for data storage. Caption: Screenshot of Cryptgeon. Cursor went ahead and created 3 (!) files immediately: - , - , - I then hand-configured to provide an command that I could run at the terminal as follows: such that I would be able to run the following command and view the development server: Running that, I was able to get the development server up and running. The UI wasn't stunning, but wasn't shabby either. I wish I had grabbed a screenshot, because this was just 7 minutes of coding to making it happen thus far, but alas, in my excitement I forgot to. 8 minutes to expand app functionality and upgrade the app Cryptgeon is capable of doing more than just one-time notes, however. One can configure the app to provide a time limit for viewing as well. To add that functionality, after work, I took a screenshot of cryptgeon with its advanced settings and prompted: Based on the screenshot, there is one other functionality that I need: to enable users to configure Pigeon's notes to be non-viewable after X minutes OR views. Add this functionality for me. Within a minute, Cursor had modified the relevant files. I double-checked the functionality to make sure it was correct (it was!), and did a, ahem, cursory check on the files to make sure they didn't violate any obvious patterns, and decided it was good enough, and committed the changes. This was 3 minutes total. From developing Shortmail, I picked up quite a number of patterns that I had to figure out the first time round (which is partly why it took 2 hours to get to the first prototype while this app took 15 minutes), such as deciding where to mount static assets, the SQLite database file, and configure relative paths. Having wrestled with these issues in one app before helped me a lot. I did a few file moves (such as moving and to live side by side with ) as well as changing hard-coded paths to environment variables (e.g. the location of the database file). This took a bit longer than I had expected, but wasn't too difficult either -- just 5 minutes of reconfiguring, and I had the core functionality of Cryptgeon working in a way that I would be confident to deploy. A day or two (on-and-off) deploying I followed the deployment pattern that I had previously set up for Shortmail (and other utility apps that I've deployed on my own Dokku server on DigitalOcean), using the official Dokku GitHub Actions to automate deployment so I wouldn't have to do it manually myself. Nonetheless, I still ran into some errors with deployment, getting the dreaded light blue screen. As I had some unforeseen errors that weren't showing up in the logs, it was challenging to debug, until I set up a Docker container locally to mimic what was going on in the Dokku server. (Those turned out to be more file path issues!) By fixing these, I deployed with confidence, and now I have Pigeon Secure Notes deployed: Caption: Screenshot of Pigeon Secure Notes. Exoskeleton for experienced devs What I think was very cool about this whole experience was getting a taste of what it's like to have an exoskeleton (borrowing a phrase from my friend Virgile Andreani, who found it from another written piece on AI-assisted coding). Having an AI do the metaphorical heavy lifting of typing was liberating: my brain runs faster than my fingers, so the bottleneck to operating at the speed of thought, for me, is literally the speed at which I can type. That said, while I was reliant heavily on Cursor and Claude Sonnet to do a lot of the heavy lift in typing, there was no substitute for experience in judgment to know: - What to hard-code and what to leave to environment variables, - How to organize files in a project, - Which tech stack to choose (I chose boring but composable stuff) - What data structures to reach for (e.g. Pydantic models instead of plain dictionaries) - Judging whether to use someone else's package or just reimplement the code directly That kind of practical wisdom can only come from practicing building stuff over and over and over. There's no theory class to teach this stuff, because it's phenomenologically driven by the constraints of reality -- human limitations, tech stack constraints, language bottlenecks, and more. In building Shortmail and Pigeon Secure Notes, I spent most of my",
    "tags": [
      "ai",
      "programming",
      "productivity",
      "software development",
      "automation",
      "cryptography"
    ],
    "pub_date": "2024-10-06",
    "type": "blog"
  },
  {
    "id": "blog-explain-your-jupyter-notebooks-using-llamabot",
    "url": "/blog/2024/9/27/explain-your-jupyter-notebooks-using-llamabot/",
    "title": "Explain your Jupyter notebooks using LlamaBot",
    "summary": "In this blog post, I introduce a new tool that uses LLMs to automatically generate explanations for Jupyter notebook cells. I discuss the motivation behind creating this tool, its current capabilities and limitations, and provide recommendations for optimal usage. The post also includes instructions on how to install and try out the tool for yourself. Curious about how it can streamline your data science projects? Why not give it a try?",
    "body": "Recently, I've been exploring ways to leverage Large Language Models (LLMs) to enhance my documentation practices. After developing the LlamaBot documentation writer system (), which creates initial drafts of routine documentation using author intents and source files, I've now added another tool to my toolkit: a system for drafting Markdown cells within Jupyter notebooks. How it works The process is straightforward and seamless. You begin by coding in your notebook as usual, allowing your coding flow to remain uninterrupted. Once you've finished coding, you simply run the command. (Link here!) As if by magic, your notebook will then be populated with Markdown cells that explain what to expect in the upcoming code cells. Design considerations In developing this tool, I took into account several key factors. First, I considered the lifecycle of a notebook. Data scientists typically prefer to focus on coding without interruption. While they might leave occasional comments, creating detailed Markdown cells can disrupt their flow. It's more efficient to add explanations after the fact. Another important consideration was the automation-friendly CLI interface. Implementing this feature directly within a notebook could interrupt the analysis process, so a command-line interface allows for easier automation. Lastly, I aimed for narrative-style explanations. Rather than generating robotic comments about each cell's contents, the tool provides context-aware explanations that contribute to an overall narrative throughout the notebook. Current limitations While powerful, the tool does have some limitations. Much of the \"why\" behind the code remains in the human's mind. Even the best LLMs can typically only guess 30-60% of a developer's thought process, necessitating human input to fill in the gaps. Currently, the tool uses a single style: every cell receives an explanation in three sentences. This approach may be excessive for simple, single-line cells used for output inspection, and insufficient for complex cells that perform multiple tasks. Recommended usage To get the most out of this tool, I recommend completing your notebook code to your satisfaction before running the notebook explainer. It's also helpful to include code comments to provide additional context, especially regarding the \"why\" behind your code. These comments can be incorporated into the generated Markdown cells for a more natural narrative flow. If anything is unclear in your head right now that you want the LLM to attempt to explain, write down what's in your head, however muddled, into the notebook as code comments. There's a chance the LLM will pick it up and attempt to clarify it. Finally, try to keep individual code blocks focused on performing one specific, useful task. Try it out! I'd love to have you give it a try. You can install it using pip:",
    "tags": [
      "jupyter",
      "notebooks",
      "llm",
      "automation",
      "documentation",
      "productivity",
      "data science",
      "python",
      "llamabot"
    ],
    "pub_date": "2024-09-27",
    "type": "blog"
  },
  {
    "id": "blog-recreating-shortwhale-with-ai-assisted-coding",
    "url": "/blog/2024/9/23/recreating-shortwhale-with-ai-assisted-coding/",
    "title": "Recreating Shortwhale with AI-Assisted Coding",
    "summary": "In this blog post, I share my experience recreating Dan Ariely's Shortwhale using a tech stack that includes HTMX, SQLite, FastAPI, CloudMailin, and DigitalOcean. I highlight the transformative role of AI-assisted coding with Cursor, which allowed me to build core functionality in under two hours. The project, now live, was an experiment and a learning opportunity, emphasizing the speed and ease of AI-assisted web development. Curious about how AI can accelerate your web projects?",
    "body": "This past weekend, I had the chance to hack on a small project inspired by Dan Ariely\u2019s Shortwhale, an email service that provides a contact form where users can add FAQs, and others can send emails using that form. I was able to recreate the whole thing, and it turned out to be a fun exercise with some surprising insights. The tech stack For the build, I used a combination of tools that made this project come together quickly: - HTMX for the frontend, which is great for minimizing the use of heavy JavaScript frameworks. - SQLite as the database. - FastAPI for the backend. - CloudMailin to handle email routing. - DigitalOcean, where I\u2019m running a $12/month server to host everything. The most magical piece in the whole process, though, was using AI-assisted coding with Cursor. With the help of an AI assistant, I was able to get the core functionality up and running in less than two hours. That was pretty mind-blowing, especially since I don\u2019t consider myself a web development expert. I was clearly inspired by @levelsio's simple setup, including his famed use of a SQLite database for his services. The build process The core functionality came together quickly, but the rest of the project involved refining the UI, making sure email routing was working, debugging the deployment on my Dokku machine, and setting up GitHub CI/CD for automatic deploys. Despite these tasks, the overall process was remarkably smooth. The project is now live at on my personal website, and everything is fully functional. Thanks to GitHub CI/CD, as soon as I've tested a change locally, I can quickly push it to deployment without issue. Reflections I\u2019m still undecided about whether I\u2019ll release this as a full service for others to use. If I do, it would be to cover hosting costs rather than to make significant money. This was more of an experiment and a learning opportunity than anything else. That said, if you're interested, drop me a note via Shortmail to let me know! What really struck me throughout this project was how fast AI-assisted coding can be. As someone who\u2019s proficient in Python but not as comfortable with web development, using AI to assist with building this project was an eye-opening experience. It allowed me to bypass a lot of the usual headaches that come with web projects, while still giving me a low grade onramp to making web apps with Python and HTMX. I also got more used to coding with Cursor, iteratively shaping how I frame and scope requests (as well as when I reset the Composer) to balance Claude's accuracy in changes. Also, HTMX was a joy to use! By minimizing the need for JavaScript frameworks, I could focus on getting things done without getting bogged down in complex tooling. Final thoughts Overall, this was a rewarding exercise in AI-assisted coding. It has definitely shifted my perspective on the speed and ease with which we can build fully functional web apps today, especially with the right tools.",
    "tags": [
      "coding",
      "email",
      "htmx",
      "sqlite",
      "fastapi",
      "cloudmailin",
      "digital ocean",
      "deployment",
      "web development",
      "ai coding"
    ],
    "pub_date": "2024-09-23",
    "type": "blog"
  },
  {
    "id": "blog-how-to-set-up-pixi-with-codeartifacts",
    "url": "/blog/2024/9/19/how-to-set-up-pixi-with-codeartifacts/",
    "title": "How to set up Pixi with CodeArtifacts",
    "summary": "In this blog post, I share my experience integrating Pixi with AWS CodeArtifact, detailing the steps needed to configure Pixi for internal package publishing at work. I discuss the installation of and , editing configurations, and setting up Pixi's global configuration. The guide aims to help others overcome similar integration challenges (obviously without revealing company-specific details). Curious about how these configurations can streamline your development process?",
    "body": "Following my endeavors test-driving Pixi, I encountered a critical challenge that could hamper its adoption at work: integrating Pixi with AWS CodeArtifact. At my workplace, we publish internal packages on CodeArtifact, and I was uncertain about how to configure Pixi to work with AWS authentication. To address this, I took the following steps: 1. I filed an issue on the Pixi issue tracker (https://github.com/prefix-dev/pixi/issues/1783). 2. I sought assistance from my colleagues, Anand Murthy and Albert Lam. 3. I received guidance from Tim de Jaeger and Olivier Lacroix of the Prefix dev team. Through these collaborative efforts, I successfully resolved the integration challenges. This blog post serves as a comprehensive guide on how to achieve this integration, excluding any work-specific idiosyncrasies. Pre-requisites Firstly, I'm assuming you have the ability to login to CodeArtifacts. You should be able to successfully login to CodeArtifacts using a command that might look like this: Running that command should give you: If this isn't true, then you can skip this blog post altogether! Set up globally installed tools If you don't already have it, install globally: Now, you'll have available globally: Now, you'll need to ensure that and are installed globally using : The second command injects into the same virtual environment that set up for . We also explicitly specify the in case your CodeArtifact repository doesn't have a mirror of PyPI public packages. Pro tip: If you don't use but instead try to do , it'll get installed into its own isolated environment separated from , and that will result in not being able to pass CodeArtifact credentials to /. Now, check the configuration for . It should look something like this: We'll want to edit the config: (Don't judge, I love using nano!) Now, edit to have the following contents: Note: The official README specifies an example with more keys, but it turns out , , and are all optional if you are logged into AWS and CodeArtifacts already, which I am assuming is true. Pixi global configuration Now we need to set up pixi's global configuration. On UNIX-style systems, this should be: Make sure your configuration includes the following: Pro tip: Don't mis-spell anything here! I had mis-spelled as and that caused me so much confusion that I left this effort on the backburner for over a week before Anand live-troubleshooted with me. This configuration is super important on our machines, as it enables us to avoid needing to specify on every call, which we'll see below. Now, you should be able to pull in from your company's CodeArtifacts! Let's see how to verify this. Verify correct configuration Navigate to an empty directory, and then run: This will give you a file. Now, try adding a package that can only be found on your company's CodeArtifacts: This should successfully work. If you're familiar with , you may notice that we didn't have to add . This is because we configured the keyring provider globally above. Keyrings? What's that? This was the new thing for me: using the Python package and its associated extension to magically pull in AWS credentials from my system and injecting it into 's (underneath the hood, it's 's) call to CodeArtifacts. Without this, will not be able to access CA. A few other minor notes. Firstly, yes, there is a in the name of the package. I don't know why that's okay, because I freaked out when I saw being installed. Apparently in Python, package names get kebab-cased. Secondly, MUST be in the same global virtual environment when managed by ! Can't repeat this enough. Conclusion In conclusion, setting up Pixi with CodeArtifacts involves several steps. First, you need to ensure you can successfully log in to CodeArtifacts. Then, you need to install globally and ensure that and are installed globally using . You also need to configure and Pixi's global configuration. Finally, you can verify the correct configuration by initializing Pixi in an empty directory and adding a package that can only be found on your company's CodeArtifacts. The, ahem, key to this setup is the Python package and its associated extension , which pull in AWS credentials from your system and injects them into Pixi's call to CodeArtifacts. Now that I've figured out how to get working with CodeArtifacts, I'm in a better position to socialize at work!",
    "tags": [
      "pixi",
      "aws",
      "codeartifacts",
      "package management",
      "devops",
      "python",
      "configuration"
    ],
    "pub_date": "2024-09-19",
    "type": "blog"
  },
  {
    "id": "blog-sync-github-secrets-with-your-env-and-gh-cli",
    "url": "/blog/2024/9/15/sync-github-secrets-with-your-env-and-gh-cli/",
    "title": "Sync GitHub secrets with your .env and gh CLI",
    "summary": "Today, I learned that we can easily sync our local .env file with GitHub secrets using the GitHub CLI (gh). This method is much faster and less error-prone than manually entering secrets through the web interface. Curious to see how it works?",
    "body": "So I learned something new today. We can set GitHub secrets using a file and the CLI. This is how it's done. Suppose you have a file with the following content: You can set these secrets using the following command: This will set the SECRETKEY and DATABASEURL secrets in your GitHub repository automatically. The pre-requisite is that you have the CLI installed and you have set up your local git config with your GitHub user. Additionally, your git remote should be set to your GitHub repository. If it's not set to your GitHub repository, you can set it with the following command: (Remember to change YOURUSERNAME and YOURREPOSITORY to the appropriate values!) Now, instead of doing the tedious work of setting each secret manually using the GitHub web interface, you can use the CLI to set them. This streamlines the process and reduces the chance of human error. This will set the SECRETKEY and DATABASEURL secrets in your GitHub repository automatically.",
    "tags": [
      "github",
      "secrets",
      "environment-variables",
      "gh-cli",
      "automation",
      "devops",
      "productivity",
      "security",
      "til"
    ],
    "pub_date": "2024-09-15",
    "type": "blog"
  },
  {
    "id": "blog-cursor-is-a-jetpack-for-coders",
    "url": "/blog/2024/9/14/cursor-is-a-jetpack-for-coders/",
    "title": "Cursor is a jetpack for coders",
    "summary": "In this post, I explore how Cursor, an AI-powered IDE, transformed my coding workflow and supercharged my productivity. Learn about its standout features and why it's become my secret weapon for efficient development and writing. Are you ready to revolutionize your coding experience?",
    "body": "Last Friday (September 6th), I finally took the plunge and downloaded Cursor for the first time. I know, I know\u2014I was late to the party. Looking back, I can't believe it took me this long to give it a try. But boy, am I glad I did! To put Cursor through its paces, I decided to build something out of curiosity: an app that takes a plain text description of a calendar event and spits out an ICS file ready for import into my calendar. I leveraged LlamaBot's StructuredBot to translate natural language into the ICS file format. Now, I'll be the first to admit that CSS isn't my strong suit, so I turned to Claude 3.5 Sonnet (Cursor's default language model) to style it for me using Bootstrap. But I didn't stop there. I asked Cursor to help me implement a login page, email-based authentication (to avoid storing passwords), and user sessions to keep multiple simultaneous users from interfering with each other. The kicker? I accomplished all of this in under four hours of actual work time (not counting breaks to entertain the kids). Since that initial project, Cursor has become an integral part of my workflow. Take this blog post, for instance. I started with a rough outline and some stream-of-consciousness narration, then asked Cursor to flesh it out into a coherent article. I've also used it to improve LlamaBot, following a similar workflow: declare what I want, review the proposed code changes, and iterate. I even attempted to create an email version of my calendar event generator. While I hit a snag with timeout issues, I was still able to get it to return a calendar invite successfully. Cursor: A game changer for coding productivity So, what makes Cursor stand out? For me, it boils down to four key features: 1. The intuitive composer UI 2. Its ability to show a diff of proposed file changes 3. The capability to edit multiple files simultaneously 4. Most importantly, its accuracy in inserting diffs in the right places (I estimate GitHub Copilot misses the mark about 20% of the time) Figure 1: Screenshot of the Composer UI in action But wait, there's more! Cursor's user interface is full of delightful surprises. As you interact with the AI, you'll notice it starts to anticipate your needs, surfacing contextual suggestions that can save you even more time. For instance, I asked Cursor to add line breaks to my blog content. After applying the change to the first paragraph using Command+K, Cursor proactively suggested extending the same formatting to the next three paragraphs. With a single tab press, the changes were instantly applied across all four paragraphs. It was a moment of pure magic that left me thinking, \"Wow, this is absolutely incredible!\" Figure 2: Screenshot of inline edits in action These intuitive features demonstrate Cursor's ability to not just follow instructions, but to understand the broader context of your work. It's this level of intelligent assistance that truly sets Cursor apart from other coding tools. Now, I want to be clear: I'm not being paid by Cursor to sing its praises. I genuinely believe it's a game-changer. In my experience, it's significantly more effective than GitHub Copilot. And when paired with macOS's dictation feature, I can breeze through tasks like writing this blog post with unprecedented speed and ease. Supercharge your workflow: A summary of Cursor's usefulness Let's recap the key points that make Cursor a game-changer for coding productivity: 1. Intuitive composer UI for seamless interaction 2. Accurate diff display of proposed file changes 3. Multi-file editing capabilities 4. Precise code insertion (outperforming GitHub Copilot) 5. Contextual suggestions that anticipate your needs 6. Intelligent understanding of your work's broader context These features combine to create a powerful tool that not only follows instructions but enhances your entire coding and writing process. Whether you're crafting code or composing blog posts, Cursor acts as your intelligent assistant, streamlining your workflow and boosting your productivity to new heights. In short, Cursor has quickly become my go-to tool for coding productivity. I think of it as more than just an IDE; to me, it's a jetpack for coders and writers! If you haven't tried it yet, don't make the same mistake I did by waiting. Give it a shot, and prepare to have your coding and writing productivity transformed. Addendum I wanted to address some commentary I've heard about Cursor's Composer view. In particular, Chris Albon on Twitter has noted how Cursor + Composer can make large-scale mistakes across multiple files in the codebase. I've observed this myself several times as well, though I also noticed it primarily occurred in situations where my requests were vaguely worded and I blindly accepted the changes without review. Granted, reviewing large-scale changes is challenging. However, I still believe the Composer UI is a net positive if we know how to effectively guide our interactions with the underlying LLM model. Th",
    "tags": [
      "coding",
      "productivity",
      "ai",
      "cursor",
      "developertools",
      "ide",
      "automation",
      "programming",
      "efficiency",
      "innovation"
    ],
    "pub_date": "2024-09-14",
    "type": "blog"
  },
  {
    "id": "blog-on-writing-llm-evals-in-pytest",
    "url": "/blog/2024/9/6/on-writing-llm-evals-in-pytest/",
    "title": "On writing LLM evals in pytest",
    "summary": "In this blog post, I explore the process of writing evaluations for LLM systems using pytest, aiming to move beyond subjective assessments to more structured testing. I detail the creation of specific tests to assess if LLMs can accurately determine documentation staleness, using various models and criteria. The challenges and insights gained from setting up these evaluations reveal the complexities involved in ensuring that LLMs perform as expected. Could this method enhance the reliability of your LLM evaluations?",
    "body": "This long weekend, I tried my hand at writing evaluations for my LLM doc writing system, not unlike what we might do with software tests, so that I could move beyond the \"vibe test\" in my work. My experiment this time round was to see whether we could write an LLM evaluation in pytest without relying on any other frameworks but pytest. Turns out this is doable! Evals (short for evaluations) check that an LLM system is doing what we want. In the vast majority of applications I've seen, human checks are the most important, but, with a little critical thought, other validity checks on an LLM system can be designed too! Continuing what I was building in my last post, I wanted to check that my prompts for doc checking \"worked\" across different LLMs. If I were to do this manually, it would be challenging to stay disciplined, especially since I experienced criteria drift while observing the behaviour of my doc writing system. Using automated software testing systems gives me a more structured and disciplined approach to writing LLM evals. Implementing evals in pytest Here's the reveal: one of the tests I wrote illustrates the pattern for writing LLM evals as Python software tests. To arrive at this test, I went through a winding and twisted process -- driven mainly by a need for more clarity about what I wanted to test. But things became much clearer once I quieted my head and treated this task like an experiment. I sketched out the axes of variation that were all possible in this system: - The prompts in my documentation intent list, - The system prompt for the judge bot, - The prompt phrasing in , which formats a prompt that effectively serves as the human message sent to the bot, - The LLM model used -- admittedly, the easiest to vary. Since the last bullet point was the easiest to set up, I went with it for this test. However, varying the prompts (the other three bullet points) would have allowed me to be more rigorous, even though it would have been a messier and more laborious task. (For example, I'd have to develop reasonable variations of documentation intents.) The biggest challenge: eval criteria Thinking about the evaluation criteria was the hardest part. The goal was to evaluate whether an LLM could provide a way of judging that the documentation needed to be updated as a human would without me going into excessive text parsing detail. Intuitively, this was the right scope of work for an LLM to handle, but it could have been better. This seemed okay until I did some ad-hoc testing in a notebook with my source files... which was slow, cumbersome, and laborious. (This pain motivated me to try doing this automatically and systematically.) Eventually, I settled on three sub-criteria to test: 1. Source files contain content not covered in the documentation. 2. Documentation contains factually incorrect material that contradicts the source code. 3. Documentation fails to cover the content as specified by the intents. Each of them would be a boolean ( vs. ) check, and we could measure the performance of an LLM by providing an example of stale documentation that a human (at least myself) would rate as stale under source code changes. For each sub-criteria, I had to provide an example of a change that should prompt the model to return a boolean , and I would also ask the LLM to give reasons. Once I had this level of clarity, I could construct a plausible example for each criterion that illustrated the kind of issue an LLM should be able to catch. Criteria drift isn't a bad thing, but it is real But let's first talk about criteria drift. As I mentioned above, I experienced it myself. When I first built the documentation staleness evaluator, my goal was to produce an \"out-of-date\" evaluator -- one that said, \"Is the documentation out of date for the source code file(s) specified?\" This is effectively using LLMs as a judge. I implemented this as a Pydantic model that was provided to StructuredBot: This is a hugely simplistic way of looking at documentation staleness. To evaluate the behaviour of the out-of-date judge, I created a synthetic example of a 'next prime number' finder using ChatGPT and then asked ChatGPT for an alternative implementation. Then, I did the following: 1. Asked a docwriter bot (using ) to generate documentation based on the intended goals in the documentation, 2. Swapped the source code implementation for a new one, and 3. Asked the LLM judge (using with a different prompt) to independently evaluate whether the docs were out-of-date. To my surprise, the LLM judge would return most of the time! Upon deeper inspection, it was because the generated docs were written at a high level and did not reference a particular detail in the source code. I injected into the generated docs a reference to that specific detail in the source code that changed in the new implementation and fundamentally changed what I was evaluating; only then would the LLM judge get the \"staleness\" evaluation correct. However",
    "tags": [
      "evaluations",
      "pytest",
      "documentation",
      "automation",
      "testing",
      "validation",
      "changes",
      "criteria",
      "staleness"
    ],
    "pub_date": "2024-09-06",
    "type": "blog"
  },
  {
    "id": "blog-llamabot-now-has-structuredbot",
    "url": "/blog/2024/8/31/llamabot-now-has-structuredbot/",
    "title": "LlamaBot now has StructuredBot!",
    "summary": "In this blog post, I discuss the latest updates to LlamaBot, particularly focusing on the StructuredBot feature introduced by Elliot Salisbury. StructuredBot leverages JSON mode of LLMs for structured outputs, significantly simplifying the process of generating reliable and type-safe outputs without manual string parsing. I illustrate its application in an automated documentation checker and writer, enhancing productivity by integrating LLM-based and traditional programming methods. Curious about how StructuredBot can streamline your documentation process?",
    "body": "I've been hacking on LlamaBot on the side since SciPy, and I wanted to share some of the updates that have been coming. Barring any other time-sensitive topics I feel the need to address, LlamaBot will be the topic of the next few blog posts. The first update I wanted to discuss is LlamaBot's ability to provide structured outputs using . Elliot Salisbury contributed the first implementation during SciPy 2024. Underneath the hood, it relies on the JSON mode of LLMs (routed by LiteLLM), and like Instructor, it requires the passing in of a Pydantic class. Once Elliot\u2019s PR was merged, I had an easy and reliable way to start with structured generation. Not needing to do string parsing of LLM outputs was a huge mental burden lifted off my shoulders. Being able to use any API via LiteLLM also allowed me to test the capabilities of a variety of LLMs, including but not limited to Llama3.1 (via Groq), Gemma2 (via Ollama), Claude, and GPT-4o. With that came a bunch of experimentation, and with that experimentation, I saw a lot of applications in my work! How to use StructuredBot Let's see how to use StructuredBot in an LLM-as-a-judge setting. I use this within an automated documentation checker and writer setting, which I will touch on in a later blog post. Firstly, we set up a StructuredBot called , to judge whether documentation is out-of-date. It accepts a Pydantic class that has just one field: One thing to note here is that the description of each field is provided to the LLM as part of its intended context, so the LLM is supposed to be \"aware\" of your intent for the field. Naturally, the more granular, the better! I then set up a StructuredBot with its system prompt to check whether the docs are outdated. The prompt looks like this: And then the bot is defined as such: Finally, I have a function that returns the user prompt formatted correctly: Once these components are in place, keeping documentation up-to-date becomes a matter of writing a Python program: I wrapped this all into a LlamaBot CLI tool, which you can run on your Markdown documentation: The result is on Llamabot's doc writer documentation, which was generated using this exact program! Structured generation brings reliability As the .txt folks (the creators of Outlines) have espoused, structured generation provides a much more reliable text generation method than relying on an LLM to conform to a schema in free-form text generation mode. In the case of StructuredBot, we take it one step further: we return a Pydantic model, which guarantees the Python types of the attributes stored underneath the hood. The attribute above is boolean, thus allowing me to check its truthiness with a simple . This level of reliability makes it much easier to write automation that involves LLMs! Moreover, any custom validation that can be coded up in Python is valid too! This can include the length of strings generated (which is fed back to the LLM for self editing) and more. As an example, LlamaBot's git commit message writer is restricted to 160 characters, and the LLM uses the length information and validation error messages to edit the strings. Further thoughts While working with and hacking on , I had a few other thoughts that I wanted to share. Agentic workflows? Agentic workflows are the hot rage, but they have issues. While I like the premise for decision-making automation, there are compounding error probabilities when one relies solely on LLM-based agents to coordinate work, one that makes it infeasible to construct extensive chains of LLM agents to do things reliably. In my testing of using to evaluate a decision point (i.e. \"Is the documentation out of date?\"), there is always a risk that the will be incorrect and not behave as intended. This would lead to the documentation writer either (a) rewriting documentation when it was not supposed to or (b) ignoring changes in source code and failing to fix them. A more reliable, productive path forward is to blend LLM-based flow control with deterministic flow control, effectively fusing agent-centric program design with traditional programming. The documentation writer is a minimally complex example of this idea. The example above shows that even though we have a bot to make a judgment call (agent-centric design), a user can override the LLM's judgment through a flag (traditional programming). LLM as a Data Engineer through StructuredBot StructuredBot can be used in other use cases. I may provide an example of some of these in the future. The biggest I can think of is a structured information extractor from documents. If we have an extensive collection of plain text documents for which we want to extract standardized and structured information, we can create a Pydantic model that captures the fields that we are interested in: Then, we create a program that loops over the documents and feeds each of the docs to a StructuredBot to extract the information: And finally, we can turn it into a pandas DataFrame: This is p",
    "tags": [
      "structured generation",
      "llamabot",
      "python",
      "documentation",
      "llm",
      "pydantic",
      "software development",
      "testing",
      "structuredbot",
      "technology"
    ],
    "pub_date": "2024-08-31",
    "type": "blog"
  },
  {
    "id": "blog-dissecting-the-esm3-model-architecture",
    "url": "/blog/2024/8/25/dissecting-the-esm3-model-architecture/",
    "title": "Dissecting the ESM3 Model Architecture",
    "summary": "In this blog post, I explore the ESM3 model, focusing on its handling of missing modalities in multi-modality training. I dissect the model's architecture, input and output configurations, and the strategic use of default values for absent data. By examining the source code and conducting a toy example, I illustrate how embeddings are calculated and how they shift in vector space when modalities are missing. This deep dive reveals the model's elegant design and its potential for multi-modality integration. Has this piqued your curiosity yet?",
    "body": "I recently read the ESM3 paper. Setting aside the more marketing-oriented title (\"Simulating 500 million years of evolution...\"), I was genuinely curious to see how this multi-modality model was trained. Previously, when writing about multi-modality models, the one thing that baffled me was this problem: How do we handle the situation where some of our modalities are missing from a training set sample? ESM3 was the first model in which I observed how accommodating missing modalities worked. Here are my notes after digging into the model's source code. Model Inputs and Outputs Firstly, we start with what the neural network model takes as inputs and returns as outputs. We will also study the tokenization scheme of the ESM3 model, which is crucial for understanding the inputs to ESM. | Argument | Embedding | Semantics | Default Value | | :-------------------------- | :-------: | ---------------------------------------------------------------------- | ------------------------ | | | \u2705 | tokenized sequence | | | | \u2705 | tokenized 3D structure | | | | \u2705 | average of predicted LDDT (i.e. model confidence) across the structure | | | | \u2705 | per-residue pLDDT | | | | \u2705 | secondary structure tokens | | | | \u2705 | solvent accessible surface area (SASA) tokens | | | | \u2705 | function tokens | | | | \u2705 | residue annotation tokens | | | | \u274c | structure 3D coordinates | | | | \u274c | chain ID, usually \"A\", \"B\", or some other name | | | | \u274c | sequence ID | | Crucially, the model only requires one of the inputs to be passed in. We can know this by studying the pass of the model. For the arguments not provided, each has default values, as described in the table above. These are set per batch of sequence that is passed in. The outputs of the ESM3 model are as follows, with the following shapes: | Output | Dimensions | Dimension Semantics | | ---------------------------- | ------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | | | is set by humans. | | | | \"8\" refers to , 260 is the function token vocabulary size after using TF-IDF. | | | | 15 boundaries imply 16 levels, + 3 tokens for , , and (unknown) | | | | 20 standard a.a. + XBUZO + + , , , , . Not sure how we get to 64; I suspect it's in preparation for a codon model, or just extra capacity. | | | | 4096 is the dimension of the VQ-VAE that's used to embed structure. | | | | 8-class vocabulary for secondary structure + 3 special tokens , , and . | I'm noticing that the Evolutionary Scale folks have chosen to discretize properties considered continuous, such as SASA. Though this choice is arbitrary, it fits the language modelling paradigm. Presumably, one may choose a smaller number of discrete levels while trading off granularity. Model Architecture At a high level, the model's architecture looks roughly (but not exactly!) like the following, with the relevant attributes assumed to be set up in the : Most crucially, we need to examine what conceptually happens within on a per-sample basis: As you can see, we sum the embedding vectors together. I had quite a few questions about this, and I will address them later. Dealing with missing modalities So, we now come to the key question of this post: how does the model deal with missing modalities? The \"Default Value\" column above gives us the answer: if we are missing a data modality, then we use a default value to fill it in! Looking at the table above, any modality involved in calculating the embedding uses a default value, such as the respective pad token, mask token, or a semantically relevant floating point value. Thinking back to a conversation I had with colleagues at work, this was also an idea we thought of implementing, but we weren't 100% sure whether this would so-called 'work' or not. Now, from studying the model source code, we know that there are 8 modalities that are used for calculating the final embedding. When a modality is missing, we use a sentinel value as a default value. What effect will this have on the learned embedding layers, and what effect will this have on the model's encoder's outputs? To understand this, we need to remember two facts. Firstly, addition in vector space represents a location shift. Secondly, tokenization effectively indexes into a trainable vector representation of text. Let's see how these two play together by exploring a minimally complex toy example. We will begin with the following setup: - Two modalities: - String letters with alphabet - String numbers with alphabet - in both cases represents the token. - Sequence of length 7. - Modality 1's sequence is: - Modality 2's sequence is: - Modality 2 is the one we will choose to be optionally missing, and when missing, will be represented by the following string: - Embeddings are 3-dimensional. For simplicity's sake, we will draw them from an isotropic Gaussian distribution, except for ",
    "tags": [
      "esm3",
      "neural network",
      "multi-modality",
      "model training",
      "data",
      "tokenization",
      "model architecture",
      "vector embedding",
      "machine learning",
      "protein modeling",
      "journal club"
    ],
    "pub_date": "2024-08-25",
    "type": "blog"
  },
  {
    "id": "blog-its-time-to-try-out-pixi",
    "url": "/blog/2024/8/16/its-time-to-try-out-pixi/",
    "title": "It's time to try out pixi!",
    "summary": "Post SciPy 2024, I had a chance to try out , a new environment manager from the prefix.dev team. I went cold turkey on my laptop, removing , and haven't looked back. In this (very long) blog post, I detail my experience switching from to , the ways that makes it easier to manage environments, how helps with onboarding onto a project, supports containerization, GPU access, and seamless integration with Docker, and how it facilitates publishing to PyPI and running tests. The switch has streamlined my workflow significantly. Was this enough to get you curious about how can optimize your development process too?",
    "body": "I recently switched LlamaBot and my personal website repository to , and having test-driven it for a few weeks now, I think the time is right to do so. During this test-drive, I went cold turkey: I deleted from my home directory, leaving me with no choice but to use 100%. In this post, I'd like to share what I've learned from my perspective as a data scientist and software developer. What is pixi? In my mind, pixi is a package management multi-tool. plays many roles, but here are the big, salient ones: | Role | Analogy | | --------------------- | -------------------------------------------------------- | | Package installer | , | | Global tool installer | , and | | Environment manager | + , or + | | Environment lock | or | | Task runner | | My needs under two personas To motivate the content in this post, we first need to understand what I see as needs from a data scientist's and software developer's perspective. Data scientist Reproducible Environments I need to replicate my computational environment from machine to machine. Strict versioning will be handy but shouldn't be unwieldy -- especially with lock files. Compared to my older ways of working using files, where I manually pin versions when something broke, I now prefer to have my environment management tool automatically produce a lock file that locks in package versions defined when solving the environment. Containerization Containerization is also important. At work, we ship stuff within Docker containers. Whatever environment or package management tool I use must work well with Docker containers. Additionally, as a bonus, we need it to have GPU access, too! Moreover, the built container needs to be as lightweight as possible. Composable GPU-enabled and CPU-only environments With a single file, I can only define a GPU-enabled or CPU-only environment. In most cases, we would default to GPU-enabled environments, which induces a huge overhead when the code is run on CPU-only environments. Ideally, I'd like to be able to do composable environment specification: a default setting for CPU-only environments, with the ability to specify GPU-only dependencies in a fashion that composes with the CPU-only environment within a single, canonical configuration file (e.g. ). Software developer Publishing to PyPI As a Python tool developer, I create stuff that needs to be distributed to other Pythonistas. As such, I need to be able to leverage existing Python publishing tooling (e.g., PyPI or conda-forge). Running tests Whatever tool I use, I also need to be able to run software tests easily. Ideally, this would be done with one command, e.g., , with minimal overhead. The software testing environment should be the same as my development environment. Both personas' needs Simplified environment specification As mentioned above, I currently use and to specify runtime and development dependencies. Runtime dependencies are declared in , while development dependencies are declared in . However, this way of separating concerns means that we have duplication in our dependencies specification! Ideally, we'd like to do this with a single file. Tooling for fast environment builds Whether we are in a Docker container or not, I'd like to see much faster container and environment build times with tooling for caching when compared to what I currently experience. (Note to self: Product-oriented folks keep talking about \"don't solution, tell me the problem\", but I have to say -- I only really knew how much of a problem this was once I touched the solution I'm about to talk about!) Automatic updates of lock files When I add a dependency to my environment, I'd like to see the lock file automatically updated so that I don't have to remember to do that in the future. How pixi operates With the desiderata outlined above as contextual information, let's now see how works. Installation What's awesome is that is installed via a simple, one-bash-script install. Official instructions are available here, but as of the time of writing, the only thing we need to run is: This sets up within your home directory and adds the path to your PATH environment variable. I also recommend getting set up with autocompletion for your shell. Once again, official instructions are here, and what I used for the (macOS' default shell) was: Starting a new project with enabled For those who are used to idioms, won't create a centralized environment but instead will create a sub-directory within the directory of your configuration file (either or ). This is akin to for Pythonistas, or for those coming from the NodeJS ecosystem and for Rustaceans. In line with my desire to be able to specify sets of dependencies for runtime environments and development environments and also minimize the number of configuration files present, it makes sense to initialize with a configuration rather than the default configuration file. This is done by executing: This gives me the following file: What's important to note here is that ",
    "tags": [
      "pixi",
      "tooling",
      "software development",
      "data science",
      "environment management",
      "containerization",
      "gpu",
      "packaging",
      "docker",
      "reproducibility",
      "testing"
    ],
    "pub_date": "2024-08-16",
    "type": "blog"
  },
  {
    "id": "blog-a-survey-of-how-to-use-protein-language-models-for-protein-design-part-3",
    "url": "/blog/2024/8/9/a-survey-of-how-to-use-protein-language-models-for-protein-design-part-3/",
    "title": "A survey of how to use protein language models for protein design: Part 3",
    "summary": "In part 3 of the series on protein language models, I explore the critical phase of evaluating protein sequences generated by language models, emphasizing the importance of practical, bioinformatics-based evals to narrow down candidates for lab testing. I explore both sequence-based and structure-based evals, highlighting their roles in filtering and ranking sequences to prioritize for experimental validation. Additionally, I offer insights on fostering collaboration between computational and laboratory teams to enhance protein design efforts. How can these evals and collaborations accelerate protein engineering?",
    "body": "This is part 3 of my three-part series on protein language models, focused on evaluations of PLMs. The previous two parts were focused on defining what a PLM is (part 1) and outlining how they are trained (part 2). Evals, evals, evals Once we have a trained protein language model, sequence generation becomes the easy part. What's hard is knowing which one to pick. After all, as alluded to above, we don't care as much about model training metrics (e.g. final loss) or model properties (e.g. perplexity) as we do about the performance of generated sequences in the lab. Unfortunately, my colleagues in the lab are absolutely not going to test millions of generated sequences to find out which handful will work, as the realities of gene synthesis and labor costs associated with assays meant to measure protein function means in most situations, we'll only get to test anywhere from 10s to 1000s of generated sequences. How do we whittle down the space of sequences to test? The answer lies in evals. Evals are the \"hot new thing\" in the LLM world, but there isn't anything new under the sun here for those of us in the protein engineering world. There have been probabilistic models of protein sequences since bioinformatics became a thing (which precedes the LLM world by at least two decades), and evals of such models involved a rich suite of bioinformatics tooling: multiple sequence alignments, secondary structure prediction, motif and protein domain detection, and more that have been developed over the years. (It certainly helps that there have been decades of bioinformatics research into discovering emergent patterns within related biological sequences.) These are all tools that can be directly applied to evaluating generated protein sequences as well. In my paper review of Profluent's preprint, I discussed using domain detection as a filter for sequences. I think this is a great illustrative example, so I'll re-hash it here (with minor edits to the original text for grammatical correctness). If we assume that each of the sequences above was generated from a language model, and the three coloured domains were present in a naturally occurring and functional variant of the protein, then we may deduce that of the five sequences generated above, only sequence 2, which has a missing domain, may be non-functional. (There are more nuanced situations, though, such domains being unnecessary for function.) Overall, I like this idea as a sanity check on generated sequences, as a ton of prior biological knowledge is encoded in such a check. Beyond just this single example, I wanted to outline two points when thinking about protein sequence evals. The first is that functionally, they either give us a way to rank-order sequences or they give us a way to filter them. In both cases, we can use such ranking and filtering to prioritize sequences to test in a lab. The second is that evals on generated sequences can be a leading indicator of success in protein sequence generation. Still, like all leading indicators, a key caveat remains: they are no guarantee of success in the lab! An ontology of evals Having thought about protein sequence evals, I would like to attempt to categorize PLM evals, particularly contextualized within the domain of protein engineering. Sequence-based evals These are evals that can be applied based on the sequence alone, and are applicable universally regardless of the protein that is being engineered (which is why I call them universal). Language model pseudo-likelihoods, or the mutational effect scores as outlined above, can serve as one metric. As shown in the paper above, there are no guarantees of perfect correlation with functional activity of interest, but nonetheless they may be useful as a sequence ranker when assay data is absent. A note is that this is most easily used with masked language models rather than autoregressive models. The Profluent CRISPR paper described degenerate sequences based on consecutive k-mer repetition. This is essentially asking if a k-mer showed up X times consecutively. Within the paper, the authors described this as no amino acid motif of 6/4/3/2 residues was repeated 2/3/6/8 times consecutively. using the first set of numbers, we didn't want to see the motif (or other 6-mer) repeated twice consecutively within a sequence, yielding . It should not be too hard to intuit that this kind of check can be applied regardless of the protein sequence being engineered and the property it is being engineered for. As for why this is a reasonable check, it's because such short duplications do not typically show up in natural sequences. Also, in the same paper, we see the use of BLAST and HMMer coverage scores. These essentially ask how much the generated sequence overlaps with a target sequence or an HMM model of a collection of sequences. High coverage scores often indicate similarity to the target protein family being engineered, which can give more confidence that we have generated a p",
    "tags": [
      "protein engineering",
      "language models",
      "sequence generation",
      "bioinformatics",
      "protein",
      "sequence",
      "evals",
      "protein structure",
      "computational biology",
      "machine learning"
    ],
    "pub_date": "2024-08-09",
    "type": "blog"
  },
  {
    "id": "blog-a-survey-of-how-to-use-protein-language-models-for-protein-design-part-2",
    "url": "/blog/2024/8/2/a-survey-of-how-to-use-protein-language-models-for-protein-design-part-2/",
    "title": "A survey of how to use protein language models for protein design: Part 2",
    "summary": "In part 2 of my three-part series on PLMs in protein engineering, I do a deep dive into the training methods of protein language models, specifically focusing on masked language modeling and autoregressive training. I explain how these models are trained, highlighting the complexities and considerations involved in training, such as model choice, masking fraction, and the need for curated training sets. With these insights, I aim to shed light on the intricate process of preparing protein language models for protein design. Curious about how these models could revolutionize protein engineering?",
    "body": "This is part 2 of my series on protein language models. If you're looking for part 1, please find it here. How exactly are protein language models trained? If you're familiar with how natural language models are trained, then this should come as no surprise to you: protein language models are trained in exactly the same way! There are generally two ways to train a protein language model: masked language modelling and autoregressive training. I will provide a brief overview of these training methods. Masked Language Modeling Masked language modelling training can be thought of like this: 1. taking a protein sequence 2. mask out a fraction of the positions - we call this the masked sequence 3. pass the masked sequence into the masked language model and have it predict what amino acid letter should have been at the masked position, given the rest of the un-masked letters 4. score the performance by measuring how wrong the model was That\u2019s it! To prepare the training data, we would using Python code that looks like this: This thus produces training set sequences that look like this (this example has 15% of sequences randomly masked): And so the model is trained, over thousands to millions of examples, with the dataset designed by a data scientist, to reconstruct the original sequence from the masked sequence. Examples of models that were trained this way include: - CARP, by Microsoft Research, and - ESM 1 and 2, by Meta AI, and now Evolutionary Scale (ESM3) To use the model for generating new sequences, one starts out by masking out a fraction of positions and calculating the probability of an amino acid at each masked position. Then, one has a choice: - Independent Sampling: Either sample all masked positions at one shot, or - Iterative Decoding: Iteratively sample a subset of masked positions, recalculate the positional probabilities, and repeat sampling until all positions are sampled. Independent sampling assumes that the amino acid probabilities per position are independent of one another, while the iterative decoding allows for conditional dependence on previously sampled positions. For the technically-minded, the second is akin to an MCMC sample across all potential position-wise conditional dependencies, and in spirit looks similar to the next method we will discuss. Autoregressive Training Autoregressive training is another way of training a protein language model. Here, the model is trained with a \"prompt\", which, depending on the model goal, can be different. One example might be setting up the training data set by prompting with a natural language description of the protein and getting back a generated protein sequence, as seen below: But what I have seen more commonly done is prompting with the first N amino acids of a sequence and asking the model to generate the rest of the sequence: Examples of such protein language models include: - ProGen by Salesforce AI, which was used by Profluent to design OpenCRISPR1 - ProteinMPNN by Baker Lab (UW) Wait, are the training paradigms really that simple? Good instincts! I've made some simplifications above to introduce the training paradigm, but that means I've masked out (pun intended!) some of the complexities in training these models. Here are a few considerations that one needs to think about when training these models and designing their training data. Decision 1: Which modelling paradigm? The first is to choose which model family to use. As you probably can tell, the masked language modelling paradigm fits naturally with sampling point mutations across the sequence. On the other hand, autoregressive generation fits quite naturally with a de novo design paradigm, where one prompts with the N-terminus of a protein and ask the model to generate the rest of the protein. With the right training modifications, it is possible have an autoregressive model generate sequences in the C-to-N direction (rather than N-to-C); this entails training the model with sequences reversed. It is also possible to train a masked language model to do larger-scale edits, such as by masking 80-90% of a sequence randomly and training the model to reconstruct them. Decision 2: What fraction to mask/prompt? If doing masked language modelling, one must ask the question: how much of a sequence should we mask? Do we follow convention and only mask 10-30% of a sequence? Or do we go more aggressively and mask a large fraction of the sequence? Is there a threshold at which masking too much becomes infeasible? Or if we are doing autoregressive training, how much prompting do we do? Do we prompt it with only 10 amino acids, or do we prompt it with 50? Keep in mind that even though autoregressive generation is stochastic after the prompt, the prompt remains constant. Is this a desirable property? Or will this cause issues with the diversity of sequences needed? Another question for autoregressive generation: when a model is used to autoregressively generate sequences, it needs to know when to",
    "tags": [
      "protein modeling",
      "machine learning",
      "bioinformatics",
      "data science",
      "protein engineering",
      "autoregressive training",
      "masked language modeling"
    ],
    "pub_date": "2024-08-02",
    "type": "blog"
  },
  {
    "id": "blog-a-survey-of-how-to-use-protein-language-models-for-protein-design-part-1",
    "url": "/blog/2024/7/26/a-survey-of-how-to-use-protein-language-models-for-protein-design-part-1/",
    "title": "A survey of how to use protein language models for protein design: Part 1",
    "summary": "In part 1 of a three-part series on protein language models, I do a deep dive into the fascinating world of protein language models (PLMs) for protein engineering, drawing parallels between PLMs and GenAI models like GPT-4. I explore three distinct applications: patent-breaking, prioritized deep mutational scans, and candidate expansion via natural mimicry, highlighting the goal of generating plausible protein sequences for diversification. I also touch upon the potential of PLMs for optimization, using mutational effect scores for predictive purposes. How can PLMs revolutionize protein engineering and what lies ahead in this promising field?",
    "body": "This is part 1 of a three-part series on protein language models for protein engineering. I hope you find it useful! With the explosion of Generative Artificial Intelligence (GenAI) and its applications in the life sciences, I thought it would be helpful to write out what I see as the technical state of the world for protein sequence generation. Here is my attempt at summarizing what I know about the state-of-the-art in protein language models are when it comes to generating protein sequences, and how to actually use them in practice. What is a protein language model (PLM)? Put simply, it is a neural network model that is trained in a fashion not unlike natural language GenAI models. All we've done is change the inputs from a \"chain of words\" to a \"chain of amino acids.\" Indeed, one of the best analogies that I've encountered is that: a protein language model is a model, like GPT-4, trained to generate protein sequences rather than natural language text. What kind of situations call for the use of PLMs in protein engineering? There are many creative use cases for a PLM! Below, I will outline three specific examples and their commonalities. Example 1: Patent-breaking In this case, we wish to design a protein with a low sequence identity to an existing patented protein. The team has the necessary laboratory budget to test tens, hundreds, or even thousands of PLM-generated variants. The goal is to identify a new variant that is sufficiently different from the patented protein, with success defined as finding a protein variant that has equal or better assayed functional activity at less than 80% sequence identity. Example 2: Prioritized deep mutational scan In this case, we wish to test the effect of point mutations on a protein's functional activity, but because the protein is 900 amino acids long, it would be cost-prohibitive to test all 17,100 single point variants -- we only have a budget to test two 96-well plates worth of variants, and factoring in the need to have appropriate experimental positive, negative, and well position effect controls, we are down to just 148 variants that we can test. Here, we can use a PLM to prioritize the list of single mutations to test (without other functional assay data available) by taking advantage of its ability to generate pseudo-likelihood scores that are also treated as mutational effect scores. We may choose to divide up the laboratory budget of 148 variants such that 75% of them (111 in total) are top-ranked by PLM pseudo-likelihoods. In contrast, the other 25% (37 in total) may be the bottom-ranked mutations, excluding proline and tryptophan substitutions (as they are often known to be deleterious in most contexts). Here, because our goals are to answer the question of mutational effects, success is defined by successfully ordering the variants and testing them, as well as producing an evaluation of how strongly correlated mutational effects were with PLM pseudo-likelihoods. Example 3: Candidate expansion via natural mimicry We want to generate thousands of variants of a protein family without explicitly doing ancestral sequence reconstruction (ASR) as an alternative to the dozens of sequence candidates proposed by ASR. In spirit, the goals of ASR and PLM-based candidate expansion are identical. Additionally, the methods by which we do candidate expansion and patent-breaking are also mostly identical, with the differences most likely being in the training dataset and parameters. For the technical folks reading this, you should note that for these goals, other sequence generation methods, such as a variational autoencoder trained on aligned protein sequences, are also a viable way. Common threads above: plausibility The three examples above constitute a non-exhaustive list of applied situations where we may want to use a PLM. What's common about them? Firstly, the goal is to generate plausible-looking protein sequences. But apart from that, did you notice how we didn't predicate the use of PLMs on the availability of assay data? This is because our goals here weren't to generate optimal protein sequences, they were just to generate plausible ones. The situations above called for the direct use of protein language models, where our goal is diversification. This is also referred to as library expansion or diversification in the chemistry world. What about optimization? As described above, direct use of a protein language model usually implies using the model contrasts with the optimization goal, where we wish to generate protein sequences that maximize (or minimize) one or more protein properties (as measured by laboratory assays). But wait, we know that protein language models like the ESM family can produce mutational effect scores, which are essentially derived from the model itself without knowledge of any assay data. Can't this be used directly for optimization? Indeed, one of the earliest protein language model papers (for ESM-1) included a benchmarking exercis",
    "tags": [
      "protein engineering",
      "generative ai",
      "protein language models",
      "neural networks",
      "bioinformatics",
      "protein sequences",
      "life sciences",
      "optimization"
    ],
    "pub_date": "2024-07-26",
    "type": "blog"
  },
  {
    "id": "blog-conference-report-scipy-2024",
    "url": "/blog/2024/7/14/conference-report-scipy-2024/",
    "title": "Conference report: SciPy 2024",
    "summary": "In this blog post, I share my enriching experience at SciPy 2024, from attending insightful tutorials on Quarto, LLMs, Anywidget, and handling large datasets, to delivering talks on fostering an open-source culture and LlamaBot. I also highlight the vibrant lightning talks, the collaborative sprints, and the engaging social activities that made this conference memorable. Not to forget, the delicious Tacoma cuisine that added flavor to the whole experience. Curious to know which tutorial inspired me to recreate my talks just for fun?",
    "body": "And just like that, SciPy 2024 is a wrap! Because I missed last year's conference, attending it this year in person was a real treat. I met old friends, made new connections, and got caught up with the Python scientific computing and data science tooling ecosystem in line with my goals. Tutorials This year, I was not chosen as a tutorial presenter. That was all good, as I delivered two talks. (There will be separate posts going out on them.) As such, I decided to attend tutorials and learn what I could. Tutorial 1: Quarto The first tutorial I attended was the Quarto tutorial \"Unlocking Dynamic Reproducible Documents: A Quarto Tutorial for Scientific Communication\", led by Mine \u00c7etinkaya-Rundel. Though I had tried Quarto two years ago, I have not tried it since. As such, I thought it would be prudent to see if I could find a new trick or two from the tutorial, and I was not left disappointed. Quarto has improved immensely over the past two years! Of the many improvements I saw, the two that stuck with me the most were special syntax used in creating presentations: creating tabs within presentations and enabling incremental reveal of elements on slides. Two days before I was slated to speak, I was inspired enough to re-create my talks in Quarto markdown just for funsies. Additionally, I learned that Quarto comes with the ability to publish pages to Confluence! This is definitely something worth testing when I'm back at work. Tutorial 2: LLMs The second tutorial I attended was an LLM tutorial titled \"Pretraining and Finetuning LLMs from the Ground Up.\" In this tutorial, famed machine learner Sebastian Raschka led a tutorial that did a fairly deep dive into the fundamentals of training LLMs: tokenization, model architecture, formatting data for training, and actually training an LLM. Lightning.ai provided a workspace for executing code, but I opted to remotely SSH back into my home GPU server and follow the tutorial from there. From my anecdotal interactions with others, the response seemed a mixed bag. This tutorial was too basic for those who were well-versed in LLMs. On the other hand, those whose skill level with language models was elementary but with prior machine-learning experience found this tutorial quite informative. This commentary shouldn't detract from the level of preparation that Sebastian put in; I think it's just a reflection of the hype cycle surrounding LLMs at the moment, where it's hard to target a particular skill level given the diverse groups who are interested in LLMs. My skill level was closer to the advanced side, but I haven't yet prepped data for training LLMs (or protein language models for that matter), so seeing that in action was informative. Tutorial 3: Anywidget The third tutorial I attended was the Anywidgets tutorial, \"Bring your \\\\repr\\\\\u2019s to life with anywidget\", led by Anywidgets creators Trevor Manz and Nezar Abdennur. I believe this tutorial was the best taught of the ones I attended! Trevor and Nezar taught it in such a way that it gave us a very gentle on-ramp to Anywidget. It also included much commentary on how to map JavaScript idioms to Python idioms. This was essentially a 3-notebook tutorial, paced at one notebook per hour (as I perceived it). The first notebook really boiled down the basics of an Anywidget: a Python class that enables a developer to add JavaScript to control the DOM associated with the next Jupyter notebook cell and link it back to Python objects. The second notebook added some complexity by creating widgets that interacted with others. But the real fun was the third notebook: . The goal of that notebook was to create an Anywidget that sounded the Mario chime! If you can, picture yourself being in a tutorial room where the Mario chime randomly rings out from laptops across the room. That's how it was like, and it contributed immensely to the fun that we had. Tutorial 4: Handling Unusually-Sized Data The final tutorial was \"Data of an Unusual Size (2024 edition): A practical guide to analysis and interactive visualization of massive datasets\" by Pavithra Eswaramoorthy and Dharhas Pothina. This tutorial covered many concepts that I already knew, but there were nuggets of information I absolutely cherished: how to store data in the cloud. It turns out that Parquet files are an excellent choice for storing large tabular data in the cloud, as the tooling has caught up to allow us to read slices of data from Parquet files natively in the cloud, (a) omitting the need to set up databases for small and medium data, while (b) ensuring rapid read/write access. Tying this point with a recent tweet from Nikita Melkozerov (essentially how to use a $5 VPS + SQLite to rival hefty Postgres deployments), our choice of data storage system can make a lot of difference when accessing large data via the cloud. Talks My experience with talks this year was primarily from a speaker's viewpoint rather than an attendee. I'll summarize what I spoke about below, knowing t",
    "tags": [
      "scipy2024",
      "python",
      "data science",
      "quarto tutorial",
      "llms",
      "anywidget",
      "large datasets",
      "open source",
      "llamabot",
      "conference activities"
    ],
    "pub_date": "2024-07-14",
    "type": "blog"
  },
  {
    "id": "blog-use-native-formats-when-storing-data",
    "url": "/blog/2024/7/2/use-native-formats-when-storing-data/",
    "title": "Use native formats when storing data",
    "summary": "In this blog post, I share a cautionary tale from my work experience about the pitfalls of using pickle files for data storage, particularly highlighting their dependency on the computing environment and specific package versions. I encountered an issue when a notebook failed to run due to a pickle file not being compatible with the updated version of pandas. This experience led me to advocate for using native formats over pickles for better stability and reproducibility, and underscored the importance of software skills like continuous testing. How can we ensure our data storage methods don't become obsolete with evolving dependencies?",
    "body": "The subtext to this blog post is to \"avoid using pickles\"! I won't repeat other reasons why we should avoid pickles -- the fact that they execute arbitrary code upon reload is the primary one, as it makes pickles a security hazard. But I have an alternative take on this from an experience at work recently. There was a notebook that was created about two years ago. I wanted to re-run the notebook to verify that the code was still usable. The code depended on , but we didn't pin the version of as we were happy to ride the wave of innovations coming on the horizon and because I was eagerly awaiting the releases of . One of the cells in this notebook also depended on programmatically pulling in a pickle file from our internal data versioning systems. It was supposed to be an ArviZ InferenceData file, but I uploaded it as a Python pickle back in the day. This came about because I used our tooling's object upload interface rather than saving it in its native (netCDF) format and checking in the file. Two years later, when I recreated the environment on a new AWS instance and attempted to re-run the notebook, I was heartened to know that it mostly ran but would hiccup on loading that pickle file. It would throw an error whose message was . Baffling! Until I realized that the module in question was removed in Pandas >=2.0, but it was still available pre-2.0. Downgrading and re-creating the environment turned out to be the fix. But it exposed a huge flaw in saving Python objects as pickle files: they will depend on certain package versions, and if those package versions are not pinned in the environment, then we are virtually guaranteeing the inability to reproduce results from computational notebooks later on. My takeaways from this incident are two-fold. Firstly, use native formats over pickles! Native formats are stable and independent of the computing environment, giving us stronger stability guarantees. Pickles, on the other hand, depend on the computing environment\u2014specifically, the dependencies present at the time of pickle creation. If we use pickles, then it will be on us to know which packages to pin to ensure the continued usability of those pickle files. Secondly, software skills pay handsome dividends! Part of a software developer's mental toolkit is knowing the ecosystem of dependencies and how they are evolving. It turns out that I just happened to know that the offending module in question was removed from , because I had to figure it out in another context. (It was in that other context that I first hypothesized that missing modules were likely due to package version changes -- something I would only intuit because I am a software developer as well.) Of the three skills I often refer to (refactoring, documenting, and testing), constant testing would have been the key here. If we invested in continuous testing, we would have caught this issue much earlier than two years later as the PyData ecosystem evolved.",
    "tags": [
      "software development",
      "data science",
      "python pickles",
      "code security",
      "pandas",
      "programming best practices",
      "version control",
      "computational notebooks",
      "dependency management",
      "software skills"
    ],
    "pub_date": "2024-07-02",
    "type": "blog"
  },
  {
    "id": "blog-two-years-of-docathons-insights-and-lessons-learned",
    "url": "/blog/2024/6/30/two-years-of-docathons-insights-and-lessons-learned/",
    "title": "Two years of docathons: Insights and lessons learned",
    "summary": "In this blog post, I reflect on two years of running quarterly docathons at work, dedicated two-day events focused on writing high-quality documentation. I discuss what docathons are, their purpose, and the simple yet effective way to organize them, emphasizing the importance of food, documentation, and optional workshops. Based on both cost and the invaluable benefits of improved documentation practices, I also discuss the significant return on investment these docathons have yielded. How can such a straightforward event substantially enhance the quality of documentation and team engagement? Read on to find out.",
    "body": "At work, we recently concluded our 8th quarterly docathon. This means we've officially run two years of docathons. I wanted to reflect on the docathon: what it is, why we run it, how to run it, and how to make it effective. What are docathons? Docathons are what you might think they are: two days set aside to focus on writing high-quality documentation about our work. We ask colleagues to block off time on their calendars, minimize meetings (if it's impossible to eliminate them), and cater specialty food that we don't usually get to eat. Why a docathon? The subtext question is usually, \"Shouldn't you be doing documentation as part of your work anyway?\" The answer is \"yes, we should,\" but the reality is \"no, we don't.\" Work documentation can easily fall by the wayside when there's constant pressure on shipping stuff. To help foster the habit of documentation, I hypothesized that teams need a chill, lightweight, and socially fun setting in which to write documentation. With the blessings of my then-manager, Andrew Giessel, we organized the first docathon. The intent was simple: get everybody engaged in the same activity for two days so that it was socially acceptable to be laser-focused, silent, and relatively meeting-free. Doing so lowered the energy barrier to making time for documentation. Leaders took part in the exercise as well; we modelled the values that we wanted to put on. Two days a quarter is a sweet spot in terms of time investment: it's sufficiently long for focused work and a sufficiently small proportion of time in the grand scheme of things that it's not too disruptive to daily work (if planned). How does one organize a docathon? A docathon may seem like a big task to organize, but in practice, it can be made super lightweight. One needs three core things: a calendar invite, a sign-up sheet, and food. That's it. Physical space is optional (though helpful), workshops are optional (though helpful too!), and prizes are not necessary at all (the work product is the prize). Here's what a possible timeline might look like, playbook-style. Beginning of quarter - Decide on the date of the docathon, ideally at least two months in advance. Send out calendar invites to groups of people who are invited to participate. - Decide on what food to order. Don't over-rotate on this one; the most important criterion is that it's special but not too expensive. - Set up a sign-up page (e.g. Confluence page, Notion page) that everyone invited can edit. A possible sign-up table might look like this: | Name | Documentation Topic | Reviewer(s) | Day 1 Food | Day 2 Food | Work Product Link | | ------- | --------------------- | -------------------------- | -------------------- | -------------- | ----------------- | | Eric Ma | How to run a docathon | Wade Davis, Andrew Giessel | Spicy Crispy Chicken | Hawaiian Pizza | | | ... | ... | ... | ... | ... | ... | The Work Product link is filled in during or after the docathon. It usually links to a confluence page, a Word document (permissions need not necessarily be visible; just a link is sufficient), or a pull request. Seeing the work products being populated can be superbly motivating for everyone, and it can also be useful documentation if you ever need to justify the money spent on the docathon. - Book a room for a kickoff - where you (re-)introduce the docathon to all participants, new and old. (More on this below). - Call for special workshop topics that individuals may want to lead. - Set up reminders to yourself to send a reminder to invitees to sign up for food on the sign-up page, while also declaring what they will work on, at the following times before the start of the docathon: - one month, - two weeks, and - one week before - two days before NOTE: Setting the calendar reminders right at the beginning is highly recommended! Automate reminding yourself of what you need to do. Throughout the quarter - Send reminders out as previously decided. Day 1 of Docathon - Kickoff the docathon, explaining: - Logistics for the two days (where and when food will take place, primarily) - Why the docathon exists and the importance of documentation, - Commentary, advice, or policies regarding the use of LLMs in writing documentation, - Call out any special workshops that may be organized, and invite their organizers to speak to those workshops' logistics, - And then let everyone go for documentation. - Enjoy food and social time over lunch, and - Continue documenting until the end of the day. Day 2 of Docathon - Continue documenting till lunch - Enjoy food and social time over lunch, - continue documenting until the debrief session, - Debrief with everybody, reminding participants to: - Fill out the table with their work products, - Provide feedback on the docathon, and - Ensure reviewers have reviewed the documentation - Conclude docathon That looks very lightweight...? Is it really that simple? Yes, I've solo organized docathons at least 4 times without workshops, and collabor",
    "tags": [
      "docathon",
      "documentation",
      "team productivity",
      "event planning",
      "work culture",
      "knowledge sharing",
      "writing tips",
      "collaboration",
      "project management",
      "continuous improvement"
    ],
    "pub_date": "2024-06-30",
    "type": "blog"
  },
  {
    "id": "blog-hire-for-communication-skills-not-conversational-skills",
    "url": "/blog/2024/6/26/hire-for-communication-skills-not-conversational-skills/",
    "title": "Hire for communication skills, not conversational skills",
    "summary": "In this blog post, I share insights from a co-op performance calibration, highlighting the crucial difference between conversational and communication skills in the hiring process. I recount an experience where a candidate's excellent conversational abilities masked their technical skills, leading to a dilemma on whether to hire them. Drawing from Andy Grove's 'High Output Management,' I emphasize the importance of using interviews to gauge technical abilities effectively, advocating for code reviews as a high-bandwidth method to assess candidates' skills. This approach minimizes the risk of being misled by mere conversational charm. How can we better distinguish between conversational prowess and genuine communication skills in interviews?",
    "body": "I was recently conversing with a colleague, Srinivasan Varadarajan, about a co-op performance calibration we performed. During that conversation, we discussed the difference between communication and conversational skills. The context was this: Srini and another colleague of mine, Zeran Li, were interviewing co-ops. There was one candidate that both liked -- they had great conversations with the candidate, who had a pleasant personality, and their 30-minute face-to-face time went smoothly and without hiccups. However, they also acknowledged that, even after the interview, they did not know whether the candidate had good technical skills. It's hard to blame Srini and Zeran for this: the candidate had great conversational skills! The problem was that the candidate's conversational skills masked their ability to communicate about their work, making it difficult to make a judgment call about the candidate's technical skill. However, as someone who has been burned before -- I took on an under-skilled student based on being able to get along with that student -- I knew that they would be taking a very big risk if either of them were to hire this candidate. I asked our campus recruiting colleague on the call if we could re-interview that candidate specifically for technical skills. Unfortunately, to ensure fairness in the hiring process, the campus recruiting folks didn't want us to engage in this. As a backup, I looked at the code repositories that the candidate had on their GitHub account. Still, the account was sparsely populated, with mainly generic forked repositories. Of what little original code (i.e. code committed under the GitHub handle), the candidate's code left very few hints that the candidate knew how to do basic organization of code, commenting, documentation of intent, and other signals of someone who is writing code that is intended to be shared with others. Though the final call was in Srini's and Zeran's hands, I advised them to continue looking. But I also told them that if they took on this candidate, I would be there for them if anything went wrong. The interview was over a year and a half ago at this point. In the intervening time, I read Andy Grove's \"High Output Management.\" In Chapter 11, he talks about a manager's \"Two Most Difficult Tasks\": hiring and retaining talented individuals. During the interview, we cannot waste our only asset -- the interview time, in which you have to get as much information and insight as possible... The interview is yours to control, and if you don't, you have only yourself to blame. A good conversationalist is not necessarily a good communicator; a good communicator will not always be a good conversationalist. But an interview that ends up being merely a conversation will waste both a hiring manager's and a candidate's time! A good conversationalist will make you feel like you were chummy pals by the end of the conversation, but a good communicator will leave you with a good grasp of what they know and how they think. A good conversationalist can leave you with warm and fuzzy feelings within an interview setting, but a good communicator will bring clarity in a concise format. For this reason, my preferred interview format is to do a code review of what the candidate considers to be their best work. Not only is code review a proper simulation of how we work, but it is also a high bandwidth opportunity for candidates to communicate clearly what they know and think. By placing actual code up front, there's hard evidence of the candidate's technical skill in their work product. It leaves little room for the candidate to use conversational skills to mask technical skill deficiencies. In terms of evidence of technical skill, nothing speaks louder than a work product!",
    "tags": [
      "interviews",
      "technical hiring",
      "communication",
      "code review",
      "hiring",
      "technical skills",
      "job interviews",
      "collaboration"
    ],
    "pub_date": "2024-06-26",
    "type": "blog"
  },
  {
    "id": "blog-headache-free-portable-and-reproducible-handling-of-data-access-and-versioning",
    "url": "/blog/2024/6/18/headache-free-portable-and-reproducible-handling-of-data-access-and-versioning/",
    "title": "Headache-free, portable, and reproducible handling of data access and versioning",
    "summary": "In this blog post, I explore the importance of reproducibility and portability in data science, focusing on data access patterns. I introduce , an open-source tool that enables data scientists to reference data from a central source of truth and manage data versions explicitly. By using , we can avoid common pitfalls like non-reproducible analyses and streamline the process of accessing and versioning data. This approach not only enhances productivity but also ensures that data is accessed in a consistent and error-free manner. Curious about how and analogous tools can robustify your data science workflow?",
    "body": "Recently, I discussed the following question with colleagues: \"How do we ensure that data scientists work in a reproducible fashion?\" It turns out that related to reproducibility is being able to work portably. Some tools provide what I call \"technological guardrails\" to ensure that one's work doesn't fall into bad patterns, but those can only go so far. Adopting and, more importantly, learning a certain way of working is the long-term sustainable way of ensuring that a data scientist's work is portable and reproducible. Software skills help, and I have written multiple blog posts about them, but today, points about software skills are adjacent to my main point; this article is primarily about data. In the open source world, a tool from Posit PBC called [](https://rstudio.github.io/pins-python/) provides tooling that supports a superbly ergonomic pattern of interacting with data, one that mirrors the way we do so at Moderna. I want to highlight these data access patterns through examples that use . Here are the key ideas that I'd like to highlight: 1. Always reference data from a centrally accessible source of truth. 2. Reference data versions explicitly rather than implicitly. Let's explore them in detail below. Idea 1: Reference data from an accessible source of truth Because of its convenience, it is tempting to reference files on one's local filesystem. Example code that one ends up writing might look like this: This is a guaranteed recipe for non-reproducibile disaster. If your colleague clones the repository after you've pushed up your changes, chances are they won't have access to , allowing them to re-run your notebook to reproduce your data. They will need to ask you for your copy of the data, which comes with its own boatload of problems that I discuss later. A better solution would be to use , which lets one reference data from an authenticated source of truth, such as Google Cloud Storage, AWS S3, or Dropbox. With , what one would do is the following: (If you need a refresher on environment variables, this article may help.) Alternatively, if one wants to work with file paths (my preferred way, more explicit) rather than objects in memory with magical read/write methods (less preferred, less explicit), one can use the API: Using file paths is more flexible. It allows us to pull in any data to and from the pin board before deciding how to load it into memory. is not the only tool available for defined access to data; Intake by Anaconda is another framework that can accomplish similar goals. Idea 2: Reference data versions explicitly In an ideal world, data would never be committed to source code version control systems. Instead, they would be treated as separate entities and version-controlled separately. Doing so allows us to cleanly separate the major entities comprising a project: code and data. If we treat data as immutable and versioned in the same way as code within a repository, we can reference data by explicit version numbers. provides such a facility; specific versions of a file can be pulled in and referenced by a particular version hash: Using this pattern, we can avoid the pathologies of \"versioning by filename\", such as that illustrated by slide 14 of Isabel Zimmerman's PyData Global 2022 talk, copied and modified shamelessly below: Benefits What are the benefits of thinking about and accessing data in this fashion? What's wrong with ? The biggest problem is the ease of portability. In the absence of data versions accessed by code, for the next person to access data, they need to (a) manually request a teammate to provide them with the data file and (b) ensure that the data file is placed in the same directory as specified by the relevant relative path. Point (a) means additional viscosity in requesting access: imagine pinging a teammate for data only to find out she is busy in back-to-back meetings and hasn't responded all morning. Regardless, you've just lost an entire morning of potential productivity. Additionally, point (a) means that you have absolutely zero guarantees that the file your colleague sends to you will be the exact file that has been distributed to everyone else. Any changes in the input data can introduce insidious bugs in the downstream analysis code. Point (b) introduces further viscosity in the form of confusion when running code that depends on data. What if another colleague, operating in a separate notebook, placed the same data file in a different directory () and referenced it there? Now, you have two code files referencing what should be the same data source but with different file paths. By referencing data from an immutable source of truth and pulling it in via code, we get around the aforementioned problems: - we no longer need to ask someone else for a file, - we can guarantee that we will access the exact that is intended, and - we will never encounter the issue of the same file being referenced in two different paths. The only major obstacle left i",
    "tags": [
      "data science",
      "reproducibility",
      "portability",
      "open source",
      "data management",
      "software skills",
      "data access",
      "version control",
      "data patterns",
      "technology guardrails"
    ],
    "pub_date": "2024-06-18",
    "type": "blog"
  },
  {
    "id": "blog-the-neural-von-mises-mixture-model",
    "url": "/blog/2024/6/8/the-neural-von-mises-mixture-model/",
    "title": "The Neural Von Mises Mixture Model",
    "summary": "In this blog post, I do a deep dive into the paper 'The Continuous Language of Protein Structure' by Billera et al., which explores generating protein backbones using autoregressive models and the von Mises mixture model for sampling dihedral angles. This approach challenges the traditional discrete output of autoregressive models by producing continuous values, specifically for modeling protein structures. I discuss the technical and scientific premises, the role of the von Mises distribution, and the potential issue of non-identifiability in mixture models. How does this method open new avenues in protein structural modeling? Read on to find out.",
    "body": "I came across this paper, \"The Continuous Language of Protein Structure\" by Billera et. al., shared via Twitter thanks to Kevin Yang of Microsoft Research. This paper talks about generating protein sequence backbones through autoregressive generation. The technical premise of this paper, which I distinguish from the scientific premise, is that autoregressive models do not inherently require their inputs and outputs to be discrete. Indeed, most of the autoregressive neural network models we have seen generate discrete outputs, such as tokens from a tokenization scheme that maps letters, words, or word fragments to those tokens. However, if one stares hard enough at the structure of such models, they generate a multinomial distribution over tokens, from which we then sample the discrete tokens (categories). So, strictly speaking, the outputs of an autoregressive language model are nothing more than continuous values that we \"interpret\" as being the probability distribution parameter values associated with this long multinomial probability vector, thus drawing a discrete categorical value from that distribution. If so, what's stopping us from producing the probability distribution parameters associated with other things that may also be continuous-valued? Why not produce the parameters of a probability distribution from which we sample continuous values? The Von Mises Mixture Distribution What, then, does all of this have to do with the von Mises distribution and the neural von Mises mixture model? This is related to the scientific premise of the paper, which is to generate protein backbones. Every protein structure comprises amino acids arranged in some 3-dimensional conformation, and between the backbone chain of $N$, $C\\alpha$, and $C$ atoms, there exists phi ($\\phi$), psi ($\\psi$), and omega ($\\omega$) angles associated, which are also known as the torsion or dihedral angles. Below, I simplify an illustration from Justas Dauparas' excellent blog post: All credit to Justas Dauparas for the original illustration, from which I created a simplification. In the illustration above: 1. The nitrogen atom of the amino acid backbone is in blue, 2. The alpha carbon ($C{\\alpha}$) is in green, 3. Side chains, also known as R-groups, are in a yellow circle with the letter $R$, 4. The other carbon involved in amide bonding is in red, and 5. The yellow shaded triangle indicates that those three atoms are considered planar with one another. While in the figure above, all of the angles are depicted on a single plane, it is possible to rotate each of those angles to move other atoms along the chain along the depth, width, and length axes of the 3-dimensional cartesian axes. Thus, a protein backbone chain can be numerically represented as a chain of dihedral angles. As such, one way to generate a protein backbone is to sample each of $(\\phi, \\psi, \\omega)$ from a distribution. The von Mises distribution is a natural distribution to sample from, as it is effectively used to model angular distributions! Moreover, because we may need extra flexibility in modelling $(\\phi, \\psi, \\omega)$ we can draw angles from a mixture of von Mises distributions, which allows for multiple modes in its angular likelihood function. Here is what it looks like, alongside the code that generated the plot, which is a reproduction (with modifications) of what the paper's authors produce. The von Mises mixture model is thus parameterized by the following parameters, as illustrated above. $w$, which is a vector of mixture weight components that sum to 1.0. The number of slots in the vector dictates the number of components present. On the left plot, this is reflected in the heights of each of the dots from the $(\\mux,\\muy)$ plane. For each component, $\\mux$ and $\\muy$, two parameters that control both the von Mises distribution's location and spikiness. These are indicated by the location of the dots on the $(\\mux, \\muy)$ plane on the left plot. The angle induced by the vector $(\\mux, \\muy)$ gives us the location on the unit circle where the highest density is. The magnitude, on the other hand, controls how spiky the probability density is. The greater the magnitude, the less the variance. These are indicated by the component von Mises distributions color paired to the $(\\mux, \\muy)$ dots on the left. Together, the mixture distribution is shown on the right, and it is the summation of the individual probability density functions. (Vertical lines mark out how the mixture pdf is projected to the unit circle. ) According to the paper, each residue's angles are calculated from a factorized joint distribution, in which $\\phi$ is sampled first, then $\\omega$, then $\\psi$. This implies the following graphical structure: (Fig. 2 from the paper.) I've modified the original figure provided by the authors to annotate clearly (for myself) where neural networks exist and what their outputs are. Here is how information flows through the model. If we focus in on $S{t+",
    "tags": [
      "protein structure",
      "autoregressive models",
      "neural networks",
      "von mises distribution",
      "protein backbone generation",
      "mixture models",
      "dihedral angles",
      "machine learning",
      "scientific research",
      "probability distribution"
    ],
    "pub_date": "2024-06-08",
    "type": "blog"
  },
  {
    "id": "blog-how-to-manage-cuda-libraries-within-conda-environments",
    "url": "/blog/2024/6/1/how-to-manage-cuda-libraries-within-conda-environments/",
    "title": "How to manage CUDA libraries within Conda environments",
    "summary": "In this blog post, I share how to resolve CUDA backend initialization issues when installing JAX with CUDA, specifically addressing outdated cuDNN versions. I detail a method using Conda environments to manage CUDA installations and set environment variables correctly, offering two solutions: configuring LDLIBRARYPATH through Conda's activate.d and deactivate.d scripts, or directly within a Python session using a .env file. Both approaches aim to ensure that JAX utilizes the correct CUDA libraries, but each has its tradeoffs regarding portability. Curious about which method might work best for your setup?",
    "body": "UPDATE 9 June 2024: With thanks to the kind efforts of Vyas Ramasubramani, there is actually no need to set at all, as long as one's environment is set up correctly! I detail what works right at the end of the blog post. If you're like me, you've tried to install JAX pre-compiled against CUDA, and have probably also wrestled with issues that look like this: And then you may have sat there scratching your head wondering, \"but wait, I swear that I have the latest CUDA drivers, CUDA libraries, and cuDNN! Why is CUDA so hard?!\" Well, now, there's a way out! As it turns out, independent of your system-level CUDA installation, if you're using Conda environments, you actually can manage your own environment-level CUDA installation! And as it turns out, this is mostly a matter of setting environment variables correctly. Step 1: Ensure that the CUDA metapackage is available in your file Your file should look like this: Now, run to install the packages. What will happen is the following: - The CUDA libraries will be installed into your directory. - and will be installed as Python packages as well, within the environment specified by the environment variable. You may be tempted to run your JAX code at this point, but you'll still run into the aforementioned error. Step 2: Ensure that your includes Now, we need to set environment variables, specifically the environment variable. The is used to specify a list of directories to look for \"dynamic libraries\" before searching the standard UNIX library paths. At this point, options multiply: we may need to make judgment calls or tradeoffs. As I see it, there are two sane places where we can configure to be correct. Option 1: Use conda's and and are folders housing shell scripts that are automatically run whenever we do and , respectively. Jaerock Kwon wrote a constructive blog post on ensuring that these environment variables are set correctly. Essentially, it is the shell script below, which should be run after activating an environment: The tradeoff here is that this is not portable from machine to machine. We'd have to run that chunk of code every single time we create a new environment that uses CUDA libraries or try to recreate the environment in a new machine or a Docker container. Ideally, we would specify this information within environment.yml (link found via StackOverflow). Option 2: Explicitly define this in and load it into your Python session As it turns out, there is another way! Before importing JAX and executing JAX code, we can directly set the environment variable within our Python session through environment variables that we load at runtime. To start, we need to create a file in our repository: Then, within a Jupyter notebook (or a Python session more generally), we use to inject those environment variables into the currently running Python session: We should be able to verify that is set correctly by running: What gets printed should follow the pattern set in the file. Now, you can create NumPy arrays and shouldn't observe any issues with outdated cuDNN: To verify that you're also using GPU, at the terminal, run the following: And you should see something like the following: Which way should I use? Both are good, and both come with a common tradeoff that we need to remember: the environment configuration isn't portable from machine to machine. In the case of the scripts housed under and , they need to be recreated any time the conda environment is recreated. This means that if one deletes the environment and recreates it, or simply moves to a different machine and recreates it, one will need to re-run the shell commands listed above to recreate the environment variables. In the case of configuration, this is much more lightweight, but as a matter of common idioms, files are not committed to source control, which makes distributing them a hassle. My preference is to use files, as they are more easily accessible to us as we work within a project repository. My solution to the problem of portability is to ensure that the specific configuration of individual files is at least initialized as part of a standard project structure (akin to what I do with [](https://github.com/ericmjl/pyds-cli) or what [](https://cookiecutter-data-science.drivendata.org/) does), so that newly created repositories come with these environment variables. And in the absolute worst case, one can simply copy/paste those two lines between two repos' files. UPDATE (9 June 2024): Thanks to Vyas Ramasubramani from NVIDIA, who did a deep dive after reading my blog post, it turns out it is unnecessary to set environment variables as long as the correct CUDA packages are installed. was necessary with CUDA11, but is no longer necessary with CUDA12. An example file that I verified as working on my system are: As I learned through the discussion thread that Vyas created, is needed for CUDA11, while is usable for any version of CUDA running back to version 9.2. This turns out to be the result of ",
    "tags": [
      "cuda",
      "jax",
      "conda",
      "environment variables",
      "cudnn",
      "python",
      "gpu",
      "dynamic libraries",
      "nvidia",
      "software installation"
    ],
    "pub_date": "2024-06-01",
    "type": "blog"
  },
  {
    "id": "blog-multi-modality-deep-learning",
    "url": "/blog/2024/5/27/multi-modality-deep-learning/",
    "title": "Multi-modality Deep Learning",
    "summary": "In this blog post, I explore multi-modality deep learning based on two papers from the biomedical world, in which we explore the definition of data modalities, what fusion is and how it takes place within a model, and possible training objectives. In this post, I also considers how to utilize these models with only one input modality available, highlighting the potential for protein function prediction and sequence generation. How can multi-modal deep learning transform our approach to complex data analysis?",
    "body": "What is multi-modal deep learning? It is perhaps better named \"multi-modality\" deep learning to help distinguish it from probability distributions with multiple modes. (This is both the statistician and deep learning personalities in me speaking simultaneously.) Modality refers to data modalities like text, video, and audio. However, if we go into the subfields of application, multi-modality can refer to granular data types. For example, within biology, we may call an ATAC-Seq dataset to be one data modality, functional assay data from a high throughput screen another, and multiple sequence alignments yet another, even though they strictly speaking are tabular, tabular, and string data. To understand what multi-modality deep learning is, I am going to review and summarize content from two papers: - Fusion of medical imaging and electronic health records using deep learning: a systematic review and implementation guidelines by Huang et al., 2020 - ProtST: Multi-Modality Learning of Protein Sequences and Biomedical Texts by Xu et al., 2023 Fusion: the key to multi-modality models is how data comes together When training a multi-modality deep learning model, a critical point is how to fuse the various modalities. Here, there are multiple ways of doing so, with the best review coming from Huang et al. (2020): We see the major fusion types: 1. Early fusion 2. Joint fusion 3. Late fusion Early fusion is when data modalities are fused early on. Type I early fusion is where we directly fuse (concatenate, add, multiply, or whatever operation is chosen) modalities together before being input into the model. Type II early fusion instead takes extracted features for one or more data modalities as the input to the neural network model. A distinction can be made between extracted features and calculated features. For example, with two distinct imaging modalities, one may calculate a Fourier transform (calculated feature) of the first imaging modality and flatten it into a vector while passing the second image through the CLIP model to obtain an extracted feature instead. Both become vectors that can be concatenated and fed into a downstream model. As another example, one can pre-train encoder-only models for protein sequences and biomedical texts through masked language modelling and then use extracted features from those pre-trained encoder-only models as embeddings for downstream usage. Joint fusion is when two or more data modalities are fused at the level of extracted features. Here, a distinction is made between early fusion and joint fusion. In early fusion, the output is compared to ground truth and a loss score is calculated, from which we tweak the model's (say, a neural network) parameters to minimize the loss. The loss does not affect how the calculated or extracted features are generated. With joint fusion, the loss function affects how the feature extractor performs by back-propagating the gradient of the loss function w.r.t. the neural network's parameters. Late fusion, however, is distinguished by having individual models perform a prediction, on which an aggregation model then combines them. This can be compared to model stacking in Kaggle competitions, where one trains a Random Forest, a Logistic Regression, an XGBoost, and an SVM model to produce a prediction. Then, a final model is trained on top of the individual model's predictions to produce a final model prediction. Training Objectives How do we train a multimodality model? The most obvious answer is \"gradient descent!\" But that would be facetious. Instead, we must ask, \"What are you trying to make the model do?\" To answer that question, we must first understand the model's training objectives. I will focus on how Xu et al. did so in the ProST paper. Objective 1: Unimodal Mask Prediction The ProST paper's first objective is masked language modelling on a single data modality. Specifically, the authors randomly mask 15% of residues and predict each masked token, using cross-entropy as the loss function. The loss function at the end compares the masked positions' probabilities against the ground truth (which might be one-hot encoded). In code, it'd probably look like this: This standard learning objective ensures that the protein language model \"learns\" the presumed complex grammar associated with evolutionarily generated sequences. Objective 2: Semantic Alignment of Embeddings At the same time, to ensure that embeddings between pairs were semantically aligned, the authors of the ProST paper use a contrastive loss as well. Focusing on the specific loss function that the authors use - the InfoNCE (Info Noise Contrastive Estimation) loss - which has several key features: - It requires the use of a similarity function, , that calculates the similarity between the embeddings of pairs of samples (whether it is a \"positive\" (paired) or \"negative\" (non-paired) set). This is a generic function that requires that the two embeddings be of the same size. - It h",
    "tags": [
      "deep learning",
      "multi-modal learning",
      "data fusion",
      "protein sequences",
      "biomedical texts",
      "gradient descent",
      "semantic alignment",
      "masked language modelling",
      "model architecture",
      "embedding conversion"
    ],
    "pub_date": "2024-05-27",
    "type": "blog"
  },
  {
    "id": "blog-how-to-control-pymol-from-jupyter-notebooks",
    "url": "/blog/2024/5/16/how-to-control-pymol-from-jupyter-notebooks/",
    "title": "How to control PyMOL from Jupyter notebooks",
    "summary": "In this blog post, I share my journey of learning to script PyMOL directly from Jupyter notebooks, a skill I picked up with the help of GPT-4. I detail the process of installing PyMOL, setting up the environment, and scripting to process and visualize protein structures, specifically nanobody VHH structures. I also demonstrate how to plot these structures in a grid using . But the more important lesson here is how quickly I was able to pick it up, thanks to GPT-4! Are you able to leverage LLMs as a skill-learning hack?",
    "body": "For a small request at work, I recently learned how to control PyMOL from Jupyter notebooks, specifically to script it without leaving the Jupyter notebook UI, and I thought it was pretty good! Here's how we do it. We first need to ensure that we have installed on our environment: This gives us the package, which we can import: Suppose we have a collection of PDB files, say of nanobody VHH structures: Using the package alongside the built-in package, we can now define two helper functions: Finally, we can do a for-loop over PDB files: Bonus: I wanted to plot all of the PDB files in a grid. This gives us an image that looks like this: I picked this up within minutes using GPT4. The subtext of this post is that GPT-4 is an incredibly useful tool for helping me learn new things. I haven't had many chances to use PyMOL throughout my career, and even when I did, it was mostly in manual mode to visualize protein structures. Scripting PyMOL wasn't something I tried to do, as this was before I felt proficient with Python, and I wasn't processing more than a handful of structures. However, with modern data science idioms, such as working within a Jupyter notebook for exploratory work, using PyMOL \"the old way\" would have disrupted my workflow by necessitating a context switch away from notebooks into PyMOL. As such, I wondered if we could control PyMOL through Python code directly. To discover whether this was possible, my first instincts were that if I could think of this idea, someone smarter than me probably already had thought of it. They probably came up with a solution a few years ahead of me, implying that it should exist within GPT4's training set! My first instinct was to go and ask GPT4 how to do so, which led me to discover the on the channel alongside the code above! Since scripting PyMOL is a form of programming and since the syntax is usually the hardest to remember, the encoded knowledge within GPT-4's weights is a great way to ramp up my exposure to the syntax quickly. Additionally, since I know that I learn fastest in an applied setting, where I pick up something new en route to doing something I already need to do, prompt-hacking GPT-4 as a personalized tutor is incredibly compatible with how I learn.",
    "tags": [
      "pymol",
      "jupyter notebooks",
      "python scripting",
      "protein visualization",
      "pdb files",
      "data science",
      "gpt-4",
      "matplotlib plotting",
      "bioinformatics",
      "automation"
    ],
    "pub_date": "2024-05-16",
    "type": "blog"
  },
  {
    "id": "blog-paper-review-design-of-highly-functional-genome-editors-by-modeling-the-universe-of-crispr-cas-sequences",
    "url": "/blog/2024/5/12/paper-review-design-of-highly-functional-genome-editors-by-modeling-the-universe-of-crispr-cas-sequences/",
    "title": "Paper Review: Design of highly functional genome editors by modeling the universe of CRISPR-Cas sequences",
    "summary": "In this blog post, I do a deep dive into a fascinating paper on designing CRISPR-Cas sequences using machine learning. The authors develop a generative model to produce novel protein sequences, validated in the lab, aiming to circumvent intellectual property restrictions. They curate a vast dataset, the CRISPR-Cas Atlas, and employ various models and filters to ensure sequence viability. My review highlights the methodology, emphasizing the importance of filtering and the challenges of using 'magic numbers' without justification. How many sequences are enough to train a generative model, and what makes laboratory experiments faster? Curious to find out more?",
    "body": "I recently caught wind of a new paper designing CRISPR-Cas sequences. Upon reading it, I thought it was very interesting for multiple reasons. Within this paper, the authors fine-tune protein language models and use those models to generate new protein sequences, which they were then able to validate within the lab. Overall, I thought the paper was well-written, though rough in some places. Here's my review, with a focus on the ML-in-bio methodology. Goal As mentioned in the paper, the author's goal is to develop a generative model of CRISPR-Cas sequences that they could then screen in the lab for activity. Within the industry, one motivation for doing this is to discover novel sequences for intellectual property (IP) reasons to circumvent IP restrictions imposed by patents. Dataset 1: The CRISPR-Cas Atlas To train this generative model, there were multiple datasets curated, each of them biologically motivated. To start, the authors curated \"26.2 terra-bases worth of assembled microbial genomes and metagenomes\". I can guess as to why assembled genomes and metagenomes are important -- if genomes are sequenced using short read sequencing, then those reads (~100 base pairs in length) are often not enough to cover the size of CRISPR-Cas9 complexes (which are in the ~1,000s-10,000s of base pairs). Metagenomes are also important; CRISPR-Cas9 complexes may reside within microbes that are not culturable (i.e. we can't grow them in a lab), and metagenome sequencing enables us to know what may be present within a microbial community without needing to go through the laborious process of figuring out how to culture the microbe(s) in that community. As described in the paper, the dataset also \"spans diverse phyla and biomes.\" From a genome mining perspective, this is important because it expands the universe of potentially undiscovered proteins, and from a protein generation perspective, it gives us a more diverse base of proteins upon which to generate novel, undiscovered sequences. In total, based on the author's description of the dataset, this constituted: - 1,246,163 CRISPR-Cas operons, - including 389470 single effector systems - a total of 5.1 million proteins in the database This became a new resource called the CRISPR-Cas Atlas. No details were provided in the paper on how to access the compiled data, so I think the authors are not planning to release it, especially given this quote: All data used to create the CRISPR-Cas Atlas are available in their original form via the IMG/M, ENA, and NCBI databases. This also gives us, the readers, a hint that the real secret sauce of the model isn't the model itself but the training data. This mirrors the broad sentiment within the deep learning world that fancier models won't do the trick, but getting creative with datasets will. One of the claimed upsides of this dataset is that it represented an increase in diversity for Cas families of proteins, as measured by the number of protein clusters at 70% identity, compared to what's available on Uniprot. While this is a great metric to understand how newly diverse the dataset is, I also wonder how the cluster identity threshold value was chosen for reporting; though shared, it isn't explained. (This is a theme throughout the paper, and I think it's worth highlighting that pure ML papers also fall into the same trap.) Model 1: A CRISPR-Cas Generator The authors train a series of models here. The first model is trained by fine-tuning the ProGen2 protein language model on the CRISPR-Cas Atlas. ProGen2 is a transformer decoder model that autoregressively decodes protein sequences. During fine-tuning, the authors mention that the training set composition enabled \"balancing for protein family representation and sequence cluster size\" but do not appear to refer to any methods section details on how this was done, making it difficult to ascertain how this was accomplished for the reader. Setting this opaque detail aside, the authors used the model to generate 4M protein sequences. 2M were generated \"directly from the model\", and what this means is that the authors provided the (beginning of sequence, or equivalent) token and let the model free to autoregressively generate 2M sequences. Another 2M were prompted \"with up to 50 residues from the N- or C-terminus of a natural protein to guide generation towards a particular family\". This point intrigued me; it's easy to see how one can do autoregressive decoding from the N-terminus since, by convention, the N-terminus is at the beginning of the sequence. Still, I struggled to understand how one can decode autoregressively from the C-terminus until I saw in the paper that sequences \"were provided to the model in forward (N-to-C) and reverse (C-to-N) directions,\" the latter, therefore, allowing for autoregressive decoding while providing the tail of the sequence. However, if one used an encoder-style model (e.g. AbLang), we could get away with feeding in sequences in the N-to-C direction.",
    "tags": [
      "crispr-cas",
      "protein language models",
      "genomic data",
      "machine learning",
      "bioinformatics",
      "sequence generation",
      "protein engineering",
      "gene editing",
      "dataset curation",
      "computational biology",
      "data science",
      "generative model",
      "generative artificial intelligence"
    ],
    "pub_date": "2024-05-12",
    "type": "blog"
  },
  {
    "id": "blog-data-science-in-the-biotech-research-organization",
    "url": "/blog/2024/5/5/data-science-in-the-biotech-research-organization/",
    "title": "Data Science in the Biotech Research Organization",
    "summary": "In this blog post, I share discussion insights from a hands-off tutorial I led at ODSC East on setting up a successful data science team within a biotech research organization. We explored formulating a mission, identifying problem classes, articulating value, and addressing challenges. I used my experience at Moderna to illustrate points, emphasizing the unique aspects of biotech data science. Despite not covering all topics due to time constraints, the discussion was enlightening, highlighting the contrast between biotech and other industries. How can these insights apply to your organization's data science team?",
    "body": "On 23 April 2024, I delivered a hands-off tutorial at the ODSC East. This was my first time attempting a topic like this; I've usually focused on hands-on technical tutorials, e.g., Network Analysis Made Simple. Attempting a non-technical tutorial was a change; I would like to document what I've learned in this blog post. The topic The topic of this tutorial was a discussion about how to set up a data science team for success within a biotech research organization. I specifically chose this topic because of its relevance to my current role at Moderna but also because a biotech's research setting offers some fairly unique challenges that other data science teams may not face. The agenda The specific sub-topics that we discussed in this session covered the following four items: 1. Formulating a team's mission 2. Identifying the kinds of problems that the data science team works on 3. Accurately describing and articulating the value of the work that the team engages in 4. Identifying the challenges en route to setting the team up for success. In my usual nerdy style, I actually flashed up this pseudo-code on the screen: Rather than lecturing, I opted to engage in discussion instead. , however, is a rather ambiguous thing, so I broke it down further into: Here, I provided a small preface based on how I think about each topic but then opened up the floor for the audience to discuss amongst their neighbours how they thought about each topic, done for a few minutes. Once that was done, I then invited a small group of audience members to share what they had discussed in their own group, offering my own synthesis at the end. Based on an informal poll of the audience, about 1/3 of the crowd came from the biotech/pharma space, while 2/3 did not. Of the latter group, approximately 1/5 of them were exploring a career change into biotechs in general, while the rest were generally curious to see what kind of differences existed. Acknowledging the diversity of domain expertise within the audience, I made it a point to document interesting and insightful points made by members of the audience who were not from a biotech setting. Mission We started off by discussing why a data science team has to exist within the biotech research organization. Specifically, I was looking to explore what different team charters looked like, and what each team defined as their north star. During the large group discussion, one audience member's commentary was insightful: the team needs to build a narrative around what the team enables the company to do, something that the company wouldn't be able to do without them, that also links to business outcomes. One example of this might look like: 10x-ing the efficiency of material usage in aerospace manufacturing During the session, I offered why the Moderna DSAI (Research) team exists: to accelerate discovery science to the speed of thought and to quantify the previously unquantified. By no means is this mission a final statement, never to be changed. Rather, it reflects the current state of the DSAI team. The statement was derived from a synthesis of the kinds of work that we do and is a reflection of the team's desire (and my own desire, too) to have a broad impact. Within other biotech research organizations, I can see varying mission statements for the data science team. A data science team situated within a company focused on the production of AAV capsids might have a narrower and deeper mission statement, such as: to enable rapid and IP-safe AAV capsid designs On the other hand, a research data science team within a company focused on microbial ensembles might have one that aligns with the company: enabling robust identification of microbes that enhance crop yields Each team's mission statement is going to be situation-specific, and also dynamic. As such, I would not fuss over getting it right the first time around. It's best to craft something sufficiently bold to inspire the team at first and adapt it to the team's and company's changing ambitions. Problem Classes From the mission statement, we moved on to the topic of problem classes. The key question I posed to the audience to discuss here was: What does your biotech data science team work on? What does it not work on? How does one delineate the work? Here, because of the more practical nature of the topic, the discussion was livelier than the first topic. One lucid point that was raised was as follows: To delineate work, one can define it at the point of handoff. To draw on an example from my home team at Moderna, we collaborate with both computational scientists and wet lab scientists and vary the capacity in which we partner based on our collaborators' needs. For work with wet lab teams doing protein engineering that have less support from data engineering and statistics teams, we may play a more end-to-end role by taking care of standardization of data, data reduction/statistical estimation by Bayesian estimation, and designing new lib",
    "tags": [
      "data science",
      "biotech",
      "team management",
      "tutorial",
      "odsc east",
      "mission statement",
      "problem solving",
      "value delivery",
      "hiring challenges",
      "leadership"
    ],
    "pub_date": "2024-05-05",
    "type": "blog"
  },
  {
    "id": "blog-how-llms-can-accelerate-data-science",
    "url": "/blog/2024/4/17/how-llms-can-accelerate-data-science/",
    "title": "How LLMs can accelerate data science",
    "summary": "In this blog post, I share insights from my talk at the BioIT World conference in 2024, focusing on how LLMs empower data scientists and the necessity of software development skills in data science. I discuss practical applications of LLMs, such as code completion, documentation, debugging, and learning new domains, highlighting their role in enhancing productivity and efficiency. LLMs not only automate mundane tasks but also facilitate rapid knowledge acquisition, proving to be invaluable tools for data science teams. How could LLMs transform your data science work?",
    "body": "I recently delivered a talk at the BioIT World conference in 2024. The slides can be found online here. This post is a written version of that same talk. (As Joyce, the moderator, alluded to afterwards, this talk probably could have been titled \"Prompt Engineering for Data Scientist Efficiency\".) Two threads The talk is inspired by two parallel threads running through my professional life. LLMs are an empowerment tool The first is that LLMs are incredibly empowering as a tool. Moderna, where I work, has fully embraced generative AI as a productivity and efficiency tool, where everyone in the company has access to GPT-4's longest context (128k) model by one means or another. There is an internal academy that everyone in the company is asked to attend that provides training on how artificial intelligence works and a forum for individuals to brainstorm use cases on how to use AI to improve individual productivity. The result has been a flowering of citizen-generated use cases - and some have followed up on it with concrete action! I have seen non-programming scientists chat with LLMs to help them build Python tooling for mass spectrometry analysis, and I have seen colleagues take the contents of documents and translate them into their native language to help with better understanding the jargon inside. At the same time, while we know that LLMs contain super-human knowledge, they are also prone to mistakes through hallucination. As such, we need to be skilled enough to verify LLM outputs. Data science involves writing software The second thread is that data science necessarily involves writing software. This argument starts with Hadley Wickham's talk, \"You can't do data science in a GUI.\" The crux of the argument is that GUIs are too restrictive for the complexity of problems that data scientists need to solve. As such, programming skills are necessary to wrangle that level of complexity. The corollary is that if we coded in a programming language without getting organized like a software developer, our codebase would rapidly become chaotic. As such, software development skills are necessary and can be developed. (This skill is trainable, motivating why I created the Data Science Bootstrap Notes.) Recap So, to recap, there are two threads: 1. LLMs are incredibly empowering as a tool, and 2. Data science necessarily involves writing software What does this mean for data scientists using LLMs? LLMs enable data science teams to build great software At a basic level, LLMs can enable a data science team to build software at a higher skill level than they might otherwise by drafting boilerplate code and lowering the energy barrier to adopting good practices. Code completion Code completion using LLMs is probably the most well-known. We know that if you start typing something along the lines of: Then, code copilot tools such as GitHub Copilot can use the comment as a prompt to complete the code necessary to accomplish what you're trying to do. Being as specific as possible helps you get closer to completing the code one-shot. In-line documentation For code blocks that become functions, with the function body filled out, what's left is writing the docstrings. 98-99% of the time, GitHub Copilot (as an example code completer) can accurately deliver what I need in the docstring, enabling me to use it as a fancy tab completer. As an example, documenting the arguments to a function is usually pretty annoying, but I can usually start with and, Copilot will fill out the rest of the line for me. Test writing Once you have a function body, the following prompt can help you with test writing: As with the first case, the more specific the instructions, the easier it will be for LLMs to help draft test cases. LLMs give your data science teams superpowers But those three use cases are rather pedestrian -- and typical between data scientists and software engineers. What other kinds of use cases can we have for LLMs to aid data scientists? Refactor entire notebooks Data scientists work in notebooks, often churning out entire notebooks full of exploratory code. Some of that code could be organized better. What if we had a bot that could automatically parse the entire notebook and help me propose refactored code? Suppose you have access to an LLM with a very long context window (e.g. GPT4-128k or Claude 3). In that case, you can do just that: either copy and paste the entire JSON of the notebook or export it as a Python script and then use the following prompt to obtain a refactoring of the notebook: Debug stack traces If you're an experienced data scientist, there may be errors for which one glance at the stack trace, and you know exactly what the problem is. However, for the relatively junior data scientist, debugging code can be incredibly challenging, but having access to a well-trained LLM can help this persona grow in debugging skills. The key challenge is overcoming the feeling of being overwhelmed when faced with a stack trace ",
    "tags": [
      "bioit world",
      "conference",
      "data science",
      "llms",
      "software development",
      "productivity tools",
      "ai training",
      "code completion",
      "debugging",
      "documentation",
      "commit messages"
    ],
    "pub_date": "2024-04-17",
    "type": "blog"
  },
  {
    "id": "blog-how-to-make-distributable-pre-commit-hooks",
    "url": "/blog/2024/4/9/how-to-make-distributable-pre-commit-hooks/",
    "title": "How to make distributable pre-commit hooks",
    "summary": "In this blog post, I share my journey of creating my first distributable pre-commit hook, , using the pre-commit framework. This hook automatically converts images to the format before they're committed to a repository, ensuring optimized image storage. I detail the essential configuration files, the creation of a Typer CLI for the hook, and how to make the hook available for others by tagging versions and adding it to a project's .pre-commit-config.yaml file. Curious about how to streamline your codebase with automated checks? How might this improve your project's efficiency?",
    "body": "Following my previous blog post on how to make pre-commit hooks, I finally made my first distributable pre-commit hook that is installable by the [](https://pre-commit.com/) framework! The pre-commit hook is called , and is housed in the repo [](https://github.com/ericmjl/webp-pre-commit). Pre-commit hooks are fantastic! They enable us to run automatic checks on files to be committed into a repository before they are committed. This allows us to automatically ensure that, for example, large files are not committed into the repository or, in the case of , images are converted to the highly optimized format before being committed. Having just learned how to make a distributable pre-commit hook, I am documenting how it works here for future reference. Essential configuration files in the source repository The most important configuration file we need within the repository is called . This defines what hooks are available from within the repository. In my case, I have only one defined, , and it is configured as follows: Let's explain what each line is: - : This is the name identifier of the pre-commit hook to run. - : This is the display name of the hook when it is run at the terminal. - : Something human-readable. - : This is the command that gets run. All relevant files are added as arguments automatically, so in this case, it'll be etc. - : This is the programming language that it's written in. - : A regex pattern for the kind of file extensions that will be subject to checks by this hook. - : An array of dependencies necessary to run the hook. In this case, I have a list of package dependencies. A few questions may be lingering here: what is this command that I wrote? It turns out to be a Typer CLI! This is implemented within the directory: To ensure that the CLI gets run as a command entry point at the command line, we configure this in : Because there's only one command in (defined under the function) I can get away with using as the command to run rather than as the command. Another crucial thing is to have tagged versions of the repository. To ensure this is done efficiently, I use (but should upgrade to soon) when cutting a new tagged release. This tagged version is referenced in my individual project repo's configuration files. Obtaining the pre-commit hook To obtain the pre-commit hook, all we need to do is add the following line into the (not to be confused with the ) file: Now, the framework will automatically manage the installation of the pre-commit hook within its own virtual environment, install dependencies, and run the command upon committing new files. The output should look like this: Summary of steps Let's recap how do we create a pre-commit hook that can be distributed to others. The key steps, in order, are: 1. Create a new repository in which you develop an installable and distributable command-line tool. 2. In that repository, ensure that you have a configuration file that declares that the pre-commit hook is. 3. Use (or manual git tagging) to cut new tagged releases of your pre-commit hook. 4. In a different project repository, add the hook to your file. As you develop the hook, continue cutting new releases through tags. When you cut new releases, go to your downstream project code and ask to update the hooks by running . For teams... Now that you know how to build your own pre-commit hooks, you can be empowered to create custom checks for your code that hold it to your company's own internal standards! I can also imagine many creative uses, including the use of LLM APIs within the pre-commit hooks that get run that parse code for issues that are difficult to detect using programmatic rules, not unlike writing commit messages based on git diffs! Happy code checking!",
    "tags": [
      "pre-commit",
      "webp",
      "optimization",
      "python"
    ],
    "pub_date": "2024-04-09",
    "type": "blog"
  },
  {
    "id": "blog-pyds-cli-version-040-released",
    "url": "/blog/2024/4/7/pyds-cli-version-040-released/",
    "title": "pyds-cli version 0.4.0 released!",
    "summary": "In this blog post, I share the latest updates to , including the use of templates for easy repo scaffolding and a new talks initializer for creating talk presentations using . These updates simplify the CLI and offer a streamlined approach to project and talk setup, reflecting my commitment to promoting best practices among data scientists. With these tools, I aim to make it easier for data scientists to adopt standardized project structures. Curious about how these updates can enhance your workflow?",
    "body": "I have released a new version of [](https://github.com/ericmjl/pyds-cli), and I wanted to share what's new there! Following on the heels of my last blog post, I decided to update to include the following upgrades: 1. The package now uses templates to scaffold a repo. 2. Included in the package is a new initializer that scaffolds out a talk based on the use of . Let's take a look at each of them. Cookiecutter Templates Previously, pyds-cli had a directory within the repo from which we copied files over to a new repository. However, maintaining the code that copied over files was tedious, especially if I wanted to evolve the structure of new repositories to keep up with evolving Python community standards. (One example would be the adoption of as a single configuration file for Python projects.) By switching over to cookiecutter templates, knowing which files get copied over is much easier -- it is everything as templated out in the source directory: An additional benefit of using is the simplification of the CLI! Because comes with its own tools to prompt users for information, I could remove a lot of CLI code that was concerned with the same task. (If you're wondering why there are so many files inside a repo, it follows the core idea of treating data science projects as software. You can read more about it on my previous blog post.) Talks Templating Apart from using templates, I also took the chance to write , which initializes a new repository with materials necessary to build a talk that gets hosted on GitHub pages, with auto-publishing using GitHub actions. As you can see, it's a relatively simple setup with only an file for the talk. There is a Makefile to host the talk locally. This was prompted by my distaste for flashy PowerPoint slide decks and my preference for using plain text to create content. My upcoming talks at BioIT world and ODSC East 2024 will be written using this templating system! Try it out I wrote to democratize standardized project initialization for data scientists. Making it easier for data scientists to do the right thing goes a long way to encouraging best practices. If you're in the same camp, please give a shot!",
    "tags": [
      "pyds-cli",
      "data science",
      "standards",
      "cookiecutter",
      "templates",
      "github actions"
    ],
    "pub_date": "2024-04-07",
    "type": "blog"
  },
  {
    "id": "blog-how-to-grow-software-development-skills-in-a-data-science-team",
    "url": "/blog/2024/4/5/how-to-grow-software-development-skills-in-a-data-science-team/",
    "title": "How to grow software development skills in a data science team",
    "summary": "In this blog post, I share insights from my 7 years in the industry on how to enhance a data science team's software development skills, focusing on the necessity of tooling and practices that make it easy and normal to do the right thing: moving from notebook explorations to production-ready code. I also discuss the importance of community practices in fostering a culture of quality software development within data science teams. How can these strategies streamline your team's workflow and elevate their software development capabilities?",
    "body": "I recently read this article on Martin Fowler's website; the contents resonated powerfully with me! Essentially, the article explains why notebooks in production are a terrible idea and that: This requires moving out of notebook style development after the initial exploratory phase rather than making it a continuing pattern of work requiring constant integration support. This way of working not only empowers data scientists to continue to improve the working software, it includes them in the responsibility of delivering working software and actual value to their business stakeholders. The article didn't touch on how to ensure that one's data science team avoids notebook-in-production syndrome. A related question is what I'd like to discuss in this blog post: How do we grow a data science team's software development skillsets? Distilling what I've learned from my 7 years in the industry, the key ideas I want to touch on are as follows: - We need tooling that makes it easy to do the right thing. - We need practices that normalize doing the right thing. These are tied together in a system of working, ideally, one that is generally stable over time but can be evolved according to changing internal needs or external conventions, backed by a philosophy that none of our work is solely our own but shared, and therefore needs to be accessible by others. What \"the right thing\" is I should first define what I mean by \"the right thing\". Within the context of this article, it'll mean only one thing: taking one's work to production without deploying the notebook in production. Backing this idea is the philosophy that: Data scientists in most companies must treat their work as software to deliver lasting impact. Tooling that makes it easy to do the right thing We need tooling that makes it easy to migrate our work from exploratory notebook code into production-quality code. Production-quality code is essentially code that has been (a) refactored, organized, and standardized to a basic level of quality, (b) tested for correctness and subject to automated testing, and (c) documented. For most data scientists, the most significant challenges about writing production-quality code include remembering minutiae such as: 1. What to put inside each of the myriad of configuration files, 2. The structure of a Python code library, 3. Where to place tests, 4. How to automatically run tests on every commit, 5. Where to place documentation, 6. How to get documentation to become auto-publishable, 7. Where to publish documentation, 8. How to automatically ensure that code stays compliant with minimum code style standards, 9. Commands to deploy code to deployment endpoints (e.g. a pip-compatible store like CodeArtifacts), We can break this down into the following categories of things to remember: - File structure - Configuration file contents - Deployment target These problems can be solved through: 1. initiating a new code repository that is templated with an opinionated and minimally complete file structure, 2. templating the files that are created with the necessary information for development and automated deployment, 3. defining a stable set of endpoints for deploying one\u2019s work and 4. executing any routine commands automatically for environment creation to minimize the mental overhead of remembering those commands. What is an example of this? Allow me to provide an example inspired by how we do it at Moderna: | Step | Example | | ---------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | | Initiation | CLI tool with a command to scaffold out entire Python package: , , and ; also creates a directory for storing notebooks. | | Templating | Source directory files have an and python modules, each equipped with docstrings to illustrate their use. | | Endpoints | Configuration files exist for continuous deployment (i.e. on every commit to main) to internal compute backend ( files), CodeArtifacts, and Confluence for documentation. | | Commands | CLI tool automatically creates project's environment, installs custom source package into project's environment, installs pre-commit hooks for automatic code checking, and, as a bonus, installs the Conventional Commits-based automatic commit message writer that we developed on top of the OpenAI API! | Accompanying this is reference documentation about why each file exists and what it is intended for within the context of a new project. Contextualized within development workflow Let's consider how a data scientist can leverage this file structure to support moving from prototype to production. Early on, when the code is being actively developed and is unstable, the data scientist only needs to operate within the directory (which has",
    "tags": [
      "data science",
      "data science team",
      "software development",
      "upskilling",
      "tooling",
      "environment",
      "productivity"
    ],
    "pub_date": "2024-04-05",
    "type": "blog"
  },
  {
    "id": "blog-llamabot-040-released",
    "url": "/blog/2024/3/24/llamabot-040-released/",
    "title": "Llamabot 0.4.0 Released!",
    "summary": "In this blog post, I share the latest updates of LlamaBot 0.4.0, highlighting the decoupling of document storage from text generation in QueryBot, the introduction of the ChatUIMixin for easy web UI integration, and the switch to LanceDB for its lightweight, SQLite-like handling of vectors. I also touch on enhancements to repo chat, making it simpler to launch web-based chatbots on repository content. If you're a llamabot user, I'd love to hear from you about how well it works for you!",
    "body": "LlamaBot 0.4.0 was released on 23 March 2024! I wanted to share some of the new things that have happened in the repo since I last wrote about it here on my blog. QueryBot Decoupling document storage and retrieval The first version of QueryBot had document storage coupled tightly with an LLM -- a pattern I had picked up from LlamaIndex. But this pattern turned out to cause a world of pain when experimenting with local LLMs. It was only when I recognized that our LLM generator doesn't have to come from the same provider as our text embedder that I could refactor QueryBot into its current form, with separate systems for document storage + retrieval and text generation. Underneath the hood, I did a major refactor of QueryBot (which I only briefly alluded to before) that encoded this information, resulting in the separation of document storage/retrieval systems from text generation. Concretely, we have the following implemented in : - : backed by ChromaDB and configured to use the as the embedding model provider - : backed by LanceDB and configured to use as the embedding model provider - : in-memory document store (literally just a list of strings) using the Python package to retrieve documents. exists to enable hybrid search manually as ChromaDB doesn't support it (as of 24 March 2024), but in the long run, I think will go away as more vector DBs implement hybrid retrieval. It's still there as a vestige of my experimentation, though! ChatUIMixin for composable web UIs Additionally, I created a that enables any of LlamaBot's s to be served as a Panel chat app. (You'll see an example below!) I first encountered the Mixin pattern for real when working with fellow PyMC developer Bill Engels on a project and thought it might be relevant for the ability to compose a bot with different pieces together. The code looks essentially like this: The key lies in the , which by default ensures that every message sent by the user will trigger a on the bot that inherits from the , and the class method that launches the Panel server and loads a browser. With this, one can create a new Bot and, with a single line change, add in web UI capabilities. Previously, you would have: Now, you would do: LanceDB LanceDB has been getting rave reviews from DS, ML, and AI practitioners whose opinions I respect a ton, in particular, because it's as lightweight as SQLite. Vincent Warmerdam has stated it as much: I've been toying with LanceDB and yeah ... I totally dig it. It gives me the SQlite feeling for vectors which is very motivating. Being able to index it to disk and send it off to a service afterwards is exactly what I want for a bunch of hobby projects. - Vincent Warmerdam on LinkedIn I decided to give it a test drive and compare it to ChromaDB, which I've already been using as a default vector DB for LlamaBot's QueryBot. My first impressions were so good that I made LanceDB the default vector database storage system in LlamaBot. The most significant selling point that won me over was the availability of a full-text index alongside the vector store. This enabled me to incorporate hybrid search much more quickly as part of the document retrieval system. (Hybrid search is when we combine keyword search + vector similarity + combined ranking of documents retrieved by the two to get a (potentially) better retrieval of context for LLMs.) As a developer constantly searching for sane defaults, I was excited about this. Now, with LanceDB as the default document storage system for QueryBot, every QueryBot instantiated as of version 0.4.0 will come with hybrid search capabilities for document retrieval by default! Repo Chat I also spent some time upgrading llamabot repo chat. In particular, I restored the ability to chat using the Panel app. Now, serving up a web-based chatbot on any repository's content has become much easier! As an example, if you want to chat with my website (as a way of accessing historical snapshots of my brain), you can do it with the following command: It may take ~1-2 minutes to finish cloning my website and embedding the text using the default local text embedder -- I'm too cheap to configure OpenAI as my default embedder! Once done, will launch a Panel app that you can use to chat. Try asking it some questions about stuff like leadership, career development, or LlamaBot: Or, if you want to chat with the repo to get a personalized tutorial on how to use it, you can launch with the following command: Repo chat is nothing much more than a lightweight customization around QueryBot. Most of the customization involves automatically cloning a particular repository branch and automatically launching the Panel server. Other notes Because I rely heavily on Ollama for local LLMs, I reported an upstream issue in LiteLLM to ensure compatibility for JSON mode with OpenAI's API. (See this issue.) LiteLLM v1.32.7 should now have a uniform JSON mode API between Ollama and OpenAI! Speaking of LiteLLM, Rena Lu of TigerEye also made her",
    "tags": [
      "llamabot",
      "querybot",
      "refactor",
      "chromadb",
      "lancedb",
      "vector database",
      "hybrid search",
      "chatui",
      "mixin",
      "panel",
      "llamabot",
      "repo chat",
      "litellm",
      "contributions",
      "open source"
    ],
    "pub_date": "2024-03-24",
    "type": "blog"
  },
  {
    "id": "blog-how-to-organize-and-motivate-a-biotech-data-science-team",
    "url": "/blog/2024/3/23/how-to-organize-and-motivate-a-biotech-data-science-team/",
    "title": "How to organize and motivate a biotech data science team",
    "summary": "In this blog post, I discuss about organizing and motivating a data science team within a biotech research setting, focusing on structuring team activities around key research entities and methodologies. I highlight the importance of aligning team members with projects that match their interests and professional goals, and suggest ways to foster leadership skills without formal management roles. How do we balance the technical and career aspirations of data scientists to maintain productivity and motivation?",
    "body": "How can we organize a team of data scientists for maximum impact, and how do we keep them motivated? There are many factors which I have discussed before, but in this blog post, I would like to home in on the activities that a data science team engages in. How do we organize the team's activities? And how do we keep the team motivated? Organization Let's answer the first question, \"How do we organize the team's activities?\" To answer this question, we must start with the key entities a research organization engages in. We need to look at Target, Indication, and Molecule. Within small molecule-focused research organizations, the \"Molecule\" entity will be relatively simple (though not trivial by anybody's standards) -- discovering a small molecule will be of prime importance. Within biologics-focused companies, the biologics product being produced should be a focal point. As such, we can see an organizational axis emerging that can include: - Small molecules - Nucleotides - Proteins - Therapeutic antibodies can be split into an independent category depending on the company's scale. - Cellular therapeutics - Microbial ensembles - Gene editing At the same time, there may be a secondary axis of organization, which is methodological in nature. These are primarily organized by the entity delivered to the group(s) that the DS team serves. For example, a lead optimization team looking to accelerate its pace will want delivered libraries of small molecules to increase their odds of finding a good hit. Another team looking to do image quantification may ask for a computer vision model that helps them perform segmentation and geometric + intensity quantification, with a PDF report and charts returned as output. Yet another team may look for a custom algorithm that enables them to automatically design chromatography gradients using Bayesian optimization, with the output being a gradient list created by a machine to maximize separation. With this framing, we are decidedly in the product-oriented (rather than service-oriented) camp. As such, we can see a secondary organizational axis emerging, which might look like: - Machine-designed libraries (including AI-designed ones) - Computer vision - Probabilistic models - Custom algorithms - Bayesian Optimization One can also imagine a third axis that deals with laboratory methods instead. While interesting, to keep this post brief, this will be dealt with during my ODSC tutorial, \"Data Science in the Biotech and Pharma Research Organization,\" at ODSC East 2024. Within the Moderna DSAI (Research) team, we adopt the following organizational axes and the main reason why for each: - Entities: - mRNA - the product we make - Proteins - the therapeutic that gets expressed - LNPs - the cargo delivery method - Methods: - Machine-designed libraries - to serve wet lab teams directly - Computer vision - for coverage on imaging. - Probabilistic models & custom algorithms - our catch-all for other things that don't fit neatly into the first two. Having a clear set of organizational axes gives us two advantages: 1. It allows us to communicate how work is organized and gives colleagues an easy framework for understanding when they should come to us and when they shouldn't. 2. The same framework provides a scaffold for assigning assignments to team members in a way that panders to their interests and professional development goals. Nothing is perfect, though; there is a risk that our 3x3 limits our thinking about delivering value to our organization. For example, there is nothing that dictates that this 3x3 matrix has to remain this way; it can also become a 3x4 matrix, with methods expanding to LLM-based methods that aid in discovering each of the three entities. As team leads, it is imperative that we continually evaluate whether it is necessary to evolve (ideally without negatively disrupting) our work's organizational axes. Motivation The second point is a perfect segue into the second question: \"How do we keep the team motivated?\" Mastery is deeply satisfying, and I would argue that satisfaction from technical mastery is a fairly universal trait among data scientists. We chose such a technical career path for a reason, after all. At the same time, career progression is essential to many people, so it's unwise to pigeonhole people for too long within any box. People may also want to take on leadership duties, but the organization may not yet be mature enough to support those ambitions through formal management duties. At the same time, it is vital to focus on a few threads to complete things so that the team has a portfolio and a constant stream of delivered value over time. With so many factors existing in tension with one another, how do we ensure that our teammates remain productive, connected, and motivated all at the same time? The obvious thing to do is to ask what they want for their future career right when they start and through check-ins during the year. As such, within the Modern",
    "tags": [
      "data science",
      "organization",
      "motivation",
      "research",
      "biotech",
      "team activities",
      "product-oriented",
      "service-oriented",
      "career development"
    ],
    "pub_date": "2024-03-23",
    "type": "blog"
  },
  {
    "id": "blog-mixtral-8x7b-instruct-works-on-an-old-gtx1080",
    "url": "/blog/2024/3/10/mixtral-8x7b-instruct-works-on-an-old-gtx1080/",
    "title": "Mixtral-8x7b-Instruct works on an old GTX1080!",
    "summary": "In this blog post, I share my experience running the Mixtral 8x7b-Instruct model on my old Linux GPU tower. I used the 4-bit quantized model and was pleasantly surprised that it worked. I generated keywords for a paper on protein engineering and machine learning using the model, and the results were comparable to GPT-4. Although the model was slower than running mistral-7b, it was still functional on older hardware. Have you tried running large language models on older hardware? Read on to find out more about my experience.",
    "body": "Today, on a whim, I decided to try running the Mixtral 8x7b-Instruct model (via Ollama) on my old Linux GPU tower. Specifically, I am using the 4-bit quantized model. To my surprise, it works! As always, in LlamaBot, this is relatively easy. To start, on my GPU server, I ran: Then, within my Jupyter on my MacBook Air: In generating keywords for a paper on protein engineering and machine learning, I had the following: This wasn't too bad at all; it feels similar to what GPT-4 would provide, which has been on par with what I've observed with Mixtral-8x7b's output quality. The thing is qualitatively much slower than running mistral-7b (I have not measured tokens per second yet), but it does work. As I mentioned in my previous post, running LLMs on my old GPU tower helped me breathe some new usage life into it. Running Mixtral-8x7b was another hardware challenge that I was eager to see, and I'm glad to have more evidence that LLMs can run on older commodity hardware!",
    "tags": [
      "mixtral 8x7b-instruct",
      "old gpu",
      "linux tower",
      "4-bit quantized",
      "llama bot",
      "keyword generator",
      "protein engineering",
      "machine learning",
      "older commodity hardware"
    ],
    "pub_date": "2024-03-10",
    "type": "blog"
  },
  {
    "id": "blog-industry-career-panel-for-phd-students-and-post-docs-at-mit",
    "url": "/blog/2024/3/9/industry-career-panel-for-phd-students-and-post-docs-at-mit/",
    "title": "From Academia to Industry: Career Advice from MIT Industry Careers Panel",
    "summary": "In this blog post, I share insights from a career panel at MIT where we discussed advice for Ph.D. students about to graduate. We covered the importance of studying job postings, networking effectively, maintaining a public profile, and understanding business needs when applying for jobs. We also touched on the value of a publicly viewable portfolio and the challenge of balancing work and home life. What other advice have you heard?",
    "body": "On 6 March 2024, I was a panelist at a career panel hosted by the MIT Career Advising & Professional Development Office. Alongside me were: - Cynthia Barber: Senior Director, Portfolio and Program Management at Vertex Pharmaceuticals; PhD, Molecular Biology, MIT 2009; Postdoc, Brandeis University 2013; Industry experience: medical writer, clinical scientist, portfolio and program management - Tobias Kaiser: Director, Discovery Research at Emugen Therapeutics (startup); PhD, Brain and Cognitive Sciences, MIT 2021; Postdoc, McGovern Institute, MIT 2023; Industry experience: R&D, entrepreneurship, startups. - Katie Worringer: Director, Neuroscience Targets & Technologies at Novartis; PhD, Biochemistry and Molecular Biology, UCSF 2007; Postdoc, Gladstone Institutes 2014; Industry experience: R&D, principal scientist, director This was an event organized by: - Simona Rosu, Sr. Asst. Dir, Postdoctoral Career and Professional Development, CAPD - Erica Long, Prehealth & Career Advisor, CAPD - Shen Wang, Postdoctoral Associate & President of MIT Postdoctoral Association Shen Wang was our moderator. One question posed to us was what advice we would offer to Ph.D. students about to graduate. I can vividly remember the significant points offered, so I'd like to record my synthesis of the ideas here to share more broadly. Research the market The first idea offered by Katie was to study job postings now to get a pulse check on the market for (a) technical skills and (b) domain knowledge. Doing so may inform one's choice of thesis topic, steering it towards more marketable skills if one so desires. That said, the disclaimer is that one's thesis topic does not necessarily need to match marketable skills and that the market is sometimes challenging to discern and pivot at a moment's notice. The broader value of a Ph.D. is in demonstrating that one can take a problem that is very hard to solve and distill it into a solvable problem with a toolkit that one either possesses or builds out. Build a network The second idea offered by Tobias is to network. This is critical for finding opportunities, as weak ties are known to be sources of new opportunities and information. When networking, especially when sending a LinkedIn request, one needs to anticipate what the other party may perceive when reading one's request to connect. Sending personalized messages is critical for helping to build a good first impression, as they help build confidence in the other party that you are a person worth connecting with (because of thoughtfulness and other traits). We discussed some specifics regarding networking, in particular, writing a personalized message to those whom you want to connect with. We discussed thinking about the matter from the other party's viewpoint, to ask, \"what's in it for the other party\" that they would want to connect with you. For some, the benefit to the other party would be feeling good about mentorship; yet for others, it would be the opportunity to know someone skilled who could bring value to their organization. The worst way to approach this, of course, is to send a connection request without leaving a note. Build a public profile The third idea I discussed was maintaining a public profile. Specifically, I discussed having a public profile as a computational person, but the same idea applies to life science Ph. D.s who are pursuing a computational topic for their thesis. To illustrate the point, I highlighted how I approach hiring as a data science team lead, in particular, how the public profile of two Biological Engineering graduate candidates led me to directly invite them to interview for positions that I had open. Cynthia's fourth idea eludes me at this point, but if I recall it, I will update this post. Other Questions During the interactions afterward, I had the chance to field many questions. Some of them were memorable to me, so I'd like to record my answers to them below. Is it ever too early to apply for a job? This was asked by a PhD candidate who was about to graduate. In response, I offered that it entirely depends on the business need; the corollary is that this is out of a job candidate's control. (I wrote a blog post detailing my thoughts on this.) For one of the positions I was hiring, we had a pressing business need such that we couldn't afford to wait 8 months for the candidate to graduate. For another position I was hiring in early 2023, we were in a state where I knew we would have a bolus of work emerging when the candidate was about to defend and graduate, which was also about 8 months away. These two examples illustrate why the answer is, more often than not, \"depends on business need.\" What goes into my publicly viewable portfolio? This question responded to my point about building a publicly viewable portfolio. For computational types, their work should be on publicly viewable code repositories in addition to their publications. For wet lab-focused students, I suggested that their paper",
    "tags": [
      "career panel",
      "professional development",
      "phd advice",
      "networking",
      "job search",
      "public profile",
      "portfolio management",
      "work life balance"
    ],
    "pub_date": "2024-03-09",
    "type": "blog"
  },
  {
    "id": "blog-your-first-90-days-at-work-what-should-you-do",
    "url": "/blog/2024/2/29/your-first-90-days-at-work-what-should-you-do/",
    "title": "Your first 90 days at work - what should you do?",
    "summary": "In this blog post, I share advice for those starting a new job, focusing on the first 90 days. I discuss the importance of automating your calendar, recording your accomplishments, building a company committee, and choosing a manageable project with tangible impact. These strategies can help you gain control over your career direction and make a positive impression in your new role. How can you apply these tips to your own career journey?",
    "body": "Congratulations! You're fresh out of school and landed a new job. What should you be doing within those first 90 days? Based on conversations I've had with many people, here's a collection of specific actions to help you gain agency over your career direction. Automate your calendar You will likely be an individual contributor, not a manager. As an individual contributor, your calendar should follow the \"Maker's schedule\". As such, you need to learn to be efficient with your time, blocking off long periods of focus time to get into a flow state and accomplish work. I recommend finding ways to schedule blocks of time on your calendar to do work. If that doesn't happen, someone else will take that open slot for a meeting. And meetings, when stuck in at 10 am or 2 pm, can be incredibly disruptive to achieving a flow state of productivity. If you're on the Microsoft Office365 stack, use Microsoft Viva to schedule focus time blocks automatically. They will then show up on your calendar as \"Focus Time\" entries. You can edit them later as blocks used for specific activities. Customize Focus Time to your preferences! It can be 1, 2, or 4 hrs, and it can be mainly in the morning or afternoon, depending on your work habits. The most important thing you can do here is to protect time from meetings to execute work and sharpen your technical saw daily. Also, save time scheduling ping-pong using Microsoft Bookings or Calendly to schedule meetings with external people. If you've ever found yourself trying to schedule meetings without Bookings or Calendly, you'll invariably find yourself knee deep in the following email pattern: Me: Sounds good! Can next week on Friday at 2 pm work! You: Actually, that might not work, can you do Friday at 3 pm? Me: Oh no, that doesn't work either. Can you give me a list of times that work for you? You: I can do the following:... It's a total waste of two people's time! Given current technology, we should automate this problem away. Start recording your accomplishments You need a brag doc. Steve Huynh, former Amazon Principal Engineer, says you need a brag doc. I want to echo that sentiment. What is this brag doc? This document lives outside your company's systems. In it, you record your accomplishments at work without revealing the company's core intellectual property. You have complete control over how it should look -- your only audience is your future self. Nobody else needs to look at it but you. All you need to do is record the following kinds of statements: Delivered {{ thing }} to {{ collaborator/stakeholder group }} using tools {{ put your tools here }} that enabled {{ put quantified value statement here }}. The more of these you have, the better. Why do you need a brag doc? As it turns out, statistically, you're not likely to work for the same manager at the same company for the rest of your career. I know because I've switched once. There may be job cuts, or you might get bored or frustrated with your environment. In those circumstances, you'll leave the company you work for, and you're going to need to update your resume. As Steve Huynh puts it, populating your brag doc will enable you to start from a position of abundance when you need to update your resume. Ask any of my teammates, and they'll tell you that I give them the same advice: I know they'll graduate from my hands one day, so this is one way for them to be equipped for whatever role they may take on next. Can someone else apart from you write your brag doc? Possibly, but it'll never be as good as you. The best managers will try their best to record your accomplishments for you, but even they will miss some things. The person who can record them best is you. (As for me, in keeping with my core ways of working, I have a Markdown file within my Obsidian notes that serves as my brag doc, and it's spartan -- nothing fancy, just a bunch of \"delivered X with tool Y with value Z\" in there.) Build your company committee If you did a PhD or Masters thesis, this shouldn't be a foreign concept. You'll want to find people whom you can trust to seek advice from. In effect, you want to build a committee similar to your thesis committee. But unlike your thesis committee, where your advisor would be part of it, in this case, I advise that your manager be outside of your company committee. Your manager should be playing the role of a coach and mentor already. But their perspective will be biased. You will want a view that isn't theirs. So, consider who can be on your company committee as you do your daily work. Best thing? You don't have to tell them they're on your committee! You only need to schedule one-on-one time with them on some regular cadence. (With my committee, I met with them one-on-one about twice a year, and they never knew!) What should be the traits of your committee? They should be the type of person who is a coach at heart. You need to trust them with information you might not be 100% comfortable sharing with your man",
    "tags": [
      "career development",
      "productivity tips",
      "time management",
      "brag doc",
      "professional development",
      "first job"
    ],
    "pub_date": "2024-02-29",
    "type": "blog"
  },
  {
    "id": "blog-how-to-keep-sharp-with-technical-skills-as-a-data-science-team-lead",
    "url": "/blog/2024/2/25/how-to-keep-sharp-with-technical-skills-as-a-data-science-team-lead/",
    "title": "How to keep sharp with technical skills as a data science team lead",
    "summary": "In this blog post, I share my strategies for maintaining technical skills as a data science team lead. Balancing management duties with technical tasks, I use strategies like performing lower-level tasks, pair coding, code reviews, asking questions, and prototyping. These methods help me stay sharp and credible with my team, while also fostering personal growth. How do you keep your technical skills up-to-date?",
    "body": "I've been a data science team lead for 2.5 years now. Over that period, the team grew from myself + 1 to now myself + 5, with four co-ops cycling through our team. The amount of time that I spend on the following stuff has gradually decreased: - Developing and deploying models - Dissecting papers in-depth - Being the primary collaborator relationship holder On the other hand, the amount of time that I spend on the following stuff has gradually increased: - Project Meetings - Reporting progress - 1:1 coaching and mentorship Over time, I should expect my technical skills (mathematical modelling, software development, and scientific) to wane; indeed, Andy Grove makes this point in his book, High Output Management. But I'm determined not to let that happen -- only by maintaining those skills can I retain credibility with my teammates. If I do not practice Stephen Covey's and Jim Collins' 7th habit (\"sharpen the saw\"), how can I offer coaching, mentorship, and growth to those within my influence? Reflecting on my time in this role, I wanted to share some things that have worked for me. Constraints The primary constraint I have to deal with is time. That constraint becomes even more pronounced when one factors in two kids, one in preschool with set hours that bookends the day. Working long hours is not tenable in the long run. As such, maintaining technical skills means I have to be in perpetual learning mode from everyone around me. Strategy 1: Opportunistically perform lower-level tasks There are times when I will find an opportunity to perform lower-level tasks. One example of this might be working on a machine learning project that has the rough shape of using routine and automated ML methods, leveraging packages like [](https://pycaret.org/) or similar. Another example of this might be deploying a Compute task for a colleague. Doing these things occasionally (but not actively seeking them out routinely) has helped me to remain abreast of the challenges my teammates face when they do these things themselves. At the same time, they free up my teammates' mental bandwidth to focus on thinking deeper and executing excellently on challenging and fun problems. Strategy 2: Pair coding Pair coding is another strategy that I have used to keep my coding muscles warm. I remember a recent pair coding session where my teammate Marcus and I were doing some defensive exploratory data analysis. For two of the sessions we did, we switched roles multiple times based on where we were in our notebook -- him taking over when it was imaging-related code and me taking over when we hit the need to write and code. Apart from having a ton of fun figuring out the most method-chained way to write data processing code, we also had a chance to share our thought processes in structuring code and figuring out how to design dataframes to fit our problem. Strategy 3: Code review Code review is another place for me to stay sharp. With my teammates' work, I ask to be included as a reviewer on pull requests for their work. Doing so allows me to keep up-to-date on our team's work. Given the breadth of topics the team covers collectively, I may be unable to keep up with every last detail. Still, I have a chance to look for the key lines of code that map to critical concepts in my head to ensure I understand how those pieces are implemented. Strategy 4: Asking lots of questions This is a hack I learned in graduate school. Verbal discussions, especially those involving a whiteboard in the mix, are among the highest bandwidth avenues for knowledge acquisition, especially when compared to independently reading a paper. As such, the easiest way to learn something is to ask. Nowadays, when I don't get something, or when something doesn't match my mental model, I'll invariably be the person who asks the dumb question to clarify my misunderstandings. (FYI, for new grads, I have observed that asking dumb questions is a rare trait, but it is a personal development hack!) A stand-in for that is to go whole-hog Socratic by chatting with papers. With the rise of LLMs, doing so has become more accessible. The real skill here lies more in devising questions and less in finding the answers, as the questions, driven by curiosity and led by the paper's narrative, are what end up sticking (at least for me). (In fact, I built LlamaBot in order to do that at the command line!) Strategy 5: Work things out by hand and prototyping The next thing I'd like to share is occasionally working things out by hand. Victor M. Zavala, a University of Wisconsin at Madison professor, mentioned this on LinkedIn post, which I also archived on GitHub Gists. Part of this practice is finding ways to distill a complex problem down to its minimally complex version -- one that is at least tractable on a whiteboard but still contains the essential elements of the complexity of the real problem. When doing this with my teammates, this practice enables us to focus on the essence of the problem ",
    "tags": [
      "data science",
      "leadership",
      "coaching",
      "mentorship",
      "continuous learning",
      "technical skills",
      "machine learning",
      "pair coding",
      "code review"
    ],
    "pub_date": "2024-02-25",
    "type": "blog"
  },
  {
    "id": "blog-llamabot-with-ollama-on-my-home-virtual-private-network",
    "url": "/blog/2024/2/21/llamabot-with-ollama-on-my-home-virtual-private-network/",
    "title": "LlamaBot with Ollama on my home virtual private network",
    "summary": "In this blog post, I share how I breathed new life into my idle GPU tower by running an Ollama server on my home's private network. I connected all my devices via a Tailscale virtual private network and installed Ollama on my GPU server. I then used LlamaBot to build bots that utilized the Ollama server. This turned out to be an effective way to extend the usable life of my GPU box. Curious about how you can do the same with your idle GPU? Read on!",
    "body": "Introduction At home, I have a relatively idle GPU tower. It's something I bought way back in 2016 to do deep learning. It has an NVIDIA GTX1080 GPU in there with 8GB of RAM. By today's standards, it's puny. Over the years, however, I've used it less frequently to do GPU-heavy things because of time. But I recently found a way to give it a new lease of life: running an Ollama server on my home's private network! I wanted to share how I made that happen in this blog post. Setup Tailscale I have all my personal devices (my M1 MacBook Air, phone, tablet, a DigitalOcean server running Dokku, NAS, and my home GPU box) running on a Tailscale virtual private network. Since my home GPU box is running Ubuntu Linux, I used the official Tailscale Linux installation instructions to get Tailscale installed on my GPU box, ensuring that it was on the same VPN as my MacBook. Install Ollama on GPU box Once I did that, I then installed Ollama on my GPU box. While -ed into my GPU server, I executed the command on the Ollama Linux installation page, which was: To verify that Ollama was installed correctly, on my GPU box, I executed the command: Doing so allowed me to verify that Ollama was installed correctly. Configure Ollama for network access By default, the Ollama web server runs on 127.0.0.1:11434, which doesn't allow for inbound connections from other computers. To change that behaviour, we must change the environment variable to . I followed the instructions in Ollama's documentation. To start, we edit the service: Then, we add the following contents to the text file that gets opened up: Finally, after saving and exiting the text file, we reload and restart Ollama: Test Ollama access remotely Now, Ollama will be running on host . To verify that it is running correctly, I went back to my laptop and ran the following command: I got back a long stream of JSONs: I thus verified that I could connect to the Ollama server running on my GPU box! Check GPU usage Knowing Ollama's behaviour, I knew that the mistral model should be loaded into GPU memory for a little while before being taken down. To verify that it was indeed using the GPU, I ran: Which gave me: Perfect! Interact with Ollama using LlamaBot Taking it one step further, I decided to connect to my Ollama server using 's class. In principle, it should be easy to do so because we have a LiteLLM pass-through for additional keyword arguments, and that meant I should be able to do so with: And indeed, it works! I get back my usual mistral bot response: Try a different model I can even easily swap out models (as long as they've been downloaded to my machine): This gives me: (Llama2 appears to have a goofier personality.) Limitation: models need to be downloaded and available One limitation (?) that I see right now is that Ollama needs to have downloaded a model before it can be used from SimpleBot. As an example, I don't have the Microsoft Phi2 model downloaded on my machine: Thus, when running SimpleBot using Phi: I get the following error: The way I solved this was by SSH-ing into my GPU box and running: You can think of the Ollama server as being a curated and local library of models. Conclusion Because of Ollama, running an LLM server on my home private network was much easier than I initially imagined. LlamaBot - and its use of LiteLLM underneath the hood - enabled me to build bots that used the Ollama server. This turned out to be a great way to extend the usable life of my GPU box!",
    "tags": [
      "gpu",
      "deep learning",
      "ollama",
      "llm",
      "tailscale",
      "linux",
      "ubuntu",
      "gpu",
      "llamabot"
    ],
    "pub_date": "2024-02-21",
    "type": "blog"
  },
  {
    "id": "blog-dashboard-ready-data-is-often-machine-learning-ready-data",
    "url": "/blog/2024/2/18/dashboard-ready-data-is-often-machine-learning-ready-data/",
    "title": "Dashboard-ready data is often machine learning-ready data",
    "summary": "In this blog post, I discuss the overlap between dashboard-ready and machine-learning-ready data. I share an example from a chemical screening campaign, where the same data used for a dashboard can also be used for machine learning models. I explore the reasons behind this from both a statistical and business perspective. How can you gain leverage in your data for both purposes?",
    "body": "This may be something obvious to some but not others: Dashboard-ready data is often machine-learning-ready data. Dashboards, in my experience, are the place where data science projects go to die, and that's why the DSAI teams at Moderna generally don't build UIs or dashboards but instead have a laser focus on delivering CLI tools that can be run on the cloud, or Python packages. But one thing recently became clear: the data that might power a dashboard is often ML-ready data. Here's an example from my past life: a chemical screening campaign. For a team embarking on a campaign, it would be nice to know the lead molecules within a campaign and how they stack up against all of the other molecules tested in the same campaign. We'd usually collect extremely raw assay data, ideally with replicates but sometimes without. We may collect a dose-response curve, for which the IC50 value needs to be quantified. As such, we first need to transform that raw measurement data into IC50s by fitting them through a dose-response curve. The process looks like this for two molecules, one blue and one red. But now, we have a table that can be made with the following schema: - Molecule: a SMILES string that can be parsed in RDKit and - IC50 (in nM units): the quantitatively measured IC50 value according to the assay. | Molecule | IC50 (nM) | | :--------: | :---------: | | A | 25.2 | | B | 31.3 | | C | 10.8 | | D | 28.7 | One could use that data table to build a dashboard; at the same time, that data is also ML model-ready! From here, we can build Quantitative Structure-Activity Relationship models that can be used to prioritize new molecules to study. It's no secret that modern QSAR models are often machine learning models, but their underlying foundation is the same as dashboard-ready data: molecule-property mappings. But why? Why would dashboard-ready data often be ML-ready data? From a statistical perspective, ML models of the predictive flavour operate on summarized data. Dashboards also often need summarized data, as opposed to un-summarized data. From a business perspective, dashboards are built to support business decisions. Machine learning models are often built with business decision support in mind, which is why they, in practice, also use the same data as dashboard-ready data. So, whether we are curating datasets for dashboarding or machine learning purposes, keep in mind that there is a very high likelihood that the same data can be used for both.",
    "tags": [
      "data science",
      "machine learning",
      "data engineering",
      "python packages",
      "chemical screening",
      "predictive models",
      "data curation"
    ],
    "pub_date": "2024-02-18",
    "type": "blog"
  },
  {
    "id": "blog-success-factors-for-data-science-teams-in-biotech",
    "url": "/blog/2024/2/7/success-factors-for-data-science-teams-in-biotech/",
    "title": "Success Factors for Data Science Teams in Biotech",
    "summary": "In this blog post, I shared my insights from the SLAS 2024 conference on how a data science team can deliver long-lasting impact in a biotech research setting. I discussed the importance of bounding work, identifying high-value use cases, possessing technical and interpersonal skills, and having the right surrounding context. I also shared some personal experiences and lessons learned from my work at Moderna. How can these insights help your data science team become successful agents of high-value delivery? Read on to find out!",
    "body": "I spoke this year at the SLAS 2024 conference. The track that I spoke in was the Data Science and Artificial Intelligence track. I'd like to share the contents of that talk here, while also fleshing out additional content that I didn't put in there. First off, the slides can be found on GitHub pages. I wanted to answer two key questions with this talk: 1. What does it take for a data science team to successfully deliver long-lasting impact through data products? 2. How do we do that in a biotech research setting? For that, there were four key ideas that I shared: 1. A framework for bounding work 2. Clarity on high-value use cases 3. Technical and interpersonal skillsets 4. Necessary surrounding context Behind this is a philosophical stance. I'm not interested in building things that live only for the duration that I'm at a company (or shorter). I'm also not interested in my impact, and my teammates' impact, being measured by how many times we've been acknowledged on a colleagues' internal presentation. I'm most interested in wanting to build lasting solutions that live beyond my duration at a company, so that we have maximum leverage delivered. Once we've got lasting solutions deployed against stable business/research processes, we get to divert our time and energy to more fun and impactful problems instead. Now, let's talk about each of the four ideas. Idea 1: Framework for bounding work Here, I used the example of the DSAI (Research) team at Moderna, where in our team charter, we have a 3x3 matrix that outlines what we work on: | | mRNA | Proteins | LNPs | |---|------|----------|------| | AI library design | \u2705 | \u2705 | \u2705 | | Computer vision | \u2705 |\u274c | \u2705 | | Probabilistic models + custom algorithms | \u2705 | \u2705 | \u2705 | Our work fills 8 out of the 9 cells in that matrix, and we cover that much work with only 5 people. There's a ton of leverage that we have in there because of the ways of working that we've adopted. Note, however, how there's a distinct lack of certain things that other companies might associate with a data science team: - Bioinformatics - Computational chemistry - mRNA design - Computational protein structure modelling We have professional teams that are capable of doing these things. Our colleagues are very clear that the DSAI team doesn't have a primary mandate to do these lines of work. That clarity doesn't necessarily imply that we don't touch some of the tools used by our non-DS colleagues when necessary, for e.g., in protein engineering campaigns, we will bust out AlphaFold + PyMOL when needed. That clarity helps us find appropriate collaborations and places for high value delivery. Idea 2: Clarity on high-value use cases We have a framework for defining highest-value use cases. 1. Primarily, we look at the trade between a data scientist's time and another team's time. 2. Secondarily, it is about new capabilities unlocked for colleagues and their expected value gain. If there's a request for which we're unable to shape up a sense of value along these two lines, we generally won't work on those. On the other hand, if there is extreme clarity on the value of a line of work from those two lenses, we'll prioritize that line of work. I'm reminded of a conversation I had with Dave Johnson, in which Dave reminded me of the importance of high value work. Stephane (Moderna's CEO) once asked me, what's the value here? You've only traded your time for one other person's time. Where's the ROI? That story has stuck with me ever since. Along those lines, how does this look like in a biotech research setting, then? What are examples of high value use cases? I offered three examples: 1. Automated chromatography analyses: trading one data scientist's time for many multiples of PhD-trained analytical chemists' time sitting in front of their computers wrangling the Chromeleon software. 2. Predictive models used to generate AI-designed molecule libraries: trading one data scientist's time for many multiples of laboratory researchers' time exploring sequence or structure space. 3. Quantitative image analysis: unlocking the ability to geometrically quantify on images what we might otherwise qualitatively state. At its core, we have to think about the value that is being delivered. Technical and Interpersonal Skillsets These come back to the five key skillsets that I interview for. 1. People skills 2. Communication skills 3. Scientific domain knowledge 4. Software development skills 5. Modeling skills In fact, one may conceive of them as a pyramid: The bottom three are technical foundations that form the base of credibility for our work. The top three are boosters for our credibility in our colleagues' eyes. And credibility is what I'm optimizing for with this criteria. I've written about these criteria before in the essay, \"Hiring and Interviewing Data Scientists\", but I'll dive into what I mentioned at the talk. Modelling Skills With respect to modelling skills, I am of the conviction that it's insufficient for us to",
    "tags": [
      "talks",
      "conferences",
      "slas2024",
      "data science",
      "biotech"
    ],
    "pub_date": "2024-02-07",
    "type": "blog"
  },
  {
    "id": "blog-an-incomplete-and-opinionated-survey-of-llm-tooling",
    "url": "/blog/2024/2/1/an-incomplete-and-opinionated-survey-of-llm-tooling/",
    "title": "An (incomplete and opinionated) survey of LLM tooling",
    "summary": "In this blog post, I explore the rapidly evolving landscape of large language model (LLM) tooling, discussing APIs, self-hosting, API switchboards, Python-based LLM Application SDKs, vector-based retrieval, prompt experimentation, evaluation, UI builders, and command-line interfaces. I share my experiences building LlamaBot and offer principles for making smart tech stack choices in this ever-changing field. How can you navigate this dynamic ecosystem and make the best decisions for your LLM projects? Read on to find out!",
    "body": "As the large language model (LLM) world has exploded in popularity, I see the need for clarity on the state of LLM tooling. More critical than specifying specific tools, my goal with this blog post is to outline a framework for thinking about the components of LLM tools. I hope you find it valuable! Disclaimer: As someone who was professionally raised in the community-driven open source software (CDOSS) world, you will probably see my biases shine through company-backed open source software (CBOSS) that is used as a play to attract business. (CBOSS that is a derivative of one's core business, is a different story.) As such, I can confidently assure you that I am not paid by any tool developers or the companies that back them. Access To build anything that uses LLMs, one needs access to LLMs. There are two sources: APIs and locally hosted. APIs The easiest way is to get access using an API provider, and the one that has the most mindshare is OpenAI's API. (Much of the LLM Python tooling I've seen is built around OpenAI's API.) Another contender is the Mistral API, which provides the Mixtral models hosted. Anyscale also has an offering for hosted LLM APIs. As we will see in the API Switchboards section below, there are more! Self-Hosted Alternatively, you can self-host open-source models. Self-hosting requires more tooling but allows for control of costs. To this end, Ollama is a fast-growing alternative to hosted APIs (its community Discord is active with ~2k members online on 30 January 2024). Ollama provides a dead-simple (and, most straightforward) mechanism for exposing an API endpoint for open-source models. The alternative would be to build a FastAPI server and combine it with HuggingFace's Transformers models, which would take more technical finesse. API Switchboards One may desire to switch between different models or even model providers. Hence, one will need an API switchboard for experimentation. To that end, LiteLLM provides an API switchboard that enables OpenAI API compatibility for the broadest subset of API providers: Claude, Anyscale, OpenAI, Mistral, Azure, AWS Bedrock... you name it! Python-based LLM Application SDKs Regarding the Python toolkits needed to build LLM applications, LangChain is possibly the first thing most people will think of. Indeed, it has grown massively in popularity, hitting 75K stars on GitHub (as of 29 January 2024) through a concerted community-building effort, content marketing, and (I am imagining) VC dollars. LlamaIndex is another application-building toolkit focusing on Retrieval-Augmented Generation (RAG) applications. It is also another hugely popular toolkit, hitting 25K stars on GitHub (as of 29 January 2024). There are also high-level developer tools, such as EmbedChain, an open-source retrieval augmented generation (RAG) framework. (More on RAG below.) Vector-based Retrieval For RAG applications, one needs a vector database to store vector embeddings of text and retrieve them based on vector similarity. The number of different vector databases available has exploded in number, and so it may be challenging to keep track of what to use. My choice of vector database has been ChromaDB, but there are other compelling offerings as well: Pinecone, Weaviate, LanceDB and FAISS are all examples of vector storage systems, and packages like RAGatouille provide state-of-the-art vector search & retrieval systems (like ColBERT). Additionally, LlamaIndex implements vector indexes that live entirely in memory and are cacheable to disk as standalone files. The primary use case for vector-based retrieval systems is RAG. RAG is emerging as an essential method for augmenting LLMs with additional data that lives outside the scope of its training data. As such, the criteria used to evaluate RAG tools are worth considering. But what should one focus on? As a builder type, I focus on the following criteria: ease of setup, ease of retrieval, cost, and quality of documentation. Here's what I think about them. For ease of setup, this meant gravitating towards anything that could be stored locally (or in memory) without manually setting up a server/client interface, akin to what SQLite does for relational databases. (Both ChromaDB and LanceDB do this.) For ease of retrieval, this meant picking tools that, within a single API call (e.g. ), would automatically embed the query string and return texts rank-ordered. For cost, the key driver here is money spent on embedding texts to obtain vector representations of it; ChromaDB, for example, defaults to the HuggingFace SentenceTransformer (which is free to run locally), while LlamaIndex defaults to using the OpenAI Embedding models, which requires a modest (but affordable) amount of spend for prototyping. Finally, the documentation quality matters as always: we developer types will spend much time wrangling tools, and good documentation will deliver us from immense headaches. (Good documentation will also help developers robustly build the",
    "tags": [
      "language model",
      "open source",
      "api",
      "python",
      "vector retrieval",
      "prompt experimentation",
      "ui",
      "command line interfaces",
      "llms",
      "framework",
      "zotero"
    ],
    "pub_date": "2024-02-01",
    "type": "blog"
  },
  {
    "id": "blog-exploratory-data-analysis-isnt-open-ended",
    "url": "/blog/2024/1/28/exploratory-data-analysis-isnt-open-ended/",
    "title": "Exploratory data analysis isn\u2019t open-ended",
    "summary": "In this blog post, I challenge the traditional approach to exploratory data analysis (EDA) in data science. I argue that EDA should be directed and purposeful, not aimless. I share key principles for effective EDA, including falsifying our assumptions, having a clear end purpose, and embracing iteration when purposes are invalidated. I also emphasize the importance of practice and domain expertise in developing this skill. How can we make EDA more purposeful and effective in our data science work? Read on to find out!",
    "body": "Misconceptions A common thing that data scientists are taught to do in our line of work is to do \"exploratory data analysis,\" also known as EDA. The way I've seen curricula teaching data science approach EDA is to emphasize the tools: pandas, matplotlib, seaborn, and techniques: find correlations, look for \"anything interesting.\" This pedagogical approach misses the mark. Exploratory data analysis should be directed and purposeful, not undirected and purposeless. Let me break this down with an example. Directed and purposeful EDA At work, we are exploring training our generative models with end-goal use cases in mind. (I will not describe them in detail here, but it all pertains to molecules and biological sequences.) Our space of generative models needs to be tailored to the therapeutics that we make. Therefore, a generative model's inputs (i.e., the data) must also be appropriately tailored. We've got many public data sources and internal proprietary datasets that could be used. So, what does EDA look like in this context? Do we go about \"just exploring\" the sequence data and its metadata for new insights? Do we open and and go to town on the keyboard, hammering out every visualization possible? Heck no! Within our generative model exercise, we came up with specific questions of the following patterns: what subgroups of sequences were there that could bias our generation process? Which sequences were annotated with valid subgroup names and which had invalid names (so we could re-annotate them with alternative computational methods to better curate the subset we are interested in)? Did the biological sequences we wanted to use as training data match the synthetic sequences we hoped to generate? What was the distribution of pairwise sequence similarity to one another (affecting whether we would be over-training on a very narrow sliver of sequence space)? Notice how none of the EDA questions are asked in isolation. Instead, they are logically linked up to 2-3 steps downstream to our overarching goals. They serve as defensive sanity checks on our data to ensure that the data are suitable for our purposes, with the ability to disprove our hypotheses around the data's suitability up-front before investing further time into model-building. Key Principles Principle 1: Falsify your purpose The first key idea that I want to communicate here is that we need to do is to falsify our assumption/hypothesis that the datasets we have on hand are suitable for our purposes, and I hold that this is the real goal behind exploratory data analysis! We needed to find warts in the data that may negatively impact our ability to make that tailored generative model. If you remember, we're data scientists, so there has to be science somewhere: The science in data science includes the scientific discovery process, the constant checking and falsification of hypotheses that are core to our role. Principle 2: The end purpose must be clear A key operative word here is \"our purposes.\" Do you know what you're building towards? Can you describe it with sufficient clarity to multiple diverse audiences? If not, your EDA will wander, meander, and likely be fruitless. Principle 3: Iteration emerges when purposes are invalidated When viewed from the lens of our daily activities, EDA is the \"backward\" process counterpart to the problem ideation (\"forward\") process. Going backwards from the problem, we start searching for warts in our data that'll invalidate our effort to solve the problem and, in doing so, allow us to fail fast and early. If our problem statement survives the data-backed scrutiny, we use that data to solve the problem. On the other hand, if we find that our data invalidates our problem statement, then we need to go back to the drawing board and figure out whether (a) we need to look at a different data source, (b) generate our own data, or (c) reformulate the problem entirely into something else that is of value. Principle 4: Practice How do we develop this muscle? With experience and, more crucially, domain expertise comes the judgment necessary to begin by asking these questions. Classroom teaching can only go so far; much more important is constant practice and feedback from others, or in other words, research training. For me, it's come from 11 years of research-oriented training + career work to finally realize this, including twists and turns being confused about how to do EDA and then how to teach others to do EDA. Conclusions I hope this post helps shortcut your learning journey! What were your thoughts after reading this post? Let me know below!",
    "tags": [
      "data science",
      "eda",
      "exploratory data analysis",
      "pandas",
      "matplotlib",
      "seaborn",
      "correlations",
      "generative models",
      "therapeutics",
      "biological sequences",
      "metadata",
      "visualization"
    ],
    "pub_date": "2024-01-28",
    "type": "blog"
  },
  {
    "id": "blog-your-embedding-model-can-be-different-from-your-text-generation-model",
    "url": "/blog/2024/1/15/your-embedding-model-can-be-different-from-your-text-generation-model/",
    "title": "Your embedding model can be different from your text generation model",
    "summary": "In this blog post, I debunked the misconception that embedding models must match the text generation model in retrieval augmented generation (RAG). I explained how these models are decoupled, with the choice of embedding affecting only the quality of content retrieved, not the text generation. I also shared my preference for SentenceTransformer due to its cost-effectiveness and performance. Finally, I updated LlamaBot to reflect this understanding, allowing for more flexible model composition. Curious about how this could change your approach to RAG? Read on!",
    "body": "This is a misconception I used to have: I assumed that my embedding model had to be from the same provider when doing retrieval augmented generation. For example, if I were using an OpenAI model like gpt-3.5-turbo, I would assume that if I wanted to do RAG, I would need to use the OpenAI embedding model. This turns out to be untrue! We embed a document to perform a semantic (or vector similarity) search, a distinct step from text generation. As long as two texts are embedded using the same model, their embeddings are comparable. Using embeddings and semantic search, we are retrieving a collection of documents (or document chunks) purportedly relevant to a query text we provide. Vector databases usually store the text and embedding next to one another, or at least link them together, so that once we know which embedding is similar to our query, we can also learn what text was associated with that embedding. Once we have that text retrieved, the rest of what we do is to use that text as part of the prompt that we provide to an LLM, which can generate text without using the embeddings, just the text. See the diagram below to make this clear: In this way, embedding models are decoupled from the text generation model \u2014 the latter does not use the output of the former. But they are nonetheless linked by the text associated with the embedding. Which embedding should we use? Will embedding choice affect our text generation model? For RAG purposes, the short answer is \"probably not directly.\" But the long answer is, \"It's a bit more complicated.\" To start, we should dispel the notion that our choice of embeddings will directly affect the performance of our text generator. As my diagram showed, the retrieval function in the most common RAG setup separates text generation and embedding usage. At most, the embedding choice will affect the quality of our similarity measure, affecting the kind of content we retrieve from the vector database and the content we pass to the text generator. I prefer to have sane defaults in life to handle most Pareto-distributed scenarios. The same applies here. The most important thing to look at here is which embedding model has the best performance, measured along a few axes. As a pragmatic start, I would consider the following criteria: 1. Hardware requirements 2. MTEB (Massive Text Embedding Benchmark) performance (see this paper and the leaderboard here) 3. Cost of operation A detailed comparison is likely the topic of a different blog post. However, after test-driving a few embedding models (OpenAI's , Mistral's embedding model, and the that ChromaDB uses by default), I now use on the basis that it is the cheapest -- it is free and runs on my hardware. It is also the (sane) default encoded within LlamaBot's new and shiny . LlamaBot encodes this knowledge. Speaking of LlamaBot, I updated it to reflect this new knowledge. In doing so, we can now freely compose embeddings from various models with text generation performed by other models. Come check it out!",
    "tags": [
      "embedding models",
      "retrieval augmented generation",
      "semantic search",
      "text generation",
      "vector databases",
      "llamabot",
      "documentstore",
      "sentence transformer"
    ],
    "pub_date": "2024-01-15",
    "type": "blog"
  },
  {
    "id": "blog-github-actions-secrets-need-to-be-explicitly-declared",
    "url": "/blog/2024/1/11/github-actions-secrets-need-to-be-explicitly-declared/",
    "title": "GitHub Actions secrets need to be explicitly declared",
    "summary": "In this blog post, I share my experience of debugging GitHub Actions for LlamaBot. I encountered a challenge with setting the Mistral API key as an environment variable in my GitHub action. After hours of frustration, I discovered that GitHub Actions can only read a secret if it's explicitly included in a workflow. I explain how to include it in a workflow step. Curious about how to securely manage your API keys in GitHub Actions? Read on!",
    "body": "Today, in releasing a new version of LlamaBot, I also made a change for one of the s that I wrote to use the Mistral AI models instead of GPT-4. That meant that I needed to use a Mistral API key as part of the environment variables. However, no matter what I tried, I couldn't seem to get the environment variable set within my GitHub action, even though it was declared as an Actions Repository-level secret. Turns out, declaring an Actions Repository secret doesn't immediately make it available to the GitHub Actions runner! According to the official documentation, GitHub Actions can only read a secret if you explicitly include the secret in a workflow. (Source: GitHub Actions docs) This turns out to be a great security practice, even if it caused me no end of frustration for over 3 hours this evening. But how do we include it in a workflow step? It is done this way:",
    "tags": [
      "llamabot",
      "mistral",
      "gpt-4",
      "api key",
      "environment variables",
      "github actions",
      "repository secret",
      "workflow step"
    ],
    "pub_date": "2024-01-11",
    "type": "blog"
  },
  {
    "id": "blog-evolving-llamabot",
    "url": "/blog/2024/1/10/evolving-llamabot/",
    "title": "Evolving LlamaBot",
    "summary": "In this blog post, I discuss the major changes I've made to LlamaBot, a project I've been working on in my spare time. I've integrated LiteLLM for text-based models, created a new DocumentStore class, and reimagined the SimpleBot interface. I've also experimented with the Mixin pattern to create more complex bots and switched to character-based lengths for more user-friendly calculations. How did these changes improve the functionality and efficiency of LlamaBot? Read on to find out!",
    "body": "Reworking LlamaBot In my spare time (if I can find any), I've been hacking on LlamaBot to make a bunch of internal improvements to the package. It's about ready, and I'd like to document what's changed here. Change 1: LiteLLM The first thing worth discussing is how the text-based models now use LiteLLM behind the scenes. With the explosion of models, switching between them without building out extensive internal code infrastructure is something that's going to facilitate experimentation. In my case, I was initially building out against OpenAI's GPT-4 API. Later, I experimented with Ollama for local LLMs on my tiny MacBook Air. Later, I became curious about Claude (by Anthropic) and Mixtral (by Mistral) and realized what a headache it would be to maintain my own switchboard for dispatching out to different APIs. LiteLLM fixed that problem for me efficiently, providing a uniform API interface to the various models I wanted to try. In short, LiteLLM became the API switchboard I desperately needed, and I'd recommend checking it out! Change 2: Document Store and History The second thing I'd like to discuss here is the new DocumentStore class available in LlamaBot. I ripped out the internals of QueryBot, made DocumentStore an independent class, and reinstalled DocumentStore into the QueryBot class. What was the motivation for doing so? It was primarily my realization that I needed an interface on document storage and retrieval that (1) could be consistent across different storage backends and (2) customizable internally to work with different forms of storage + retrieval logic. As such, I started out with the following API: Here, , , , and are the core APIs, while is a higher-level API that lets us add documents to the based on file paths. What's cool is that this core interface (init, append, extend, retrieve, and reset) applies to document storage and chat history! Let's think carefully about it, with chat history. We are always appending (or extending) it and retrieving (the last K messages) within a ChatBot setting. Re-imagining and treating chat history as a collection of documents allows us to build a uniform interface for both, simplifying the internals of our Bots. Here's what the class looks like, then, with the exact same interface and adding a method: You may have noticed that I used ChromaDB instead of LlamaIndex's tooling. There were multiple reasons for doing so, but the primary drivers were as follows. The first was the layers of complexity in LlamaIndex's tooling, which primarily revolves around a , an , a , and a . The documentation does not clearly explain what each of these abstractions is all about. Additionally, it felt peculiar to pair an LLMPredictor with a VectorStoreIndex when the LLMPredictor was primarily present to synthesize answers - at most, we need an embedding function. By contrast, ChromaDB's abstractions are much more natural: we have documents (text) that are stored as vectors (numbers), and they are linked together; when we the vector database with a , we get back a collection of results that have the document and metadata linked together. The other was the so-called \"cost of sane defaults\": ChromaDB defaults to using SentenceTransformer from HuggingFace to compute embeddings (which is zero-cost out of the box), while LlamaIndex's examples commonly default to using OpenAI's API, which costs some money. Taken together, though I had already become somewhat familiar with LlamaIndex's API, ChromaDB felt much more natural for the way that LlamaBot's internal APIs were being designed -- bots that do text-in/text-out and document stores with customizable retrieval. Change 3: SimpleBot as the core natural language programmable bot interface One critical insight I arrived at in building LlamaBot is that there is one general and valuable use case for LLMs: we can use natural language to build text-in, text-out robots. Granted, this is less rigorous than using formal programming languages, but this is really useful for applications that need human-sounding natural language outputs. I'm also not the first to arrive at this conclusion: OpenAI's \"GPTs\" feature is also the result of this insight! Additionally, the mechanics of sending messages out to APIs often means that we need to compose a collection of messages that get sent to the API. The APIs (OpenAI, Mistral, etc.) are stateless, meaning they do not remember the previous context. This is a natural consequence of how the neural network models are trained behind the scenes. What's usually different between , , , and perhaps future , would be how the messages are composed before being sent to the API. Finally, the mechanics of streaming, which usually involve copying and pasting the same chunk of code, feel stable enough that it should be abstracted behind a boolean toggle. Taking all of this together, I thought that if I could further use to simplify the interface to LiteLLM, we could then use in such a way that we could bang out speci",
    "tags": [
      "llamabot",
      "api",
      "chromadb",
      "openai",
      "mistral",
      "anthropic",
      "claude",
      "mixtral",
      "simplebot",
      "chatbot",
      "querybot",
      "llm",
      "large language model"
    ],
    "pub_date": "2024-01-10",
    "type": "blog"
  },
  {
    "id": "blog-lessons-learned-optimizing-alphafold-at-moderna",
    "url": "/blog/2023/12/13/lessons-learned-optimizing-alphafold-at-moderna/",
    "title": "Lessons Learned Optimizing AlphaFold at Moderna",
    "summary": "In this blog post, I share my experience of refactoring the AlphaFold execution script at Moderna, which led to significant cost savings and efficiency. I discuss the challenges faced, including hitting AWS' GPU instance availability limits, and the lessons learned, such as the importance of static analysis tools, CI/CD caching, reading code before writing, and working openly. Curious about the technical details and the lessons I learned from this experience?",
    "body": "At work (Moderna), we use AlphaFold. I was one of the people who deployed it to run on our internal infrastructure. At the same time, my initial implementation could have been more efficient: we ran the entire AlphaFold pipeline on a p3 or g5 instance, which was costly per folding run. On a small scale, it got the job done. That was, of course, until we had power users hammer AlphaFold. That's when the flaws in our current implementation became evident - we started hitting limits on AWS' GPU instance availability. Our initial deployment of AlphaFold was inefficient because the multiple sequence alignment (MSA) step, which was CPU-bound, was running on the GPU instance while not using the GPU. In my final week of work (week of December 5) before I went on vacation for the rest of the year (to work on house renovation matters), I made a successful attempt to refactor the AlphaFold execution script such that it would run the CPU-bound MSA steps separately from the GPU-bound structure folding steps. While the original implementation was initially designed to work on a beefy workstation or High-Performance Computing (HPC) cluster, we had scalable infrastructure that allowed us to run many more jobs than before, which also meant passing information from the MSA job to the structure prediction job via the filesystem. At its core, the key object passed from the MSA job to the structure prediction job is the features dictionary, which (by default) gets saved to disk as a Python pickle by AlphaFold. Once we identified this key interface (which took me days to figure out by digging through source code), it was immediately apparent where to perform code lobotomy. Once the refactor was complete, we could run MSA jobs on r6a instances and structure prediction jobs on g5 instances while orchestrating the two jobs from a dirt-cheap c5 instance. In doing so, we estimated that running jobs AlphaFold would cost 1/3 the original price. But beyond the impact, I recognized that I had a bunch of technical lessons that I thought would be useful for others, and I'd like to share them here. Lesson 1: Static analysis tools can help catch code errors early The first lesson I wanted to share is investing in mastering an integrated development environment (IDE). VSCode, when fully configured, can be a super powerful tool! While hacking on the deployment codebase, I used static analysis tools (pyright, flake8, ruff) to check that we had the correct code inside the Typer CLI we built for AlphaFold. Along the way, when splitting functions up, if we weren't using an argument in a function or didn't declare it in the function signature, I would get a red squiggly line under that variable, immediately giving me a visual check that the code contained errors. Catching these errors early on helped with development cycle time. Lesson 2: CI/CD caching is incredibly useful This lesson is an inverse lesson for me because we use Bitbucket at work and don't have the kind of cache sizes available to GitHub Actions users that I'm used to in my open-source projects. As such, my cycle times for testing were 30-45 minutes rather than 2-3 minutes, which can be incredibly distracting. On the other hand, I could work around the long cycle times by setting a watch timer for AlphaFold. I am sure that we would have finished the refactor faster if we had better caching! Lesson 3: It pays to read code first before writing code For a while, I was intimidated by what it might take to refactor the AlphaFold executor script. Our original rewrite included mixed abstraction levels, making it difficult to know whether I had broken something. In some ways, I was paying the price for my own design choices made earlier. That said, taking the time to slowly read the code while making small breaking changes, which was approximately 1-2 days worth of time, really helped build momentum for the breakthrough on 3rd day when the needed abstractions finally clicked in my head. Greg Brockman of OpenAI once said, Much of the challenge in machine learning engineering is fitting the complexity into your head. Best way to approach is with great patience & a desire to dig into any detail. Twitter This statement resonated so much during this refactor. Lesson 4: Scaling is hard, and not for the reasons you might think When scaling from hundreds of AlphaFold jobs a month to thousands, we ran into all sorts of limits that I was unaware of. A majority of these were AWS' limits: they have a limited pool of GPU instances that we share with the entire set of users of , and when we hit NeurIPS season, that pool of available GPUs also shrunk. Additionally, AWS limits the number of IP addresses we can use within a subnet, the total amount of EBS storage provided for EC2 instances, the total number of instances we can run, and more. A lot of these limits help prevent accidental surprise bills that may show up. However, these limits also put an upper limit on the amount of actual computation we might",
    "tags": [
      "alphafold",
      "moderna",
      "technology",
      "cloudcomputing",
      "aws",
      "infrastructure",
      "code optimization",
      "refactoring",
      "scaling challenges",
      "continuous integration"
    ],
    "pub_date": "2023-12-13",
    "type": "blog"
  },
  {
    "id": "blog-classes-functions-both",
    "url": "/blog/2023/12/12/classes-functions-both/",
    "title": "Classes? Functions? Both?",
    "summary": "In this blog post, I discuss the choice between class- or function-based programming for data scientists. I argue that objects are best for grouping data, while functions are ideal for processing data. However, configurable functions that need to be reused can be implemented both ways. I lean towards a functional programming style, using classes to organize related data. But sometimes, like with callable objects, I adopt a different approach. Curious about when to use each style in your data science projects? Read on!",
    "body": "Should I adopt a class- or function-based programming style as a data scientist? Recently, one of my colleagues asked me this question, and his question reminded me that I get asked this question pretty often. Additionally, I've written one take on this question before, too. But with the benefit of additional years of experience from that previous post, I figured it'd be good to do another take. So here it is: Objects are for data, and functions are for processing data. We can implement configurable functions both ways. Ok, let's break it down. Objects are for grouping data Objects are good places for storing related pieces of data together. One example would be neural network configuration. Another example would be a collection of file paths accessed throughout your code. The key here is to require those variables to be present when instantiating the object, ensuring they are present when needed. Here's an example: Because all of the arguments are required when instantiating the object, we have the safety and guarantees necessary to access or anywhere in the program. Doing so necessitates declaring all of the paths up-front rather than scattering the paths throughout the program, which turns out to be a good pattern to follow anyway. If, for whatever reason, you need to reorder operations within the program, then by declaring all of the paths beforehand and encapsulating them within the object, you can avoid NameErrors due to objects not being found. As a side note, there's a refrain about programming that comes from Linus Torvalds, creator of Linux and Git: Bad programmers worry about the code. Good programmers worry about data structures and their relationships. (I would recommend reading through the discussion on StackExchange as well!) Thinking hard about our data structures, which by definition are always going to be classes, makes writing functions much more effortless later. Functions are for processing data Wherever possible, I recommend that colleagues use functions for data processing. In doing so, we help encourage a state-less pattern of programming. What do I mean by that? Let's use an example to illustrate. Let's say we're writing a data-processing chunk of code. One could choose to do this in an object-oriented way, which might look like: I struggled hard to write that example because it contains so many anti-patterns that I would avoid, to begin with: We lack a single location where class attributes are defined, making it difficult to reason about what class attributes should exist. , a higher-order function, is at the same indentation level as . This organization makes it difficult to reason whether or should have precedence when reading the code; indeed, as mentioned in my other blog post, one could (in theory) call any of the class methods in any order, only to be frustrated because we did not set a class attribute. We cannot easily read off the flow of information within the class method, as there are no return values here. The critical overarching problem here is using state where state is not warranted. By contrast, using a function helps alleviate some of these issues. Rewriting the code above as functions instead gives us the following: There were two design decisions here. The first is that there are only primitive data transformation operations () that are declared as functions, while disk I/O is left unencapsulated. Doing so assures the next reader of code that we did not sneak in auxiliary data processing code into disk I/O functions for convenience. Second, we express each logical data transformation unit as a Python function. Its scope should match existing domain knowledge about how that transformation should work. By adhering to these design decisions, we make it easy for anyone with domain knowledge to follow the code. Configurable functions can be implemented both ways This idea is where we muddy and blur the lines between objects and functions. We can implement configurable functions as partially initialized functions or as callable objects. One example from deep learning is our neural network layer. As a function, we might implement it as follows: The neural network layer is nothing more than a Python function, though we have to initialize ourselves. As a configurable class, we might implement it according to the following pseudocode: Indeed, this is the pattern used by major neural network libraries, such as Chainer, from which PyTorch adopted the pattern, which was then further propagated in the Equinox library as well. In both cases, we initialize our neural net so we only have to pass in . We have to initialize parameters with the partially initialized function pattern, which may be a bit hasslesome. However, with the callable function pattern, parameters are attached to the object directly as part of the initialization. The latter is a generally helpful pattern when we reuse data associated with the callable function and when we can configure the data. In this case,",
    "tags": [
      "data science",
      "programming style",
      "function-based programming",
      "class-based programming",
      "data processing",
      "object-oriented",
      "data structures",
      "neural network",
      "data transformation",
      "callable objects"
    ],
    "pub_date": "2023-12-12",
    "type": "blog"
  },
  {
    "id": "blog-elevating-team-performance-feedback-strategies-for-data-science-leaders",
    "url": "/blog/2023/12/11/elevating-team-performance-feedback-strategies-for-data-science-leaders/",
    "title": "Elevating Team Performance: Feedback Strategies for Data Science Leaders",
    "summary": "In this blog post, I share my experiences and insights on providing effective feedback in a data science team. I discuss the importance of positivity, specificity, self-reflection, effusiveness, in situ technical feedback, connecting accomplishments to broader impacts, and uplifting when mistakes occur. These strategies foster a supportive environment, promote continuous improvement, and align team members with the broader mission. How can these feedback strategies improve your team's dynamics and performance? I hope my experiences shared here can give you inspiration!",
    "body": "Introduction In data science (and most) teams, how team leads give feedback can significantly influence team members' growth and development. Being close to the end of the year, I reflected on 2.5 years of providing feedback and coaching to my teammates. I decided to document the nuances of delivering constructive feedback with concrete (but anonymized) examples from my work. These essentially are things that I've learned over the years. \ufffc Stay Positive and Specific When giving feedback, you want to stay positive. Doing so is essential for fostering a supportive and motivating environment. Additionally, you want to be specific. Doing so is necessary for coaching an individual contributor toward improvement. For example, I remember one of our teammates delivered an excellent presentation on behalf of a project team. At the same time, I noticed that this public speaking session revealed some pointers where he could have improved. I could have just said, \"Great job! I loved it!\" and moved on, but instead, I said something along the lines of: That was an excellent presentation - based on feedback from others in the audience, they could follow along the main concepts. Your delivery was smooth and confident - no noticeable hiccups, even if there were any. In the spirit of continual improvement, I noticed that your eyes made more contact with the speaker's monitor than the audience. If you switched your attention to the audience, that would double the effectiveness of your presentation instantly. Consider that for the future! Some key pointers I strived to accomplish include: Opening off with the positives in concrete ways. Pivoting to improvements (and not to critiques). Providing actionable suggestions that my teammates could put into action in the future. This kind of feedback acknowledges the strengths while offering specific areas for improvement, making it constructive and encouraging. By contrast, without specific feedback, your teammates would be left wondering what they did well and what they needed to improve upon. In a worst-case scenario, change something they did well (which they should preserve) while continuing to do something that wasn't effective (which should be changed). Encourage Self-Reflection In providing feedback, I avoid injecting my opinions first. Instead, I encourage self-reflection first. My opening lines in a feedback session often are: What did you think went well, and what would you like to improve? Even though I ask that my teammates begin first, I still take notes and have an opinion on where they can improve. The key idea I'm going for here is to encourage self-reflection. From personal experience, I am more receptive to feedback once I've had the chance to rewind and play back my performance tape in my head first. Following the Golden Rule, I want to afford my teammates the same opportunity. From that point onwards, there is a discussion rather than a top-down lecture. If I've done the hiring part right, my teammates will be self-conscious and technically adept. As a result, we will agree with most of what we discuss and only have to focus our energies on places where I, or they, may have missed something. Be Effusive in Asynchronous Feedback As with general feedback, code review feedback must be specific and positive. Unlike general feedback, however, asynchronous code review needs additional care. In asynchronous code review, being biased towards being effusive is essential. If we were to metaphorically measure the bandwidth available for carrying intent, text is limited compared to in-person reviews or reviews done over video calls. Being effusive is a way to help communicate assumed positive intent. Doing so can help foster a psychologically safe environment within the team. Suppose you are the reviewer of an algorithm that requires refactoring. A poor way to leave a review comment might look like this: This code needs refactoring. A better way to communicate here would be: Thanks for implementing the algorithm -- this is a non-trivial and technically challenging piece of code. How could we refactor this further to make the implementation match our understanding of the problem better? And how could we test it so that we can gain a guarantee of the mathematical correctness of the overall algorithm? In that response, I followed the abovementioned principles - being positive, encouraging specificity, and encouraging self-reflection. Also, I avoid using the term \"you\" to convey that this code is co-created by us as a team, even though my teammate is the reviewer. (Using \"you\" can also come across as being accusatory, even if that's not the intent.) There will be times when I find a nitpicky error that requires a nitpicky response. In those cases, I will prefix my review comment with a clarification that my comment is nitpicky. To illustrate, here is a nitpicky comment without that clarification: A blank line between the function description and argument docs is needed. Imagine b",
    "tags": [
      "data science",
      "team management",
      "culture",
      "feedback",
      "coaching",
      "code review",
      "asynchronous feedback",
      "technical feedback",
      "team",
      "morale",
      "continuous improvement"
    ],
    "pub_date": "2023-12-11",
    "type": "blog"
  },
  {
    "id": "blog-embracing-leadership-my-journey-at-moderna",
    "url": "/blog/2023/12/3/embracing-leadership-my-journey-at-moderna/",
    "title": "Embracing Leadership: My Journey at Moderna",
    "summary": "In this blog post, I explore my thoughts on leadership as a data science team lead. I discuss the power of leadership, the importance of technical skill and credibility, encouraging independent growth, fostering psychological safety, embracing graduation, sharing credit, and the challenge of humility. I conclude with my belief in servant leadership. What does leadership mean to you and how do you navigate its challenges?",
    "body": "Recently, I found myself discussing leadership on a podcast. On multiple occasions in the past, I've been hesitant to pen down my thoughts. It's because this topic often feels squishy and somewhat intangible compared to the (qualitatively) more concrete world of coding. But then I realized where my hesitation stemmed from -- it was the perfectionist in me desiring to write my own comprehensive guide to leadership. Rather than do that, though, I decided to own the imperfection and simply share what I've been thinking about along these lines. I decided that it'd be fruitless to try to provide a complete thesis of leadership from my incomplete experiences being a data science team lead. Rather, I thought I could pull on as many threads as I can muster within one blog writing session. Here's what they are. Thread 1: Heaven or Hell - The Power of Leadership The team lead position comes with inherent power and authority. This power can shape the work environment dramatically \u2013 making it either a heaven or a hell for my team. I consciously choose to strive for the former. Amidst the inevitable frustrations of work, why not strive to bring a taste of Eden instead? Thread 2: Technical Skill and Credibility In my role as a DS team lead, technical expertise is non-negotiable to have any kind of credibility in my teammates eyes. I need bring to the table advanced knowledge that I can share with my teammates. For me, I've chosen Bayesian stats, network science, deep learning, and now LLMs as part of my toolkit. Staying technically grounded is crucial! Thread 3: Encouraging Independent Growth As part of career coaching, I have advocated to my teammates to form their own 'Moderna committees' \u2013 groups of 2-4 colleagues, ideally more experienced than themselves in career trajectory or technical skill, whom they can consult independent of me. This stems from knowing the importance of diverse perspectives beyond my own limited one. Same goes for skip levels - every once in a while, my teammates should feel empowered to meet with my manager, and freely share what they see as my weaknesses, especially if they do not want to share those with me directly. How else am I supposed to become a better team lead without that feedback? Thread 4: Fostering Psychological Safety Referencing Google's research, it's clear that psychological safety is key to high-performing teams. This means protecting my team's time from overwhelming demands and normalizing conversations about mistakes. It's about creating an environment where learning from errors is valued over reprimanding them. Thread 5: Embracing Graduation I know that none of my teammates will work with me forever; they will all \"graduate\" from my hands at some point. A fundamental belief I hold is that my team members should leave my hands better than when they started. Whether they move on due to new opportunities or personal reasons, I focus on their growth and development. The idea of training team members to grow, even if it means they might eventually leave, is far more appealing than the risk of stagnation within the team. I believe there is a quote, often attributed to Henry Ford, that goes, The only thing worse than training your employees and having them leave is not training them and having them stay. It's also reflected in the cartoon below: Original source: Twitter Thread 6: Just Get Things Done and Share Credit President Obama once said in an interview, \"Just learn how to get stuff done.\". Rather than jostling for the plum assignment, just nail the thing we're doing. Also, credit is not a zero-sum game; credit multiplies the more we give it away. As such, it's important that we continually recognize the contributions of our teammates and collaborators. Thread 7: Humility This one is my biggest challenge; I'm naturally an egotistical person. Humility is the antidote - it isn't thinking less about myself, it's thinking about myself less. Conclusion I've been most influenced by the ideas of servant leadership: guiding, supporting, and elevating others before myself. This post has touched on just a few facets of what leadership means to me \u2013 there are undoubtedly more! What does leadership mean to you? I'd love to hear your thoughts!",
    "tags": [
      "leadership",
      "podcast",
      "hesitation",
      "coding",
      "perfectionism",
      "team",
      "power",
      "environment",
      "technical",
      "credibility",
      "bayesian",
      "network",
      "deep learning",
      "llms",
      "growth",
      "moderna",
      "skiplevels",
      "feedback",
      "psychological safety",
      "google",
      "mistakes",
      "graduation",
      "development",
      "obama",
      "humility",
      "servant"
    ],
    "pub_date": "2023-12-03",
    "type": "blog"
  },
  {
    "id": "blog-speeding-up-ci-pipelines-with-micromamba-a-llamabot-case-study",
    "url": "/blog/2023/11/26/speeding-up-ci-pipelines-with-micromamba-a-llamabot-case-study/",
    "title": "Speeding up CI Pipelines with Micromamba: A LlamaBot Case Study",
    "summary": "In this blog post, I experimented with speeding up LlamaBot's CI system by switching from Miniconda to micromamba. The results were impressive, with more consistent timings and a significant reduction in build and test times. The primary advantage was the built-in, turnkey caching of the entire environment. This change made a noticeable difference, especially when testing against bleeding-edge packages. Could micromamba be the solution to your CI delays? Read on to find out!",
    "body": "Nobody likes waiting around for continuous integration (CI) pipelines to finish. I'm sure many of you can relate to the frustration of slow build times. In my recent post, How to choose a (conda) distribution of Python, I touched upon Python distributions. However, a comment on LinkedIn by Wade Rosko, referencing Hugo Shi's post about speeding up a Saturn Cloud CI job with micromamba, got me thinking. They achieved a 40X speedup! This was something I had to try. The Experiment I decided to use LlamaBot's CI system as a test case. The original setup used Miniconda with the following YAML configuration: In my new approach, I switched to using micromamba with the following YAML: The rest of the YAML file remained unchanged. Benchmarking Results I meticulously recorded the timings, and you can find the full record here. | Configuration | Run 1 | Run 2 | Run 3 | Run 4 | |---------------|--------------------------------------------------------------|--------------------------------------------------------------|--------------------------------------------------------------|--------------------------------------------------------------| | Old YAML | 2m 4s | 3m 16s | 2m 3s | 6m 34s | | New YAML | 1m 1s | 1m 19s | 2m 9s | N/A | Analysis and Conclusion The timings on my latest PRs with the new setup feel more consistent. Not only is there a noticeable reduction in build and test times, averaging around 1 minute, but there's also no difference in the final environment, and all tests pass. The primary win here comes from the built-in, turnkey caching of the entire environment, a stark contrast to the more complicated caching methods suggested by mambaforge. Opting for caching based on the date seems to be a practical compromise, especially since I often test against bleeding-edge packages. Wrapping Up Switching to micromamba for the LlamaBot's CI system was a rewarding experience. It's a straightforward and effective way to reduce CI times significantly. If you're dealing with similar CI delays, consider giving micromamba a try. It could just be the solution you're looking for.",
    "tags": [
      "continuous integration",
      "python",
      "micromamba",
      "llamabot",
      "conda",
      "yaml",
      "mambaforge",
      "caching"
    ],
    "pub_date": "2023-11-26",
    "type": "blog"
  },
  {
    "id": "blog-zephyr-vs-gpt4-whats-your-verdict",
    "url": "/blog/2023/11/19/zephyr-vs-gpt4-whats-your-verdict/",
    "title": "Zephyr vs. GPT4: What's your verdict?",
    "summary": "In this blog post, I benchmarked Zephyr, a new language model by HuggingFace, against GPT-4 using GitBot. I found that while Zephyr shows promise, GPT-4 seems to offer a more out-of-the-box solution for accurately interpreting and summarizing code changes. However, different models may require different prompts to perform optimally. Curious about how these language models could change up your coding workflow?",
    "body": "Zephyr, a new LLM by the HuggingFace H4 team, has been making the rounds as being super competitive with GPT3.5. I thought I'd take it for a drive and decided to benchmark Zephyr on a thing I've been building, GitBot, but with a twist: I decided to benchmark it against GPT4 instead of 3.5. Here's what I did. Benchmarking Setup The Bots I set up two LlamaBot s, one using the Zephyr model, and the other using GPT4: Prompt Then, I pulled out the original Git diff commit writing prompt that I used for prototyping: Git Diff The diff I had was one that I had previously used for benchmarking other local LLMs such as Llama2: Comparing Zephyr vs. GPT4's Outputs Following that, I executed the bots and observed what they produced. First the Zephyr bot: And then the GPT4 bot: Commentary Before I go on, I should explain what constitutes a good \"commit\" message. A good commit message (1) accurately describes the changes in detail while also summarizing it, and it (2) also describes the intent of the change. I would expect that an LLM that writes commit messages for me should be able to do (1) pretty well and (2) moderately okay, occasionally guessing the intent incorrectly unless I have documented the intent in-line as a code change as well (which is good practice too), in which case it should just \"parrot-phrase\" (repeat but summarize) my comments. What I'm noticing here is a few things. Conditioned on the prompt, GPT-4 gets the change type (\"refactor\") more precise compared to Zephyr (\"fix\"), though both are, strictly speaking, writing this part of the commit message correctly. Secondly, GPT-4 is more compact and less verbose. Thirdly, GPT-4 describes the changes more accurately than Zephyr -- the diff has nothing to do with connectivity issues at all, which Zephyr guesses incorrectly. Re-running the models Knowing that the models are sampling tokens at each step, I decided to re-run the bots one extra time just to compare their outputs. Zephyr produces: While GPT-4 produces: Consistently, both of them are producing the (or variation) at the end of the commit message. Notably, Zephyr is also producing a meta-level commentary on the commit message, which I personally find hilarious. Prompt Engineering I then surmised that it may be the case that different models need variations on the prompt in order to do the task they're asked to do correctly. Observing the patterns that Zephyr was producing, I decided to modify the commit message prompt in the following ways: 1. Format the prompt a bit differently to get rid of the [BEGIN/END COMMIT MESSAGE] fences. 2. Changing the final instructions to steer the Zephyr bot to exclude meta-level commentary and section headers. The prompt then looked like this: The Zephyr model then produces this commit message: While GPT-4, for comparison completeness, produces: Reflections on this experiment It seems to me that GPT4 is still the model to beat here. GPT-4, in my own experience, seems to have a higher probability of \"just working\" out of the box with little prompt design, while Zephyr needs a bit of prompt tweaking (not really \"engineering\" per se) to produce the desired results. More important is the lesson that different models will need different prompts; the one prompt designed for one model won't likely \"just work\" with another model out-of-the-box. Overall, while Zephyr shows promise and could potentially be tailored to perform better with more specific prompts or further training, GPT-4 seems to offer a more out-of-the-box solution for accurately interpreting and summarizing code changes. As the field of LLMs continues to evolve, it'll be interesting to see how models like Zephyr develop and what niches they find themselves filling.",
    "tags": [
      "huggingface",
      "zephyr",
      "gpt4",
      "benchmarking",
      "gitbot",
      "llm",
      "language models",
      "code summarization",
      "prompt engineering",
      "machine learning"
    ],
    "pub_date": "2023-11-19",
    "type": "blog"
  },
  {
    "id": "blog-navigating-the-hiring-process-what-to-expect-and-when-to-expect-it",
    "url": "/blog/2023/11/12/navigating-the-hiring-process-what-to-expect-and-when-to-expect-it/",
    "title": "Navigating the Hiring Process: What to Expect and When to Expect It",
    "summary": "In this blog post, I share insights from my experience as both a job candidate and a hiring manager. I break down the hiring process into five stages: application submission, TA phone screen, hiring manager phone screen, onsite interview, and offer negotiation. Each stage has its own expectations and timelines, which can help you plan your job-seeking journey. Patience is key, and remember, each step is an opportunity to showcase your skills. Curious about what to expect at each stage and how long it might take? Read on!",
    "body": "Having sat on both sides of the hiring table, I've been frustrated and puzzled by how long and opaque the hiring process can be, especially as a job candidate. Now that I've been both a job candidate and a hiring manager, I wanted to write my observations of the job hiring process down for your benefit. Whether you're fresh out of your education or eyeing your next career move, understanding the timing and components of this process can offer a great deal of relief. So, let's bring some clarity to the table and discuss what each stage looks like, what you should expect, and how long it might take. This could be your road map for your next career adventure! Submitting the Application Unless there are some exceptional circumstances at play, your journey likely begins with submitting an application. What to Expect You've probably filled out applications through LinkedIn, directly on a company website, or via a 3rd party system like Glassdoor. After hitting that 'Submit' button, your application lands in the hands of the Talent Acquisition (TA) team. Despite popular belief, your resume isn't only processed by some faceless algorithm; there are actual humans on the other side of the screen reviewing your application. The hiring manager may or may not be involved at this stage, depending on their schedule and priorities. Timelines So, how long before you hear something back? It can be immediate, or it may take up to four weeks to find out whether you've made it to the TA screen. In short, patience is key at this stage. Talent Acquisition Phone Screen If you made it past the resume screen, you'll likely move onto the talent acquisition phone screen. What to Expect If you've caught the TA team's (or hiring manager's) eye, you'll be among roughly 12 candidates selected for a TA phone screen. During this call, expect to discuss your legal status and any need for sponsorship (some companies don't offer this), as well as your ability to relocate if the job calls for it. This initial chat also dives into your cultural fit with the company, with questions often tailored to the company's values. Timelines You're looking at a timeframe of anywhere between 1 to 4 weeks after application submission to hear back. Stay positive, and keep an eye on your phone and inbox! Hiring Manager Phone Screen If you pass the TA phone screen, you'll be one of the subset of the candidates selected to move onto the Hiring Manager Phone Screen. What to Expect Around six candidates, yourself possibly included, will move on to a 30-45 minute phone screen with the hiring manager. The focus of this interview is a bit more fluid and depends largely on the hiring manager's preferences. You could be quizzed on your skill set, academic background, motivation for applying, or even what you\u2019re expecting from your next job. Timelines You'll need to be patient for another 1-3 weeks as the hiring manager works through their candidate shortlist. And if they're really keen on a candidate (maybe you!), they might expedite the next steps while keeping others on hold. Onsite Interview If you pass the hiring manager phone screen, you'll be invited to an onsite interview. What to Expect The lucky few (around three candidates) make it to this stage, which is a more comprehensive assessment of your abilities and fit. Expect a panel consisting of the hiring manager, a few team members, and a TA representative. Interviews at this stage often require a job talk where you present your work. Skills such as communication, domain expertise, coding, and mathematical modeling are evaluated. For a deeper dive, check out my essay Hiring and Interviewing Data Scientists. Timelines Coordinating onsite interviews is a logistical puzzle, often requiring multiple days. Expect a timeline of 4-6 weeks for all candidates to complete this stage. Offer and Negotiation If you've shined throughout the entire interview process, you'll be made an offer that you can negotiate. What to Expect Finally, the moment you've been waiting for: the job offer! At this stage, I can't offer much advice on negotiation since my primary motivation has never been monetary. If you do want to negotiate money, your best shot is to have a competing offer. However, if you're starting out in your career, I would prioritize work environment, mentorship from your immediate manager, and opportunities for professional development over money, as these are the things that can have a lasting effect on your career. Timelines Timeframes can be unpredictable at this point, especially if there are other candidates in the mix. If you weren't the top choice candidate, it's likely you'll hear a conditional or tentative timeline from the TA team. But if you're the hiring manager's slam dunk top choice, it's very likely that you'll hear back immediately! Summary Table Here's a summary of what to expect. | Step | What to Expect | Timelines | |-----------------------------|---------------------------------------------------------",
    "tags": [
      "job hunt",
      "hiring process",
      "career advice",
      "application tips",
      "interview preparation",
      "talent acquisition",
      "job offer",
      "negotiation tips",
      "career development",
      "job seeking"
    ],
    "pub_date": "2023-11-12",
    "type": "blog"
  },
  {
    "id": "blog-navigating-the-transition-from-academia-to-industry-in-biomedical-data-science",
    "url": "/blog/2023/11/5/navigating-the-transition-from-academia-to-industry-in-biomedical-data-science/",
    "title": "Navigating the Transition from Academia to Industry in Biomedical Data Science",
    "summary": "In this blog post, I share strategies to bridge the gap between academia and industry. I discuss enhancing your public profile, adapting your academic skills for industry, networking effectively, and communicating your value. These strategies can help PhDs, Master's grads, and those considering their academic future navigate their career more confidently. Curious about how to translate your academic achievements into industry value? Read on!",
    "body": "Navigating the professional world with an advanced degree can sometimes feel like steering a ship through uncharted waters. Many of us enter a PhD or a Master's program with a deep-rooted passion for research and academic development. But how do we translate those years of scholarly endeavors into value for the industry? How do we make ourselves not just known, but understood and appreciated in the business environment? In this blog post, I'm going to share some strategies that have helped me and many others successfully bridge the gap between academia and industry. Whether you're a PhD, a Master's grad, or someone just pondering their academic future, these insights may help you maneuver through your career more confidently. Strategy 1: Enhance Your Public Profile When you're in academia, your reputation often precedes you in the form of publications, academic conferences, and perhaps some teaching experience. In the industry, however, you need to diversify the ways you make your skills known. Show, Don't Tell The most effective way to showcase your skills is by demonstrating them. Rather than just telling people you're good at something, show them. There are numerous ways to do this: - Papers: If you've published research, make sure it's easily accessible. Use platforms like ResearchGate, arXiv, or even a personal blog to publicize your work. - Blogging: Start a blog to discuss topics in your field of expertise. This not only helps you clarify your thoughts but also proves you have a good grasp of the subject matter. - Recorded Talks: If public speaking is your strength, leverage it. You can upload your talks to YouTube or share them via webinars. This adds another layer of credibility to your professional profile. - Teaching Sessions: If you have the opportunity to teach \u2014 whether it's a semester-long course or a single workshop \u2014 make sure to document it. Recorded teaching sessions or even just the syllabi can demonstrate your expertise and commitment to education, teaching, learning, and mentorship. Strategy 2: Adapt Your Skills Academic training provides a set of skills that need a bit of tweaking to fit smoothly into industry roles. For PhDs By the time you complete your PhD, you should be able to: - Independently lead technical projects: This ability translates directly into technical independence within an industrial context. You're not just a cog in the machine; you're capable of overseeing complex tasks from start to finish. - Communicate to various audiences: Whether it's breaking down complex theories for students or presenting your research to seasoned experts, you need to communicate effectively. In industry, this ability is crucial for tasks like presenting to stakeholders or explaining technical procedures to non-technical team members. For MSc Grads If you're wrapping up a Master's degree, aim to: - Master a defined set of tools: Unlike PhDs who often need a broader toolkit, you should focus on becoming highly proficient in a more specific set of skills or tools. - Be on the cusp of independent creativity: While you may not lead projects right away, you should be prepared to offer creative solutions within your scope of expertise. Note: These are general rules of thumb; there are exceptions based on individual capabilities and job requirements. Strategy 3: Network Effectively The saying goes, \"It's not what you know, but who you know.\" I would slightly modify it to be a bit more true, \"It's not just what you know, but also who you know\". This rings especially true when transitioning from academia to industry. In that vein, here's how you can network effectively. Choose Relevant Meetups and Conferences Attend local meetups, such as the Boston Python or Boston Bioinformatics Meetup, as well as conferences like PyCon and SciPy. These are great platforms to meet people who share your professional interests. Leverage Weak Links You may find your best opportunities come from acquaintances rather than close friends. Or it may come from friends who refer you to speak with their colleagues. To make these interactions more meaningful, offer to make a small donation to a charity of their choice as a thank-you for their time. This creates a positive impression and adds a unique element to your networking strategy. Ask Insightful Questions Information is key. During your networking conversations, ask questions that help you understand the landscape of the industry roles you're interested in. These could relate to team structures, job titles, or even the day-to-day tasks associated with a specific role. This information will help you target your applications more effectively. Strategy 4: Communicate Your Value When translating your academic achievements into industry lingo, remember that hiring managers are generally looking for one thing: value. The best way to communicate this is by clearly stating how you used a particular method or skill to solve a problem and what value it added. In academic terms,",
    "tags": [
      "professional transition",
      "public profile",
      "skill adaptation",
      "networking",
      "phd careers",
      "masters careers",
      "career strategies",
      "career development",
      "career advice"
    ],
    "pub_date": "2023-11-05",
    "type": "blog"
  },
  {
    "id": "blog-how-to-debug-a-modulenotfounderror-in-the-interrogate-pre-commit-hook",
    "url": "/blog/2023/10/29/how-to-debug-a-modulenotfounderror-in-the-interrogate-pre-commit-hook/",
    "title": "How to debug a ModuleNotFoundError in the interrogate pre-commit hook",
    "summary": "In this blog post, I discuss a I encountered while using the pre-commit hook, interrogate, in Python 3.12. The issue arose due to a missing package, , which is no longer included by default in Python 3.12's virtual environments. I proposed a solution and provided a workaround by using Python<3.12 for pre-commit installation. This experience highlights the importance of tracking dependencies and adapting to language and library updates. Have you ever faced similar issues in your development workflow? Read on to find out more about my debugging journey.",
    "body": "Pre-commit hooks are an indispensable tool in the modern development workflow. They help enforce code quality and consistency right at the stage of committing the code, well before it becomes a part of the codebase or enters the CI/CD pipeline. Recently, while working with , a popular pre-commit hook for checking Python docstrings, I encountered a puzzling . Here's how I debugged the issue and some potential solutions to the problem. The Error I ran the command like usual within LlamaBot's CI/CD pipelines, and all seemed well until I hit this snag: Diagnosis Upon investigating, I narrowed down the problem to a few key points: 1. Python Version: Pre-commit was defaulting to the latest Python version, which at the time was Python 3.12. 2. Missing Package: Python 3.12 no longer ships with , which provides the missing , by default within a virtual environment (venv). 3. Dependency Gap: The package itself doesn't explicitly depend on . The Solution Given the diagnosis, one way to fix this would be to ensure that explicitly lists as a dependency. Doing this would ensure that is available, eliminating the . To that end, I submitted a bug report and immediately put in a pull request to fix the bug. Workaround the issue While waiting for an official fix, the workaround for the pre-commit hook is to ensure that you are running Python<3.12 when installing . In my case, I set the Python version to 3.11 within the relevant GitHub Actions YAML file: This ensured that was installed within the virtual environment for each hook, and should solve any similar issues in the short-term. Conclusion This experience underscores the importance of keeping track of dependencies and being mindful of how language and library updates can break existing setups. The good news is that this issue is quite fixable, either through an update to or through manual intervention. Issues like these are bound to happen in a continually evolving ecosystem like Python, and troubleshooting them is part and parcel of a developer's life. If you've faced similar issues or have suggestions, feel free to share them in the comments below!",
    "tags": [
      "pre-commit hooks",
      "code quality",
      "debugging",
      "python version",
      "setuptools",
      "dependency management",
      "github actions",
      "code style checks"
    ],
    "pub_date": "2023-10-29",
    "type": "blog"
  },
  {
    "id": "blog-the-importance-of-skip-level-11s",
    "url": "/blog/2023/10/24/the-importance-of-skip-level-11s/",
    "title": "The importance of skip-level 1:1s",
    "summary": "In this blog post, I discuss the importance of regular 1:1 meetings and the benefits of 'skip-level' 1:1s, where you meet with your direct manager's manager. These meetings provide a broader perspective, facilitate understanding of organizational dynamics, and contribute to a culture of transparency and mutual respect. They are beneficial for both individual contributors and managers, fostering personal and organizational growth. If you're a team lead, encouraging your team members to engage in skip-level 1:1s demonstrates great leadership. Curious about how these meetings could transform your work routine and relationships?",
    "body": "Wherever you work, 1:1s (one-on-one meetings at a regular cadence) are likely something you'll encounter. These are important: they allow us to maintain a regular cadence, usually weekly, of facetime with your internal network of colleagues, usually the ones with whom you work most closely. Doing so has multiple benefits, including: building better working relationships, aligning on goals, providing a private avenue for feedback and conflict resolution, and more. Your 1:1s are usually with the following categories of people: - your direct manager, - your direct reports, and - close collaborators within the company I would venture that you should consider also having a \"skip-level\" 1:1. What's a \"skip-level\" 1:1? This is when you skip one level upwards on the org chart and have a 1:1 with your direct manager's manager. Doing so has many benefits. Let's take a look at a few of them. Benefits for you as the individual Whether you are a manager or individual contributor, skip-level 1:1s give you the same benefits mentioned above, except the context of information that you'll be accessing (via your skip-level manager) is going to be broader than what your direct manager can provide. This can help you understand how decisions are being made outside of the scope of your own work. This understanding, in turn, can benefit how you conduct your work tactically, or how you strategically choose courses of action. Benefits for the team If you are a team lead, I am of the conviction that you should absolutely encourage your teammates to do a skip-level with your manager. Doing so is a signal of trust and confidence in your teammates. It is also part of their training: to grow via coaching from someone who, presumably, has more experience than yourself. Within my home team at Moderna, I do exactly that, and take it one step further: they should feel free to have conversations with my manager without needing to tell me when they do it, and without telling me what they talked about. I'm always happy to hear about their skip-level conversations, but will never ask. Conclusion The importance of clear communication and relationship building cannot be overstated. Regular 1:1 meetings serve as an invaluable tool for achieving these ends, with benefits that reach both you and your team. The addition of skip-level 1:1s adds another layer of depth, providing a broader perspective and facilitating a more nuanced understanding of organizational dynamics. By encouraging open channels of communication at multiple levels, you not only bolster your own career development but also contribute to a culture of transparency and mutual respect. This culture, in turn, translates to a more engaged, efficient, and ultimately successful work environment. If you're a team lead, empowering your team members to engage in skip-level 1:1s shows great leadership. It's a clear indication that you are invested in their personal and professional growth, and it shows that you trust them to handle the responsibility that comes with such an opportunity. So, whether you're an individual contributor or a manager, consider making 1:1 meetings \u2014 skip-level or otherwise \u2014 a regular part of your work routine. The payoffs, both immediate and long-term, are well worth the investment of your time. In the end, it's all about creating stronger connections, fostering meaningful dialogue, and driving personal and organizational growth. So, if you haven't yet scheduled your next 1:1 or skip-level meeting, maybe now's the time to get it on the calendar.",
    "tags": [
      "work",
      "relationships",
      "feedback",
      "conflict resolution",
      "meetings",
      "organizational dynamics",
      "team building",
      "career development",
      "transparency",
      "professional growth"
    ],
    "pub_date": "2023-10-24",
    "type": "blog"
  },
  {
    "id": "blog-how-to-run-ollama-with-llamabot",
    "url": "/blog/2023/10/22/how-to-run-ollama-with-llamabot/",
    "title": "How to run Ollama with LlamaBot",
    "summary": "In this blog post, I explore the integration of local Large Language Models (LLMs) with my LlamaBot project using Ollama. I discuss how Ollama simplifies the setup of local LLMs and demonstrate how to use Ollama models with LlamaBot. I also share a quick demo with Zotero chat using Ollama models. While OpenAI's GPT-4 remains the benchmark, local models offer cost-free alternatives. Curious to read more?",
    "body": "If you've been following the LlamaBot project, you know it's my pet Pythonic project to interface with Large Language Models (LLMs). I've had fun building some cool stuff, like GitBot, a chatbot for your Zotero library, and even a blogging assistant (more on that later, promise!). However, there's been one area I've shied away from until now: using local LLMs. The setup could be daunting, but I finally found a way to simplify it with Ollama. A request sparks an idea A community member posted an issue on GitHub: \"Hi @ericmjl, thank you for making this! I'd sent a PR but it's a little beyond me but I was wondering if there is a simple way to use local models such as Ollama?\" And I thought, why not? Can it work? Let's find out. First steps with Ollama Ollama made setting up local LLMs a breeze. I was pleasantly surprised at the smooth installation process; you simply need to follow the instructions on their main page. Two ways to run Ollama models Following that, there are two ways to access Ollama models. 1. Chat in the Terminal: Run 2. Local API Mode: Run One thing to note: Ollama pulls in models on the fly. They range from 3GB to 16GB, so you may need to be patient while they download. Ollama + LlamaBot: How I integrated them A happy architectural decision My earlier decision to use LangChain paid off, even with all of the frustrations I had trying to track a fast-evolving Python package. LangChain's architecture made it straightforward to write a model dispatcher. Making the adjustments I made some tweaks in SimpleBot, ChatBot, and QueryBot to ensure that they work with Ollama models. How to use Ollama models with LlamaBot So, how exactly do we use Ollama models with LlamaBot? Firstly, start by serving an Ollama: Secondly, in your Jupyter notebook, initialize the bot: And that's it! A Quick Demo with Zotero Chat I\u2019ve already enabled Zotero chat to use Ollama models to give you a taste. Try it out: What's more While OpenAI's GPT-4 still sets the benchmark in speed and response quality, local models offer the freedom of being cost-free. This opens up new avenues for fine-tuning and prompt engineering! Thanks for reading, and happy coding!",
    "tags": [
      "python",
      "large language models",
      "llms",
      "gitbot",
      "zotero",
      "local llms",
      "ollama",
      "langchain",
      "openai",
      "gpt-4",
      "prompt engineering",
      "llamabot"
    ],
    "pub_date": "2023-10-22",
    "type": "blog"
  },
  {
    "id": "blog-how-to-ensure-that-bump2version-only-updates-the-correct-string",
    "url": "/blog/2023/10/18/how-to-ensure-that-bump2version-only-updates-the-correct-string/",
    "title": "How to ensure that bump2version only updates the correct string",
    "summary": "In this blog post, I share a solution to a problem I encountered with , where it was incorrectly updating all pattern-matched strings instead of just the version number. I found that by modifying the file, I could ensure only the version number was updated. This has been a game-changer for me and I'm excited to implement it into my default project templates. Curious about how this could streamline your own version control process? Read on to find out more!",
    "body": "I recently discovered how to ensure that bump2version only bumps the exact version number and not other pattern-matched strings. Within a file, I had the following configuration: If I wanted to do a patch release, would pattern-match on the exact string and give me the following updated file: This is undesirable; really, we should be only updating the string . Turns out, the configuration that we need within is: With this configuration, we get: Which is correct! I am going to add this to the configuration to make sure I never encounter the same problem again.",
    "tags": [
      "bump2version",
      "version control",
      "pyproject.toml",
      "configuration",
      "pattern matching",
      "patch release",
      "dependencies",
      "llamabot",
      "pyds-cli",
      "problem solving"
    ],
    "pub_date": "2023-10-18",
    "type": "blog"
  },
  {
    "id": "blog-how-i-made-a-local-pre-commit-hook-to-resize-images",
    "url": "/blog/2023/10/14/how-i-made-a-local-pre-commit-hook-to-resize-images/",
    "title": "How I made a local pre-commit hook to resize images",
    "summary": "In this blog post, I share my experience creating a custom pre-commit hook for resizing images within a repository. This hook automates the process of ensuring all logos meet a defined width, saving time and maintaining consistency. It uses Python and integrates with the pre-commit framework, running in an isolated environment to keep the main project clean. I also discuss the potential of distributing these hooks for wider use. Curious about how you can automate checks and streamline your development process with pre-commit hooks?",
    "body": "Today I learned how to make a pre-commit hook that lives locally within a repository. Pre-commit hooks are a powerful tool in any coder's arsenal -- whether they are a data scientist or software developer -- enabling us to automate certain checks before changes are committed to the repository. This ensures that every commit meets the defined standards and can save countless hours in code reviews. So... what does this hook do? The primary function of the pre-commit hook I made is to resize images, particularly logos, within the repository. I wanted this hook so I could avoid manually resizing blog banner images, which I've been creating using DALLE-3. Let's dive into a high-level overview of the script: At its core, this script: 1. Checks images: It examines all the logos in the repository, specifically those named \"logo.webp\". 2. Resizes oversized images: If any logo exceeds a set maximum width (600 pixels in this case), the script resizes it to fit within the defined width while maintaining its aspect ratio. 3. Provides feedback: Depending on whether any logos were resized, the script either rejects or accepts the commit, informing the user of its decision. Configuration breakdown To understand how this script integrates with the pre-commit framework, let's break down the configuration for the pre-commit hook: - repo: Specifies that the hook is local to the repository. - id: A unique identifier for the hook. - name: A descriptive name for the hook. - entry: Path to the script that will be executed. - language: The programming language of the hook, which is Python in this case. - languageversion: Specifies the Python version. - additionaldependencies: Lists external libraries the script depends on. Here, is for image processing and helps in finding the root of the project. - types: Indicates the file types the hook applies to. It's set to PNG images. - files: A regex pattern to match specific filenames, ensuring the hook targets only \"logo.webp\" files. - pass_filenames: This is set to , meaning the script does not expect file names as command-line arguments. Behind the scenes Now, how does this all work together? The magic of the pre-commit framework is that it creates an isolated Python environment specifically for the hook. This means the script doesn't run using the Python interpreter in your PATH. Instead, it uses a hidden, separate Python interpreter. This might initially seem confusing or even redundant. However, it offers a significant advantage. By having a separate environment, there's no need to mix dependencies required by the hook with those of your main project. This separation ensures that the main project environment remains clean and free from unnecessary dependencies. Conclusion Harnessing the power of pre-commit hooks, especially custom ones tailored to specific project needs, is super empowering. They help maintain code and asset quality, automate checks, and streamline the development process. Moving forward, I'd like to explore how to distribute these hooks, enabling other developers to benefit from them in their projects. The world of pre-commit hooks is vast, and there's always something new to learn and implement!",
    "tags": [
      "pre-commit",
      "pre-commit hook",
      "automation",
      "python",
      "python script",
      "software development",
      "data science",
      "dalle-3",
      "til"
    ],
    "pub_date": "2023-10-14",
    "type": "blog"
  },
  {
    "id": "blog-when-to-write-tests-for-your-data-science-code",
    "url": "/blog/2023/10/10/when-to-write-tests-for-your-data-science-code/",
    "title": "When to write tests for your data science code",
    "summary": "In this blog post, I discuss the importance of testing in data science code. I explain how research code can transition into production code and the potential implications of errors or oversights. I suggest three levels of testing: adding assertions within your notebook, migrating code into functions and testing them, and refactoring code into a library with associated test functions. By testing and refactoring our code, we can ensure its accuracy and reliability. Are you curious to see how you can test your code as a data scientist?",
    "body": "As data scientists, our work often exists in a continuum ranging from initial, tentative exploratory analysis to mature, production-ready solutions. The ephemeral nature of exploratory code can sometimes mislead us into believing that it's fleeting and doesn't warrant the rigor of tests. However, I think this perspective deserves a re-evaluation. Production vs. Research Code One might think, \"It's just a preliminary analysis, not the final product.\" But herein lies a crucial distinction: When decisions are derived from your research code, that code transitions from being 'just research' to pivotal 'production'. This perspective mirrors that of Niels Bantilan, who insightfully [commented on my LinkedIn post][neils]: \"but it's not production!\"... if you make conclusions from research findings based on incorrectly implemented code, then research is production. [neils]: https://www.linkedin.com/feed/update/urn:li:ugcPost:7112753423216730112?commentUrn=urn%3Ali%3Acomment%3A%28ugcPost%3A7112753423216730112%2C7112806327323693056%29&dashCommentUrn=urn%3Ali%3Afsd_comment%3A%287112806327323693056%2Curn%3Ali%3AugcPost%3A7112753423216730112%29 Such code, veiled as 'research', affects stakeholders. The implications of errors or oversights can range from minor hiccups to major organizational setbacks. I don't have to look too far from my professional neighborhood to find places where testing will be worthwhile. In the discovery of a drug, buggy machine learning code that is used to prioritize candidates can lead us to erroneously prioritize duds. As for the range of impacts, it could range from days (if caught early) to years (if caught very late) of wasted laboratory time and effort on prioritized candidates. Not everything requires testing... initially Initiating a new project or analysis doesn't always imply diving straight into writing tests. In the nascent stages, especially during exploratory phases, the terrain is still unfolding. You're sketching possibilities, and the path isn't concrete. But, as your code matures and starts to take shape, bringing in testing becomes crucial. The evolution of code demands an evolution in its verification. The Golden Principle Here's how you know when to test: test the stuff that needs guarantees! This isn't just about the mechanics of testing; it's a philosophy. Pieces of code that possess the potential to wreak havoc or disseminate incorrect information demand rigorous testing. At this point, we're not just talking about lines of code; we're talking about the bedrock of trustworthiness. Levels of test-writing for data science code So, how do we embark on our testing journey? I'd like to suggest three levels of testing. Consider them as progressive milestones; ascending these levels not only instills confidence in your code but also strengthens its foundation. Level 0: Add assertions within your notebook Even seasoned data scientists, while engrossed in their work, might resort to a mere visual inspection of variables as a sanity check. If you find yourself doing visual checks of variable values, then consider elevating this informal process to structured statements. Doing so lends your code an added layer of validation: The key here is to formalize what we're intuiting and translating it into code. It may take a bit of critical thinking to do that formalization, but it'll help us make the code much more reliable. If your desired code logic has changed, then these assertions can help you catch the effects of those changes, giving you additional confidence that your change is what you indeed desire. Level 1: Migrate Your Notebook Code into a Function and Test It For those chunks of code that are bound within the confines of a single Jupyter notebook, structuring them into functions can be a game-changer. This compartmentalization allows for a more streamlined testing process. Here's a caveat: Ensure that your tests don't become entangled with the specific state of your Jupyter notebook. Maintain their independence! The function being tested should be self-sufficient and not rely on any other state within the notebook. Additionally, the test function itself should also be self-sufficient. Level 2: Refactor code into a library and associate it with a test function When you find yourself revisiting certain functions or realize their broader applicability, it's a signal to house them in a dedicated library. This not only amplifies reusability but also facilitates structured testing. Library Code: Test Function: Bonus: The right time to refactor The art of refactoring isn't just about the 'how' but also about the 'when'. A pragmatic rule of thumb is: If you find yourself cloning a notebook to make minor tweaks or frequently copying and pasting code segments, it's a beacon signaling refactor time. Each refactoring iteration brings with it not just the promise of cleaner code but also the commitment to testing. Summary Your data science journey might begin with uncharted, exploratory pa",
    "tags": [
      "datascience",
      "testing",
      "machine learning",
      "best practices",
      "production",
      "research",
      "exploratory analysis"
    ],
    "pub_date": "2023-10-10",
    "type": "blog"
  },
  {
    "id": "blog-its-time-to-upgrade-to-ruff",
    "url": "/blog/2023/10/9/its-time-to-upgrade-to-ruff/",
    "title": "It's time to upgrade to Ruff",
    "summary": "In this blog post, I discuss the benefits of using Ruff, a blazing fast linter for Python code. With its speed and performance, Ruff can significantly reduce linting and code style checking times. It is written in Rust, known for its performance and safety features. I provide step-by-step instructions on how to integrate Ruff into your workflow, including installing the pre-commit hook and configuring Ruff in pyproject.toml. If you're looking to improve the quality and efficiency of your Python codebase, give Ruff a try. Are you ready to switch to Ruff and experience lightning-fast code checking?",
    "body": "Admittedly, I'm a bit late to the party. However, if you're using Python and you care about the cleanliness and consistency of your codebase, I've got news for you: it is time to switch to Ruff! So, What Exactly is Ruff? Ruff isn\u2019t just another linter in the market; it's a blazing fast one. What makes it stand out? - Speed: Ruff can reduce your linting and code style checking times from minutes to mere seconds. - Crafted in Rust: A fun fact about Ruff is that it's written in Rust, which is renowned for its performance and safety features. - For Python Projects: If you\u2019re working with Python, this is specifically for you. No need for adjustments or tweaks to make it fit for Python projects \u2013 it\u2019s already tailored for it. Getting Started with Ruff Ready to integrate Ruff into your workflow? It\u2019s simpler than you might think. Here\u2019s how to do it: Firstly, install the pre-commit hook into your repository. Pre-commit hooks are beneficial to ensure that the checks run automatically before your code gets committed, ensuring its quality. Set it up using the following configuration: Then, configure Ruff in : Now, to ensure Ruff knows what you want, you'll need to specify some configurations: Summary Linting and code style checking is a crucial step in ensuring the quality of your codebase. With tools like Ruff, this process not only becomes efficient but also lightning-fast. If you haven't tried it yet, give it a go \u2013 you might just get hooked! Happy coding!",
    "tags": [
      "python",
      "ruff",
      "tips and tricks",
      "rust",
      "pre-commit"
    ],
    "pub_date": "2023-10-09",
    "type": "blog"
  },
  {
    "id": "blog-check-docstrings-blazing-fast-with-pydoclint",
    "url": "/blog/2023/10/9/check-docstrings-blazing-fast-with-pydoclint/",
    "title": "Check docstrings blazing fast with pydoclint",
    "summary": "In this blog post, I discuss the importance of documenting code and the risks of using outdated tools like darglint. I introduce pydoclint as a faster alternative and share a case study of how it solved a problem for the pyjanitor project. I provide instructions on getting started with pydoclint and highlight its default configurations. As a data scientist and tool developer, I'm always on the lookout for better tools, and pydoclint promises a smoother experience. Are you ready to embrace the future with pydoclint?",
    "body": "Documenting your code is essential, not only for others but for your future self. Tools like have been lifesavers, ensuring our docstrings align with the actual code. But as we all know, technologies evolve, and sometimes the tools we rely on become outdated or no longer maintained. The Issue with Darglint I used to be a fan of , but sadly, it's no longer maintained. This poses risks; using outdated tools might result in overlooking newer conventions or not being compatible with the latest Python updates. Enter Pydoclint Thankfully, I stumbled upon [](https://github.com/jsh9/pydoclint). Not only does it serve the same purpose as , but it's also considerably faster. Case Study: 's Dilemma Let's discuss a real-world scenario. The project faced a recurring issue with causing timeouts during Continuous Integration (CI) runs. To keep the CI process smooth, the team had to introduce multiple workarounds. This isn't ideal; CI tools should enhance our workflow, not impede it. Switching to solved this problem. Drawing a comparison, is to what is to other code checking tools. In my own benchmarking, it's nearly 1000x faster! Pyjanitor's checks used to take on the order of 5-8 minutes. With pydoclint, we're down to milliseconds. Getting Started with Pydoclint If you're convinced to make the switch, here's how to get started: Integrate as a Pre-commit Hook Ensure code quality before even committing. Add the following to your : Configuration One of the strengths of is its sensible default configurations. I generally don't recommend tinkering with these settings unless you have very specific needs. The out-of-the-box settings are well-balanced for most projects. However, if you do find the need to customize its behavior, you're in good hands. Detailed configuration instructions are available here. Closing Thoughts As a data scientist and tool developer, I'm always on the lookout for better tools - tools that enhance our workflow, ensure quality, and save time. is one such tool that promises a smoother experience when it comes to validating docstrings. While I've got to tip my hat to for its service, it's time to embrace the future with !",
    "tags": [
      "coding",
      "documentation",
      "darglint",
      "docstrings",
      "tools",
      "technologies",
      "pydoclint",
      "pyjanitor",
      "continuous integration",
      "til"
    ],
    "pub_date": "2023-10-09",
    "type": "blog"
  },
  {
    "id": "blog-vscode-tip-cmdp-lets-you-switch-to-any-file-within-a-repository",
    "url": "/blog/2023/10/8/vscode-tip-cmdp-lets-you-switch-to-any-file-within-a-repository/",
    "title": "VSCode Tip: Cmd+P lets you switch to any file within a repository",
    "summary": "In this blog post, I share a quick tip for using VSCode. I show how to easily locate and open any file within a repository using the Command Palette. By typing keywords in the file-browsing mode, you can quickly narrow down the exact file you want to open. Have you ever struggled to find a specific file in VSCode? Read on to discover this time-saving trick!",
    "body": "Sharing a quick tip here when using VSCode. I have noticed colleagues sometimes struggle to locate an exact file that they want to open when using VSCode. As it turns out, the Command Palette gives us a quick way of navigating to any file within a repository. The trick here is to do a , which will open up the command palette in file-browsing mode. From there, you can start typing keywords to quickly narrow down the exact file that you want to open. Any portion of the file path is fair game! It looks something like this (you can click on the image to zoom in): [](./cmd-palette.webp)",
    "tags": [
      "vscode",
      "tips and tricks",
      "til",
      "navigation",
      "repository",
      "productivity"
    ],
    "pub_date": "2023-10-08",
    "type": "blog"
  },
  {
    "id": "blog-how-to-choose-a-conda-distribution-of-python",
    "url": "/blog/2023/10/7/how-to-choose-a-conda-distribution-of-python/",
    "title": "How to choose a (conda) distribution of Python",
    "summary": "In this blog post, I discuss the differences between the Anaconda, Miniconda, and Miniforge distributions of Python. Anaconda is the official distribution from Anaconda and comes with a wide range of data science packages. Miniconda is a smaller version of Anaconda, intended for use in Docker containers. Miniforge, developed by the conda-forge team, pulls packages from the conda-forge repository and includes mamba. The choice of distribution depends on your needs and preferences, with Miniforge being recommended for lightweight and open-source use, and Anaconda for enterprise support and backing the Python open source world.",
    "body": "Note: This is an excerpt from my Data Science Bootstrap Notes, which is freely available online here. If you find the notes useful and wish to support my work, please consider either purchasing a digital copy on LeanPub or sending coffee money via GitHub Sponsor or Patreon. If you're a user, you may have heard of the Anaconda distribution of Python. In this set of notes, however, I've also referenced the Miniforge distribution of Python. What's the difference here? How do you pick which one to use? To answer those questions, we must first understand what is a distribution of Python. Python distributions Python can get distributed to users in many ways. You can download it directly from the official Python Software Foundation's (PSF) website. Or you can install it onto your system using the official Anaconda installer, through Homebrew, or through your official Linux package manager. Each way of installing Python can be thought of as a distribution of Python. Each distribution of Python differs ever so slightly. Official Python from the PSF comes with just the standard library. Anaconda, however, ships with the standard library and many other packages that are relevant for data science. What is common across all Python distributions, however, is that it will ship with a Python executable that, at the end of installation, should be discoverable on your environment variable. Most commonly, there will be a Python package installer that ships with the distribution as well. This can be , the official tool for installing Python packages, or it could be , which was developed by the company Anaconda. As such, the anatomy of a distribution is essentially nothing more than: - A Python interpreter that can be discovered on your , - A Python package manager, and - Any other default Python packages that the distributor thinks you might want With that aside, let's look at three distributions of Python that are relevant to this set of notes. Anaconda Python The Anaconda distribution of Python is the official distribution from Anaconda. It ships with a modern version of Python, both and package managers, and a whole slew of default data science packages (, , , , , for example). With the Anaconda distribution, is configured such that packages are installed from the repository of packages, hosted by Anaconda itself. Its default installation location is or . Miniconda Python The Miniconda Python distribution also comes from Anaconda. It looks like Anaconda except it ships with fewer packages in the base environment. You wouldn't, for example, find installed for you. This was mostly intended to keep the base environment small for use within Docker containers. Its default installation location is or . Miniforge Python This distribution of Python comes from the open-source developer team behind . Miniforge looks like Miniconda, but instead of configuring to pull packages from the repository, packages are instead pulled from the repository of packages by default. This has the advantage of being able to pull more bleeding-edge versions of packages that you may use. Additionally, Miniforge Python ships with as well. Summary Table Here's a summary table of these features. | Attribute | Anaconda Python | Miniconda Python | Miniforge Python | |------------------------------|-------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------| | Origin | Official distribution from Anaconda | Comes from Anaconda | From open-source developer team behind | | Version of Python | Modern | Similar to Anaconda | Similar to Miniconda | | Package Managers | and | Similar to Anaconda | , and | | Default Data Science Packages | , , , , | Fewer packages (e.g., not pre-installed) | Similar to Miniconda | | Conda Configuration | Pulls from repository | Pulls from repository | Pulls from repository by default | | Primary Use Case | General-purpose with lots of pre-installed data science packages | Keeping base environment small, e.g., for Docker containers | Access to bleeding-edge versions of packages. | Tip: All of the distributions can be installed into the directory if you desire consistent behaviour regardless of the installation source. All three installers provide the flag when executing it, thus allowing us to specify the prefix directory in which to install. We would thus do something like: Which to use? Depends on your persona! If you're an indie hacker type, I would strongly recommend the Miniforge Python as it is lightweight and fast to get set up with and fully open source. On the other hand, if you're more inclined to want enterprise support, vetting of packages, and wish to support a company that backs so much of the Python open source world, then I would r",
    "tags": [
      "conda",
      "anaconda",
      "miniforge",
      "python distribution",
      "data science",
      "pip",
      "tooling",
      "python"
    ],
    "pub_date": "2023-10-07",
    "type": "blog"
  },
  {
    "id": "blog-how-to-use-python-functions-as-a-template-engine-for-prompts",
    "url": "/blog/2023/10/6/how-to-use-python-functions-as-a-template-engine-for-prompts/",
    "title": "How to use Python functions as a template engine for prompts",
    "summary": "In this blog post, I explore the use of Outlines for prompt management in Python, specifically for LlamaBot. However, due to its heavy dependencies, I decided to reimplement the functionality using GPT-4 as a coding aid. The result was a successful reimplementation that allowed me to organize prompts within source modules more easily. A lesson from this experience. is the importance of clarity in programming even when we're using LLMs to help us code.",
    "body": "I previously noted how Outlines had an excellent implementation of prompt management through a Python decorator, and as such, I decided to explicitly add Outlines as a dependency for LlamaBot, the Pythonic LLM interface with sensible defaults that I've been working on. However, Outlines has a heavy dependency chain itself, requiring, in particular, PyTorch (and a bunch of NVIDIA packages on PyPI) as well. That felt like too much weight for the one functionality I really wanted, which was that prompt decorator. So I decided to reimplement it in my own way... using GPT-4 as a coding aid. What Outlines does Let me back out and explain what Outlines does and how it does so. Outlines lets us to write prompts as Python docstrings. Doing so offers us the ability to organize our prompts within Python submodules, thereby enabling us to import them as Python objects. When compared to LangChain's prompt templates, Outlines' Jinja2-templates also felt much more lightweight. Here's an example of what we're going for. First off, the definition: And then secondly, the result of calling : That text can now be passed into, say, a LlamaBot SimpleBot and thus we get back a response. Now, as I mentioned above, Outlines' dependency chain was a bit heavyweight, so I wanted to find a way to replicate the functionality within LlamaBot so that I could avoid the heavy dependency chain. I studied the original implementation here but it did feel a tad too complex to understand at first glance. Additionally, I was hesitant to copy and paste verbatim the file from Outlines as that felt like stealing, even though it is, strictly speaking, an open source project. So I embarked on my journey to write a reimplementation of the functionality. GPT-4 designs the function As it turns out, GPT-4 was incredibly good at writing a reimplementation of the desired functionality. I gave it the following specification: One of the solutions I settled on, which I arrived at after a bunch of back-and-forth, was this: This was a great solution that worked! It's now implemented within , allowing me to organize prompts within source modules more easily than before. What did I learn? What's instructive of this experience, I think, is how to construct the prompt for a chunk of desired code. It's infeasible to ask an LLM to do something rather generic and expect it to read my mind. Rather, in order to get back desirable code from an LLM, we need to have a fairly specific idea of what we actually want. I think this really means having a level of clarity about programming -- one that can only be borne from prior experience and training doing programming itself. For coding, the outputs of an LLM are pretty much going to mirror the level of clarity that we have as programmers. On the other hand, the interactive nature of chatbots (e.g. ChatGPT or LlamaBot's ChatBot class) means we can always refine the output interactively. Sometimes, this can be incredibly helpful for gaining clarity over a problem, especially if we prompt the LLM to help us clarify our thoughts. I might explore this thought further in a later blog post. As such, it isn't necessary for us to presume that we'd get our prompts right the first time round; after all, I did have to do a bit of back-and-forth with the LLM to get to the code I eventually used.",
    "tags": [
      "python",
      "llm",
      "gpt-4",
      "coding",
      "outlines",
      "llamabot",
      "jinja2",
      "prompt management",
      "chatbots"
    ],
    "pub_date": "2023-10-06",
    "type": "blog"
  },
  {
    "id": "blog-shape-up-and-data-science-a-match-closer-to-agile-than-you-think",
    "url": "/blog/2023/10/5/shape-up-and-data-science-a-match-closer-to-agile-than-you-think/",
    "title": "Shape Up and Data Science: A Match Closer to Agile Than You Think",
    "summary": "In this blog post, I explore the limitations of Scrum for data science. I introduce Shape Up as a potential alternative. I discussed how Shape Up's ways of working align better with the unique needs of data science, such as deep domain specialization and varied feedback durations. I also highlighted how Shape Up embodies Agile's core values while suggesting modifications to suit data science projects better. Ultimately, I emphasized the importance of adaptability and delivering value, staying true to Agile's core principles.",
    "body": "As I've delved deeper into the realms of data science and software methodologies, I've often reflected on Agile principles. Agile's core ethos \u2013 adaptability, swift iterations, and collaborative engagement \u2013 have always resonated with me. Yet, over the years, I've observed a myriad of interpretations, with Scrum emerging as a dominant favourite. But here's a question I've grappled with: Does Scrum genuinely cater to every discipline's unique needs? Recently, I stumbled upon Shape Up, Basecamp's brainchild. In this piece, I want to dissect whether Shape Up aligns more seamlessly with data science. Let's see if, in some unexpected ways, it might even echo the spirit of Agile more closely than current Scrum interpretations do. Understanding Agile Principles Before we go on, we must establish what Agile really is about. At its core, Agile values: 1. Individuals and interactions over processes and tools. 2. Working solutions over comprehensive documentation. 3. Customer collaboration over contract negotiation. 4. Responding to change over following a plan. Agile isn't about specific rituals; it's about mindsets and principles emphasizing adaptability and delivering value. (That Agile isn't about particular rituals makes the structured approach of Scrum, with all of its, ahem, ceremonies, particularly jarring and ironic.) Scrum's Assumptions About Engineers vs. The Reality of Data Scientists At its core, Scrum was developed with software engineering projects in mind. This lineage inherently carries certain assumptions about the team members involved: Interchangeability of Engineers: One of the implicit assumptions in Scrum is that engineers are somewhat interchangeable. In a Scrum environment, tasks are often broken down from user stories into smaller tickets. In theory, any developer on the team should be able to pick up most tickets. This \"generalist\" approach assumes a relatively uniform skill set among team members. While specialization does exist in software engineering, Scrum teams often aim for a degree of cross-functionality where members can interchange roles to some extent. This is why we hear statements in Scrum teams like, \"Can anyone take this ticket?\" Short Feedback Loops: Scrum thrives on rapid iterations and quick feedback. It assumes that engineers can produce a piece of functionality, however small, that can be reviewed, tested, and potentially shipped within a short sprint duration. As such, Scrum assumes that the nature of the work allows for such quick turnarounds. Homogenized Progress Tracking: Using tools like burndown charts and the concept of velocity assume that tasks (or story points) can be somewhat uniformly estimated and tracked across sprints. This presupposes a certain consistency in the type and complexity of tasks across sprints. Contrast this with the world of data science: Deep Domain Specialization: Data scientists often possess deep domain specialization. One might be an expert in natural language processing. At the same time, another might specialize in deep learning for computer vision. Yet another may be a recovering wet lab biologist with over a decade of molecular biology domain expertise. Their expertise isn't just about knowledge; it's often about years of experience and intuition built over time. This makes it challenging to have interchangeable roles, as we might find in Scrum-based software teams. Complex Tooling Expertise: Beyond domain knowledge, data scientists need expertise in specific computational toolsets. Building a neural network in TensorFlow differs from setting up a distributed data processing pipeline in Spark. This isn't about merely knowing a programming language but mastering a very complex and deep stack of computational tools. Varied Feedback Durations: Unlike software tasks that can often be demoed quickly (I am thinking of those that involve building user interfaces or standing up a new database), a data science task, such as training a machine learning model, might require days or even weeks before its efficacy can be evaluated; additionally, data scientists often need to use their deep domain expertise in working with collaborators to really dig at the heart of a problem. The feedback loop can be long and is not always predictable. Non-uniform Task Complexity: In data science, two tasks that sound similar in scope can vary wildly in complexity due to the nuances of data or the intricacies of algorithms. This makes standardized estimation techniques, like story points, challenging to apply consistently. In essence, while Scrum's assumptions about engineers foster a fluid, iterative environment for software development, data science requires a more nuanced approach. The deep specialization and intricate tooling knowledge needed in data science demand methodologies that acknowledge and cater to these unique challenges, something methodologies like Shape Up aim to address. Why Shape Up Aligns Well with Data Science Let's first understand what Shap",
    "tags": [
      "data science",
      "agile",
      "scrum",
      "shape up",
      "software methodologies",
      "product development",
      "deep work",
      "team autonomy",
      "adaptability"
    ],
    "pub_date": "2023-10-05",
    "type": "blog"
  },
  {
    "id": "blog-how-automating-git-workflows-improves-data-scientists",
    "url": "/blog/2023/9/30/how-automating-git-workflows-improves-data-scientists/",
    "title": "How automating git workflows improves data scientists",
    "summary": "In this blog post, I discuss the importance of commit messages for data scientists and how automated commit message writers can improve their workflows. I highlight the psychological barrier of committing in-progress work and the benefits of having informative commit logs. By using automatic commit message generation, data scientists can create a digital lab notebook that summarizes their work and aids in resuming tasks. This blog post emphasizes the value of good commit logs in maximizing productivity for data scientists.",
    "body": "As I've been working on automated Git tooling (i.e. automated commit messages and release notes powered by LLMs), I've been thinking about how this applies to data scientists' workflows. When I was a front-line data scientist, I found that committing work was sometimes a psychological barrier. The reason for this was because of the commit messages. Having been previously taught that we commit code when it feels done, committing in-progress exploratory work seemed to break the rule of not committing work that was in-progress. However, the exploratory nature of data science work means that we often need to commit work that is in progress. That may look like a Jupyter notebook with commented code (but not fully-fleshed out commentary in the Markdown cells). Committing the notebook in its unfinished state at the end of the day with an uninformative commit message feels like a high cognitive barrier. However, if we had an automatic commit message writer, we might have a commit message that looks like this: Notice the phrase: and will be further tested with a 5-genotype system This is the kind of thing I wouldn't write on my own! Leaving notes for myself in the commit log, based on the notes I leave for myself in my notebooks, actually makes the commit log useful for me. My own commit message would look more like this: My lazy brain's commit message is uninformative and unhelpful for resuming work later on. I think data scientists don't take advantage of commit logs as much as we possibly could because we're too lazy to write commit logs ourselves. But if we have good commit logs, then we'll have the equivalent of a digital lab notebook that summarizes our work automagically as we go about our work. And that lab notebook is going to be incredibly helpful for our work. If you're interested in trying out a service I've built that will automatically compose commit messages on every commit, please check out GitBot!",
    "tags": [
      "automation",
      "git",
      "commit messages",
      "release notes",
      "data workflow",
      "data science",
      "jupyter",
      "notebook",
      "lab notebook"
    ],
    "pub_date": "2023-09-30",
    "type": "blog"
  },
  {
    "id": "blog-how-to-crisp-up-your-resume-with-chatgpt",
    "url": "/blog/2023/9/26/how-to-crisp-up-your-resume-with-chatgpt/",
    "title": "How to crisp up your resume with ChatGPT",
    "summary": "In this blog post, I share my discovery of using ChatGPT and GPT4 to enhance a PhD student's resume. By utilizing the prompt and interactive process, you can efficiently condense bullet points without losing important information. I explain how the AI model suggests rephrasing and offer tips on how to further shorten bullet points.",
    "body": "Today I figured out a way to use ChatGPT (and GPT4, by extension) to crisp up a PhD student's resume. The prompt looks like this: Help me reduce this bullet point to: \"Used (method X) to solve (problem Y) with (impact z)\" (then put your bullet point here) The way this works is as follows: Firstly, GPT will give you a proposed rephrasing. If it's not short enough, you can ask GPT: Can you shorten it further? Then, you then interactively work with ChatGPT to wordsmith your bullet point until it's as short as it can possibly be (without loss of information). Obviously, the human needs to judge whether it's \"without loss of information\"!",
    "tags": [
      "resume",
      "career development",
      "gpt",
      "large language models",
      "chatgpt"
    ],
    "pub_date": "2023-09-26",
    "type": "blog"
  },
  {
    "id": "blog-how-to-automatically-write-git-commit-messages",
    "url": "/blog/2023/9/23/how-to-automatically-write-git-commit-messages/",
    "title": "How to automatically write git commit messages",
    "summary": "In this blog post, I discuss how I used LlamaBot, a Pythonic interface to Large Language Models (LLMs), to automatically write git commit messages following the Conventional Commits specification. By feeding the git diff into the LlamaBot SimpleBot, I was able to generate informative commit messages that make it easy to track project history and create accurate change logs. I also explain how to install the prepare-commit-msg hook to run the LlamaBot after pre-commit hooks and before editing the commit message. Interacting with LLMs requires precision and clarity in thinking to effectively utilize their capabilities.",
    "body": "(The alternative title to this blog post is actually \"How I made LlamaBot able to write commit messages automagically.\") I've finally figured out how to automatically write git commit messages properly within the workflow! Commit messages are handy for tracking the history of a project, but it is challenging to write informative ones. There are conventions for writing commits -- one is aptly named Conventional Commits. If you read through it, you'll find it's quite a handful to remember, just like Semantic Versioning. Yet, the conventions exist because they're helpful! If we write commits that follow the Conventional Commits specification, we gain a git history that makes it easy to: 1. Figure out on which commit a specific change was done (and hence be able to study it or even undo it directly), 2. Use the collection of commit message headers and bodies to write out accurate and succinct change logs. For data scientists, the commit messages can be even handier: if we commit one notebook that corresponds to one experiment, then an accurate commit message that summarizes the experiment goals (as described at the top of a notebook) and results (as described at the end of a notebook) essentially lets us have a summarized laboratory notebook. But here's the kicker: great commit messages take time to write. And nobody has that luxury of time, right? (Unless you're a PhD student!) Nobody can write that level of great commit messages, right? No longer! As you might know, I'm hacking on [](https://github.com/ericmjl/llamabot), which is a Pythonic interface to Large Language Models (LLMs) hosted by OpenAI. One of the things I thought LlamaBot would be able to do is to write commit messages based on the between the current state of the repository and the previous commit. The way this works is that we feed in the output into a LlamaBot SimpleBot and prompt it to write a commit message that follows the Conventional Commits specification. The Prompt So, what does the prompt look like? It's essentially the Conventional Commits specification plus a few more instructions! Here it is taken directly from the source code: But that alone doesn't help us much. This prompted bot is magical when it is inserted at precisely the place that's needed: after pre-commit hooks run and before we edit the commit message. Composing the Commit Message To compose the commit message, we must first get the diff and feed it into the commit message bot. You can see it below, lifted from the original source: As you can see, if there's any error encountered, we still need to enable code committers to write their own manual commit message, so we have a graceful fallback by echoing any errors that show up but not throwing the error. (h/t Andrew Giessel for this design choice.) But this alone isn't good enough. We must also run the bot after commit hooks and before editing the message. hook Turns out, there's a git hook that runs right after the pre-commit hooks and right before editing the message, and it's called the hook. Installing hooks manually is often foreign to data scientists (and I'm guessing a significant fraction of software developers too), so provides a way of installing the hook, once again lifted from the source: So to install the hook, one just has to do the following: This is done once per repository. Note that must be on the for the hook to work! How in the world did I come to know this?? feel like esoteric knowledge. The most commonly known one is the pre-commit hooks. However, the hook is probably not very well-known. As things turned out, I had to ask GPT-4 how to do this a few times with a slightly more refined question each time. My first question was, \"how do I compose a commit message as part of the pre-commit hooks?\" which, given my faulty model of git at that point, was the entirely wrong question to ask. It took me 3-4 tries to get to \"how do I compose a commit message using the diff between my current staged changes and the previous commit and use that diff with a SimpleBot program that can automatically write messages to COMMITMESSAGEEDIT\" (notice how I even got the filename wrong!). That's when GPT-4 alerted me to the presence of the , which I then read up on the Git book. Only then was my mental model clarified and correct. I've learned from this that interacting with LLMs requires a certain finesse. GPT-4 can give great code solutions (i.e. \"the how\") if I can describe with sufficient precision what I'm trying to accomplish (i.e. \"the what\"). A general description of \"the what\" doesn't cut it. If you're trying to solve a problem with LLMs, know that possessing precision and clarity in your thinking can go a long way to effectively using LLMs.",
    "tags": [
      "commit messages",
      "conventional commits",
      "git workflow",
      "git",
      "llamabot",
      "python",
      "pre-commit",
      "software development",
      "data science"
    ],
    "pub_date": "2023-09-23",
    "type": "blog"
  },
  {
    "id": "blog-how-to-extract-query-params-from-fastapi",
    "url": "/blog/2023/9/17/how-to-extract-query-params-from-fastapi/",
    "title": "How to extract query params from FastAPI",
    "summary": "In this blog post, I learned how to extract query parameters from FastAPI. I discovered how to access key-value pairs from a GET request using the dictionary. Additionally, I found a solution to properly format URLs by using the submodule. This information was crucial in developing a blog writing assistant with a frontend in HTMX and a backend in FastAPI. Overall, it was a valuable learning experience that I hope will be useful to others as well.",
    "body": "Today, I learned how to extract query parameters from FastAPI. This was within the context of a blog writing assistant that I was making. I used HTM[L/X] for my frontend and FastAPI for the backend. (HTMX rocks, btw, go check it out!) What I wanted to do was to make an HTML tag's depend on another . As it turned out, the HTMX devs already had foreseen many people needing this example and wrote it up as part of the examples. What intrigued me was that the example used a GET request to the API endpoint. This meant that I would get a URL that looked like this: There were two problems that I didn't know how to solve at first. First, how do I access the key-value pair within Python? Second, given that I knew how to access the key , how do I get rid of those s and s? A bit of Googling did the trick. Access GET request key-value pairs To access key-value pairs, the API endpoint must accept a Request object. Then, we can access , an immutable dictionary, from the request object. The code looks like this: I got this answer from this Stackoverflow post. Properly format the URL To properly format the URL, we need to use the submodule, part of the Python standard library. The code looks like this: I got inspiration for this problem from this StackOverflow post. That was the trickiest part for me here. I hope this piece of information is of use to you as well!",
    "tags": [
      "til",
      "htmx",
      "fastapi",
      "web development",
      "programming",
      "frontend",
      "backend",
      "python",
      "requests"
    ],
    "pub_date": "2023-09-17",
    "type": "blog"
  },
  {
    "id": "blog-centaurs-and-cyborgs-interacting-with-artificial-intelligence-tooling",
    "url": "/blog/2023/9/17/centaurs-and-cyborgs-interacting-with-artificial-intelligence-tooling/",
    "title": "Centaurs and Cyborgs: Interacting with Artificial Intelligence Tooling",
    "summary": "In this blog post, I discuss the concept of Centaurs and Cyborgs in relation to how consultants interact with AI tooling. Centaurs have a clear division of labor between humans and AI, while Cyborgs deeply integrate the two. I explain how I personally work in Centaur mode for tasks like writing blog posts, delegating certain aspects to AI, and in Cyborg mode for modeling work. I also explore how this framework can be applied to integrating ML tooling into biotech research. Overall, the two modes are not mutually exclusive and can be further refined.",
    "body": "I just had a chance to read this excellently written article on how Boston Consulting Group consultants interacted with Artificial Intelligence tooling. Within there was this concept that described the two kinds of users of AI tooling: Centaurs and Cyborgs. Quoting from the article, Centaur work has a clear line between person and machine, like the clear line between the human torso and horse body of the mythical centaur. Centaurs have a strategic division of labor, switching between AI and human tasks, allocating responsibilities based on the strengths and capabilities of each entity. When I am doing an analysis with the help of AI, I often approach it as a Centaur. I will decide on what statistical techniques to do, but then let the AI handle producing graphs. In our study at BCG, centaurs would do the work they were strongest at themselves, and then hand off tasks inside the jagged frontier to the AI. On the other hand, Cyborgs blend machine and person, integrating the two deeply. Cyborgs don't just delegate tasks; they intertwine their efforts with AI, moving back and forth over the jagged frontier. Bits of tasks get handed to the AI, such as initiating a sentence for the AI to complete, so that Cyborgs find themselves working in tandem with the AI. This is how I suggest approaching using AI for writing, for example. It is also how I generated two of the illustrations in the paper (the Jagged Frontier image and the 54 line graph, both of which were built by ChatGPT, with my initial direction and guidance) Both reflect how I work with AI tools. My blog posts are written in \"Centaur\" mode: I write the posts, but delegate tag creation, summarization, and social media post composition to GPT4. The same goes for my day-to-day work. Intellectual synthesis is done by me, the human, but summarization is done by LLMs. On the other hand, modelling work, for me, is done in Cyborg mode. Model conceptualization is done by me, the human, but model code writing is done by an LLM, but I switch back to human-driving when doing verification of the correctness of the code. This framework also gives me a vocabulary to talk about how we can integrate high-power ML tooling into AI-enabled biotech research. For example, in order to integrate the use of ML models into protein engineering and design, for example, we can adopt a Centaur mode where a library designs an initial library, while a human refines the library design before sending it off for testing. Or we can have Cyborg mode, where AI models automatically decompose a chromatogram into constituent peaks, humans annotate experimental designs onto a collection of chromatograms, and more automated standardized statistical analysis takes place to infer key parameters. This framework also reinforces another point: the term \"AI\" can also be reinterpreted as augmented intelligence, not just artificial intelligence. I don't think the two modes are mutually exclusive; the idea probably can be refined further. I'd be curious to hear what your thoughts are on this!",
    "tags": [
      "artificial intelligence",
      "centaurs",
      "cyborgs",
      "ai tooling",
      "integration",
      "biotech research",
      "ml models",
      "automation"
    ],
    "pub_date": "2023-09-17",
    "type": "blog"
  },
  {
    "id": "blog-article-review-4-skills-the-next-generation-of-data-scientists-needs-to-develop",
    "url": "/blog/2023/9/9/article-review-4-skills-the-next-generation-of-data-scientists-needs-to-develop/",
    "title": "Article Review: 4 Skills the Next Generation of Data Scientists Needs to Develop",
    "summary": "In this blog post, I reflect on the importance of four key skills for data scientists in the biotech field: problem spotting, problem scoping, problem shepherding, and solution translating. These came from an article in the Harvard Business Review. I show by example the need to understand the real issues faced by our collaborators, ask probing questions, maintain regular communication, and speak the language of the audience -- the last one being crucial. These skills are crucial in building trust, understanding the underlying science, and developing effective solutions.",
    "body": "Today, I had a chance to read the article, 4 Skills the Next Generation of Data Scientists Needs to Develop. (h/t Eric Yang for sharing this one on LinkedIn.) This article resonated massively with me, and I wanted to share how some of the ideas in here translate over to the DSAI Research team at Moderna. 1. Problem Spotting: Seeing the real issue The first skill is distinguishing the real issues faced by our collaborators from the apparent problems. The real issues might be different from the issues they think they have. Within biotech research, scientists come with differing levels of savviness with machine learning methods and the capabilities they bring to laboratory science. As such, our collaborators may come to us with requests for help that may need refining before they match what they need. One example that I can recall was related to the hype of ChatGPT. A news and perspectives article in Nature wrote that Language models similar to those behind ChatGPT have been used to improve antibody therapies against COVID-19, Ebola and other viruses. And pretty soon, we were getting inquiries about whether we could use an internal implementation of GPT-4 to design protein sequences. An inexperienced data scientist would say, \"That sounds like a cool idea! We should try it out!\" But an experienced data scientist well-versed in the training of ML models and laboratory science would immediately see the flaws in that logic and instead ask what the real problems are -- why would one want to design proteins in the first place? Here are some examples of questions we might ask: 1. Is the protein you're trying to engineer particularly difficult to express, and is your goal to improve expression? 2. Is your protein chemically modified under manufacturing conditions, and do you need to re-design the protein to minimize chemical modifications while retaining function? 3. Is your protein degrading too quickly within the cell? Notice how the questions we're asking aren't related to ChatGPT at all! Instead, they're all questions about the laboratory science itself. These questions are designed to address the lab scientist's fundamental scientific challenges. Doing so is paramount for our team to continue being valuable and innovative: uncovering the fundamental problems orients us to build Model Ts instead of breeding faster horses. 2. Problem Scoping: Gaining clarity and specificity The second skill is asking probing questions that narrow down the space of problems and the possible solutions that our collaborators could use. Let's continue the previous example by expanding on option #2, which is antibody-related. Gaining clarity would involve asking follow-up questions that look like these: 1. What chemical modifications are you worried about? 2. How do you detect these chemical modifications? Is it mass spectrometry? 3. How is the mass spec data analyzed? What are the data transformations involved to get to the endpoint measurement? 4. How do you know your antibody continues binding to its target? What is the assay that you're using? Can you describe the experiment? What are the controls used? 5. What's your hypothesis behind using machine learning for your protein engineering problem? How will there be recurring benefits from our effort? 6. What's the experimental budget in terms of libraries that you can screen? Can you turn an arrayed screen into a pooled screen? With answers to these and other questions of the same spirit, we can enhance our understanding of the problem space. We can also begin to see connections between what our collaborators need and what other collaborators may need and begin to design entire systems that can benefit more than one group at a time. 3. Problem Shepherding: Getting updates, gathering feedback As mentioned in the article, it's tempting for a data scientist to dive into the problem for an extended period and come back up with a solution that may impress a collaborator. Within a biotech context, this is junior-level thinking -- and an easy way to lose the trust of sharp and well-trained PhD-level scientists and leaders. Universities train PhDs to question everything they don't understand. No assumption will be left unturned. A junior-thinking data scientist who keeps themselves disconnected to do a \"big reveal\" will likely not understand the questions the laboratory scientists will ask. They may build something they don't understand and are unwilling to adopt and use. By contrast, the senior-thinking data scientist who makes the concerted effort to jointly co-create the solution with the laboratory scientist will cultivate trust in the solution, build their vocabulary within the scientific domain, and construct a library of misconceptions and assumptions they can anticipate and address in future builds. 4. Solution Translating: Speaking in the language of the audience Following the original article, speaking the audience's language is paramount. As you might have guessed from the previou",
    "tags": [
      "data science",
      "biotech research",
      "machine learning",
      "problem spotting",
      "problem scoping",
      "problem shepherding",
      "solution translating",
      "laboratory science",
      "protein engineering",
      "antibody therapies"
    ],
    "pub_date": "2023-09-09",
    "type": "blog"
  },
  {
    "id": "blog-interviewing-data-science-candidates-with-code-reviews",
    "url": "/blog/2023/9/6/interviewing-data-science-candidates-with-code-reviews/",
    "title": "Interviewing Data Science Candidates with Code Reviews",
    "summary": "In this blog post, I discuss a different approach to evaluating a candidate's coding skills during an interview. I ask them to bring a piece of code they're proud of and conduct a code-review style discussion. This method reveals their standards of excellence, their ability to explain and defend their work, and their thought process behind their design choices. I also share a rubric for assessing coding skills, which includes factors like code organization, documentation, and the candidate's response to feedback.",
    "body": "Note: This is an excerpt from my longer-form essay on Hiring and Interviewing Data Scientists I have found one way to deeply evaluate a candidate's coding skills, which I would like to share here. In this interview question, I would ask the candidate to bring a piece of code they are particularly proud of to the interview. We would then go through the code together, code-review style. We intentionally ask candidates to pick the code they are proud of, which gives them a home-court advantage. They should be able to explain their code better than anyone else. They also won't need to live up to an imagined standard of excellence. Additionally, because this is their best work, we gain a glimpse into what standards of excellence they hold themselves to. During the code review, I would ask the candidate questions about the code. These are some of the questions that I would cover: - What's the purpose of the code? - How is it organized? Are there other plausible ways of organizing the code? - What tradeoffs did the candidate make in choosing that particular code organization? - Are there parts of the code that remain dissatisfactory, and if so, how would you improve it in the future? That 4th question is particularly revealing. No code is going to be perfect, even my own. As such, if a candidate answers \"no\" to that question, then I would be wary of their coding skills. A \"no\" answer usually betrays a Dunning-Kruger effect, where the candidate thinks they are better than they actually are. That said, even a \"yes\" answer with superficial or scant details betrays a lack of thought into the code. Even if the code is, in my own eyes, very well-written, I would still expect the candidate to have some ideas on where the code could be extended for a logical expansion of use cases, refactored, or better tested. If the candidate cannot provide details on these ideas, it would betray their shallow thinking about the problem for which they wrote the code. Here is my rubric for assessing coding skills. Did the candidate offer the details mentioned above without prompting? This is a sign of experience; they know how to handle a code review, which we often do, and are usually confident in their knowledge of their code's strengths and weaknesses. How well-organized was the code? Does it reflect idiomatic domain knowledge, and if so, how? Organizing code logically is a sign of thoroughly thinking through the problem domain. Conversely, messy code usually suggests that the candidate is not well-versed in the problem domain and hence does not have a well-formed opinion on how they can organize code to match their domain knowledge. Additionally, because code is read more than written, a well-organized library of code will be easily reusable by others, thereby giving our team a leverage multiplier in the long run. How well-documented was the code? In what ways does the documentation enable reusability of the code? Documentation is a sign of thoughtfulness. In executing our projects, we consider the problem at hand and the reusability of the code in adjacent problem spaces. Documentation is crucial here. Without good documentation, future colleagues would have difficulty understanding the code and how to use it. Did the candidate exhibit defensive behaviors during the code review? A positive answer to this question is a red flag for us. We want to hire people who are confident in their skills but humble enough to accept feedback. Defensive behaviors shut down feedback, leading to a poor environment for learning and improvement. How strong were reasons for certain design choices over others? This question gives us a sense of the candidate's thought process. Are they thorough in their thinking, or do they rush to a solution? Because our team is building out machine learning systems, we must be careful about our design choices at each step. Therefore, if a candidate does not demonstrate thinking thoroughly through their design choices, then it means we will need to spend time and effort coaching this habit.",
    "tags": [
      "data science",
      "hiring",
      "interviewing",
      "code review",
      "coding skills",
      "candidate assessment",
      "documentation",
      "design choices",
      "machine learning"
    ],
    "pub_date": "2023-09-06",
    "type": "blog"
  },
  {
    "id": "blog-promotions-vs-bonuses",
    "url": "/blog/2023/9/4/promotions-vs-bonuses/",
    "title": "Promotions vs. Bonuses",
    "summary": "In this blog post, I debunk the misconception that promotions are rewards for excellent work at your current level. Instead, I share a framework I learned before that promotions should be rewards for demonstrating sustained excellence at a higher level. Bonuses, on the other hand, are the appropriate reward for outstanding work at your current level. But are promotions and bonuses enough as a motivator?",
    "body": "One of my previous managers, Holger Hoefling, shared a framework for thinking about rewards in the office. I found it to be quite helpful when addressing a common misconception that many fresh grads in the industry may have: The misconception The misconception that I had was this: Promotions are a reward for doing sustained, awesome work at your current level. It's natural to think about promotions this way, flawed as the logic might be. It's natural for fresh grads, mainly because in school, we are rewarded with moving on to the next course, next year, or next school by simply showing that we've mastered what we're expected to learn within a class. But suppose one thinks carefully about the logical consequences of believing that promotions are a reward for doing excellent work at our current level. In that case, that leads to the Peter Principle rearing its ugly head: The Peter principle is a concept in management developed by Laurence J. Peter which observes that people in a hierarchy tend to rise to \"a level of respective incompetence\": employees are promoted based on their success in previous jobs until they reach a level at which they are no longer competent, as skills in one job do not necessarily translate to another. A corporation would be insane to promote people for doing their current job well without knowing whether they can take on a greater scope of responsibility first. The second-order effects can end up being disastrous: a poorly motivated employee, poor morale in those surrounding them, and, with the stigma surrounding a \"demotion\" back to their original happy place, one may even face the loss of a teammate who carries precious institutional knowledge. A saner way to think about promotions The saner way to think about promotions is as follows: Promotions are a reward for doing sustained awesome work at a higher level than your current one. When phrased this way, we can avoid promoting people to their level of incompetence. Promoting an individual only after they have shown the ability to perform a higher level of work ensures that they have been trained for that new level, which increases the likelihood that the individual will be able to succeed in the new role. The second-order effects are much more attractive: a motivated teammate who can do what's expected of them at the new level, who is likely to stay around and positively impact the team, and teammates who can become inspired by a role model who has done excellent work. This is half of the framework that Holger shared with me. Then what about bonuses? If we accept the previous statement on promotions, then: Bonuses are a reward for doing sustained awesome work at your current level. Bonuses are the right incentive for doing awesome work at your current level. Bonuses are a timely reward, and their application does not add a burden of responsibility onto the rewarded individual. This is the other half of the framework that Holger shared with me. The way I think about promotions In my Career FAQ blog post, I wrote the following: I don't consider promotions my primary indicator of advancement. For me, it is a side-effect of doing good work. What profoundly satisfies me the most at work is that the projects I work on accelerate science to the speed of thought. I hold the conviction that if I'm advancing the field of science that way, I'll eventually share in the team's success. I stand by that point. If you've made it this far, my questions to you, my dear reader, are as follows: - What significant problem are you trying to solve? Does it really matter? Why? - What is the value that you are going to bring to the world? Does it really matter? Why? Also, - What are the tools in your toolbox that you're going to need to make a dent on that problem? - What tools and skills will give you maximum leverage? - How will you lift others up while you solve that significant problem of yours? Pursue the answers to those questions, not promotions and bonuses, and I can guarantee your life will be orders of magnitude more satisfying.",
    "tags": [
      "career growth",
      "promotion",
      "bonus",
      "peter principle",
      "work rewards",
      "motivation",
      "morale",
      "professional development",
      "incentives"
    ],
    "pub_date": "2023-09-04",
    "type": "blog"
  },
  {
    "id": "blog-whats-the-difference-between-setupcfg-pyprojecttoml-and-setuppy",
    "url": "/blog/2023/8/31/whats-the-difference-between-setupcfg-pyprojecttoml-and-setuppy/",
    "title": "What's the difference between `setup.cfg`, `pyproject.toml`, and `setup.py`?",
    "summary": "In this blog post, I explored the differences between , , and in Python packaging. I explained their historical context and usage, and recommended using pyproject.toml as the setup configuration file for Python packages in 2023. I also discussed the importance of Python packaging for data scientists, and the distinction between and . The former defines a project's development environment, while the latter provides pip with installation and usage information for a Python package that I might be working on.",
    "body": "If you're in a rush, then here's the tl;dr: Of the three, use ! Yesterday, one of my new teammates Jackie Valeri asked me a relatively challenging question about , and : what's the difference between those three files? Let's start with . This is a legacy format in the Python packaging ecosystem. Essentially, is a script executed when installing a Python package. For good reasons, it was the de facto format for a long time in Python. However, it has its flaws. Paul Ganssle has an excellent article on why you shouldn't invoke directly. I won't repeat his points here; I encourage you to read through it. The most common structure of is that you'd declare your package metadata as arguments to the function: That brings us to . The history of is that it initially started as a complement to . Looking through this StackOverflow thread will give us many answers on what the differences are, but the short version is that allows us to further configure the behaviour of . In theory, you can actually put all of the metadata that we originally put into the function into instead: And then your call can be left empty instead. But now we have two files. And so the Python packaging ecosystem evolved. With PEP517 and PEP518 (PEP = \"Python Enhancement Proposal\"), became a new option to configure Python builds. Over time, it has also evolved into the de facto standard for configuring Python packages, providing a one-stop shop for configuring (almost) everything about your Python project. For package metadata configuration, it'd look like this: What should I use...? Since it's 2023, and PEP517 and 518 have become widely accepted, I recommend using as the setup configuration file for a Python package. In other words, unless you've got special reasons for it, your project should not have or . In fact, in [](https://github.com/ericmjl/pyds-cli), a tool I built to standardize the creation of my Python projects, I have removed and and rely only on . Bonus: Why would a data scientist care about Python packaging? The main reason is as follows: if we seek maximum leverage from our work, then our data science work should result in reusable tools that others make. This often takes the form of a Python package. Even if our work doesn't currently have the need to build such a tool, we give ourselves the optionality to quickly build it out in the future, as long as we adopt a standard project structure that is inherently a Python package. Bonus: Then what about ? Anyone who has seen my development environments will probably notice the duplication of package dependencies between and . The way I treat the two files is as follows. defines a project's development environment. provides to what is necessary for installation and usage of my Python package. In most cases, these will be slightly different, with packages being a superset of dependencies declared in . How so? Usually, will include packages that are needed for development (e.g. ), code quality (e.g. ), and documentation (e.g. ). On the other hand, 's dependencies section are parsed by to install into an environment so that one can use the project when deployed as a standalone and distributed package. In most of the project repositories I've worked on, we have core dependencies of a package listed under both and . This came from a not-too-distant legacy when the package manager didn't play well with . As such, a dual listing is probably unnecessary. Strictly speaking, we can declare core dependencies in . When we do an editable install (), we will still see all those dependencies installed.",
    "tags": [
      "packaging",
      "setup.py",
      "setup.cfg",
      "python",
      "pyproject.toml",
      "enhancement proposal",
      "project configuration",
      "dependencies",
      "package management",
      "conda",
      "project structure"
    ],
    "pub_date": "2023-08-31",
    "type": "blog"
  },
  {
    "id": "blog-research-code-benefits-from-unit-testing",
    "url": "/blog/2023/8/30/research-code-benefits-from-unit-testing/",
    "title": "Research code benefits from unit testing",
    "summary": "In this blog post, I discuss the importance of unit tests in research code. I share an experience from a code review with our intern, Matthieu, where we realized the need for rigorous testing of a non-standard splitting strategy in our ML model. We concluded that even research code, which might be discarded eventually, can benefit from thorough testing to ensure its correctness. This is particularly crucial when the code is used for comparisons or collaborations.",
    "body": "Today, during a code review with one of our interns, I had an epiphany. Research code can still benefit from unit tests. Especially if that research code is going to be used in a head-to-head comparison of methodologies. Let me explain. Within the ML for chemistry world, random splits are downright unacceptable when constructing training, testing, and validation sets, particularly if one wants to claim that one's model generalizes beyond seen chemistry. (For a comprehensive introduction on how best to split data for molecular property prediction, I refer you to Pat Walter's blog, Practical Cheminformatics. Without going into proprietary details, one of our datasets involved replicate measurements for each molecule tested. For reasons beyond my realm of influence, one of our external collaborators built a model that included all replicate measurements rather than doing a Bayesian estimation of property for those molecules before predicting the property from an ML model. The splitting strategy there was to split on molecule, so that we ensure that no molecules were represented in both the training set and test sets. In order to build a comparator model internally, one of our interns, Matthieu, wrote code to implement that splitting strategy, as the splitting code was not shared by our collaborators just yet. During code review, Mattheiu, his direct supervisor, my teammate Zeran and myself all came to the realization that because of the non-standard nature of the splitting strategy, we needed to have guarantees that the code that Matthieu wrote was correct. As Matthieu voiced out ways to test the correctness of his code, it dawned upon me: as long as he did a refactor of the splitting code into a function, he'd have the target for a unit test! Here, he could test certain properties of the splitter function. Firstly, given data that had replicate measurements for a molecule, he could test that in any random split tested, no molecule showed up in both the train and test sets. Secondly, he could test that the total number of rows of data, when re-combining the train set and the test set, equalled the original number. These were, by no means, the only two properties of the function that he could test. And that basically brings me to the main thesis of this post: even research code, one that might be thrown away eventually, still can benefit from rigorous testing! Since this code was intended to be used in a bake-off between an internal effort and a collaborator's effort, we needed to be absolutely sure that the code did what we claimed it did, and there was no better way to have that guarantee than by having a unit test. That is, after all, the point of software tests: to find ways to prove the correctness of our code.",
    "tags": [
      "code review",
      "unit tests",
      "research",
      "chemistry ml",
      "chemistry",
      "data splitting",
      "property prediction",
      "software testing",
      "code correctness"
    ],
    "pub_date": "2023-08-30",
    "type": "blog"
  },
  {
    "id": "blog-service-vs-product-oriented-data-science",
    "url": "/blog/2023/8/28/service-vs-product-oriented-data-science/",
    "title": "Service vs. Product-Oriented Data Science",
    "summary": "In this blog post, I explore the two flavours of data science work: service-oriented and product-oriented. Service-oriented data science serves others in a one-off fashion, while product-oriented data science builds a reusable tool for a well-defined problem. Both have their value depending on the situation. I discuss the challenges in navigating between the two and emphasize the importance of adopting a product-first orientation. As an individual contributor or team lead, it's crucial to shift from being mere consumers of tooling to makers of tools, enhancing efficiency and scalability!",
    "body": "As the field of data science progresses and evolves within the biotech space, I'm seeing two flavours of data science work showing up. The first is service-oriented data science. The second is product-oriented data science. These two can be conceived along a continuum. What is service-oriented data science? As its name suggests, this flavour of data science is all about serving others, mainly in a one-off fashion. In this flavour of data science, the insights derived from a model-building effort are the object of value. Examples of this kind of data science might be building a mechanistic model of a biophysical measurement to quantify some key parameter of interest (e.g., half-life decay or thermal stability), which can be used downstream. In this mode, we encounter a wide range of problems, and our impact is primarily on the individual or group being serviced. What about product-oriented data science? As its name suggests, this flavour of data science is all about building a product that can serve users over and over on a narrowly defined but valuable problem. Examples of this kind of data science might be building a predictive model of protein stability or a mutation sampler from evolutionary sequences that one can use repeatedly in future protein engineering campaigns. In this mode, we are building a reusable tool that solves a well-defined problem, effectively expanding our colleagues' capabilities in a scalable fashion. (h/t to Andrew Giessel and Dave Johnson, from whom I first heard this concept articulated.) In both cases, the model itself is of less value than the capabilities it enables or the insights it delivers. Is one mode more valuable than another? My answer is yes... depending on the situation. How do we know which? I've identified two plausible scenarios, one for each mode. When one's broader organization is in a position where people need convincing of the value of a methodology, a service-oriented posture is necessary. On the other hand, when one's organization needs automation that cannot be accomplished by simply chaining off-the-shelf tools together, then a product-oriented posture will be necessary. Even within a team project, one can switch between service- and product-orientations. In favour of the project's goals, we can adopt a service-oriented posture and help deliver insights, library designs, or more. But behind the scenes, we can adopt a more product-oriented stance and build out reusable tools and components for the general class of problems being solved. What challenges are there in navigating the spectrum? Our collaborators will always care about being served first. The \"how\", on the other hand, doesn't really matter as much, whether through white-glove service or through a tool we build for them. In that respect, it is tempting to adopt a service-oriented posture when building a higher ROI tool. An example of a very challenging instance of this problem I've seen before is when my teammates were asked to design a protein library using rules similar to a previously designed one but with minor differences for a new protein. This ask precludes blindly re-running code for generating protein libraries; instead, modifications are needed. Here, a product-oriented service posture (yes, I did chain those four words together) can be beneficial. With simple and good software engineering practices, which necessarily means thinking clearly about our work, we can make routine things easy while incorporating the new and varied things modularly. On the other hand, if one adopts a product-oriented posture, the challenge lies in becoming progressively detached from the collaborators our products are supposed to serve. In this situation, it can be tempting to delve into a problem for an extended period, tinkering in circles without the necessary feedback from our collaborators on whether our product solves their pain points. Thankfully, my teammates and I have not yet been in this situation. One of the ways we stay vigilant here is by constantly asking ourselves the core question, \"Does this serve our collaborators?\" What can I do as an individual contributor or team lead? In general, I prefer that my teammates and I adopt a product-first orientation, building tools with high leverage while using a service-first orientation to help demonstrate the value of those tools that we build. For some, this is a challenge: if one's instinct is to \"just solve the problem\" without stepping back to think of the general case of the problem, then one ends up in a habitual service orientation, which I consider to be of extremely low leverage. For an individual contributor, then, I think a mindset shift is necessary. Rather than being mere consumers of tooling, one needs to become a maker of tools as well. Being able to make tools for oneself is a data science superpower that will differentiate one from the pack. This is in-line with knowing every last detail of your computational stack, and being able to bui",
    "tags": [
      "automation",
      "biotech",
      "collaboration",
      "data insights",
      "data product",
      "data science",
      "model building",
      "predictive models",
      "product oriented",
      "protein engineering",
      "service oriented",
      "software engineering",
      "team collaboration",
      "tool building"
    ],
    "pub_date": "2023-08-28",
    "type": "blog"
  },
  {
    "id": "blog-deploy-to-dokku-from-github-actions",
    "url": "/blog/2023/8/27/deploy-to-dokku-from-github-actions/",
    "title": "Deploy to Dokku from GitHub Actions",
    "summary": "In my latest blog post, I share my experience of hosting a Dokku server on DigitalOcean and how I've managed to automate the deployment process using GitHub Actions. I delve into the cost benefits of using Dokku on DigitalOcean over other services like Heroku and Fly.io. I also provide a step-by-step guide on how to configure GitHub Actions to deploy apps to DigitalOcean automatically. If you're interested in saving time and money on app deployment, this post is a must-read.",
    "body": "I host a Dokku server on DigitalOcean. (If you're interested in signing up for a DigitalOcean account, I have a referral link here.) I use Dokku on DigitalOcean because of cost: for a given instance size's price, I can host about 5-10x more apps on a given server than I could otherwise on competing services such as Heroku and Fly.io. That said, deploying to my Dokku server is often done manually. I recently figured out how to get out of doing that manually, by using GitHub Actions to deploy my apps to DigitalOcean on every merge to the branch. The GitHub Actions Workflow YAML Dokku has an official GitHub Action for doing so. Under there, I found the most minimal and simple configuration that's needed: What's going on here? Underneath the hood, the Dokku action is simply taking the repository and doing a to the Dokku server. Doing so requires the following information to be configured on GitHub Actions, as secrets (for security purposes). Firstly, you need : which should be of the format . I mistakenly configured it as , which is how remotes are configured, but that yielded an error. Secondly, you need : which should be the SSH private key that Dokku is configured to accept. For this, I would strongly suggest: 1. Creating a new SSH key pair (following the excellent instructions on GitHub's docs, and then 2. Adding the public key half of the SSH key pair to Dokku (following the instructions on the Dokku User Management page). Both and need to be configured on GitHub Actions' secrets. This is done by navigating to the repository's Settings --> Secrets and Variables --> Actions, and then adding the appropriate secrets as described above.",
    "tags": [
      "til",
      "github actions",
      "cicd",
      "continuous integration",
      "continuous delivery",
      "dokku",
      "digitalocean",
      "deployment",
      "coding",
      "devops",
      "cost efficiency",
      "dokku server",
      "git",
      "dokku deployment"
    ],
    "pub_date": "2023-08-27",
    "type": "blog"
  },
  {
    "id": "blog-enable-github-actions-to-push-code-changes",
    "url": "/blog/2023/8/26/enable-github-actions-to-push-code-changes/",
    "title": "Enable GitHub Actions to Push Code Changes",
    "summary": "Today, I learned how about a hidden setting that's needed to enable GitHub Actions to push code to its associated repo, and wrote it out as a tutorial. As a bonus, I also share how to correctly configure within a GitHub actions workflow. Discover the trick with me!",
    "body": "Today I learned how to ensure that GitHub actions is capable of pushing code to its associated repo. The tl;dr is: 1. Within a repo, click on \"Settings\". 2. Within \"Settings\", click on \"Actions\", and then \"General\". 3. Under \"General\", look for \"Workflow Permissions\" and then grant \"Read and write permissions\" to the Workflow runner. It'll look something like this: Underneath the hood, there is a default that is granted to a GitHub Actions' Workflow runner. The default permissions are read-only, as it needs to be able to clone the repo. However, we can enable it to push by enabling write permissions. Once we've configured that setting, within a GitHub actions workflow, we need to ensure that is configured correctly before pushing. In one of my workflow configuration files, I have it split up into two steps:",
    "tags": [
      "til",
      "github",
      "github actions",
      "github workflow",
      "git configuration",
      "workflow runner",
      "github permissions",
      "repo settings",
      "workflow permissions",
      "github token"
    ],
    "pub_date": "2023-08-26",
    "type": "blog"
  },
  {
    "id": "blog-use-cron-to-execute-commands-on-startup",
    "url": "/blog/2023/8/22/use-cron-to-execute-commands-on-startup/",
    "title": "Use Cron to execute commands on startup",
    "summary": "Today I learned how to execute arbitrary commands on startup on a Linux machine. It's pretty simple. Curious to hear more?",
    "body": "Today I learned how to execute arbitrary commands on startup on a Linux machine. This trick uses , and is essentially adding something like this to your crontab: There are other ways of doing so as well, according to tutorialspoint, including using , , and . While I'm quite sure there are good reasons for more than one way to accomplish this task, I think using is pretty elegant for the following reasons: 1. The syntax is very clean. 2. is almost universally installed on Linux variants. 3. is a conceptually well-accepted place for automated execution of commands on a schedule.",
    "tags": [
      "automation",
      "command execution",
      "cron",
      "cron jobs",
      "init.d",
      "linux",
      "linux commands",
      "linux startup",
      "linux tutorial",
      "rc.local",
      "startup scripts",
      "systemd",
      "til"
    ],
    "pub_date": "2023-08-22",
    "type": "blog"
  },
  {
    "id": "blog-journal-club-differentiable-search-of-evolutionary-trees",
    "url": "/blog/2023/8/7/journal-club-differentiable-search-of-evolutionary-trees/",
    "title": "Journal Club: Differentiable Search of Evolutionary Trees",
    "summary": "I've just explored a fascinating paper on differentiable search of evolutionary trees. It's a creative blend of math and biology, using mathematical structures to solve a biological problem. The authors have developed a way to infer both phylogenetic trees and ancestral protein sequences in a continuous, differentiable manner. This opens up exciting new avenues for protein engineering and design. Plus, the paper's figures are top-notch! \ud83e\uddec\ud83c\udf33\ud83d\udcca",
    "body": "Once in a while, I find a research paper that is unique and creative -- one that is not just a matter of brute force scaling but uses mathematical links creatively to solve a biological problem. \"Differentiable Search of Evolutionary Trees\" is one such paper, and I'm very happy for the authors Ramith Hettiarachchi, Avi Swartz, and Sergey Ovchinnikov that it was accepted to ICML 2023! (It was specifically accepted to the Sampling and Optimization in Discrete Space (SODS) and Differentiable Almost Everything (DiffAE) workshops.) I thought it would be worth sharing the methodology, with a specific focus on how the authors take a non-differentiable problem and turn it into a differentiable problem through interconversion between mathematical data structures. It's challenging to find a phylogenetic tree that explains the existence of a collection of sequences -- the problem is NP-complete, and the space of possible trees is combinatorially large. Moreover, the discrete nature of phylogenetic trees makes tree generation inherently non-differentiable. Yet, we should be able to make phylogenetic tree search differentiable. So how do we do that? The answer to that question is the premise of this paper. The key idea here is to recognize that: 1. protein (and their corresponding nucleotide) sequences, whose generation can be represented by a tree, can be represented by a probability matrix (a.k.a. position-specific probability matrix). 2. trees are graphs, 3. graphs have a discrete matrix representation, 4. discrete matrices can be relaxed to continuous matrices, 5. continuous matrices can be optimized using gradients, Let's explore these ideas a bit more. Sequences have a probabilistic representation To start, we should know one definition of a phylogenetic tree: Given $N$ observed sequences (also known as \"leaves\" in the tree), there will be $N-1$ ancestral sequences that are assumed to exist. This comes from the definition of a phylogenetic tree as a bifurcating tree, which implies that each ancestral sequence gives rise to two children sequences, which can either be ancestral themselves or the observed sequences. Now, the observed sequences can be represented as a one-hot encoded matrix. Using SeqLike to create the object, we can convert it to a one-hot encoding easily: Visualized, it will look like this: By default, SeqLike will construct the matrix such that positions are rows and letters are columns. However, that matrix can also be transposed if desired. One-hot encodings are a discrete representation; however, we can relax it to be a continuous representation by ensuring that each position (column) is a probability vector that sums to 1.0. Doing so gives us a position-specific weight matrix (PSWM) (or the position-specific probability matrix). This is the first trick the authors used to convert a discrete problem into a continuous problem. In contrast to the observed sequences, the $N-1$ ancestral sequences are not precisely known, so we use a PSWM to represent them. PSWMs can be visualized like this: Phylogenies, however, don't operate with just one observed sequence; there usually are a handful of sequences. For illustrative purposes, we will generate and use three observed sequences throughout this post. Trees are graphs, and graphs have discrete matrix representations, and those representations can be relaxed as well. A phylogenetic tree is a directed acyclic graph (DAG). Every graph can be represented as a matrix, such that nodes are ordered identically on the rows and columns. Since we are talking about directed graphs, we can treat the rows as \"source\" nodes and the columns as \"target\" nodes. In the case of a DAG, we only need to consider the upper triangle; due to the acyclic property of a DAG, we never need edges from the target nodes back to the source nodes. As such, for a tree (which is a DAG), we're able to always find a topological sort of the nodes, and hence only need the upper right triangle. Here's an example. This tree, which has 5 nodes (3 observed, A-C and 2 ancestral, D & E), looks like this in tree form: That same tree, in matrix form, (with rows being children and columns being parents to maintain conventions): Here, a value of - indicates that the row is a child of the column, - indicates the absence of that relationship, - fills the lower left triangle because those are not applicable to trees, and - fills the invalid relations between observed sequences. If you're being astute and thinking ahead a few steps, you'll quickly come to the realization that the s and s above are the positions of the parameterizable parts of the tree model. Those entries in the matrix are the ones that we need to infer in order to find the best tree for a given collection of observed sequences. Here are other observations that we can make: - Every row must sum to 1. This is because every child node can only be a child node to one parent node. - Every column must sum to 2. This is because every parent node ",
    "tags": [
      "machine learning",
      "phylogenetics",
      "protein engineering",
      "protein sequences",
      "ancestral sequences",
      "bioinformatics",
      "computational biology",
      "deep learning",
      "differentiable computing",
      "gradient-based optimization",
      "phylogenetic trees",
      "sequence representation",
      "tree adjacency",
      "vae",
      "protein design",
      "jax",
      "python"
    ],
    "pub_date": "2023-08-07",
    "type": "blog"
  },
  {
    "id": "blog-skating-ahead-of-the-puck-a-wayne-gretzky-approach-to-tech-adoption",
    "url": "/blog/2023/8/4/skating-ahead-of-the-puck-a-wayne-gretzky-approach-to-tech-adoption/",
    "title": "Skating Ahead of the Puck: A Wayne Gretzky Approach to Tech Adoption",
    "summary": "In this post, I discuss the 'Wayne Gretzky move' in tech adoption, especially in data science. It's about foreseeing the future of tech and making bold moves to stay ahead of the game. It's risky but with the right team, it can lead to significant gains. \ud83c\udfd2\ud83d\udcbb\ud83d\ude80",
    "body": "Wayne Gretzky was a Canadian ice hockey player to whom this quote is attributed: A good hockey player plays where the puck is. A great hockey player plays where the puck is going to be. This quote applies when applied to choosing tech to adopt, buy, or build. We want to select tech that gets us to our envisioned goals down the road. We don't merely want to choose tech that puts out fires today. A \"Wayne Gretzky move\" is one of those thought frameworks I use to help me navigate the technology ladder. What does that look like in a data science setting? For an individual contributor data scientist, making a Wayne Gretzky move means picking up something technologically ahead-of-the-game. Move away from the routine tooling you already use. Expand your horizons into emerging tech for which you can imagine the value but for which it has yet to be proven. For a data science team, it means envisioning entire capabilities that an organization may not yet have dreamed up and building out those capabilities early to let them become competitive advantages. Wayne Gretzky moves require foresight (anticipating where the puck will be), confidence in the value of the move (leading to better shots on goal), and technical execution capability (skating fast enough before competition arrives). They can be risky to execute, so having the right team is needed (teammates who will put the puck where it needs to be). But if your team can realize the move's expected value (a goal, possibly a game-winning shot), there is much profit to gain!",
    "tags": [
      "wayne gretzky",
      "tech adoption",
      "data science",
      "emerging tech",
      "competitive advantage",
      "foresight",
      "execution capability",
      "risk management",
      "teamwork",
      "profit",
      "innovation",
      "strategic thinking",
      "technology ladder",
      "individual contributor",
      "team strategy",
      "thought framework"
    ],
    "pub_date": "2023-08-04",
    "type": "blog"
  },
  {
    "id": "blog-climbing-the-tech-ladder-or-staying-grounded-a-guide-to-navigating-data-science-innovations",
    "url": "/blog/2023/7/24/climbing-the-tech-ladder-or-staying-grounded-a-guide-to-navigating-data-science-innovations/",
    "title": "Climbing the Tech Ladder or Staying Grounded? A Guide to Navigating Data Science Innovations",
    "summary": "In this post, I discuss how to decide which technological models to adopt in our work. I introduce two frameworks: the 'technological trinity' and the 'technology ladder'. The former helps us evaluate if a new technology is worth investing in, while the latter places it within a broader context. I illustrate these concepts using protein engineering as an example.",
    "body": "How do we choose the model to use? One of our DSAI interns asked an excellent question, \"One of the attractive things about DSAI is its promotion of the use of a Journal Club for reviewing the latest literature. Do we put that latest stuff into our daily work much?\" I took this question and responded with the following ideas. Firstly, keeping up with the latest literature is important because we need to know the latest methodologies. Doing so gives us a feel and, more importantly, a taste for technology. Developing that taste for tech is essential. It informs how we evaluate the performance of a model, the validity of a scientific conclusion, and more. Keeping up with the literature can help us find new talent to hire. What comes next, then, is developing judgment for whether to invest time in the latest methodologies or not. To make that judgment call, we need a goal and frameworks to make those decisions. We work backwards from our goals, and the frameworks are the compasses we use to guide our choices. I rely on two frameworks here. The first is what I call the technological trinity. The second is the technology ladder, which I adapted from my colleague Joshua Bishop. I want to expand on them here. Technological Trinity The technological trinity is us being able to answer \"yes\" to the following questions: 1. Will a new technological or methodological innovation help us achieve our goals and deliver value to those we serve? 2. Do we have the personnel to implement those methods in-house and maintain them long-term? 3. Does the method fit within a broader, unified technical strategy that already exists? Point #3 is only sometimes articulated clearly, especially for those who work to make medicines. We're only sometimes going to be sophisticated technically and mathematically. Still, without a strong foundation, we'll be assembling Rube Goldberg machines that may work now, may not be architected for the long term, and may not be attractive to individuals of the next level of technical sophistication, thus limiting innovation. If the answer is \"yes\" to all three of those questions, then we can invest our time in doing a build-out. If any of them gets a \"no\" from us, then there will be a risk that we must weigh. Technology Ladder The technology ladder is a framework for placing that new tech or method within a broader context. For a given method, we identify a simpler version of that tech/method that achieves the same goals. We also imagine a more complex version that achieves the same goals, but it may come with more bells and whistles, such as efficiency gains or capability expansions. The price we pay is the need for skilled individuals to build and maintain those shiny things. Example: Protein Engineering I used the example of library design methods in protein engineering to illustrate this. One way of designing libraries can involve prior training data for an enzyme's activity. How do we build a system that does this? At the bottom of the technology ladder, we can generate mutations by randomly sampling single, double, triple, or more. Our coverage of mutational space would be highly sparse; we would be navigating blindfolded. Anyone skilled in Python could write this code quickly, as it is essentially just a string mutation problem. One rung up the ladder, we might incorporate evolutionary information. One method would be to train a variational autoencoder on evolutionarily related sequences retrieved via BLAST. Doing so would be akin to navigating sequence space with signposts (evolutionary examples). To build this out in-house, we need to find a skilled individual who has built variational autoencoder models before, understands at least the one-hot encoding representation of protein sequences, and knows tricks for stably training a VAE. Another rung up the ladder would be to use protein language models, which came later than variational autoencoder models, trained on the space of observed protein sequences. Using this class of models productively would require someone who has worked with language models before, understands tokenization and embeddings, and understands the caveats of using language models and how they differ from evolutionary models of proteins. Another rung up the ladder is to use graph representations of protein structures. Using this class of models productively requires an individual who has worked with GNNs, ideally understands protein structural biochemistry, and has training in biophysics. They would also need to be comfortable wrangling an ever-evolving landscape of graph neural network tooling (torch-geometric, Graphein, etc.), with bonus points if they are also core developers on those tools. And yet another rung up the ladder would be to find someone who could train diffusion models of protein structure. As of writing, no established tooling exists to do this robustly; it is an active area of research. At this point, the pool of candidates would likely be current Ph.D. studen",
    "tags": [
      "data science",
      "technology ladder",
      "technological trinity",
      "decision making",
      "protein engineering",
      "methodologies",
      "innovation",
      "team building",
      "technical strategy",
      "bayesian optimization",
      "graph neural networks",
      "variational autoencoder",
      "machine learning",
      "library design",
      "lead optimization"
    ],
    "pub_date": "2023-07-24",
    "type": "blog"
  },
  {
    "id": "blog-behind-the-scenes-developing-llamabot",
    "url": "/blog/2023/7/21/behind-the-scenes-developing-llamabot/",
    "title": "Behind-the-scenes developing LlamaBot",
    "summary": "Just built LlamaBot, a chatbot using OpenAI GPT-4, and it's been a wild ride! \ud83c\udfa2 From dealing with rapid innovation and versioning issues to discovering the power of code ghostwriting, it's been a learning curve. \ud83e\udde0\ud83d\udca1 Also, I explored some cool LLM frameworks. Dive in to learn more! \ud83c\udfca\u200d\u2642\ufe0f",
    "body": "LlamaBot LlamaBot is a Pythonic interface to LLMs that I built out with input from friends and colleagues. (I previously announced that I was working on it in a previous blog post.) Within the project, I also decided to build some LLM-based apps that I felt would be useful for myself. To pace myself, I set some milestones for myself that included the following desirable features: - Making a CLI for chatting using the OpenAI GPT-4 API, - Building a commit message writer, - Making a Zotero chatbot, and - Building a Python code ghostwriter that can work with any Python library. I want to reflect on the development process in this blog post. Building this thing, after all, was challenging, especially since I was doing this way after work hours, handling two kids, and before midnights arrived. I also wanted to record some of the challenges faced, the decisions taken, and how I might redo the code if I had a chance to start again. Breakneck innovation pace means we're always building on sand First, I want to share that building on top of an evolving ecosystem feels like building a house on quicksand. I remember vividly when underwent a refactor of their class. It broke a working version of the code while I was developing more advanced features. That was bloody frustrating! The challenge here was finding time to keep pace with the ecosystem. It takes time to keep up, and that is time that could be spent building. This was a constant tradeoff that I had to consider. Eventually, I ended up temporarily pinning the version of that I depended on while I got the rest of the features of QueryBot in place; only then was I able to work following up on the refactored code. The flip side is that things that were problems before soon disappear. One example is the inability of LLMs to natively return structured data reliably; I will quote the Marvin docs: One of the most important \"unlocks\" for using AIs alongside and within your code is working with native data structures. This can be challenging for two reasons: first, because LLMs naturally exchange information through unstructured text; and second, because modern LLMs are trained with conversational objectives, so they have a tendency to interject extra words like \"Sure, here's the data you requested:\". This makes extracting structured outputs difficult. Marvin has had a solution to this problem for a short while, but because I had chosen LangChain's as the base of LlamaBot's stack (because I wanted to work with llamaindex, which depends on LangChain), I found myself hesitating to even experiment with Marvin, even though, through reading the docs, I loved the ideas the devs were building Marvin around. Yet, soon after, OpenAI released Functions capabilities, and now LangChain also can create structured data reliably (or so I think, given what I'm seeing in the docs). Additionally, I'm seeing solutions from the Outlines package for this exact problem too). At this pace, thanks to rapid feedback from the global developer community, we will see the rapid development of things that developers need to build the apps they want. Bots working with Bots: A design pattern for LLM apps This is a neat design pattern I didn't appreciate before trying to build the Zotero chatbot. Within the Zotero chatbot, building a general-purpose bot that could handle the breadth of possible user interactions with papers was impossible. Instead, I still needed to map out plausible user flows and decompose possible user interactions into something that fit within the scope of a single bot. Within , the three bot classes provide a general-purpose interface for creating a bot. SimpleBot corresponds to a Python function steered by a system prompt and can transform user input into specified output. Because it has no memory, future output requests are not confounded by messages in memory being passed in as context. These behave like stateless functions, essentially. ChatBot, on the other hand, is a general-purpose interface that lets us create ChatBots. They, too, are steered by a system prompt and can transform user input into specified output, while the implementation of memory (stuffing as many messages into the maximum context window) allows us to have longer-range conversations. However, stateful memory means that the LLM output will always be confounded by previous messages. (This is why SimpleBot exists.) Finally, QueryBot is a general-purpose interface for querying documents. By embedding documents and using similarity search for retrieval before stuffing the queried text into the context, synthesized responses can be better conditioned and steered by existing content within the context window. Within , one QueryBot embeds the entire Zotero library and picks out the entries that are most relevant to a user query, returning only the entries that the QueryBot thinks are relevant. Once the user has chosen one paper to chat with, another QueryBot embeds that paper and enables users to interact with the",
    "tags": [
      "llamabot",
      "openai",
      "gpt4",
      "chatbot",
      "software development",
      "ai innovation",
      "langchain",
      "llama_index",
      "code ghostwriting",
      "zotero"
    ],
    "pub_date": "2023-07-21",
    "type": "blog"
  },
  {
    "id": "blog-how-to-configure-a-minimum-python-version-in-pyprojecttoml",
    "url": "/blog/2023/7/12/how-to-configure-a-minimum-python-version-in-pyprojecttoml/",
    "title": "How to configure a minimum Python version in pyproject.toml",
    "summary": "In today's blog post, we dive into setting the minimum and maximum Python versions in \ud83d\udc0d. We explore how this impacts and discuss the new syntax for indicating the union of types in Python 3.10. This new syntax is visually easier to understand, making our coding journey a bit smoother! \ud83d\ude80\ud83d\udc69\u200d\ud83d\udcbb\ud83d\udc68\u200d\ud83d\udcbb",
    "body": "Today, I learned how to set the minimum version of Python in . If I want a minimum version of Python: If I want a maximum version of Python: This problem came up within the context of , where I was using relatively new syntax to indicate the union of types: Pre-Python 3.10, the only way to show this was: Visually, I think the post-Python 3.10 syntax is a bit easier to reason about. h/t my colleague Jiayi Cox who helped me identify the bug in this issue.",
    "tags": [
      "til",
      "python",
      "python310",
      "pythonversioning",
      "pyproject.toml",
      "pythontips",
      "llamabot"
    ],
    "pub_date": "2023-07-12",
    "type": "blog"
  },
  {
    "id": "blog-automatically-write-awesome-commit-messages",
    "url": "/blog/2023/6/20/automatically-write-awesome-commit-messages/",
    "title": "Automatically write awesome commit messages",
    "summary": "Tired of writing boring commit messages? \ud83d\ude34 I've got you covered! Introducing , a CLI tool that uses GPT-4 to craft meaningful commit messages following the Conventional Commits specification. \ud83d\ude80 Improve collaboration, project management, and debugging with just one command! \ud83c\udf1f Try it out with . \ud83e\udd99",
    "body": "As a data scientist, I work with Git. If you're anything like the lazy version of me, your commit messages look something like this: (This is my website's commit log.) These aren't particularly informative. They don't tell me sufficient detail on what changes were made from commit to commit. Imagine, now, we had a commit message that looked like this: That's even worse... and it's a routine thing amongst data scientists! To get around this problem, I wrote a CLI tool within that crafts commit messages according to the Conventional Commits specification. To use it, I simply stage the changes I'd like to commit together and then run: Underneath the hood, with an OpenAI API key, GPT-4-32k will write a commit message for me that looks like: Isn't that amazing?! The benefits of using meaningful commit messages are manifold. Firstly, it improves collaboration within development teams. When team members review the commit log, they can quickly understand the nature and purpose of each change. This clarity fosters efficient code reviews and reduces the time spent deciphering the intentions behind commits. Secondly, informative commit messages facilitate better project management. By examining the commit log, project managers and stakeholders gain insights into the progress of the project, the implemented features, and the bugs fixed. This knowledge allows for effective tracking of changes and helps in making informed decisions about future development directions. Moreover, detailed commit messages greatly aid in debugging and issue resolution. When encountering a bug or regression, developers can trace back the changes through the commit history and identify the specific commit that introduced the problem. This capability saves time and effort by pinpointing the exact source of an issue, enabling faster resolution and reducing downtime. Even better if we're versioning machine learning models! If you're interested in trying out , you can install it by running at the terminal!",
    "tags": [
      "git",
      "commit messages",
      "gpt-4",
      "llamabot",
      "cli tool",
      "openai api",
      "conventional commits",
      "collaboration",
      "project management",
      "debugging",
      "code review",
      "versioning",
      "machine learning",
      "outlines library",
      "coding bot"
    ],
    "pub_date": "2023-06-20",
    "type": "blog"
  },
  {
    "id": "blog-outlines-llm-prompt-management-and-more",
    "url": "/blog/2023/6/16/outlines-llm-prompt-management-and-more/",
    "title": "Outlines: LLM prompt management and more",
    "summary": "Today, I explored the library and built a coding bot that ghostwrites code, docstrings, and tests! \ud83e\udd16 With clean prompt design, I was able to create a compact and efficient Python program. The best part? It's super easy to map this to a user interface! \ud83c\udf89 Check out how I did it and learn about the benefits of using for prompt management. \ud83d\ude04",
    "body": "Outlines I recently picked up on the library [](https://github.com/normal-computing/outlines/tree/main) by Normal Computing. Reading the README, it could have great potential in prompt management - the ability to concisely manage prompts in a composable fashion. For example, I decided to build a coding bot - not just the Copilot-esque autocomplete, but one that could ghostwrite code with me. As an experiment, I installed the library and created a that would do three things: code, write , and write . With these three well-defined tasks on hand, I wrote the code + prompts needed to accomplish this task. Here is what the bot and the prompts looked like. Firstly, the bot's definition: I clearly had to do some iteration on the prompt, and I used 's to help with that. Then, the prompt for ghostwriting the code: Next up, the docstring writer: And finally, the unit test code writer: With those, I tried the following code ghostwriting request: This is what I got back: On being satisfied with the correctness of the function, I then asked the bot to add docstrings into the mix: This gives me: python prurl = \"https://github.com/owner/repo/pull/42\" prcontents = getprcontents(prurl) print(prcontents) docstring()tests()ghostwriterghostwriterdocstringtestsghostwritercode@text.promptoutlinesoutlines@text.prompt` decorator enables interpolation without worrying about tricky edge cases. Apart from that, I also wanted to emphasize how the syntax here is quite enabling! Encapsulating natural language instructions in a function makes for very compact Python programs.",
    "tags": [
      "outlines",
      "python",
      "coding bot",
      "ghostwriting",
      "docstrings",
      "unit tests",
      "prompt management",
      "composable prompts",
      "llamabot",
      "gpt-4",
      "code generation",
      "ui design",
      "jinja2",
      "clean code"
    ],
    "pub_date": "2023-06-16",
    "type": "blog"
  },
  {
    "id": "blog-how-to-craft-stellar-pull-request-summaries-with-gpt-4",
    "url": "/blog/2023/5/13/how-to-craft-stellar-pull-request-summaries-with-gpt-4/",
    "title": "How to Craft Stellar Pull Request Summaries with GPT-4",
    "summary": "Today, I discovered a fantastic use for GPT-4: writing improved pull request messages! \ud83c\udf89 I used a Python package I developed, llamabot, to generate summaries for pull requests. The results were impressive, making the task of reviewing PRs with long diffs much less daunting. \ud83d\ude0c",
    "body": "Today, I discovered an exciting use case for GPT-4: writing improved pull request messages! In this blog post, I will share the process I followed to achieve this. First, I utilized , a Python package that I developed. Next, I obtained the diff of a pull request. The simplest method to do this is by appending to the end of a PR URL, like so: https://github.com/pyjanitor-devs/pyjanitor/pull/1256 -> https://github.com/pyjanitor-devs/pyjanitor/pull/1256.diff Here's the code for generating the summary message: For PR#1262: And for PR#1261: This was an impressive outcome! Previously, I would dread having to (1) write a summary of my PRs, and (2) review PRs with very long diffs. This prompt is an excellent solution to both problems! How do other devs think about this? As pyjanitor co-maintainer Samuel Oranyeli commented, I am missing out ... Summarises my code better ... I could def use this to improve my comments and my code generally ... Now I admit mind blown",
    "tags": [
      "gpt4",
      "pull requests",
      "code review",
      "python",
      "llamabot",
      "github",
      "automation",
      "machine learning",
      "ai",
      "coding",
      "programming",
      "data science",
      "nlp",
      "open source",
      "software development"
    ],
    "pub_date": "2023-05-13",
    "type": "blog"
  },
  {
    "id": "blog-how-to-make-python-context-managers-aware-of-their-code",
    "url": "/blog/2023/5/2/how-to-make-python-context-managers-aware-of-their-code/",
    "title": "How to make Python context managers aware of their code",
    "summary": "I\u2019ve been working on and wanted to implement a feature to automatically record prompts and LLM responses. I used a PromptRecorder object with a context manager, similar to PyMC\u2019s . However, I faced a challenge: making the context manager aware of its internal code. \ud83e\udd14 After some research, I found a solution using the built-in module. By creating a globally-referenceable prompt_recorder variable, I was able to modify the instantiated PromptRecorder object\u2019s state and call its methods while keeping it hidden from the front-end code. \ud83c\udf89 Now, can easily record prompt-response pairs! \ud83e\udd16",
    "body": "While working on , I implemented a new feature: automagically recording prompts and LLM responses. I've wanted this for a while because it would eliminate a lot of friction associated with designing prompts, namely, having to keep track of what I tried and what the LLM responded with. On thinking about how to implement it, I went with a object that uses a context manager. The syntax I was going for was to match PyMC's context manager, which looks something like this: I was trying to go for the following: As an alternative, I can imagine interactively experimenting with prompts inside a Jupyter notebook cell: And we could access the prompt-response pairs from the object. But how do we make a context manager aware of its internal code? As it turns out, that's not easily doable. However, the reverse direction is! Code inside the context manager can modify the context manager's contents if referenced properly. How do we reference a context manager from within a code chunk? I'm going to show a snippet from to illustrate. Firstly, within the source file that we use, we start with the following: This gives a globally-referenceable variable we can call on later. Next, within the class, we implement the and methods as follows: Upon entering the context manager, the method sets the variable as the instantiated object. One final function gives our objects the ability to use the within them: This function accepts a prompt and a response and automatically checks if a object is stored inside the . If yes, it will record the response. If not, nothing happens. We use this function inside the SimpleBot class when calling on it: And just like that, the is made aware of the context manager! The fundamental trick is keeping a globally referenceable variable enabled by the built-in [](https://docs.python.org/3/library/contextvars.html) module. With that, we can modify the instantiated object's state, call on the object's methods, and do other things with the object while still hiding it from the front-end code we write. If you're curious, you can study the code here",
    "tags": [
      "python",
      "context manager",
      "llamabot",
      "code snippet",
      "contextvars",
      "programming",
      "til"
    ],
    "pub_date": "2023-05-02",
    "type": "blog"
  },
  {
    "id": "blog-how-to-use-npwhere-in-a-vmap-ed-function",
    "url": "/blog/2023/4/29/how-to-use-npwhere-in-a-vmap-ed-function/",
    "title": "How to use NumPy's where function with JAX's vmap",
    "summary": "\ud83e\uddd0I encountered a tricky problem when working on chromatography: I needed to find x-values on concave curves at a specific height without solving simultaneous equations. Check out my blog post \ud83d\udcdd to see how I went from a simple for-loop solution to using JAX's for a more effective, expressive, and compatible solution. This was a fun programming problem to solve! \ud83d\udd0e\ud83d\udcc8\ud83d\udcca\ud83d\ude03",
    "body": "While working on a chromatography problem, I encountered a sub-problem where I needed to find the x-values on a concave curve where the height at those points was 13.5% of the maximum height. Let me provide a concrete example. Visually, it looks like this: The most obvious solution is to write a for-loop: Indeed, this was the solution that Copilot gave me, and I ran with it for a while until I later thought, \"hmm, this isn't good enough. I can definitely do better.\" I decided to implement the function using NumPy-only functions, and it turned out like this: That gives us: Now, this is much better: compact and expressive. However, this is incompatible with JAX's . Wait, why do I want ? Well, it's because I wrote my functions to be able to operate on a single curve, but I will eventually need to do this for a rolling window of pairs of curves. This means needing to over each pair of curves and then over each pair. So why is the function above incompatible with ? The culprit here is that returns a vector with a variable size, not fixed. As it turns out, the JAX devs knew about this problem and put in a argument. This allowed me to re-express the solution as follows: Here are the key tricks: 1. We use to grab out the first point at which the y-values are greater than or equal to the desired height. This is why we grab out just one element. 2. We use to do the same, except we use as the size to return, as it is guaranteed to give us the upper-bound index. 3. Finally, we create a new NumPy array with the exact values we want. Using on the new function gives us the following: Plotting the points shows us that we got the right thing:",
    "tags": [
      "chromatography",
      "numpy",
      "jax",
      "vmap",
      "lax",
      "programming tips",
      "coding",
      "til"
    ],
    "pub_date": "2023-04-29",
    "type": "blog"
  },
  {
    "id": "blog-llamabot-an-opinionated-pythonic-interface-to-large-language-models",
    "url": "/blog/2023/4/12/llamabot-an-opinionated-pythonic-interface-to-large-language-models/",
    "title": "LLaMaBot: An opinionated, Pythonic interface to Large Language Models",
    "summary": "\ud83e\udd16 I've been playing with Large Language Models and found some common patterns. \ud83e\udde0 So, I created a new package called ! \ud83d\ude0e It lets you make your own Python LLM bot, all with just a few lines of code. \ud83c\udf2c\ufe0f It should make experimenting a breeze! \ud83d\ude80 Are you curious? Read on!",
    "body": "I've been experimenting with Large Language Models recently, and some coding patterns have emerged. The most common patterns I saw were: - Single text in, single text out (STISTO): I prime an LLM with a system message (its task/role), feed in a text, and get back a text response. - ChatBot: I prime an LLM with a system message and expect it to keep chat history alive. - QueryBot: I give it text files and ask questions about them. One thing that was common in my previous experiments with all three LLM APIs was the amount of boilerplate code I had to write before experimenting with prompts. For example, suppose I wanted to \"chat in my Jupyter notebook.\" I'd have to do manual chat history (memory) management in that case. This could have been better. I thus decided to see how close I could get to my perfect developer experience. While chatting with my friend and current manager, Andrew Giessel, we soon realized that a class-based interface may be appropriate here, especially with stateful chat history. So I put that idea into practice with a new package that I'm excited to announce: ! Equipped with an OpenAI API key, the key idea here is that users should be able to instantiate an LLM bot preconditioned with a system prompt and then reuse that base system prompt over and over during a live Python session. A natural structure for this kind of program is to use a Python class. Those who know my penchant for functional programming should know that I did experiment with closures. Nonetheless, I still concluded that a class definition is more natural here. At its core, it looks something like this: The text will be GPT4's imitation of how Richard Feynman might explain that complicated biochemical concept. We can also create an ELI5 bot: Likewise, the text will attempt to explain the difficult biochemical concept as a 5-year-old might understand it. For example, I've seen some pretty good ones about lipid nanoparticles being described as \"bubbles.\" One way to think about a SimpleBot is that it is a Python callable (like a function) that you can configure using natural language. The function body is configured upon instantiation/creation. Post-configuration, upon receiving text as an input, it will treat your wish (configuration) as its command. As a result of this simplicity, it's surprisingly easy to build opinionated bots that are preconditioned to behave a certain way and then wrap them inside a desired endpoint (e.g. a FastAPI endpoint, a command-line interface, a Panel/Streamlit app, or more). For example, if I wanted to make a CLI out of my Feynman bot, I might use: Simple Chat Bots Sometimes, you might need a simple chatbot that you can interact with in a Jupyter notebook. (In this way, you could avoid exposing your potentially sensitive data to OpenAI's data collection efforts.) To implement a proper chatbot, you will need a way to keep track of chat history, at least within a live Python session. With the OpenAI native API, you'll likely need to do this manually. implements convenience machinery to ensure your chat history is stored in memory while a live Python session runs. So you can do things like: While that might look similar to the above, the magical part here is when you call on once again: Unlike SimpleBot, which will not remember previous text entries, Virtual Richard Feynman (or ELI5, or whatever you build...) has memory over what you (the user) put into the function. QueryBot Another common use case for LLMs is to get an LLM to generate texts (summaries, LinkedIn posts, and more) based on one or more documents given to the LLM. To this end, we can use QueryBot. The QueryBot gets instantiated with document(s) to query; those documents are automatically embedded using LlamaIndex using its default . To save on costs, you can embed a collection of documents once (using QueryBot or LlamaIndex), save the index to disk as a JSON file, and then re-load the index into QueryBot directly. It looks something like this: What's coming up next? Another way of asking that question is, \"What does the future hold?\" LLMs are having their moment now. I think can help facilitate experimentation and prototyping by making some repetitive things invisible. As for things that I think could be helpful, sticking to the spirit of \"making repetitive things invisible\", I plan to put in opinionated UI-building functions that let one stand up Panel apps easily. That should enable rapid prototyping in front of end users. For each of the Bots above, adhering to good software development practice, the programmatic interface should effectively define the user interface. Figuring out a sane way to interface with multiple LLMs would be incredible too! I will need time to work on it; however, I would gladly accept a PR that implements the idea while sticking closely to good software organization practices! I'd love for you to check out Llamabot and tell me what is missing and what could be made better!",
    "tags": [
      "llms",
      "large language models",
      "chatbot",
      "llm",
      "fastapi",
      "panel",
      "software development",
      "llamabot"
    ],
    "pub_date": "2023-04-12",
    "type": "blog"
  },
  {
    "id": "blog-how-to-programmatically-download-a-file-from-dropbox-without-the-dropbox-api",
    "url": "/blog/2023/4/11/how-to-programmatically-download-a-file-from-dropbox-without-the-dropbox-api/",
    "title": "How to programmatically download a file from Dropbox using Requests",
    "summary": "Today, I cracked the code on downloading files from Dropbox programmatically! \ud83c\udf89 Using Python and the Requests library, I can now store large data files for my tutorials on Dropbox, making my Jupyter notebooks portable and internet-friendly. \ud83c\udf10 It's like having my own personal CDN! \ud83d\udcbd",
    "body": "Today I learned how to programmatically download a file from a Dropbox public folder. Storing it here so I remember how to do it in the future. I find it's useful to use Dropbox to host large data files for my Python-based tutorials, allowing me to ensure that my Jupyter notebooks are portable from computer to computer, as long as they can maintain an internet connection. This is neat because it allows me to use Dropbox, for which I pay for storage, effectively as a personal CDN (content delivery network) with easily referenced URLs.",
    "tags": [
      "python",
      "dropbox",
      "file download",
      "programming",
      "requests library",
      "data storage",
      "jupyter notebooks",
      "portability",
      "internet connection",
      "content delivery",
      "personal cdn",
      "data files",
      "tutorials",
      "linux",
      "wget"
    ],
    "pub_date": "2023-04-11",
    "type": "blog"
  },
  {
    "id": "blog-mastering-linkedin-connections-avoid-mistakes-and-boost-your-professional-network",
    "url": "/blog/2023/4/2/mastering-linkedin-connections-avoid-mistakes-and-boost-your-professional-network/",
    "title": "Mastering LinkedIn Connections: Avoid Mistakes & Boost Your Professional Network",
    "summary": "Are you looking to improve your LinkedIn game? \ud83d\ude80 In this blog post, I explore the best practices and common mistakes when connecting with others on LinkedIn. Learn how to create quality connections, avoid cold connecting without a note, and ensure your profile is up-to-date! \ud83d\udcbc I also share examples of great LinkedIn messages and resources to help you connect like a pro. Don't miss out on these valuable tips to enhance your professional network! \ud83c\udf10\ud83e\udd1d (Summary was written by GPT-4 and edited by me.)",
    "body": "I've been on LinkedIn for a while, and many people have reached out to connect with me professionally. In this blog post, I suggest practices and recommend resources to better connect with others on LinkedIn and explore why they matter. What are we trying to accomplish on LinkedIn? The purpose of LinkedIn is to serve as a professional social network with quality connections. Quality connections imply being able to: 1. Receive content and information relevant to their profession through weak contacts, 2. Associate with like-minded individuals, 3. Reach out to their network for professional assistance when needed. 4. Advertise personal skills and capabilities to potential employers. It's safe to assume that most LinkedIn users will have one or more of these goals. As users of LinkedIn, the question, then, is this: why would anyone else want to be connected or associated with me? LinkedIn has a 30,000 connection limit. Most of us are never going to get close to there, but LinkedIn also suggests one other thing: to \"only keep quality connections in your network.\" The value of a social network comes from sparsity, which is the result of active curation. When requesting to connect with others, we need to take the initiative to let them know why we would be a valuable connection for them. In other words, how would you add value to them? Common mistakes when connecting on LinkedIn With this lens, how to remedy the following mistakes when connecting on LinkedIn becomes clear. Mistake 1: Cold connecting without a note The first mistake is this: having never met the other party in person, you click that \"Connect\" button without leaving a note for them. Doing so leaves the other party guessing, \"How will connecting with this person be a quality connection for me?\" Any guesswork that we can take out is going to be better. Mistake 2: Connecting with just a generic note This mistake is related to the first one. A generic note without substance still does not let the other person know why you would be valuable to them. Mistake 3: Making an ask of the other party when connecting This mistake ignores the value exchange that takes place in professional settings. For example, asking someone to do something for you without offering something back is an unfavourable value exchange for the other party. Keep in mind the value of the other person's time. For example, a junior data scientist is paid \\$150,000 per year. Assuming an 8-hour workday and 250 days per year, that professional's effective hourly rate is approximately \\$75/hr. More senior people would have a higher effective hourly rate. One should always be mindful of the other party's time whenever one makes an ask outside of work. I've received this ask multiple times in various forms: \"please let me know of any opportunities.\" Big no-no! That's like asking me to do the job search for you! This is a terrible phrase to use in a cold connection/message. Simply doing a cold-hearted calculation on the value of my time makes it hard to justify why I should take the initiative to look at your profile when there are other, higher-value things I could be doing with my time. So instead, ask if you could chat about specific roles or if the other party might know the hiring manager and request to make an introduction to them. Mistake 4: Not having a fleshed-out LinkedIn profile One of the ways we determine whether someone would be a valuable connection for us is by seeing their LinkedIn profile. Therefore, keeping your LinkedIn profile updated - roughly the same way you would keep your resume updated - is essential to communicating your potential value to someone else. The Remedy: Exchange Value! As with all things professional, making a professional connection means asking, \"What fair value can we exchange here?\" But what would be of value to someone on the other side? Especially if you're a junior person just starting? Let's think about what kind of needs others may have professionally. They may be seeking cutting-edge knowledge By definition, industry professionals' knowledge will become outdated 2-3 years into their careers. The demands of professional and personal life mean they will likely only have the time to keep up with the latest knowledge if they were involved in producing it in their roles. Almost by definition, the new \"generation\" of graduates will most likely be equipped with the practical know-how to wield the latest knowledge productively - if they are keeping up with these trends. (Pro tip: this is a great way to stay differentiated amongst new graduates!) They may be seeking talented individuals to join their team Industry professionals who have years of experience under their belt may be looking to connect with individuals whose skills match up with their business needs. Doing so can reduce their burden when doing hiring -- which is a very, very time-consuming thing to do! In addition, if your LinkedIn profile adequately shows off your skills, they can verify th",
    "tags": [
      "linkedin",
      "professional networking",
      "connection mistakes",
      "profile optimization",
      "value exchange",
      "mentorship",
      "career development",
      "personalized messages",
      "networking tips"
    ],
    "pub_date": "2023-04-02",
    "type": "blog"
  },
  {
    "id": "blog-a-developer-first-guide-to-llm-apis-march-2023",
    "url": "/blog/2023/3/29/a-developer-first-guide-to-llm-apis-march-2023/",
    "title": "A Developer-First Guide to LLM APIs (March 2023)",
    "summary": "I built one application using three different GPT libraries \ud83d\udcda, and figured out that it can give me a 100X ROI! \ud83d\udcb0 In this blog post, I compare their outputs, developer experiences, and \u23f0 discuss how future application developments may look. Curious about the future of GPT and its value proposition? \ud83d\ude80 (The summary and title were generated by GPT-4 and edited by me.)",
    "body": "Large Language Models (LLMs) are having a moment now! We can interact with them programmatically in three ways: OpenAI's official API, LangChain's abstractions, and LlamaIndex. How do we choose among the three? I'd like to use a minimally complex example to showcase how we might make this decision. The Problem I blog. (Ok, well, that's quite the understatement, given that you're reading it right now.) As you probably can tell from my blog's design, I need to summarize the blog post for the blog landing page. Because I usually share the post on LinkedIn, I also need to generate a LinkedIn post advertising the blog post. I've heard that emojis are great when sharing posts on LinkedIn, but I can only sometimes remember the names of appropriate emojis. Some automation help would be great here. Finally, if I can make a better title than I initially thought of, that would also constitute an improvement. I've decided that I wanted a tool that can summarize my blog posts, generate appropriately emojified LinkedIn posts for me to advertise those posts, and provide a catchy and engaging title without being clickbait. These are tedious to write! Wouldn't it be great to have a tool that can generate both? That's the use case for LLMs! Desiderata Here's the list of requirements I have to build the tool I want. Firstly, I should be able to provide the text of a blog post as input and get back: 1. A proposed title according to my desired specifications, 2. A summary to put onto my website's blog listing page, and 3. A LinkedIn post that I can use to share with others. Secondly, the tool should minimize token usage. Tokens equal money spent on this task, so saving on token usage would increase my leverage trading time for money. Finally, because I'm still writing some code to implement this tool, I'd like to use a package that would provide the most easily maintained code. Of course, that means a subjective judgment of how simple the abstractions are. To that end, I will show how to implement this writing tool using three APIs currently available: the official OpenAI Python API, the LangChain API, and the LlamaIndex API. This exercise lets us see which ones are most suitable for this particular use case. We will be using GPT-4 everywhere, as its quality is known to be superior to GPT-3.5. Finally, I will focus on blog post summary generation and LinkedIn post generation when comparing the APIs before choosing one framework to implement the complete program. The test blog post The blog post I'll use for implementing this is my most recent one on the Arc browser. I will be passing in the Lektor raw source without the title. The raw source is available on my website repository. According to [](https://github.com/openai/tiktoken), this text uses 1436 tokens to encode: This is a constant throughout the post. Prompts The prompts that I will use to generate the desired text are as follows. If the three prompts are too long to remember, you can focus on the one for summarization as an anchoring example. Summary LinkedIn Post Prompt Title Prompt Setup API key Using OpenAI\u2019s API requires we set the OpenAI API key before doing anything; this is also true for using LangChain and LlamaIndex. As always, to adhere to reasonable security practices when developing locally, we should store the API key as an environment variable in a file listed in the of a repository and then load the API key in our Jupyter notebooks. Here is how we implement it. Firstly, the file: And then, at the top of our Jupyter notebook or Python script: Implementation using OpenAI's API Summarization Let\u2019s start off using the OpenAI Python API. To use GPT-4, we need to provide a chat history of messages that looks something like this: Then, we pass the message history into the OpenAI chat completion class Once the API call has returned, we can see what gets returned: The total number of tokens used here is: LinkedIn Post Now, let's make the LinkedIn post. The total number of tokens used here is: Cost Accounting Tabulating the total cost of tokens, we have 3c per 1,000 tokens for prompts and 6c per 1,000 token for generated texts. We can do the math easily here: Or about 12c to perform this operation. How much ROI do we get here? Excluding benefits, equity, and more, a new Ph.D. grad data scientist is paid about (give or take) per year in the biomedical industry in 2023. Assuming about 250 days of work per year at an average of 8 hours per day, we're talking about an hourly rate of /hr at that salary. If it takes that person 10 minutes to cook up a summary and LinkedIn post (which is about how long I take - excluding figuring out what emojis to put in because that's, for me, bloody time-consuming.), then we're looking at worth of time put into crafting that post. Looking at just the cold hard numbers, we're looking at a 100X cost improvement by using GPT-4 as a writing aid. Implementation using LangChain's API Now, let's do the same thing, except with the LangChain AP",
    "tags": [
      "llms",
      "large language models",
      "gpt",
      "gpt4",
      "openai",
      "langchain",
      "llama_index"
    ],
    "pub_date": "2023-03-29",
    "type": "blog"
  },
  {
    "id": "blog-arc-browser-first-impressions",
    "url": "/blog/2023/3/25/arc-browser-first-impressions/",
    "title": "Arc Browser: First Impressions",
    "summary": "Just tried out the Arc browser and I'm loving it! \ud83d\ude0d It's a game-changer for tab management and focus. With features like expiring tabs, tab spaces, and a side-by-side view, it's like having a personalized workspace right in my browser. \ud83d\ude80 Even has a developer mode for locally hosted sites. Can't wait to see where they take this!",
    "body": "I recently received my invite to use the Arc browser! Arc reimagines the browser as a workspace. Given how much of our work is done in the browser, this makes sense as an idea. As modern people, we also juggle multiple hats. As a result, our browser can become unwieldy by having numerous tabs from various contexts open simultaneously. Arc rethinks the traditional browser UI, and after using it for 24 hours, I like how it fits my brain. Feature 1: Tabs that expire In a traditional browser implementation, we have the browser and its tabs. But, for the chronic \"million-tabbers\" out there, finding that thing you were looking at can become a chore. That's where Arc's first feature comes in: tabs that expire. Arc's take on tabs is that most tabs should expire while some should be kept around. That's where they distinguish between pinned tabs and tabs that can expire. Chrome has pinned tabs for a while now, but Chrome doesn't enforce tabs to expire after a set number of hours. On the other hand, knowing that most tabs don't need to live around for too long, Arc defaults to opening tabs with a default 12-hour expiry schedule. Users can then drag the ones they want to keep into the pinned section. An example of where this came in handy is my drafting this blog post. I pinned the main Lektor UI but allowed my other tabs, where I'm doing the fact-checking, to expire after I'm done. This is a minor detail in the grand scheme but has incredibly high leverage for user productivity. Allowing browser tabs to expire helps clarify my browser (workspace). Feature 2: Spaces Another take Arc has on tabs is being able to group them into spaces. The modern person takes on multiple life projects at the same time. For example, I have projects that are ongoing in parallel: a home renovation, personal GPT experiments, blogging, and more. If we're not careful, we'll have a million tabs open in our browser window. To solve this problem, Chrome has implemented tab groups. It can work well until... (Image credit: this website) At that point, who's to know what is in each tab? Arc solves this problem by doing two things. Firstly, tabs are moved to the left. This lets each tab's title be much more visible. Then, if necessary, we can resize the tab to provide more browser real estate space. Secondly, tabs are grouped into spaces. Each space can correspond to one context. For example, I can have one space for websites related to home renovation and another for blogging. The critical thing is this: when I have activated one space, tabs from the other spaces are not visible. This means I no longer have the visual distraction of other things going on. This massive UI improvement supports focusing on one context at a time! Here are screenshots of a few tabs, organized into their respective spaces, that I have kept open. Bonus: each space can even be configured with different colours! As you can see, within one space, I only see the tabs relevant to that space. As long as I'm studious about making sure I only open tabs related to that space, I can focus on that task alone. Websites like YouTube, LinkedIn, and Twitter are kept together in a \"Social and Entertainment\" space, making them out of sight and out of mind. If I ever need to, there is also a UI for viewing all the spaces I have open: The best part here is that if I close the Arc browser, the tabs and spaces are restored, allowing me to switch right into the given context that I need at that moment. Feature 3: Rapid switching between tabbed and full-screen mode The left sidebar may bug some people, so hiding it with a single key combination is possible. On the Mac, it is and is configurable to be anything I want. Here's an example of Twitter in full-screen mode in my \"Social and Entertainment\" space: There are a few subtle design details with high leverage here. First, I remain aware that I'm in the \"Social and Entertainment\" space because of the yellow background. The second is that I can be completely immersed in whatever I'm doing on the internet at that point. No distractions are visible - not even the other open tabs in the same space. Perfect for focusing. Feature 4: Side-by-side view Continuing the theme of productive workspaces, I can place two tabs side-by-side like this: Say, for example, that I was writing a blog post (ahem) and needed to use Grammarly to edit my text. I could have my blog editor on the left and Grammarly on the right and easily toggle back and forth between the two tabs. Or if I needed to refer to reference material while writing, I could do a left-right combination. Without Arc, this is still doable - use something like Rectangle or Magnet to keep two browser windows open. However, we lose the benefits Arc's ability to keep related browser tabs together and in focus without distractions. Feature 5: Automatic developer mode for locally hosted sites When developing locally, Arc will automatically show a site in developer mode. In this mode, the top of the browser",
    "tags": [
      "tools",
      "productivity",
      "arc browser",
      "browser",
      "browser review",
      "ui design",
      "user experience",
      "tab management",
      "workspace",
      "focus",
      "internet browsing",
      "web development",
      "browser features",
      "tech review",
      "web tools",
      "online workspace",
      "internet tools"
    ],
    "pub_date": "2023-03-25",
    "type": "blog"
  },
  {
    "id": "blog-okrs-are-a-pacing-tool-not-a-contract",
    "url": "/blog/2023/3/12/okrs-are-a-pacing-tool-not-a-contract/",
    "title": "OKRs are a pacing tool, not a contract",
    "summary": "How can a 15th-century explorer teach us about modern goal-setting strategies? In this blog post, I explore Andy Grove's part-historical, part-fictional take on Christopher Columbus in his quest to find a trade route to the Far East. By looking at Columbus' story through the lens of Grove's book \"High Output Management,\" we can learn that OKRs are not a contract but something more useful... Come check out what I learned recently while listening to High Output Management!",
    "body": "Objectives and key results (OKR, alternatively OKRs) is a goal-setting framework used by individuals, teams, and organizations to define measurable goals and track their outcomes. The development of OKR is generally attributed to Andrew Grove who introduced the approach to Intel in the 1970s. John Doerr published an OKR book which is called Measure What Matters: How Google, Bono, and the Gates Foundation Rock the World with OKRs in 2017. - Wikipedia, accessed 12 March 2023 Popularized by Google, I learned about an earlier iteration of OKRs through Andy Grove's book, \"High Output Management.\" Here, Grove uses Columbus' discovery of the new world to describe his take on OKRs. In his retelling, the Spanish Queen aimed to increase funds for the war. Columbus aimed to find a trade route to the Far East to support the broader goal. Columbus likely had a version of OKRs suited for the medieval context. If so, he could use these to pace himself and his team on the key results. He and his team would have known how their objectives fit within the bigger picture of the Spanish monarchy's objectives. Columbus likely satisfied all his key results! But, as history tells it, he would fail spectacularly in his objective of opening a trade route to the Far East. Nonetheless, Columbus' arrival in the New World brought Spain immense wealth (albeit with destructive consequences for natives). From Spain's perspective, Columbus nevertheless helped achieve Spain's broader goals. Let's say Columbus was evaluated on his objectives stated at the outset. If so, we would rate him as a failure. And that would have been an inane conclusion to Columbus' career, given that he pivoted and helped deliver on broader objectives. The lesson from Grove's semi-historical parable is that OKRs are not a contract. Instead, they're a tool to pace ourselves. Unfortunately, in corporate settings, OKRs can get abused as a contract by management or individual contributors. If treated that way, we will lose the ability to pivot fearlessly when necessary, thus missing out on valuable opportunities to deliver immense value. That is the kind of high leverage we always need to be on the lookout for!",
    "tags": [
      "management",
      "performance management",
      "okrs",
      "objectives",
      "key results"
    ],
    "pub_date": "2023-03-12",
    "type": "blog"
  },
  {
    "id": "blog-how-to-automate-the-creation-of-google-docs-with-python",
    "url": "/blog/2023/3/8/how-to-automate-the-creation-of-google-docs-with-python/",
    "title": "How to automate the creation of Google Docs with Python",
    "summary": "I've just automated the creation of Google Docs using Python! \ud83d\udc0d I used a Google service account to authenticate, then created the doc content using Markdown. I converted the Markdown to HTML, then uploaded it as a Google Doc. I also learned how to set multi-line environment variables and programmatically set document permissions. It was a fun and enlightening project! \ud83c\udf89",
    "body": "The backdrop At our church, I teach a Sunday school class about how to lead a Bible study. As part of the training, we ask our class to collaboratively create study notes and study questions for an assigned scripture each week. Previously, I would hold a template in Google Drive, one for each of the four weeks of class, then copy and paste them into another folder in Google docs and then share the link with others in the class. I would use the most permissive setting when sharing - anyone with the link could edit. (Doing so mainly was for simplicity.) Then, I would compose an email and send it to everyone in the class. The problem As it turns out, this workflow was a hassle in several ways. Firstly, because the template looked identical to the copied doc, I would have to double/triple check that I didn't accidentally send a link to the template rather than a copy. Checking that I had the correct doc then was tedious. It involved clicking around on the screen, checking permissions, and checking filenames... and even then, sometimes, I would still send out a link to the template. Secondly, the docs were quite similar, differing only in scripture references and links to other resources. If I wanted to update the standardized formatting or text in one doc, I would have to copy it seven more times into the other docs. If I forgot to do so, that would lead to a drift between the docs. Also undesirable. Thirdly, I would compose a weekly email, which was also relatively standardized. However, copying and pasting the text felt tedious. Writing from scratch in Chinese, my second language, was sometimes a mental challenge -- compounded even more so with two kids who fragment my attention span. The solution: Python! Thinking hard about the problems above, I noticed a common thread: it was all about putting standardized text into the right places. That sounded like a problem I could use Python to solve -- and that was precisely what I did! My desired end goal for this project was to write a Python program that could: 1. Create Google docs for me, 2. Grab the docs' URLs automatically, and 3. Insert them into an HTML email that the program composed. Step 1: Create a Google service account To create Google docs with Python, one needs a Google account, specifically, a service account that can remain authenticated without needing a human to sign in. To do this, we need to head over to our Google cloud console. There, we'll do the following things: 1. Create a new \"project.\" Google keeps track of which service accounts are associated with which user by having a \"project\" between them. 2. In the new project, enable the Google Drive API. This allows the project to access Google Drive. 3. From the project's UI, we'll create a new Service Account and grant the Service Account the ability to access project APIs. 4. For that service account, we'll then create a key credential file that will be used to authenticate the service account. I compiled a series of screenshots into an HTML slideshow below. Following the instructions on the slideshow while replacing \"Test Project\" with a more relevant name will get you to the place where you will have a JSON file that will act as the credentials to authenticate with Google. (Best viewed on a desktop browser!) Step 2: Store the JSON file contents as environment variables When working with a deployed API, application, or other services, I habitually stick to the 12 Factor App patterns. These are battle-tested patterns that enable secure app/service deployments. The third principle listed in the 12 Factor document is to store configuration in the environment as environment variables. This includes credentials (a.k.a. \"secrets\" in techno-lingo). However, Google just provided me with a JSON file for authentication; the usual expectation here would be to store the JSON file on disk in a remote server, for which a Python process can read the JSON file and pass it to Google's Python API client. However, this clearly violates the idea of storing configuration as environment variables. So how can we get around this? My answer is to store the credential file's information as part of the runtime environment variables available to the app. Looking at the contents of the JSON file: It's clear that there are specific key-value pairs uniquely identify the project and its security credentials. We merely have to store those values as environment variables. How about we go about doing authentication? I had a choice between two options: one was to use the Official Google Python API, and the other was to use the PyDrive2 package. As it turns out, authentication from a Python package is only part of the whole picture; I also needed to know which Python package was, overall, easier to use. So, I had to study them first. That meant taking them for a test drive in a Jupyter notebook. After a bunch of experimentation, I concluded that although the Google Python API is officially supported, it also involved abstracti",
    "tags": [
      "python",
      "google docs",
      "automation",
      "markdown",
      "html",
      "programming",
      "api",
      "google drive",
      "google cloud",
      "service account",
      "environment variables",
      "mimetype",
      "permissions",
      "templating",
      "jinja"
    ],
    "pub_date": "2023-03-08",
    "type": "blog"
  },
  {
    "id": "blog-building-a-gpt3-based-translation-app",
    "url": "/blog/2023/2/5/building-a-gpt3-based-translation-app/",
    "title": "Building a Translation App with GPT-3: The Story Behind My Creation",
    "summary": "I built a translator using GPT3 for Ark Channel, a daily devotional written in Chinese. The translator parses the text into structured data, translates the devotional and additional texts, and generates cover images. I also built a UI using Panel and deployed it on a Dokku server on DigitalOcean. The result? My translation time went from 10-15 minutes to just 2! \ud83d\ude80",
    "body": "Since 2011, I have been doing Chinese-to-English translation for [Ark Channel][arkchannel]. For as long as I can remember, I have used Google Translate to assist me in translation because Chinese is my 2nd language (though in Singapore, I learned it as a 1st language subject). So, with GPT3's capabilities at doing translation, I wanted to test drive its abilities to do machine translation. I have not been disappointed! [arkchannel]: http://www.arkchannel.org/en/ What's Ark Channel? Ark Channel is written in Hong Kong by one person, Sam Kong. He started broadcasting short messages of Biblical encouragement in response to seeing suicide rates skyrocket in Hong Kong. Sam is not a computer person, but he writes each edition of Ark Channel in a relatively structured way. From Monday through Saturday, Ark Channel's edition looks something like this: And on Sunday, it looks like this: Can GPT3 alone do the translation? At first, I thought I would get GPT3 to do translation by using the following prompt, written inside a Python function: However, this yielded unsatisfactory results. Here is one example translation for the January 14 entry: At first glance, the translation is okay! The devotional text on the 3rd line is reasonably accurate. The additional text at the end is also translated well. There are other issues, though, that we could further tackle. Here they are. 1. Per our usual practice, should be translated into . 2. is mistranslated as . It should be or . Both are acceptable without contextual information; sometimes, we split a chapter into 3 () rather than 2 (). 3. Additionally, the quoted verse on the 2nd line might need to be corrected and verified. 4. Translating the entire text uses up a lot of tokens. To solve point 3, Ark Channel historically uses the NKJV version. However, I have sometimes used the NIV version to translate (primarily out of habit). If we can extract the Bible book, chapter, and verse number from the free text, we might be able to hit an API to automatically get an authoritative translation rather than use GPT3 to do an approximately correct translation. To solve point 4, if we could translate just the devotional text and additional text fields while using dictionary lookups for the rest, we could save a lot of OpenAI API calls. Structuring the translation task into sub-tasks So I decided to split the task into two pieces: 1. The first is to extract the information based on a template I provide and return it as a JSON object that I would read into Python as a dictionary. 2. The second would take just the parsed devotional and additional texts and do a GPT3-based translation for me. Extract information into a JSON object To make point 1 happen, I needed to provide a template inside a prompt. Templating out the Ark Channel devotionals, they look like this: It is relatively structured but definitely not easily machine parseable with Python code. Furthermore, if I had to code up a parser, I would have to write lots of code for many edge cases, which would be undesirable. Here are some examples of the issues I encountered, each of which would introduce code path forking: 1. The date may sometimes be written as or . 2. Only on Sundays will the 'day of week' field be written as \"Sunday Hope\"; on other days, it is in Chinese. 3. There is sometimes an alphanumeric letter at the end of the chapter number to signify that week's edition covers part of a chapter rather than the entire chapter. 4. The devotional text sometimes includes multiple line breaks with bullet points or numbered lists, and at other times is one single line. 5. The additional text is sometimes multiple lines long and sometimes a single line. So, I experimented a bit with the API and made a prompt that returns a structured dictionary with fields correctly populated: As mentioned above, this prompt performs relatively consistently against the test cases I designed. However, if there is a mild change in formatting, I will need to adjust the prompt. Additionally, the part that needed the most experimentation was the statements. Before including the statements, GPT3 would return a JSON string that missed out on keys that needed to be present. However, GPT3 began reliably covering all of the necessary keys in the dictionary after including the' Ensure' statements. Finally, there were places in Ark Channel where quotation mark usage needed to be more consistent. Sometimes, quotation marks were curly; at other times, they were straight. The parentheses were also sometimes the fixed-width Chinese version; at different times, they were the English font version instead. With the prompt above, I was able to control the usage of quotation marks and parentheses in the translated texts. As I'll discuss later, GPT3 became (for me) a great Natural Language API. That was pretty incredible. With this function, the original text above is parsed as: This is a great start! With it in hand, I could see a path forward to composing the English t",
    "tags": [
      "gpt3",
      "openai",
      "python",
      "blogging",
      "natural language processing",
      "machine learning",
      "artificial intelligence",
      "data science",
      "programming",
      "api",
      "dokku",
      "digitalocean",
      "deployment",
      "web app",
      "large language models",
      "translation",
      "faith",
      "christianity",
      "bible"
    ],
    "pub_date": "2023-02-05",
    "type": "blog"
  },
  {
    "id": "blog-twitter-threads-about-generative-ai-business-ideas",
    "url": "/blog/2023/1/7/twitter-threads-about-generative-ai-business-ideas/",
    "title": "Twitter Threads about Generative AI Business Ideas",
    "summary": "I discuss two Twitter threads that outline potential business ideas that could be built on top of ChatGPT3, a chatbot-based language model. What's the tl;dr? As mankind has done over and over, we build machines to solve mundane and repetitive tasks, and ChatGPT3 and other generative models are no exception!",
    "body": "I've seen many Twitter threads outline potential business ideas one could build using ChatGPT3. Here are two that came up on my timeline. - One by Alex Banks [here][alex], and - One by Ben Tossell [here][ben]. [alex]: https://twitter.com/thealexbanks/status/1610996706887467009 [ben]: https://twitter.com/bentossell/status/1598694052144320516 In summary, the ideas are: - Generating music. - Generating fitness plans. - Copywriting and translation. Here's what I see as being common between the ideas in those two threads: 1. The tasks aren't doable by simple, structured information storage and retrieval. 2. They involve either the synthesis or summarization of existing knowledge. 3. As I think through what else needs to be done in each context, it's clear that they all need expertise or domain knowledge to evaluate the correctness of the generated text. As Alex Banks writes at the end of his thread, My advice: don't force yourself to come up with the next big startup idea. Instead, keep your eyes and mind open to the problems and frustrations you encounter in your daily life. These are the seeds of potential startups, waiting to be nurtured. Solving mundane and repetitive tasks has always been the value proposition of computation. Our definitions of \"mundane\" and \"repetitive\" will evolve; humans will continue to build machines to free us up to pursue deeper creativity. ChatGPT3, and other generative models, are no exception!",
    "tags": [
      "generative ai",
      "twitter",
      "til"
    ],
    "pub_date": "2023-01-07",
    "type": "blog"
  },
  {
    "id": "blog-why-skilled-practitioners-will-benefit-most-from-ai-coding-and-writing-tools",
    "url": "/blog/2022/12/8/why-skilled-practitioners-will-benefit-most-from-ai-coding-and-writing-tools/",
    "title": "Skilled practitioners will benefit most from AI coding and writing tools",
    "summary": "I gave AI coding tools like GitHub Copilot a try and found them subtly valuable. They're not replacements but augmentations for skilled practitioners who can verify their output. Incorrect outputs can spark creative solutions, making these tools powerful. So, no, AI won't take our jobs, but will supercharge our skills. \ud83d\ude80\ud83d\udc69\u200d\ud83d\udcbb\ud83e\udd16",
    "body": "If you're still on Twitter, you've probably seen the wave of ChatGPT3 tweets. There are rave reviews and there's also a lot of skepticism. I was skeptical, at first, but I decided to give these AI coding tools a try. I happen to be in the privileged position of being an open-source maintainer, so I have access to GitHub Copilot for free. (It costs $10/month otherwise.) In VSCode, which is my daily driver text editor, I installed the Copilot extension, turned it on for Markdown, Python, and Plain Text files, and started typing away. You get suggestions as you type, and you can accept them by pressing . My conclusion? I'm not blown away, but neither am I disappointed. Rather, I'm starting to realize the subtle value in using these tools, and who would be the best target audience for them. The value of AI coding tools Firstly, I think it's important to note that these tools are not meant to replace human coders and writers. Rather, their best use is in augmenting human capabilities. Moreover, I'd argue that the best use of these tools is in the hands of people who have already accumulated sufficient experience. In other words, skilled practitioners. By skilled practitioners, I mean people who possess the ability of verification. They should be able to verify the output of AI tools for correctness. Without this ability, AI coding assistants may give us incorrect code, or in the case of writing prose, factually incorrect statements. Yet at the same time, I think there is value to the incorrectness. I saw one [LinkedIn post by Cassie Kozyrkov][linkedin] that eloquently expressed where that value lies: [linkedin]: https://www.linkedin.com/feed/update/urn:li:activity:7006408044595040256/ But here's the catch - ChatGPT's responses only touch reality at a tangent. While they may sound convincing, they are ultimately fictional creations of the GAN. This might sound like a drawback, but it actually makes ChatGPT incredibly useful. Because it isn't tied to the constraints of reality, ChatGPT can engage in completely imaginary conversations and provide creative, out-of-the-box responses. For example, you could ask ChatGPT what it would do if it could fly, and it might respond with something like \"I would soar through the skies like a majestic eagle, feeling the wind beneath my wings and the freedom of flight.\" This type of response would be impossible for a human to come up with, but it's perfectly within the realm of possibility for ChatGPT. So, why use ChatGPT if its responses only touch reality at a tangent? Because sometimes, it's exactly this type of creative, imaginative thinking that we need to solve complex problems and generate new ideas. ChatGPT allows us to explore possibilities that are beyond the constraints of our everyday reality, and that can be incredibly powerful. The internet community has a wonderful property: state something incorrectly, and someone will correct you. Outside of bad-faith actors, it is usually a skilled practitioner will be the one who will do so. They can do so because they possess the relevant background knowledge and analytical and research skills to evaluate incorrect statements and produce a correction. This skill will go a long way in establishing trust with work products that end up in the hands of other humans. Likewise, with reality-tangential (or even incorrect) responses, it is the skilled practitioner who will be able to verify the correctness of the response. AI coding skills won't take away the need for skilled practitioners; rather, I predict that it is skilled practitioners who will find their skills most supercharged, and it will be the skilled practitioners who will become the necessarily trusted partners in a world of automated generation of incorrectness. The corollary? Never before has it been more important to become a skilled practitioner and to build trust with other skilled practitioners. Will AI take our jobs? I don't think so. But I do think that AI will augment our skills, and make us more productive. For anyone willing to put in the time and effort to become a skilled practitioner, I can only see these developments in a positive light!",
    "tags": [
      "ai",
      "artificial intelligence",
      "augmentation",
      "chatgpt",
      "coding",
      "copilot",
      "github",
      "gpt3",
      "markdown",
      "opensource",
      "plaintext",
      "productivity",
      "python",
      "skilled practitioners",
      "trust",
      "verification",
      "vscode"
    ],
    "pub_date": "2022-12-08",
    "type": "blog"
  },
  {
    "id": "blog-how-to-pick-a-mastodon-instance",
    "url": "/blog/2022/11/6/how-to-pick-a-mastodon-instance/",
    "title": "How to pick a Mastodon instance?",
    "summary": "With the Twitter-to-Mastodon migration ramping up, I've received questions on which Mastodon instance to go to. Here's my answer!",
    "body": "One question I've gotten is which Mastodon instance should I go for? Answer that question is actually anyone of them. This is OK because of the premise of Mastodon being active curation instead of algorithmic recommendations. On Twitter, content gets recommended to you. On Mastodon, you proactively seek out the people you want to follow, and their content is shown on your timeline. Therefore, the first-order answer to the question, \"which Mastodon instance should I sign up for?\" is \"any one of them!\" What I just mentioned implies that there's a second-order answer. A Mastodon instance is usually a self-organizing community of people with shared interests. Therefore, joining certain instances makes it easier to discover birds of the same feather - those you would find interesting to follow. Two examples of those communities are the deep/machine learning (sigmoid.social) or biotechnology communities, two communities I would like to follow. So how do we discover these communities? Thus far, I've allowed serendipity to help me discover these communities. This happens by simply paying attention to people's Twitter profiles as the mastodon migration happens, or social links in their personal websites and blogs. Naturally, this is more time consuming and requires more effort. Still, we get a much better curated and interesting timeline in exchange.",
    "tags": [
      "social media",
      "mastodon",
      "twitter"
    ],
    "pub_date": "2022-11-06",
    "type": "blog"
  },
  {
    "id": "blog-writing-project-documentation-helps-with-future-presentations",
    "url": "/blog/2022/9/28/writing-project-documentation-helps-with-future-presentations/",
    "title": "Writing project documentation helps with future presentations",
    "summary": "Just had an insightful 1:1 with a teammate. We discovered how writing project documentation helps with presentations! \ud83d\udcdd It clarifies our mental knowledge graph, gives us practice in creating narratives, and provides a 'hard copy' of project elements for future use. \ud83e\udde0\ud83d\udca1",
    "body": "In a 1:1 session with one of my teammates, I gave feedback on a presentation she had just delivered the hour before. One really cool lesson we learned from this experience was how writing project documentation helped a lot with preparing a presentation. We contrasted two presentations that she had delivered in the past. One was on a project for which she had written a lot of documentation as part of our broader DSAI team's quarterly docathons. The other was on a project where the project execution was more ad hoc, and hence we had not had the opportunity to write stable documentation. With the first project, the presentation was much smoother, and that came from the prior practice of linearizing her internal knowledge graph of the project into a written piece of documentation. Also, because the elements (nodes) of that knowledge graph were clarified while writing, it turns out that she could easily compose those elements into alternative narratives for multiple types of audiences. With the second, the presentation delivery had more challenges, and we attributed that to the narrative being less structured, logical, and linear, and that this was the result of having less clarity on the elements of the knowledge graph for that project. On thinking about this more, it's clear to me that writing documentation gives us: 1. A reason to clarify the elements of our mental knowledge graph of a project, 1. A chance to practice linearizing that knowledge graph into one or more narratives, 2. A 'hard copy' record of those elements from which we can draw in future presentations. This was a fantastic lesson to learn!",
    "tags": [
      "audience",
      "career",
      "career advice",
      "clarity",
      "communication",
      "documentation",
      "feedback",
      "knowledge graph",
      "learning",
      "lessons",
      "logic",
      "narrative",
      "practice",
      "presentation",
      "project management",
      "structure",
      "teamwork"
    ],
    "pub_date": "2022-09-28",
    "type": "blog"
  },
  {
    "id": "blog-coding-on-an-ipad-with-codespaces-and-blink",
    "url": "/blog/2022/9/16/coding-on-an-ipad-with-codespaces-and-blink/",
    "title": "Coding on an iPad with Codespaces and Blink",
    "summary": "After many attempts, I've found a solution to code on my iPad using VSCode in Blink! All my settings sync perfectly, and I can even run Jupyter notebooks. It's like having my MacBook Air, but lighter. \ud83d\udcf1\ud83d\udcbb\ud83d\ude80",
    "body": "After many attempts at doing coding on an iPad, I finally have hit a solution that works well for me. I'm a VSCode user. I enjoy using VSCode because of the many extensions that make my work productive. While I enjoy using my MacBook Air, I've sometimes wanted to lug around nothing more than my iPad and its keyboard to do my blogging, coding, and other work. I've tried many different solutions to do coding on an iPad. For example, I test-drove GitHub's Codespaces in the built-in Safari browser, but the experience was a little less than ideal. I'd frequently find myself unable to scroll in the Jupyter notebook interface, or the notebook would never load. Then, I tried using plain text editors, such as a combination of and with syntax highlighting, but I found that I was missing the ability to execute code in a notebook. One day, however, I stumbled upon VSCode in Blink. I was trying to open up my personal website's Codespace in Safari, but instead, iOS opened up Blink. And there it was: Visual Studio Code working on my iPad! At first, I thought I was still in Safari, and tried to switch to a different tab - only for the action to not work. Only later did I realize I was in Blink! One thing notable is that all of my settings are synchronized correctly, including my theme configurations, extensions, and keybindings! Trackpad usage allows me to operate as if I were on my laptop. In other repos where I have Jupyter notebooks, I can open them up and run code in them as well. Even the built-in terminal works too! This was pretty amazing to see in action. Read more here: - VSCode in Blink on YouTube - Blink Shell - Docs for using VSCode in Blink",
    "tags": [
      "blink",
      "codespaces",
      "coding",
      "data science",
      "extensions",
      "github",
      "ipad",
      "jupyter notebook",
      "keybindings",
      "macbook air",
      "mobile",
      "nano",
      "productivity",
      "syntax highlighting",
      "terminal",
      "tmux",
      "vscode"
    ],
    "pub_date": "2022-09-16",
    "type": "blog"
  },
  {
    "id": "blog-how-to-solve-raspberry-pi-failing-to-update-because-of-repository-suite-value",
    "url": "/blog/2022/8/20/how-to-solve-raspberry-pi-failing-to-update-because-of-repository-suite-value/",
    "title": "How to solve raspberry pi failing to update because of repository suite value",
    "summary": "Today I learned how to solve yet another fairly esoteric issue with .",
    "body": "Today I learned a new thing trying to install Tailscale on the Raspberry Pi machine responsible for running Pihole. Part of the install script involves doing an . On running the command, I would get: After a bit of digging, I saw a blog post on kraaima.com that describes the solution: This will bypass the error message listed above. After that, we do an upgrade: After this, updates & upgrades should resume regular working operation.",
    "tags": [
      "raspberry pi",
      "linux",
      "til"
    ],
    "pub_date": "2022-08-20",
    "type": "blog"
  },
  {
    "id": "blog-practical-tips-for-staying-focused-at-work-as-a-data-scientist",
    "url": "/blog/2022/8/6/practical-tips-for-staying-focused-at-work-as-a-data-scientist/",
    "title": "Practical tips for staying focused at work as a data scientist",
    "summary": "For us data scientists, whose work is what Cal Newport calls \"deep work\", staying focused amongst a barrage of distractions is a challenge. In this post, I outline several tips that data scientists can use to stay focused at work.",
    "body": "One of my new PyData friends whom I met at PyCon, Sophia Yang, asked this question on Twitter: How do people deal with meetings and slack messages interrupting work? I mean they are part of the work, but so very much distracting. Sometimes I just want to log off and work... Anyone has good recommendations?&mdash; Sophia Yang (@sophiamyang) August 4, 2022 Her predicament is one I share; it's a classic one plaguing every modern knowledge worker. I would like to share some of the ways I've been able to help myself and my teammates stay calm. Calendaring Tips Time Blocking for Focus Time The first tip I have is to do Time Blocking on my calendar. Microsoft has an excellent \" My Analytics \" feature, which lets me set \"Focus Time\" blocks on my work calendar. Focus Time will automagically attempt to block off 2 hr chunks of time on my calendar on Tuesdays to Fridays (the exact days of the week are configurable). During those times, I will appear \"Busy\" to colleagues; rarely will I get scheduled meetings. Another benefit of Focus Time is that all Microsoft Teams and Outlook notifications will be silenced! This is undoubtedly the killer feature of Focus Time. For example, suppose I get distracted by messages during Focus Time. In that case, it'd be because I checked Outlook and Teams, not because Outlook and Teams sent me a notification. This notification silencing feature makes it much easier for me to accomplish work during my Focus Time sessions. If this feature extended deep into macOS, such that all notifications on macOS could be disabled during Focus Time, that would be amazeballer! Office Hours Our work is never done in isolation; there will always be some degree of coordination needed with colleagues. For these, I open up Office Hours for my colleagues. To remind myself of them, I have a \"Free\" block on my calendar explicitly labelled Office Hours, and I avoid scheduling anything recurring in those slots. Regular meetings on Mondays I have six teammates with whom I have regular 1:1 meetings, five in the Research team and my manager Andrew Giessel. I try to schedule my weekly check-in sessions with them on Mondays (with only one exception due to historical reasons) so that we kick off the week with direction and focus. This usually means I have six hours of meetings on Mondays, but that is a tradeoff I am willing to make to have uninterrupted time throughout the week. Schedule catch-ups To prioritize Focus Time, that sometimes necessarily means de-prioritizing spontaneous catch-up time. To get around this, schedule time to catch up with colleagues with whom you don't have regular 1:1 meetings. For this introvert, meetings scheduled this way are particularly rejuvenating. They are intentional, have a purpose, and aren't just about having people get together to chit-chat. Software Tips Apart from calendar tactics, some software hygiene tips can help. With chat programs, you have the option of muting them globally, on a per-channel basis, and (for some) on a per-person basis. Take advantage of the ability to mute channels! Not every channel with a ton of activity is something you need to be informed of on a blow-by-blow basis. Ask yourself, \"Do I really need to read every_ last message on this channel right away?\" Leave them unread until you have a chunk of downtime. Prioritize the most critical work for your best energy levels Every person is different and will need to make differing compromises on different teams. Regardless, you should know how your energy levels trend over the day and after certain events. You'll want to use those trends to better plan out your day. Prioritize the most critical work to be done during your peak energy levels and leave everything else for the rest of the day. My energy levels have seasonally changed over the years. There are some months when I am best in the morning, and others when I am best after finally downing the 3rd cup of coffee :-). I also generally can handle 4 hours of intense, focused work before needing a break. Therefore, as much as possible, I will plan for about two good 2-hour blocks of deep work each day, leaving the rest to meetings and coordination with colleagues. (Reality is always more complicated than I described above, but at least it's a starting point!) Conclusion In summary, I've shared calendaring, software hygiene, and prioritization as tips for staying focused at work. The struggle is real! I hope they help you stay focused and energized throughout the day. What other tips do you have to stay focused? Let me know by sharing the post on Twitter and commenting there, or leaving a comment below!",
    "tags": [
      "productivity",
      "tips",
      "work",
      "focus",
      "focus time"
    ],
    "pub_date": "2022-08-06",
    "type": "blog"
  },
  {
    "id": "blog-python-builtins-and-their-modern-and-convenient-replacements",
    "url": "/blog/2022/7/25/python-builtins-and-their-modern-and-convenient-replacements/",
    "title": "Python Builtins and their Modern and Convenient Replacements",
    "summary": "Python has a batteries-included philosophy. This means that when you install Python, you get a lot of basic functionality thrown in for free! Yet, as time progressed, we (the Python language community) gradually figured out how to accomplish that same functionality with fewer lines of code. In other words, we built opinionated tools to streamline common ways of doing things. In this blog post, I'd like to highlight some of these tools, both big and small, and the built-in modules (or otherwise patterns of usage) that they replace.",
    "body": "Python has a batteries-included philosophy. This means that when you install Python, you get a lot of basic functionality thrown in for free! Yet, as time progressed, we (the Python language community) gradually figured out how to accomplish that same functionality with fewer lines of code. In other words, we built opinionated tools to streamline common ways of doing things. In this blog post, I'd like to highlight some of these tools, both big and small, and the built-in modules (or otherwise patterns of usage) that they replace. Summary Table If this blog post is too long for you, then here's the tl;dr: | Built-in | Replacement | | :----------------------------: | :----------------: | | [][csv] | [][pandas] | | [][logging] | [][loguru] | | Disable [][warnings] | [][shutup] | | [][argparse] | [][typer] | (You can click on the package/module names to check them out!) [csv]: https://docs.python.org/3/library/csv.html [pandas]: https://pandas.pydata.org [logging]: https://docs.python.org/3/library/logging.html [loguru]: https://loguru.readthedocs.io/en/stable/ [warnings]: https://docs.python.org/3/library/warnings.html [shutup]: https://github.com/polvoazul/shutup [argparse]: https://docs.python.org/3/library/argparse.html [typer]: https://typer.tiangolo.com Handling tabular data Before came along and became the de facto library for handling tabular data in Python, the built-in module was the de facto library before it. The kind of code that we'd have to write before looked like this: Of course, with , we know that we can now write the following compact code instead: did a great job by giving us an intuitive API for reading CSV files and other tabular data formats, such as Excel spreadsheets and Matlab data files. In addition, it also gave us an interface to compact binary representations of data files, such as the or formats, which, in some cases, can result in 3-20X disk space reductions. Logging information If you use logging, you might know that Python has a built-in logging module. Logging has advantages over printing, the most significant two that I've experienced being (1) knowing (in the logs) exactly which module a logging line came from, and (2) the ability to redirect the logs to a plain text file that we can search later. As it turns out, [](https://loguru.readthedocs.io/en/stable/) makes logging in your Python code as easy as printing without too much overhead. Here's the code that you might write using the built-in logging facilities in Python: And here is the code that, frankly, feels just a tad easier to write, because we eliminate one line of boilerplate: The output looks beautiful in the terminal because of its colours! Disabling I can't remember how many times I've forgotten how to disable all warnings in a Jupyter notebook. Usually, a quick online search would lead me to [this StackOverflow post][so], but there are many options for doing so listed on that post that I invariably give up and live with warnings piling up in my notebooks. That is, until, I saw . [so]: https://stackoverflow.com/questions/14463277/how-to-disable-python-warnings Before , we would write: But with we would write the following code that is just a tad easier to remember than the builtin: The package is so Canadian and polite too! Making command line interfaces If you've built command line interface (CLI) tools using Python's built-in module, you'll know that it's incredibly convenient to turn Python scripts into CLIs. Yet, at the same time, it's also really verbose. You might end up writing code that looks like this: Looking at that code, I can imagine building CLIs composed of multiple commands will be challenging to maintain and develop. Typer makes building CLIs much more accessible by helping you easily organize your CLI code into sub-commands. As you probably can tell, it is much easier to compose command line interface programs with Typer than with `argparse; this is thanks to type hints in modern Python. Typer takes advantage of this to make building CLIs a breeze! Summary As the Python ecosystem matures and grows, as a community, we will uncover and find new sane patterns that lubricate our code development work. That will, in turn, lead to more development of what I call \"convenience packages\"; in the marketplace of ideas, good ones will usually (though not always) rise to the top and become well-known. I'd encourage you to try out these modern replacements for built-in Python modules and see if they stick!",
    "tags": [
      "python",
      "tools"
    ],
    "pub_date": "2022-07-25",
    "type": "blog"
  },
  {
    "id": "blog-pyscript-python-in-the-web-browser",
    "url": "/blog/2022/5/1/pyscript-python-in-the-web-browser/",
    "title": "PyScript: Python in the Web Browser",
    "summary": "At yesterday's PyCon 2022 keynote, Anaconda CEO Peter Wang did a big reveal on PyScript, which lets us use Python in the browser just like we would...",
    "body": "At yesterday's PyCon 2022 keynote, Anaconda CEO Peter Wang did a big reveal on PyScript, which lets us use Python in the browser just like we would use JavaScript. Having heard the Keynote and having sat in on the Open Space led by Fabio Pliger, one of the lead software developers on the project, I immediately went off and tested the thing that afternoon. My initial reactions can be described using the following phrases, in exact order: - No way this is happening! - Wow! - Oh wow! - Daaaaaaang! - I can write Python just like JavaScript?????? - Oh man, I see so many possibilities. - Yes, yes, this is for the masses. So I decided to give it a shot. That afternoon, I put in a pull request to the examples gallery. It was one describing how to do message passing on graphs, a point that left many of my Network Analysis Made Simple tutorial participants mindblown. I'm replicating the example below, with a few prose enhancements. Message Passing and Linear Algebra: A Demo - networkx - numpy Imagine we have a chain graph that looks like this: In NetworkX, we could construct a graph object that represents the graph. That graph would have the following edges: import networkx as nx G = nx.DiGraph() nodes = list(range(4)) G.addedgesfrom(zip(nodes[0:-1], nodes[1:])) print(G.edges()) It would also look like this: import matplotlib.pyplot as plt fig = plt.figure() nx.draw(G, ax=plt.gca(), withlabels=True) fig Now, as it turns out, this graph has a linear algebra representation in the form of an adjacency matrix that looks like this: import numpy as np A = np.eye(4, k=1) print(\"A:\") print(A) And imagine that we have a message that lives on the graph. It is given by an indicator vector , where indicates that the message lives on a node and indicates that the message is absent from the node. M = np.array([1.0, 0.0, 0.0, 0.0]) print(\"M:\") print(M) Now, if you did a dot product beetween the message and the adjacency matrix, you would pass the message along the chain. Try it out for yourself by copying and pasting in any one of the following lines into the embedded PyScript REPL below: (Execute the REPL by hitting or .) M @ A Reflections I'm not sure that 2022 is going to be the year of the Linux desktop, but I'm very sure that 2022 is already the year of Python embedded_ in the browser. I wrote this blog post entirely in Markdown, with only a few , and tags sprinkled throughout the post to make this happen. As someone who has programmed in Python for my entire professional life, there's a glaring inability for me to embed Python in HTML documents. As such, Python has a strong server-to-client vibe baked into the ecosystem. PyScript changes that paradigm: everything you saw above was rendered client-side only. My website is a static site, and yet you were able to run an embedded Python REPL here! The possibility of using Python just like JavaScript is pretty amazing. I can see many possibilities going forward, both at work, in education, and more. Kudos, Peter, Fabio, and the rest of the Anaconda team that put this out. I can't wait to see how PyScript evolves!",
    "tags": [
      "python",
      "web",
      "pyscript",
      "browser"
    ],
    "pub_date": "2022-05-01",
    "type": "blog"
  },
  {
    "id": "blog-matrices-and-their-connection-to-graphs",
    "url": "/blog/2022/4/2/matrices-and-their-connection-to-graphs/",
    "title": "Matrices and their connection to graphs",
    "summary": "I will be at ODSC East 2022 teaching Network Analysis Made Simple. We will learn more cool stuff about graphs, their key underlying concepts, and awesome connections to other topics through this tutorial. Hope to see you there! Meanwhile, here's a blog post for your edutainment!",
    "body": "Graphs, also known as networks, are ubiquitous in our world. But did you know that graphs are also related to matrices and linear algebra? Graphs, at their core, are comprised of two sets: - A node set - An edge set Nodes are entities in a graph, and edges are their relationships. One anchoring example you can use throughout this blog post is a social network - people are nodes, and their connections are edges. In \"Network Analysis Made Simple\", we go much deeper into particular examples and the specific NetworkX implementation. As it turns out, you can represent graphs as matrices! Let's construct a minimal complex example to illustrate the point. If you had a social network of 4 people (A through D), such that they had the following connectivity pattern: We can actually represent the graph as an adjacency matrix, which highlights which nodes are connected to which other nodes: Here, a value of in the matrix indicates a connection between the two nodes, while a value of indicates no relationship. Using NetworkX, it's straightforward to create the graph and convert it to matrix form: From the adjacency matrix, it's already easy to infer some basic information about the graph. Firstly, we can test whether the matrix is symmetric around the diagonal or not to infer whether or not the graph can be represented as an undirected graph. We do this by checking whether the lower triangle of the adjacency matrix is equal to the upper triangle of the adjacency matrix when either one of them is transposed: If the graph is undirected, we can count the number of edges in the graph by summing up either the upper or lower triangle of the matrix. If the graph is directed, then summing the matrix tells us the total number of edges: Additionally, taking the -th matrix powers of the adjacency matrix tells us the number of ways to reach another node by taking hops on the graph. Let's see the example of : As we can see, there are two ways to go from A and back to A (value = 2), by going and . There are no ways to go from B to D using 2 hops. Hence the value is 0. And if you've been astute enough to pick it up, the diagonal also happens to tell us the degree of each node, that is, the number of neighbours that each node has: Knowing how to convert between object and matrix representations of graphs is a really useful skill, because being able to do so gives you access to the necessary programming language APIs that can make your life a lot easier. I [know this from personal experience][graph]! [graph]: https://ericmjl.github.io/blog/2019/6/15/graphs-and-matrices/ I will be at ODSC East 2022 teaching Network Analysis Made Simple. There, we will learn more cool stuff about graphs, their key underlying concepts, and awesome connections to other topics through this tutorial. Hope to see you there!",
    "tags": [
      "network science",
      "networkx",
      "numpy",
      "data science",
      "graph",
      "graph theory",
      "linear algebra",
      "matrices",
      "matrix math"
    ],
    "pub_date": "2022-04-02",
    "type": "blog"
  },
  {
    "id": "blog-functional-over-object-oriented-style-for-pipeline-esque-code",
    "url": "/blog/2022/4/1/functional-over-object-oriented-style-for-pipeline-esque-code/",
    "title": "Functional over object-oriented style for pipeline-esque code",
    "summary": "I'm of the opinion that for most data science code, it makes more sense to write functions than objects. This probably isn't controversion, but if you're curious why, then this blog post explains my thought process.",
    "body": "Where possible, in my data science projects, I usually go for a functional programming style over an object-oriented style. I'd like to explain why this could be advantageous from the standpoint of a mental model, which is an idea that my friend Jesse Johnson has elaborated on [in the context of in biotech organizations][biotech]. [biotech]: https://scalingbiotech.com/2022/01/26/the-giant-hidden-problem-plaguing-tech-biotechs/ Types, standards, and pipelines It all starts with types -- basically, what objects are floating around in a program. When writing data science code, data type transformations are commonplace. Reasoning about the data types is hard enough without introducing another object -- and, therefore, another type. Introducing another type into the program [is like introducing another standard][standards] -- suddenly, you have one more thing to worry about. [standards]: https://xkcd.com/927/ Here's an example. We read in a CSV file and spit out a DataFrame. That goes into another function that constructs a PyMC model and returns the model. Next, the model goes into another function that calls on a PyMC sampler and returns an object. The object then gets parsed back into another for multiple uses -- to generate plots, get checked into a database, and more. If we were to introduce a new object, say, a object, we might encapsulate all of those steps in the following way: Now, the critical trouble I usually run into with classes-for-data-pipelining is not merely about needing to create new objects and hence, new \"types\" in the project. It's also that the class methods can, in theory, be called without any enforcement of order - and errors will result. It's challenging to reason clearly about the correct order of operations when reading just the code. The atomic class methods are attached to the class at the same level of abstraction as the higher-level class methods, and the object's internal state can be created on-the-fly -- and is implicit, not explicit. By adopting a functional style, we can make keeping atomic functions and higher-order functions mentally separate easier. We can keep the atomic functions inside a submodule and higher-order functions higher up in the source code hierarchy. And by calling on the higher-order functions, we can easily read off the correct order of operations that need to happen, i.e. the pipeline. Riffing off the above example: And now the functions are easily composable to make higher-order functions: A natural hierarchy appears here! We import lower-level functions from deeper within the library; with higher-level functions, we go closer to the top-level imports. (In the example above, I also took the liberty of annotating types in-line. They are another form of in-line documentation. They also help with reasoning about the workflow much more efficiently, especially if someone is just getting used to, say, the workflow.) As you probably can see here, too, using functions enables us to easily see the idiomatic workflow. Our mental model is more easily read off by the next person in the pipelines-as-functions paradigm rather than pipelines-as-objects paradigm. And as a side benefit, if we think carefully about the data types we pass around, consume, and generate, then we end up with easily reusable functions. That's another awesome side effect! Objects are fine, just not everywhere Now, just to be clear, I'm not advocating an abdication of objects altogether. Clearly, a PyMC Model, InferenceData and DataFrames are all objects themselves. For someone who works with data, when do objects make sense? I think in the context of data pipelines, objects make the most sense as a data container. One clear example is configuration objects (which I usually implement as a dataclass with very-few-to-zero class methods). Another clear example is when 3-4 related dataframes need to be kept together. Also, we may need convenient class methods to dump them to disk at once. Here a dataclass with 3-4 corresponding attributes and a single class method would make a ton of sense. Pipelines are best thought of as functions and are therefore best implemented as functions. Data (and data containers) are best thought of as objects and are therefore best implemented as objects. Objects as data pipelines, though? In my opinion, doing so only adds confusion. Once again, it's my conviction, having written more than 9 years of Python data science code, that pipelines-as-functions express the developer's mental model of the pipeline more cleanly than pipelines-as-objects. The twist Aaaaaand I know there's going to be someone who says, \"But you can make objects callable too, right?\" Yeah, I know that's doable :). On this point, the flexibility of Python allows for things to be implemented in multiple ways. However, in line with the Zen of Python, it's probably best to adopt one clear way of implementing things. Pipelines as functions, data as objects. This is a much saner way of establishing simp",
    "tags": [
      "data science",
      "pipeline",
      "programming style",
      "programming",
      "software development"
    ],
    "pub_date": "2022-04-01",
    "type": "blog"
  },
  {
    "id": "blog-everything-gets-a-package-yes-everything-gets-a-package",
    "url": "/blog/2022/3/31/everything-gets-a-package-yes-everything-gets-a-package/",
    "title": "Everything gets a package? Yes, everything gets a package.",
    "summary": "How treating data science projects like software development projects (plus more) helps you be maximally effective.",
    "body": "I recently read [Ethan Rosenthal's Python Data Science][pds] setup, and I loved every line of it. His blog post encapsulates some of the best of the ideas that I wrote about in my [Data Science Bootstrap knowledge base][kb]. In this blog post, I'd like to share my own data science setup and some new tooling that I've been working on to make it easy to achieve the same goals as I have. [pds]: https://www.ethanrosenthal.com/2022/02/01/everything-gets-a-package/ [kb]: https://ericmjl.github.io/data-science-bootstrap-notes/get-bootstrapped-on-your-data-science-projects/ But why all the trouble? Isn't there a ton of overhead that comes from doing software work? You now suddenly have to manage dependencies, start documenting your functions, add tests... you suddenly can't just store data with code in the repo... I can't just commit the notebook and be done with it? Let me explain. Data science is no longer solo work, but teamwork. The corollary is this: discipline enables better collaborative work. Have you encountered the following issues? - A notebook that works on your colleague's computer trips up on imports? - A notebook that ran on your colleague's machine errors out because you were missing a data file? - Your PyMC model definition doesn't agree with your colleagues', because you two were independently editing two notebooks that now each contains drifted model code? These are problems that are solved by thinking more like a software developer and less like a one-off script writer. Let me explain what to optimize for when adopting this mindset. Optimize for portability My grad school days taught me the pains of dependency management and dependency isolation, so as soon as I borked my MacBook Air's native Py27 environment, I bought into using for environment management. (That's a long, long story I can tell at some other time.) I also had to run code locally and on an HPC machine, and maintaining two separate computation environments means that from the get-go, I had to learn how to maintain portable environments that could work anywhere. By buying into the package manager and the associated file early on, it meant I could recreate any project's software environment with a single command . (Nowadays I use instead of because is faster, but the command is essentially the same.) Why ? On one hand, the ability to install into my home directory means it is a tool I can use anywhere, as long as I have privileges to install stuff in my home directory (which, yeah, I figure that should always hold true, right? \ud83d\ude42). On the other hand, I used to find myself on projects where packages could only be installed with ease from - this holds especially true for cheminformatics projects that I have worked on. Optimize for standardized structure [Cookiecutter Data Science by DrivenData][ccds] was the OG inspiration for this idea. Basically, I ensure that every project I work on starts with a standardized project directory structure that looks, at the minimum, something like this: [ccds]: https://drivendata.github.io/cookiecutter-data-science/ If you squint hard, you'll notice that the project structure essentially looks like a Python software package, except that there is also a directory added into the mix. This is intentional! Structuring every project in this fashion ensures that when future me, or someone else comes to view the project repository, they know exactly where to go for code artifacts. Need an analysis? Look it up in . Need to find where that function was defined? Look it up in submodules. Need to add a package? Update the definition and get or to update your environment. Imposing this structure ensures your code is organized, and as the CCDS website states: Well organized code tends to be self-documenting in that the organization itself provides context for your code without much overhead. But what about a directory? That's a good question - lots of people store CSV files in a directory. If working in a professional setting, it is becoming increasingly idiomatic to pull data from a single source of truth, such as an PostgreSQL database or an S3 bucket, rather than caching a local copy. At work at Moderna, we have an awesome single source of truth for versioned datasets from which we pull data programmatically into our projects via a Python client. (The data is cached locally but not in our repo -- it is cached in a special directory in our home folder.) In our personal projects, we can approximate this user experience by storing data in personal cloud storage (e.g. Dropbox) and pulling the data using a supporting Python web API (e.g. the [official Dropbox Python API][dropbox]). [dropbox]: https://dropbox-sdk-python.readthedocs.io/en/latest/api/dropbox.html#dropbox.dropboxclient.Dropbox.filesdownload Optimize for effective processes and habits Any data scientist has enough knowledge to carry in their heads: domain expertise, programming skills, modelling skill, and more. With that amount of knowledge overhead tha",
    "tags": [
      "data science",
      "software development",
      "software engineering"
    ],
    "pub_date": "2022-03-31",
    "type": "blog"
  },
  {
    "id": "blog-better-conda-environments-on-github-actions",
    "url": "/blog/2021/12/30/better-conda-environments-on-github-actions/",
    "title": "Better conda environments on GitHub actions",
    "summary": "I recently figured out two tips to make GitHub actions play nicely with conda installations. Here they are.",
    "body": "I recently figured out two tips to make GitHub actions play nicely with conda installations. Here they are. Ensure bash is in login mode The first is to use the following block: What this does is ensure that every shell command is run in login mode. As detailed in this StackOverflow answer: - to insure (sic) a login bash, where the environment is correctly set; - , a template placeholder, replaced at pipeline execution time by the actual script command to execute. Counterfactually, I would have had to use the deprecated , which always made me a bit nervous. Now, I can instead switch over to using before executing environment-specific commands, thereby providing longevity for the build. Use mambaforge The other tip is to use the mambaforge installer to get a conda installation onto GitHub actions. The block I recently used for my causality repo is as follows: This configuration guarantees the use of to solve the environment, which means we will have blazingly fast builds. Previously, I used to use a different GitHub action (), original (rather than ), and a convoluted build that involved environment caching. You can take a look at an example that I copied over from my project repository by expanding the details below. name: Build documentation on: push: branches: - master jobs: build-environment: runs-on: ubuntu-18.04 name: Build conda environment steps: - uses: actions/checkout@v2 name: Checkout repository # See: https://github.com/marketplace/actions/setup-conda - name: Setup anaconda uses: s-weigand/setup-conda@v1 with: conda-channels: \"conda-forge\" # Build cache of environment - name: Cache conda environment id: cache-environment uses: actions/cache@v2 with: path: nxviz.tar.gz # Note: Remember that whatever files the environment build depends on # should be hashed and added to the key. key: ${{ runner.os }}-env.${{ hashFiles('environment.yml') }} - name: Build environment if: steps.cache-environment.outputs.cache-hit != 'true' run: | conda env create -f environment.yml python -m pip install . - name: Install conda-pack if: steps.cache-environment.outputs.cache-hit != 'true' run: conda install -c conda-forge conda-pack - name: Run conda-pack if: steps.cache-environment.outputs.cache-hit != 'true' run: conda pack -n nxviz -o nxviz.tar.gz # See: https://github.com/actions/upload-artifact - name: Upload environment uses: actions/upload-artifact@v2 with: name: nxviz-tarball path: nxviz.tar.gz docs: name: Build static site docs runs-on: ubuntu-latest needs: build-environment steps: - name: Checkout repository uses: actions/checkout@v2 # https://github.com/actions/download-artifact - name: Download environment tarball uses: actions/download-artifact@v2 with: name: nxviz-tarball - name: Unpack environment and activate it run: | bash scripts/ci/unpackenvironment.sh - name: Build docs run: | source /tmp/nxviz/bin/activate python -m ipykernel install --user --name nxviz make docs - name: Deploy website uses: peaceiris/actions-gh-pages@v3 with: # https://github.com/peaceiris/actions-gh-pages#%EF%B8%8F-set-personal-access-token-personaltoken personaltoken: ${{ secrets.GHPAGESTOKEN }} publishdir: ./site publishbranch: gh-pages # destinationdir: manuscript allowemptycommit: false keepfiles: false forceorphan: true enablejekyll: false disablenojekyll: false By contrast, the new build is much smaller and easier to maintain: name: Build documentation on: push: branches: - master jobs: build-docs: runs-on: ubuntu-latest name: Build conda environment # https://github.com/marketplace/actions/setup-miniconda#use-a-default-shell defaults: run: shell: bash -l {0} steps: - uses: actions/checkout@v2 name: Checkout repository # See: https://github.com/marketplace/actions/setup-miniconda - name: Setup miniconda uses: conda-incubator/setup-miniconda@v2 with: auto-update-conda: true miniforge-variant: Mambaforge channels: conda-forge python-version: 3.9 activate-environment: nxviz environment-file: environment.yml use-mamba: true - name: Build environment run: | conda activate nxviz python -m ipykernel install --user --name nxviz python -m pip install . make docs - name: Deploy website uses: peaceiris/actions-gh-pages@v3 with: # https://github.com/peaceiris/actions-gh-pages#%EF%B8%8F-set-personal-access-token-personaltoken personaltoken: ${{ secrets.GHPAGESTOKEN }} publishdir: ./site publishbranch: gh-pages # destinationdir: manuscript allowemptycommit: false keepfiles: false forceorphan: true enablejekyll: false disable_nojekyll: false",
    "tags": [
      "continuous integration",
      "github actions",
      "til"
    ],
    "pub_date": "2021-12-30",
    "type": "blog"
  },
  {
    "id": "blog-what-candidates-can-and-cannot-control-in-their-job-hunt",
    "url": "/blog/2021/11/28/what-candidates-can-and-cannot-control-in-their-job-hunt/",
    "title": "What candidates can and cannot control in their job hunt",
    "summary": "I've been involved in quite a few rounds of hiring. I've also seen a lot of LinkedIn posts by junior data scientists complaining about the state of the job market. In this post, I'd like to provide personal reflections on the hiring process that I've experienced written from the perspective of someone participating in the hiring team.",
    "body": "Having been involved in quite a few rounds of hiring data scientists in a biomedical research context, I'd like to share some perspectives that may help candidates who desire a move into a data science role in biomedical research. I'll start off with the usual disclaimer that these are personal observations and thoughts; they may not apply uniformly to all biomedical data science teams, and may reflect personal biases. With that disclaimer out of the way, here are my observations. 60-70% of the hiring process is out of a candidate's control As a candidate, you probably want to focus on just the things that are in your realm of control while not fretting over the things that are out of your control. Here are a few examples of what I mean by things you cannot control: 1. Whether the role is opened up specifically for another person or not. 2. The underlying \"story\" that the hiring manager is telling themself regarding the role, such as the relative emphases and weight that the hiring manager places on different skillsets. 3. The personality, personal history, and angles of inquiry that the interviewing team brings to the process. 4. Whether or not the hiring manager can or cannot hire you because of other legal constraints. A candidate can't know any of these constraints unless you have contacts inside the company that can help you find these out. Because these pointers are inherently dependent on what I call the \"local\" hiring context, you're therefore also unable to directly control how those factors will affect your chances of being hired. For one's own sanity and peace of mind, I would strongly advise not fretting about these matters and instead, focus on the 30-40% of things you can control. 30-40% of the hiring process is within a candidate's control Here is a sampling of what I know is within the realm of control of a candidate. 1. If a candidate comes out of a research project, how cutting-edge their choice of methods is is highly likely to be within their realm of control. 2. The professionalism of their searchable digital footprint. 3. The development of their creative imagination to imagine the problem space of the firm they are applying to. 4. Mock practice sessions that a candidate gets before interviewing. I think these examples adequately highlight the kinds of things within a candidate's realm of agency as they embark on the job hunt. Once in a while, I see junior candidates (fresh grads) complain about the state of data science hiring. While I can empathize with the underlying emotion, I also think it is more productive to direct that energy towards things they can control - including better curating that digital footprint which is being affected by those publicly aired complaints. What I looked for when I was involved in hiring When I was involved in hiring, I was laser-focused on technical excellence. I chose this aspect out of the many factors because of my personal interest. I wanted to see what new things I could learn from candidates. I also wanted to continue raising the bar on technical excellence in my team because I know this is fundamental to a data science team's collective ability to deliver on projects. Having worked with individuals who possess the seeds of technical excellence that also match my own mental model of excellence, I had a blueprint for MSc-level and PhD-level from prior experience that I could benchmark candidates against. Here are the criteria by which I have evaluated technical excellence. \"Special sauce\" modelling skills Firstly, I looked for evidence of special sauce modelling skills. \"Special sauce\" skills refer to something uncommon, rare, and technically challenging. I looked for probabilistic modelling, causal inference, graph algorithms, and graph neural networks. Having taken a class can only be considered weak evidence; strong evidence implied actual projects that they had to dedicate a significant amount of time towards solving, usually for half a year or longer. This may be in a Masters or Ph.D. research thesis, or it may be in an internship. Candidates may or may not be able to go in-depth into specific business-related project details. However, they should usually be able to go in-depth into technical details. To tease out these details, I would question and probe very deeply. I would also ask counterfactual questions, say, on their choice of modelling strategy. In questioning, I would place equal weight on technical correctness and interpersonal response; a candidate that gets defensive or starts name-dropping terms without justification would usually end up getting low ratings from me. Special sauce skills, in my opinion, signal intellectual agility and relentlessness to go technically deep. Hence the premium I placed on this skillset. Strong software development skills Secondly, I looked for evidence of solid software development skills. My preferred way of evaluating this skill is not by programming puzzles; in my opinion, these are artificial t",
    "tags": [
      "data science",
      "job hunt",
      "career",
      "careers"
    ],
    "pub_date": "2021-11-28",
    "type": "blog"
  },
  {
    "id": "blog-career-faq",
    "url": "/blog/2021/9/30/career-faq/",
    "title": "Career FAQ",
    "summary": "I've been asked some questions about my career trajectory, and thanks to a call with Zarrar Shezad, I finally decided to pen down my answers to career trajectory questions.",
    "body": "Q: How was my transition from the academic world to the industry? I found the transition relatively easy because, I think, of the environment that I first joined. The Novartis Institutes for BioMedical Research, NIBR, was famed for leaning towards the academic side back in 2017. Thus, over the 3.5 years that I was there, I was given free rein to decide what to work on. The operational freedom was quite important to me as I had experienced a similar environment in graduate school. That lesson on the environment turned out to be more critical than I appreciated early on. We might live in an individualistic society, yet we're more affected by our community and environment than we might expect. (One might even make the case that a phase change might happen when a community collectively turns individualistic... that's an irony to explore in another setting.) During my time there, I found NIBR's environment to be pretty forgiving, thanks to its slower pace than other companies. Generally, my colleagues were kind and intelligent (in that order). Colleagues at the same level were idealists who strived to do good science. Having that community of similarly skilled and motivated colleagues was essential to me early in my career. Q: What essential steps/skills that I gained allowed me to advance in the field? First off, we should get one point out of the way - I don't consider promotions my primary indicator of advancement. For me, it is a side-effect of doing good work. What profoundly satisfies me the most at work is that the projects I work on accelerate science to the speed of thought. I hold the conviction that if I'm advancing the field of science that way, I'll eventually share in the team's success. That perhaps is the most critical lesson that others way more experienced than me have given to me: to stay mission-focused and to not think much about promotions, salary, and more, for they come, eventually. To transcend the material and not think much of credit gives us the freedom to pursue high-value work in the service of others. The irony here is that when you do high-value work in the service of others, you'll then have the credibility points to ask for a high-value, market-rate compensation for that work. So the most important thing I see is finding, buying into, or creating a transcendent, high-value, service-oriented mission for yourself. Then, use your powers of logic to deduce the high-value actions you need to take to serve others. All of business, science, and life, in general, should be about deep service to others. If we start focusing inwards on our own desires, we'll end up deeply unsatisfied. But, on the other hand, I firmly believe that if we keep service to others squarely in our sights, we'll always find unique and fun work to do. Now, everything I said above does not exclude looking around and benchmarking whether I'm being treated fairly or not. Still, it's important to note the order of priorities: I am first concerned with the transcendental mission and then worried about making sure I'm being treated fairly. With that order of importance, I can be free to do my best work first. Doing so lets me accumulate credibility points to advocate for fair treatment of myself and others around me. Flip that order of priority, and I am sure I will be treated as a pariah with no credibility. So our credibility points must be earned first. As for the tangibles, such as salary and title, some of my following observations could be of help: 1. Salary raises and title promotions jump most substantially when one switches roles. But I would not encourage switching roles just to get salary and title jumps. 2. Internal promotions can sometimes be biased because of the human factor involved. Therefore, you should always be aware of your internal context. Okay, so beyond the overarching \"how do you advance\" point, I'd like to address \"how to do good work.\" I'm trusting that you probably know a lot of these points anyway. So I'm expecting that these points should serve merely as reminders rather than advice. I'll explain this in the context of my idealized project lifecycle. Firstly, when you embark on a project, I would encourage you to sit down and carefully write out a two-page (~1000 word) document for yourself. This document should provide an overview of the motivation, needs, and possible solutions for a problem your colleague brings to you or that you have identified. This document is akin to a project charter. It's very tempting to dive head-first into programming as that feels like our currency of data science work. But the actual currency of data science work is our insights; code is merely our medium to get there. Writing a document that accurately, concisely, and clearly communicates an overview of the project can help you design the solution; sometimes, it also kickstarts the flow of creative ideas. In other settings, it might illuminate the most straightforward method that solves the problem. Clarity",
    "tags": [
      "data science",
      "career",
      "faq"
    ],
    "pub_date": "2021-09-30",
    "type": "blog"
  },
  {
    "id": "blog-machine-directed-evolution",
    "url": "/blog/2021/9/12/machine-directed-evolution/",
    "title": "Machine-Directed Evolution",
    "summary": "This is a blog post I've been wanting to put out for a while. It's about [a really cool paper][acscatalysis] that I had the privilege of working on with colleagues at the Novartis Institutes for BioMedical Research (NIBR). If you have institutional access, you can read the paper [online][acscatalysis]. Here is my layman's summary of the paper. [acscatalysis]: https://pubs.acs.org/doi/10.1021/acscatal.1c02786",
    "body": "The backdrop In 2019, while I was still at the Novartis Institutes for BioMedical Research (NIBR), I joined an internal startup team doing enzyme evolution in NIBR's chemistry department. This project was part of an internal innovation program, Genesis Labs, where teams are given the space and time to pursue a project idea. Our team wanted to compare and contrast how traditional directed evolution fared against machine learning-powered directed evolution (which we call machine-directed evolution in the paper). The scientific story Our enzyme of choice is IRED, which is an industrially relevant enzyme for chemistry applications. We had a choice of IREDs that catalyzed our desired reactions, some of which were good already and others that were somewhat middling and not ideal. Our goal wasn't solely to get a good IRED, however. We also wanted to compare machine-directed evolution against traditional directed evolution techniques. Therefore, we picked the \"kinda middling\" IRED-88 enzyme as our starter enzyme. That way, we could explore how quickly machine-directed evolution could help us climb the dynamic range of enzyme goodness vs. traditional directed evolution. But what do we exactly mean by enzyme goodness? We meant to measure enzyme goodness on two axes: (1) substrate conversion into product and (2) chiral selectivity. Conversion measures how much substrate gets converted into the desired product; for us, higher conversion is better. Chiral selectivity, if you remember organic chemistry from 2nd-year science curricula, is all about getting the correct chiral form of the molecule; chirality results from having a tetrahedral geometry around carbon atoms. If you're curious to learn more, Wikipedia has a great article on this. When the enzyme catalyzes our reaction, we get a mixture of R- and S-chiral molecules; we were interested in maximizing the yield of R-chiral product while minimizing the production of S-chiral product. Deep mutational scan To start, we generated a deep mutational scan (DMS) dataset. DMS requires the generation of a site-saturation mutagenesis library, where we generate all possible single-mutant variants of wild-type IRED-88. Doing so gave us a great starter dataset for machine learning purposes by covering a large proportion of single-mutant sequence space. Out of the roughly 6000+ mutants that we could have possibly generated, we cloned about 81% of them successfully. Then, we measured their activity; we also measured their chiral selectivity for a subset of them. In the subsequent paragraphs, I won't talk much about chiral selectivity. Still, you know that we always measured selectivity for a subset of highest activity mutants -- a subset, because chirality is harder to measure at scale. Fig. 2(a) from our paper, deep mutational scan measurements. X-axis: position on linear amino acid sequence of IRED-88. Y-axis: amino acid substitution. Heatmap shows activity measurements for ~81% of all 6000+ possible single point mutations. Fig. 2(b) from our paper, activity distributions. X-axis: Bayesian estimated activity of IRED-88 mutants. Y-axis, top panels: Activity cumulative distribution of our mutants. Y-axis, bottom panels: Enantioselectivity of our mutants. Even without machine learning, that DMS dataset was highly informative! We happened to have a crystal structure of IRED-88 on hand. When we mapped what positions in the linear sequence were present in the crystal structure, we noticed that part of the N- and C-termini (head and tail of the sequence, respectively) were actually not present -- they probably were too floppy to be included as part of the solved structure. Yet, because we had measurements from the N- and C-termini mutants of IRED, we found that mutations in the C-termini were beneficial for enzyme activity. I think this means that a DMS dataset can help uncover potentially good mutants that we would never have guessed to take a look at, simply because they were missing in a crystal structure of the same protein. Traditional directed evolution At the same time, we did three rounds of traditional directed evolution. These are EPPCR1-3 in Fig. 2(b) shown above. One round was based directly on the wild-type, generating roughly 4000+ mutants (with replicates, so go ahead and imagine how many colonies we had to pick). Out of those enzymes, the winner turned out to be an okay-ish enzyme. However, in the interest of obtaining a good enzyme, we decided to take a winner from the deep mutational scan as our basis for a 2nd round and subsequent round of directed evolution. The winners of those next rounds all turned out to be pretty good. However, the number of things we had to screen was close to 10,000 mutants (with replicates). So, again, keep that number in mind: it's $10^4$ mutants measured, amounting to about $10^5$ activity measurements taken. Machine-directed evolution So now we moved on to machine-directed evolution. Here, our strategy was to train a machine learning model",
    "tags": [
      "science",
      "paper",
      "catalysis",
      "enzyme engineering",
      "machine learning",
      "data science"
    ],
    "pub_date": "2021-09-12",
    "type": "blog"
  },
  {
    "id": "blog-hiring-data-scientists-at-moderna-2021",
    "url": "/blog/2021/8/26/hiring-data-scientists-at-moderna-2021/",
    "title": "Hiring data scientists at Moderna! (2021)",
    "summary": "Moderna's DSAI team, the team I'm on, is hiring! Read on to get links to the postings and get a glimpse into the inner workings of my head as I look through resumes.",
    "body": "The team I'm on is hiring! We are looking for three bold, curious, relentless, and collaborative data scientists to join Moderna's DSAI group. There are three specializations, one for public health/medical affairs, one for chemistry, and one for biological sequence modelling. Here are the LinkedIn links at which you can apply and learn more about the roles: - Commercial & medical affairs role - Chemistry role - Biology role We are looking for strong model-building skills, prior experience handling relevant data, and the compassion & competence to communicate with non-technical stakeholders. Historically, we are a Python + PyTorch shop. Bonus points for special technical skills (network science, Bayesian modelling) on top of a technical track record of building fit-for-purpose machine learning models. And now for an FAQ that disambiguates some questions that you might have. The answers, views, and observations listed below are my own; they don't represent Moderna's official hiring policies, which I'm striving to adhere to. In no order of importance, and basically in order of what I could think of to include in an FAQ, here they are. Q: Is relocation necessary? Because of the collaborative nature with colleagues who can't work remotely, we expect relocation to Cambridge, MA, with an eventual return to campus. COVID times dictated a temporary change in practice, but in-person lubricates difficult technical discussions. We do have some colleagues in other states/cities, but they plan to move to Cambridge eventually. Q: Who will the roles be reporting to? For the chem & bio roles, yours truly. For commercial and medical affairs, my wonderful colleague Adrianna Loback. Q: Who leads the broader team? Andrew Giessel, data scientist #1 at Moderna. (I am only #6!) Q: Is a Ph.D. required? The expectation is yes, because of the research-y nature of the role and because of the level of scientific depth required. However, excellent MSc candidates might be considered if a publishing track record or relevant industry experience is present. Q: How urgent is the hiring? For the bio and chem DS roles, I'm still at the point where I'm willing to wait. We want to consider a diverse slate of candidates before making a decision. For the Commercial & Medical Affairs role, I would defer to Adrianna for her thoughts, but I also want to respect her time - she is slammed with many projects. Q: What are the core skills we need to demonstrate? For the BioDS role: solving scientific problems that rely on biological sequences as input to machine learning models. We have professional bioinformaticians, so traditional bioinformatics skillsets are not really what we're looking for. For the ChemDS role: solving scientific problems that rely on QSAR modelling and the extended suite of modelling tools. We have computational chemists, so traditional comp chem skillsets are not really what we're looking for either. For both roles: showing me that you can build custom differentiable or likelihood-based models and not just grab something off-the-shelf. Or, if you did grab something off-the-shelf, show me that you really have dissected the internals to the point that you can recreate it in a different framework. I only trust modellers who know the ins and outs of the models they build. Q: If I work at Novartis, will I be considered? Unfortunately, no. I have a non-solicit clause that bars me from recruiting anyone from Novartis within a year of my last day of work, which was 30 July 2021. That means my colleagues cannot include me in any interview panel that involves a candidate currently working at Novartis. Q: Is there an algorithm filtering me out? No. Real humans are at work. Trust me. I've once thought that way too. But, now that I'm on the other side, I know what it looks like. Our friendly talent acquisition partner in HR is helping us with the process, and he handles wayyyyyyy more than we see. So... yes, we're overwhelmed with the response and have been working through your resumes. But we definitely want to pick well. Q: Is a GitHub repository required? Not really, but we do want to see your best work at some point, including how you code. All of us, including myself, have work that came from our early days of data science that we're not proud of, but don't worry about those - I'm only interested in knowing how what your best work looks like. A GitHub repo can help as a data point. If you do choose to highlight one, make sure it's done well - a README that explains why your repo exists and how to get started is an absolute necessity! Q: What is the impact of the role to Moderna and the world? I can't reveal internal workings, but the roles I'm in charge of will involve the core platforms of mRNA, lipid nanoparticle delivery, and protein engineering. There are so, so many open scientific problems that we can solve building models. My friend Jon Bloom at Cellarity characterized it on Twitter this way, \"If you\u2019d like your sequence-to-function",
    "tags": [
      "data science",
      "hiring",
      "moderna"
    ],
    "pub_date": "2021-08-26",
    "type": "blog"
  },
  {
    "id": "blog-my-experience-switching-to-github-codespaces",
    "url": "/blog/2021/8/23/my-experience-switching-to-github-codespaces/",
    "title": "My experience switching to GitHub codespaces",
    "summary": "I recently started experimenting with GitHub codespaces and I wanted to share a bit about my experience.",
    "body": "I recently started experimenting with GitHub codespaces and I wanted to share a bit about my experience. What is GitHub's Codespaces? GitHub Codespaces is a way for anyone who develops in code to work on a repository without going through the hassle of initial setup. At its core, Codespaces is based on container technology; essentially, code is cloned into a volume and mounted into a container that is defined by a single source of truth . This is known as a \"development container\", which VSCode supports excellently. GitHub Codespaces takes this one step further and runs the container on a cloud VM, thereby eliminating any local setup overhead: with one click, you get set up in a development environment inside the browser, connected to a VM that is potentially more powerful than your own. What benefits do we get by using Codespaces? To illustrate the benefits of Codespaces, and development containers more generally, I need to recount the 2019 sprints at SciPy 2019 and PyCon 2019. These were the days before I knew anything about development containers. As such, the best environment isolation tool I knew of at the time was Conda environments. As such, I had instructions for Conda environment development setup. It was, to say the least, quite a bit of work getting that documentation up and running. However, not everyone uses Conda; some Python developers either have a preference for -oriented virtual environment tooling or are already invested in it and have a shell configuration built around it. Other users were using Windows; this made things particularly challenging for me when debugging because I hadn't used a Windows machine in over 13 years. (I had switched to a Mac in 2006 and never looked back.) As you can see, the main challenge I had to deal with as a maintainer and sprint lead was the myriad and variety of possible environment setups; each contributor also had to work through their own system's quirks to get up and running. Though some found it satisfying, others found it frustrating; if left alone to wrangle environment set up, first-time open source contributors would most likely be scared off. Codespaces, and development containers more generally, solves that exact problem of development environment setup by taking responsibility for it out of the hands of the contributor and putting that responsibility in the hands of the project maintainer. The project maintainer sets up a working container definition via a and a file that is sufficient for all of the tasks necessary to work on a project. Those tasks might include running tests, building docs and previewing them on a web server, or building an app and previewing it on a web server. For education materials that I develop as a hobby, I can get new contributors set up and running on Codespaces or dev containers running Jupyter Lab in the background automagically, no setup is required on their side. I even started developing my personal website, yes, this exact site that you're viewing right now, on Codespaces. The hardware that Codespaces VMs run on can be quite hefty and impressive. For software development that involves heavy computation, having access to a 32-core, 64GB RAM machine can be quite liberating when our local machine is only an 8-core, 16GB RAM machine. Impressively, when I checked the specs of the VM, I saw that I was running on a 4-CPU, 8GB RAM box. That was more than generous for what I needed to develop this website. Even better, the entire Codespaces setup feels local, even though it is running in the cloud; if there are temporary web services that we need to run, such as the Lektor web server that I used to write this blog post in, we are given a temporary URL for that Codespaces session that lets us access the web service easily, as if we were accessing something on . And as a bonus, that URL can be provided to fellow developers if they need a live preview of the site to provide feedback, eliminating delays in feedback that may arise from using a CI system to preview the docs. (As they say, \"right tool for the job\" - CI previews are good for asynchronous development but potentially frustrating to wait for in synchronous, pair programming settings.) Another benefit, which might feel a bit niche but is highly relevant, is that with Codespaces, we need not worry about multiple SSH keys existing on a system and which SSH key we use. (I for one, prefer to have just one key for one computer, which simplifies things greatly.) Some users configure their system to use multiple SSH keys. Sometimes the reason is intentional, but at other times the reason is something specific and peculiar in history that we forgot why later on... and yet the system state remains encrusted. The confusing part comes when a relatively inexperienced user tries to clone a repository onto their system using SSH keys but forgets which SSH key they are supposed to use for accessing their remotes. Using Codespaces we completely eliminate this stumbling block, beca",
    "tags": [
      "programming",
      "ide",
      "tooling",
      "coding"
    ],
    "pub_date": "2021-08-23",
    "type": "blog"
  },
  {
    "id": "blog-beautiful-graph-visualization-with-nxviz",
    "url": "/blog/2021/8/16/beautiful-graph-visualization-with-nxviz/",
    "title": "Beautiful Graph Visualization with `nxviz`",
    "summary": "In this post, I formally introduce the package, a rational network visualization package with beautiful defaults made for Pythonistas. I created it alongside many contributors, to whom I'm supremely thankful and indebted. Come learn more about !",
    "body": "When working with network (graph) data, being able to visualize it is immensely valuable. When a graph visualization is done well, it can help you provide immediate visual insights based on the structure of a graph. While NetworkX provides a built-in graph visualization library, other tools exist that accept a NetworkX graph object and return a beautiful graph visualization. I want to illustrate one example here, the project. is a library that I originally built during my graduate school training days. My goals were simple: to produce two kinds of beautiful and rational graph visualizations: Circos plots and Hive plots, both built by one of my graph visualization heroes, Martin Krzywinski of the BC Cancer Research Center in Vancouver, BC. Back then, an easy API for making graph visualizations did not exist in the Python world. So I decided to scratch my builder's itch and created that API. Fast forward 5 years later, has become a part-time research hobby into graph visualization while also being a generally useful tool for Pythonistas who wish to visualize graphs. The goals are simple: to provide a declarative API for producing beautiful and rational graph visualizations from NetworkX graph objects. In this post, I would like to introduce you to the basic, high-level plotting API so that you can get started with it easily. What plots can you make with ? provides a family of plots: Circos, Arc, Matrix, Hive, and Parallel plots. They are unified by this idea that rational plotting starts with prioritizing the placement of nodes. Assuming you have a NetworkX graph called that you have been working with in-memory, making a Circos plot that has colour annotations for the nodes and edge transparencies is as simple as: assumes that node attribute fields, i.e. the keys in the Python dictionary that constitutes the node attribute dictionary, are consistent across every node. (There are special cases where we can break this assumption, but we won't go into that here.) The same is said for edge attribute data. With that information, we can declare that we would like nodes to be coloured by some attribute and edges to have their transparency controlled by some other attribute. Apart from Circos plots, other plots are available: Hive plots, Arc plots, and Matrix plots, as you can see below. What is the underlying philosophy of ? When analyzing a graph, structure is what we primarily pursue. Thus, when visualizing a graph, we should highlight its structure. To additionally make the graph beautiful, one needs to impose additional constraints. As such, the rational approach to visualization is to prioritize the placement of nodes. Doing so helps us avoid the construction of messy hairballs, which, though illustrative of the complexity of networks, don't actually give us much in the way of visual insights. works by first deciding where overall to place nodes. This is known as the \"graph layout\" step of network visualization. For example, in a Circos plot, we first constrain all nodes to be placed on the circumference of a circle. Next, we need to decide how to place nodes relative to one another by leveraging node metadata attributes. For example, if we plot the social network of students on the Circos plot and want to visualize how height co-varies with connectivity, we might order the students' nodes along the circumference from shortest to tallest. If students were also assigned Houses, as in Harry Potter, we might group the students by their House. Once that basic layout is done, we can add node styling, edge drawing, and edge styling into the mix. What else can you do with ? Apart from making beautiful plots, does more! For example, you can also annotate the plots using our annotation API and highlight certain nodes using the highlighting API. And because plots are built entirely on axes, you can drop down to the API easily to make fine-tuned customizations of your own. Where can I learn more about ? Please head over to the official documentation page. I'd love to hear how you use it and where it can be improved - and even better, if you'd like to work together to implement something, then let's hash it out on the issue tracker! Finally, I will be teaching a tutorial on Network Analysis on the ODSC AI+ platform - learn more about NetworkX, applied network science, and graph visualization up there! We will cover the basics of graph theory, how to use NetworkX, and how a variety of problems can be solved using graphs as a central data structure. You'll walk away with a solid grounding in NetworkX and applied network science; there will surely be seeds of inspiration for your downstream work!",
    "tags": [
      "open source",
      "network science",
      "graph theory",
      "nxviz",
      "network analysis",
      "graph",
      "network"
    ],
    "pub_date": "2021-08-16",
    "type": "blog"
  },
  {
    "id": "blog-ancestors-and-descendants-apply-to-undirected-and-directed-graphs",
    "url": "/blog/2021/8/15/ancestors-and-descendants-apply-to-undirected-and-directed-graphs/",
    "title": "Ancestors and descendants apply to undirected and directed graphs",
    "summary": "Does finding ancestors and descendants of a node apply to undirected graphs, or do they apply to directed graphs only? The answer is less intuitive than we might think.",
    "body": "Today I learned a new thing! It is inspired by PR #5017 from a GSoC student that I have been co-mentoring with Ross Barnowski (in the NetworkX dev team). The PR really forced me to think about the concepts described here. There are two functions in the NetworkX library called and . Respectively, they return: - : All nodes that have a path into a node in graph G. - : All nodes that have a path from a node in graph G. Do you think it applies to directed graphs only or both directed and undirected graphs when you think of this definition? Intuitively, we might think that ancestors and descendants apply only to directed graphs, particularly directed acyclic graphs (like trees). However, as it turns out, based on the definition provided, they must apply to both directed and undirected graphs. Here's why. We can think of undirected graphs as being equivalent to directed graphs that have bidirectional edges between nodes. When viewed this way, an undirected graph is a specific case of the more general directed graph. When we trace all ancestors of a node, we are recursively collecting nodes along the path into that node. If we continue recursively collecting nodes in the bidirectional representation of an undirected graph, then we will end up collecting all of the nodes in the connected component of the graph that are connected to the node we are asking for ancestors. The same argument applies to descendants.",
    "tags": [
      "til",
      "network science",
      "graph theory"
    ],
    "pub_date": "2021-08-15",
    "type": "blog"
  },
  {
    "id": "blog-one-killer-way-to-burst-to-the-cloud-from-your-laptop",
    "url": "/blog/2021/7/12/one-killer-way-to-burst-to-the-cloud-from-your-laptop/",
    "title": "One killer way to burst to the cloud from your laptop",
    "summary": "I test-drove Coiled Computing's product, and it's magical. Let me tell you why!",
    "body": "Today I finally had a chance to test-drive Coiled Computing's product. Oh boy, am I impressed! The first time I first tasted parallel computation on the MIT GridEngine compute cluster in 2014. Ever since then, one thing I've been dreaming of is the ability to burst my computation into remote machines from my little laptop. The computing model I'm thinking of isn't exactly an SSH tunnel, as this model presumes a few things: 1. We have permanent access to a remote beefy machine or a cluster. 2. We have the ability to SSH directly into it to manipulate it. 3. Our compute environment lives entirely on the cluster and not on our local machine. The computing model I'm thinking of is as follows: 1. We only want convenient and ephemeral access to the practically infinite cloud computing resources available. 2. We merely need to declare what our environment ought to look like using configuration files. 3. Our compute environment lives primarily on my portable machine and can be replicated in the cloud. You can think of this as akin to having 12 Factor App-like computing environments. Coiled Computing's product provides exactly the computing model that I'm thinking of. By being built on top of Dask, the critical problem that Coiled solves, which is also its fundamental value proposition, is to orchestrate the creation of ephemeral cloud compute resources into a Dask cluster with access to a replica of our local compute environment. Locally, we only need to worry about having a configuration file, such as an file, that declares what kind of compute environment is required. Coiled will read that config file to re-create the environment on the cloud, optionally on a cloud provider that you get to specify. Once the Dask cluster is created in the cloud, we only need to connect to that Dask cluster from a Python session using Dask's distributed clients. Once we've done that, if you've got code written against Dask that runs on your little laptop, then literally nothing else needs to change! Execute the rest of your code, whether in a Jupyter notebook or in a Python script. From my perspective, this is the ideal situation for parallel computation. Because the computing environment is available locally, I need not worry about not having internet/cluster access when I'm on the road. At the same time, if I'm literally flying in the cloud with WiFi access, I can burst my calculations into the metaphorical compute cloud and do my computing with ease. Additionally, easy-to-follow documentation is readily available, making it easy for me to create very ephemeral Dask clusters in the cloud that also shut down as soon as my computation is done. By contrast, at work, I had tried Databricks. It was helpful for some things that my colleagues needed, but it was also highly inflexible: back in 2018, my environment was not recognized, I couldn't develop and use custom software packages in support of a project, and I most certainly couldn't have the convenience of a local computation environment. So I quickly ditched it because of the lack of flexibility. Moreover, though I shared access to a Linux workstation with colleagues, and even though that Linux workstation could access our HPC cluster, it still required me to SSH every morning. Call me a person pampered with first-world problems, perhaps, and yes, I'll grant you that. Still, you can't deny that needing to SSH every morning is still a friction point. Seeing how easily Coiled replicates my computation environment on the cloud is a really key thing for me; on our HPC, we were guaranteed replicated conda environments because our worker nodes had access to the same filesystem, but on the cloud that was not possible. Coiled makes it possible - in a pretty magical way. By contrast, Coiled has me smiling to my ears. I'm very impressed with what I see here! [Clarke's third law][clarke] states that \"any sufficiently advanced technology is indistinguishable from magic.\" Because I have helped alpha-test Coiled's product and successfully borked it on my machines before, I sort of know what the product's internals are like. (I will respectfully decline to reveal what I know, though, as it concerns my friends' livelihoods.) However, even with that knowledge, Coiled feels like magic. That was the exact feeling I had when I first test-drove their demo code successfully on my laptop. [clarke]: https://en.wikipedia.org/wiki/Clarke%27sthreelaws Kudos to Matt and Hugo, the two whom I know most closely, and the rest of the team, who have really built a fantastic product! I am excited to try things out on my projects at home :).",
    "tags": [
      "data science",
      "dask",
      "coiled computing",
      "computation"
    ],
    "pub_date": "2021-07-12",
    "type": "blog"
  },
  {
    "id": "blog-how-to-enable-custom-source-package-installation-in-binder",
    "url": "/blog/2021/7/10/how-to-enable-custom-source-package-installation-in-binder/",
    "title": "How to enable custom source package installation in Binder",
    "summary": "I figured out how to make my custom source code libraries installable in a Binder container - and do it in a way that preserves flexibility while still being easy to use.",
    "body": "When I make tutorial material, I often choose to write a custom library in support of the tutorial. For example, for this JAX-centric tutorial repository that I made (), all exercise answers are written as Python functions that we can call in the notebook (and inspected at one's own pace). Doing so helps me ensure that I have one single source of truth for the answers and can write tests against them (if I desire so). By organizing the code answers into submodules, I also gain an organizational mapping from answers to tutorial sections - which is incredibly handy for navigating the library. When building a Binder container using a configuration file, it's sometimes difficult to include the custom source library in the Binder build. I could, in theory, use the section of an file to install the custom source as follows: However, there are cases where we might not necessarily want the current version of the custom library. In those cases, we may instead prefer a canonical version that we can reference (like the one that lives on the / branch). Sticking the in thus restricts the flexibility that we might otherwise need. Suppose this level of flexibility is needed. In that case, we need to match the flexibility with the appropriate composition of tooling. Writing a could be the way out. Thankfully the Binder team made a fit-for-purpose abstraction called the script. Essentially is nothing more than a shell script that gets executed right after the Docker container is built. We can use it to install our custom source library for use with Binder: 1. Include a file named in the root of our repository. It shouldn't have any file extensions, such as or . 2. In there, add in the following commands, assuming our source code library is in the directory : As a side note, Binder also provides [](https://mybinder.readthedocs.io/en/latest/configfiles.html#start-run-code-before-the-user-sessions-starts). This is a way of running code before the user session starts (e.g. setting environment variables that we don't want to be stored in a container). I have yet to find a use for this myself, but I'm sure the good folks on the Binder team have excellent reasons for doing so. I really do have to give it to the Binder team. They've done a fantastic job here in architecting the package to satisfy workflow_ needs. Kudos!",
    "tags": [
      "binder",
      "tutorial",
      "jupyter",
      "data science",
      "til"
    ],
    "pub_date": "2021-07-10",
    "type": "blog"
  },
  {
    "id": "blog-how-i-do-conference-talks-in-2021",
    "url": "/blog/2021/6/4/how-i-do-conference-talks-in-2021/",
    "title": "How I do conference talks in 2021",
    "summary": "I was inspired by a blog post of an old schoolmate from undergrad days, Ramon Huidoboro, who wrote about his experience pre-recording his conference talks. So I thought I'd write on the same topic.",
    "body": "This year I participated in PyCon US 2021. It's my 7th PyCon running, and I'm thankful to have been able to attend one every year since 2017. For the past year and a half, we've been on lock-down, though. This means we have had to do conferences online instead. In this blog post, I wanted to simply document what my setup and process for doing these recordings was like, in addition to writing down why I made particular choices. My recording setup can be broken down into the hardware and software setup. The hardware Let's first talk about the hardware setup. Video recording To record videos, I used a 1080p webcam that I bought off Amazon. Back then, because supplies were low, I paid a premium of around USD80 to get one of the lesser-known brands; brand-name Logitech would have cost close to USD200. Nowadays, though, these webcams are available at a much lower price, about USD40 instead. Audio recording For audio, I splurged and bought myself a RODE podcaster USB mic. Every condenser microphone that I had tried recorded way too much background noise. With the red line rolling in the background from time to time, it'd be too distracting in the recorded videos. Hence, I opted for a dynamic microphone. Of them, the RODE Podcaster USB mic seemed to be at the sweet spot for being relatively affordable at about USD200+. This included the boom arm and shock mount while still having a good enough quality for recordings. As we all know, the most crucial element in videos is actually the audio, and I wanted to make sure this was done right. Hardware for maintaining a standing posture When doing presentations, I have a preference for standing over sitting. This is because I was trained to give presentations while standing, so I'm used to it, and standing gives me this sense of formality that keeps me grounded when I'm doing these talks. So in support of standing while presenting, I have three pieces of hardware that help. The first is the monitor desk arms by North Bayou. They keep my monitor and laptop in an elevated position and are at about the right height. The second is the folding laptop desk. It just so happens to be of the correct height such that when I stand, my hands land comfortably on the keyboard and mouse. I also have a microphone boom arm, which allows me to keep the microphone at a comfortable height. With a pop filter, plosives become minimized. I tried it out at a PyMC Labs weekly meeting, and everyone could hear the difference. USB Dock Connecting the entire hardware setup is the Tobenone USB 13-port docking station. It provides power to the laptop, HDMI connectivity to the floating monitor, and USB-A connectivity for the webcam and microphone. Both my keyboards are Bluetooth (the big one is Logitech's K480, which allows simultaneous connectivity to three devices). Pics or it never happened Here's a photo of the setup! The software Open Broadcaster Software When recording, I set up the Open Broadcaster Software (OBS) with a scene that has: 1. A video capture channel 2. A screen capture channel 3. An audio capture channel The video capture takes in video from my webcam; the screen capture is recording the main presentation window; the audio capture records audio from the RODE microphone. Presentation slides I've taken a particular liking to web technologies. Hence, I typically make my slides using a combination of hand-crafted, raw HTML backed by . I like HTML slides because I can then host the slides on GitHub Pages without manually compiling them from another source, such as Keynote. Additionally, I can have complete control over the presentation. Now, one may suggest that writing Markdown followed by calling on any one of the available markdown-to-reveal converters would make authoring the slides a bit easier. I agree, especially when all I need is to quickly bang out a set of slides! That said, I sometimes also want to have fine-grained control over particular transitions. Sometimes wish to embed custom JavaScript inside the slides. And at other times, I'd like to do highly custom HTML layouts, such as two cards on the left-to-right. Of course, to do this, I'd end up writing HTML and JavaScript anyway, so I figure I might as well write the entire slide deck in hand-crafted HTML anyways. Committing the slides to GitHub also gives me GitHub pages. That means I can distribute a URL to my audience. They can flip through on their own terms if they choose to go on their own pace; no need to be constrained by my own pace. Scripting and teleprompting I use Ulysses (and, nowadays, Obsidian if I happen to be working there) to draft my script. The ability to focus on just the text and nothing else helps a lot. Because I have a two-monitor setup (laptop native and the external monitor), my OBS setup will capture the window on the laptop. My external monitor, where I have my webcam mounted, is also where I set up a makeshift. There, I use Ulysses (or Obsidian) with a window size just wide enough to manually scr",
    "tags": [
      "reflections",
      "conferences",
      "pycon"
    ],
    "pub_date": "2021-06-04",
    "type": "blog"
  },
  {
    "id": "blog-ward-a-new-python-package-for-software-testing",
    "url": "/blog/2021/5/29/ward-a-new-python-package-for-software-testing/",
    "title": "Ward: A new Python package for software testing",
    "summary": "Today I learned about a new package for Python testing called Ward. It looks pretty neat!",
    "body": "Today I learned about a new package called Ward. It claims to be an alternative to pytest with a focus on productivity and readability. Looking at the examples on the docs, I see a few cool things that are in there: Firstly, how the decorator is used to provide a descriptive name, thus allowing us to completely bypass trying to think up descriptive test function names and instead simply describe the test in plain English. I can see how this makes documenting the test a little bit easier. Secondly, how tags (which in land would be ) are simply added as part of the decorator. The syntax is definitely attractive. That said, while I like the syntax, one thing that might prevent me from using at this moment in time is the lack of clarity with how it interfaces with , which I use to do testing of machine learning model implementations. I might do some digging in the near future. For now, just leaving this note here for future reference.",
    "tags": [
      "til",
      "software testing",
      "tooling"
    ],
    "pub_date": "2021-05-29",
    "type": "blog"
  },
  {
    "id": "blog-help-mathjax-render-correctly-by-wrapping-latex-in-span-tags",
    "url": "/blog/2021/5/28/help-mathjax-render-correctly-by-wrapping-latex-in-span-tags/",
    "title": "Help MathJax render correctly by wrapping latex in span tags",
    "summary": "Today I learned how to guarantee correct rendering of LaTeX equations with MathJax when placed inside Markdown documents.",
    "body": "Today I learned that we can help MathJax guarantee correct rendering of LaTeX in HTML and HTML-converted Markdown documents by simply wrapping the latex inside a tag. For example, if the following equation gives you trouble because the underscore gets parsed by a Markdown parser incorrectly: If it is rendered correctly, you'll see: $y{alpha} = 3x^2 + 5$ However, if the rendering is butchered somehow, then we can simply add tags around it to guarantee correct parsing: With the span tags, it'll look like: $y{alpha} = 3x^2 + 5$. (Inspect the website source to see how I did it.) This tip should apply anywhere we use LaTeX inside Markdown and wish to have it render correctly in HTML using MathJax. Some examples of documents where this might be useful is in: 1. Jupyter Notebooks 2. vanilla Markdown files, 3. Markdown blocks in YAML files that get parsed.",
    "tags": [
      "til",
      "blogging"
    ],
    "pub_date": "2021-05-28",
    "type": "blog"
  },
  {
    "id": "blog-grammarly-is-surprisingly-useful",
    "url": "/blog/2021/5/19/grammarly-is-surprisingly-useful/",
    "title": "Grammarly is surprisingly useful",
    "summary": "Over close to a year of using Grammarly has told me that it's been a well-worthwhile purchase. I detail a bit on why.",
    "body": "Grammarly is surprisingly helpful. As a native speaker of English and who once nearly chose to do English Literature as part of my A-Level coursework, I pride myself on having a good command of English grammar, vocabulary, and literary devices to communicate effectively. That said, language habits that form over time become hard to change. Over-used passive voice, wrong placement of commas, over-use of a single word, and many more bad habits can conspire to muffle my message. In using Grammarly, I find that it helps me be much more aware of myself when writing. I'm not kidding when I say that my experience writing is just like the advertisements portray it: write something and use the immediate feedback to iterate quickly. Frequently I find that my writing after Grammarly edits is much more concise and to-the-point. It's something I would recommend trying out. The premium version might be worth it for individuals whose main day-to-day work relies heavily on writing; otherwise, the free version is a great preview of what it's like!",
    "tags": [
      "writing",
      "random",
      "reflections"
    ],
    "pub_date": "2021-05-19",
    "type": "blog"
  },
  {
    "id": "blog-set-environment-variables-inside-a-jupyter-notebook",
    "url": "/blog/2021/5/14/set-environment-variables-inside-a-jupyter-notebook/",
    "title": "Set environment variables inside a Jupyter notebook",
    "summary": "Did you forget to set an environment variable before launching Jupyter? Today, I learned how you can rescue that situation easily.",
    "body": "Today, I learned that one can set an environment variable from within a Jupyter notebook session. Let's say you needed an environment variable set for your Jupyter notebook, but you: 1. Don't want it set globally or project-wide, or 2. Forgot to set it before you launched Jupyter. To do so, add the following line(s) to the very, very first cell of the notebook: Within the same code cell, you can clone that as many times as you want to set environment variables of any kind. To verify that the environment variable has been set correctly, you can use the following code block to view the value of the environment variable that you set: Now, one thing has to be stated - according to 12 Factor App development principles, you might need to set environment variables for sensitive information. You should never, ever, ever set those environment variables at the top of your notebook because they will be exposed!",
    "tags": [
      "til",
      "jupyter",
      "data science"
    ],
    "pub_date": "2021-05-14",
    "type": "blog"
  },
  {
    "id": "blog-desktop-python-applications-with-flask",
    "url": "/blog/2021/5/12/desktop-python-applications-with-flask/",
    "title": "Desktop Python applications with Flask",
    "summary": "Today I learned that it's possible to create a Desktop app that uses Flask (and hence HTML/CSS/JS) to render the front-end. Read on to see what packages are involved.",
    "body": "Today, I learned that it's possible to create a desktop app that uses Flask as the webserver, Python for application logic, and HTML/CSS/JS for the front-end. The logic basically that we split the desktop app code into the front-end and back-end, and we let the front-end and back-end communicate data using JSON. The backend code does whatever complex logic is needed, while the front-end code simply does the rendering. It's basically a very familiar pattern for server-based applications. One tool that helps with this is [](https://github.com/ClimenteA/flaskwebgui), by GitHub user . I found out about this tool by reading a dev.to article on using FastAPI to create a desktop application. I also saw another article on using Svelte.js + Flask to create a Python app. That one also looks like a simple pattern to follow, with the advantage that Svelte.js makes reactivity very simple. Of course, if you're looking to stay in a single language without this separation of concerns, it's feasible to stay with a single framework, like Streamlit, ipywidgets, Panel, and more. Indeed, for some projects, it may be more pragmatic to stick with a single language depending on the language skillset of those working on that project.",
    "tags": [
      "python",
      "software development",
      "til"
    ],
    "pub_date": "2021-05-12",
    "type": "blog"
  },
  {
    "id": "blog-canada-exports-education",
    "url": "/blog/2021/5/6/canada-exports-education/",
    "title": "Canada exports education",
    "summary": "Today I learned that Canada exports education.",
    "body": "Today, I learned that one of Canada's exports is actually education. It sounds weird, but I learned this from Economics Explained, and I think I can follow the logic. A foreign student who studies in Canada brings money into the country. Four years later (or longer), Canada sends that student back to their home country. Canada's economy gets money up-front, while the foreign country gets a smarter student later on. Economists have a very different way of looking at the world. Here is the relevant video.",
    "tags": [
      "til"
    ],
    "pub_date": "2021-05-06",
    "type": "blog"
  },
  {
    "id": "blog-probability-simplex",
    "url": "/blog/2021/5/5/probability-simplex/",
    "title": "Probability Simplex",
    "summary": "Today I learned about the definition of the term \"probability simplex\". Here's what it is.",
    "body": "Doing another TIL (as inspired by Simon Willison), this time being the definition of a probability simplex. I gleaned a pretty good definition of the probability simplex from this article. From a computational perspective, it is represented as: 1. a vector of $K$ numbers ($K$ being the dimension of the simplex) that lie between 0 and 1, such that 2. their sum is equal to 1.0 (it is a probability vector, after all), and 3. each vector slot represents a choice mutually exclusive with others. Here are a few examples. - A probability simplex where $K = 2$ might be the vector $(0.15, 0.85)$. - A probability simplex where $K = 3$ might be the fector $(0.10, 0.50, 0.40)$. Probability simplices are usually used as the probability parameter in the Multinomial distribution. (Reminder: the Binomial is a special case of the Multinomial.) In Bayesian inference, we also place priors over the probability parameter (or probability simplices) to express what we might believe about the relative tendency to pick one choice over another.",
    "tags": [
      "bayesian",
      "probability",
      "til"
    ],
    "pub_date": "2021-05-05",
    "type": "blog"
  },
  {
    "id": "blog-nudge-for-your-flu-shot",
    "url": "/blog/2021/5/3/nudge-for-your-flu-shot/",
    "title": "Nudge for your flu shot",
    "summary": "Nudges are fascinating, useful, and in some cases, potentially life-saving. I recently read a paper that talked about life-saving nudges and reflected on what made these nudges effective.",
    "body": "I'm a fan of nudges that help us make better decisions for society. In this recent study published in PNAS, researchers found that text nudges could boost vaccination rates by up to 5 percentage points. The text nudge was one that, in my opinion, removed complexity for the individual. Consider the following selection of nudges (lifted from the paper): - Flu shot reserved for you - Share a joke about the flu - Improve the flu shot rate in your region If you were to rank-order the three options, which would you find to be the most effective? Looking at the results of the paper, it was the \"Flu shot reserved for you\" text, out of the 19 interventions tested (including the three above) that worked the best. This result immediately prompted thoughts: why? I can't claim to fathom what goes on in people's minds, but on doing some post-reading self-reflection, I arrived at the following answers for myself. If I were given the first text message over the third, I would: 1. Perceive myself as being treated with care by the healthcare system. (\"We reserved something good for you.\") 2. Find the decision much easier to take (\"yes\" vs \"no\"). 3. Find the interaction much more lightweight (flash binary decision vs. having to think through possibilities). 4. No additional action was needed, the flu shot is given as a byproduct of doing a routine clinical visit. In thinking through these options, I can see why such nudges would be the most effective of them all.",
    "tags": [
      "reflections",
      "til"
    ],
    "pub_date": "2021-05-03",
    "type": "blog"
  },
  {
    "id": "blog-preview-built-static-sites-on-netlify",
    "url": "/blog/2021/4/28/preview-built-static-sites-on-netlify/",
    "title": "Preview built static sites on Netlify",
    "summary": "I learned a new thing: how to enable Netlify site preview URLs to be commented back on a PR thread! A story about how the collective intellect of the world is such a wonderful resource to query.",
    "body": "Twitter is an amazing universe. I asked a question, and friends came to help. I use Netlify to host my site previews. That said, I use GitHub actions to build the site, as site-building on Netlify has a limit of 300 minutes, while for my open source projects, I have unlimited minutes. I was curious about how to enable static site previews on Netlify with an automatic comment on the PR housing the URL. Peter Bull kindly replied, and that's when I learned that GitHub user @nwtgck created a GitHub Action that makes this easy. Inside one of our GitHub actions configuration YAML files, we add this step: The result looks something like this: Now, it's much easier for me and collaborators to find the preview URL rather than dig through a bunch of logs to find it. When one can query the collective intellect of the whole wide world, that's an amazing feeling.",
    "tags": [
      "netlify",
      "web development",
      "continuous integration",
      "til"
    ],
    "pub_date": "2021-04-28",
    "type": "blog"
  },
  {
    "id": "blog-publishing-data-with-datasette",
    "url": "/blog/2021/4/26/publishing-data-with-datasette/",
    "title": "Publishing data with Datasette",
    "summary": "Recently, I saw Datasette resurface on my Twitter feed, and I started doing a deep dive into it once again. The work that Simon Willison, author of Datasette, has done -- it is amazing! Here's what I learned test-driving it once again.",
    "body": "I think the Datasette ecosystem of tools is amazing. Built basically single-handedly by Simon Willison, it provides a way to turn collections of CSV files into read-only collections of SQLite databases that one can serve on the web. Here are a few use cases of Datasette as described by Simon on the official website: 1. Journalists and academics can publish the data that they use or generate onto the web. 2. Web developers can stand quickly stand up a mock database for developing applications, complete with web APIs, without needing to go to more expensive solutions. I also remember having the pleasure of meeting Simon at PyCon 2019 while in line for food! In some sense, while I enjoy remote work, I do miss being at an in-person conference. The balance where we do remote work but getting together once in a while to do in-person events seems like the right balance for me. Combine multiple data files into a single database I recently wanted to share some data from a publication that I had co-authored with colleagues. Primarily, there were two CSV files - one that was about 100MB in size and the other was 10MB in size. The flat CSVs were a tad overloaded in information, so I did a \"data refactor\" (a.k.a. data normalization in database terms) and pulled out some of the columns into independent CSV files as needed. (I used pandas and a Jupyter notebook to do that processing.) Then, I put them back into a single SQLite database using Datasette. This is where Datasette's suite of tools really shines. One CLI tool, which is called , has a wonderful command-line interface for handling this task: Essentially, takes as many CSV files as you want and stashes them into a single SQLite database. Doing so allows you to convert logical collections of CSV files into a single database file that is now queryable using SQL syntax. Compared to sharing a collection of CSV files that might be up to hundreds of files long, sharing a single SQLite database as a logical collection is much easier. This ability to quickly get a logical collection of data into a single data container (the SQLite database) is handy! (It's no wonder Datasette is labelled as a multi-tool, like a Swiss Army knife. Publishing Publishing data is the second piece that Datasette excels in. Built-in to Datasette is a web server for browsing the contents of a SQLite database. To launch the server, you use the following command on your local machine: Now, you can go to your local host on a port that specifies for you and view it. For example, I might get port 8001, so I would go to on my browser to view the contents of the database. This allows you to serve files on a local network. But what if you wanted to publish the database across the whole wide world? That's where Datasette's dataset publishing capabilities really start to shine. Datasette comes with several publishing plugins that let you deploy a Docker container housing the SQLite database onto PaaS providers such as Vercel, Heroku, and Fly. (I think AWS is also supported, but I also remember Simon ranting on Twitter about how hard some parts of AWS were.) To use the , you will need to have installed in your environment alongside one of the hosting plugins that Simon has generously built. In my case, I used the Fly plugin , which one installs into the same environment as itself. After logging into Fly's command-line interface, we then execute the following command: After a short while, the data will be viewable at ! Now, what happens behind the scenes? Basically, automates the construction of a Docker container. The first build will usually take a while, but once the build is done and the container deployed, you'll get a URL returned to you in the Terminal at which you can view the Datasette instance with your data served up. For example, check out the Drosha data that I deployed on Fly. Some comments on using datasette In using Datasette, knowing some key concepts about databases is highly recommended. These include primary keys, foreign keys, data types, and more. For example, if you have a column in your CSV files that basically acts as an indexer across multiple tables, you might want to consider extracting it to a separate lookup table using 's flag. Here's one practical consequence of doing this. Suppose you have a categorical column with repeated values. In a CSV file, that column's values are stored as a raw string over and over; multiply it by tens of thousands of rows, and you might end up with that value occupying megabytes of data. Instead of recording the raw string repeatedly, one can store an integer-to-string mapping in a separate table while recording the integer value in the original table, thus saving on storage. (Indeed, I was able to shave off about 25% of the size of the original files put together using this trick.) It goes without saying that knowing something about the relationships in your data will help a ton too.",
    "tags": [
      "data",
      "datasette",
      "data publishing",
      "data engineering",
      "data science"
    ],
    "pub_date": "2021-04-26",
    "type": "blog"
  },
  {
    "id": "blog-learning-css-by-styling-a-resume",
    "url": "/blog/2021/4/24/learning-css-by-styling-a-resume/",
    "title": "Learning CSS by Styling a Resume",
    "summary": "Once in a while, I make it a practice to hack on my resume. The goal isn't explicitly to keep it relevant, though. Instead, I use it as an excuse to practice CSS. The result is something I'm pretty pleased with. Come read about how I did it!",
    "body": "Once in a while, I make it a practice to hack my resume. The goal isn't explicitly to keep it relevant, though. Instead, I use it as an excuse to practice CSS. (But then again, since I have it open anyways, I update the content along the way.) Since I've been on leave for a while, I decided that it was a good idea to update my resume design. The previous iteration was based on Nord, but my design choice felt a bit dark and bland simultaneously. This time around, I decided to spruce things up a bit. The result is something I'm pretty pleased with. Design process To gather inspiration, I browsed several designs that were freely available online. Amongst those that I personally found aesthetically pleasing and informative, I noticed that: 1. There were a lot of whitespaces. By contrast, my old resume site was visually dense. 2. Prose was deployed effectively for clarity. By comparison, my previous resume site was a tad brief in my skills section. 3. The colour scheme was well-defined with tasteful colour accents. By contrast, my old resume site was a bit overwhelmingly dark. 4. Resumes that were relatable had a headshot of the person. For some reason, seeing a face is quite important. One of the designs that I saw used a left-right layout for each item in education and experience. For example, with work experience, the left third of a Bootstrap card would contain the role title, with a short description placed on that card's right side. One example I saw also included an aesthetically pleasing pine green background colour on the left third. That became one of the biggest design inspirations that I used. Additionally, I revisited the Nord theme colours. I rediscovered through to , the non-blue/gray colours that are part of the standard palette. They were gorgeous colours! Since my resume already had sections, I decided that the colours could help visually identify each section. I saw two CSS features in other resumes and decided to incorporate them. They were (1) drop shadows for the cards and (2) hover scaling to give the resume a bit of additional \"pop.\" Those turned out to be easy-to-implement yet beautiful interaction elements that I could include. Along the way, I learned about CSS transitions and scaling too. Finally, in all the examples that I reviewed, the ones that were the cleanest had no icons on the headers. Icons next to headings were something I used to have because I thought it looked cool. I was also using it to getting familiar with FontAwesome's icon sets. However, my personal aesthetic has changed now. I now think the resume looks less cluttered and much cleaner without the icons. Now, far from setting requirements and sprinting one-shot towards the final product, I actually iterated over a few ideas and refined them towards the final design that I settled on. Everything I wrote above is a crystallized summary of the process, which in reality, was much messier! Some code snippets I wanted to highlight two technical pieces here: (1) implementing the scale transition on hover and (2) implementing cards with headers on the left and content on the right. Both of these assume some degree of familiarity with Bootstrap CSS and its card system for laying out content. Scale transitions in CSS The effect I was going here is to make a particular card increase in size when the user's mouse cursor hovers on top of the Bootstrap card. To do so, I wrote two CSS classes, one to control transitioning and the other to control hover on cards: I then applied the and CSS classes to every card that I wished to have hovering enabled for: Now, anytime someone mouses over a card with those two CSS classes applied to them, the card and all of its child elements will increase in scale to 1.05x in size. Left-right organization on card contents To implement left-right organization on card contents, we actually nest a zero-gutter () row inside the card, and then use the and (the numbers should add to 12, ideally) classes on two divs to split the card horizontally: To implement colours, I used the Nord CSS variables in my custom CSS class definitions. For example, you'll notice the class definition used in the left side of the card: Now, the left side of the card will have a blue background. Screenshot Having described the design process and outcomes, here's a link to the final product. And here's a full screenshot of the page in case you're feeling lazy :). Lessons Learned This was a fantastic exercise, crafting a personal resume to my liking. In particular, I enjoyed discovering new things about CSS that I didn't know before. I also enjoyed bringing some pop and colour into my resume. Having previously structured my resume in a pretty sane YAML format made it much easier for me to change what I thought needed reworking. Since a resume is primarily composed of data, this reinforced for me that storing data in a sane data structure is supremely important. Counterfactually, if my resume were entangled with the HTML by being",
    "tags": [
      "css",
      "html",
      "career development",
      "coding",
      "programming",
      "design"
    ],
    "pub_date": "2021-04-24",
    "type": "blog"
  },
  {
    "id": "blog-grammatically-composing-network-visualizations",
    "url": "/blog/2021/4/2/grammatically-composing-network-visualizations/",
    "title": "Grammatically composing network visualizations",
    "summary": "I've been working on revamping the API to much more closely align with the grammar of graphics' principles and other data visualization best practices. Come check out the ideas backing the API revamp!",
    "body": "This past month, I've been revamping the package, a package I created in graduate school to build network visualizations on top of NetworkX and Matplotlib. (You can see the work being done in [this PR branch][pr]!) [pr]: https://github.com/ericmjl/nxviz/pull/658 It's been a bit neglected! The biggest reason, I think, is the lack of a grammar for composing network visualizations, which then leads to hairball being created. Sometimes the hairballs are beautiful; most of the time they are uninformative. Flowing from a lack of grammar means the architecture of the codebase is also a bit messy, with one example being annotations being too tightly coupled to a particular plot object. Network visualization workflow To revamp the API, I started by structuring the API around the following workflow for constructing graph visualizations. As such, I decided to refactor the API, structuring it around what I know about the grammar of graphics. Firstly, because the core idea of is rational graph visualizations that prioritize node placements, the refactor started there, with node layout algorithms that group and sort nodes according to their properties. Once we have node placement figured out, we can now proceed to node styling. Mapping data to aesthetic properties of glyphs is a core activity in data visualization; in , the styling that can be controlled by data are the node size (radius), color, and transparency. Following that, we need algorithms for drawing edges. Do we simply draw lines? Or do we draw Bezier curves instead? If we draw curves, what are the anchor points that we need? Sane defaults are provided for each of the plot families in . As with nodes, we then add in aesthetic styling that are mapped to a given edge's metadata. In , the edge styling that can be controlled by data are their line width, color, and transparency. We then add in annotations onto the graph. Are there groups of nodes? If so, for each node layout type, we can add in group annotations in sane locations. Are colors mapped to a quantitative or qualitative variable? Legends or color bars can be annotated easily. Finally, in some cases, it may be helpful to highlight a node, a particular edge, or edges attached to a node; we might use such highlights in a presentation, for example. The final part of the refactor was to add in functions for adding in each of these highlights in an easy fashion. Summarized, the flow looks something like this: 1. Layout nodes. 1. Style nodes according to node properties. 1. Connect nodes by drawing in edges. 1. Style edges according to edge properties. 1. Annotate visualization with auxiliary information. 1. Add highlights. Handling node layouts with cloned node axes Some node layouts involve cloning the axis on which the nodes live. Examples of this include the matrix plot, in which we lay out the nodes on the x-axis, and then clone them onto the y-axis, canonically in the exact same order (though not always). These were much easier to write once the components underneath were laid out in a sane fashion. Supporting data structures Some data structures were immensely important in the API redesign, and turned out to be foundational for almost everything. Every graph's node set can be represented as a data table. In the node table, nodes are the index, while the metadata are the columns. In the edge table, two special columns, and , indicate the nodes between which an edge exists, and the rest of the columns are the metadata. A node position dictionary that maps node ID to (x, y) coordinates was also something extremely important. This is calculated by the node layout algorithm and used by the edge drawing algorithms and elsewhere in the library. Worrying about the positions of nodes is a page taken out of NetworkX's drawing functionality directly, and I have to give the developers much of the credit for that. The global stateful nature of Matplotlib turned out to be a blessing in disguise. Rather than having a user pass around an axes object all of the time, in we simply assume that there is a globally available axes object. Optimizing for composability and workflow In redesigning the API, I wanted to make sure that the workflow of creating a network visualization, done in a principled fashion, was something that the API would support properly. As such, the API is organized basically around the workflow described above. A lesson I learn over and over in writing software is that once the workflow is clear, the API that's needed to support it also becomes quite clear. And when does the workflow become clear? Usually, that clarity comes when the important logical steps that are composable with one another are well-defined, and the core data structures that support that workflow are present. Bringing a grammar to network visualization The Grammar of Graphics, , Seaborn, Altair, and Holoviews were all inspirations for nxviz's functional and declarative API. Network visualization appears to be an under-developed area",
    "tags": [
      "data visualization",
      "open source",
      "python",
      "matplotlib"
    ],
    "pub_date": "2021-04-02",
    "type": "blog"
  },
  {
    "id": "blog-phony-targets-in-makefiles",
    "url": "/blog/2021/3/4/phony-targets-in-makefiles/",
    "title": "Phony targets in Makefiles",
    "summary": "What are phony targets in Makefiles? Or have you not heard of them before? Here's a short post documenting a minimal example of what phony targets are.",
    "body": "Makefiles can be used to create a lightweight CLI entry point for your project. By writing a , you can automate repetitive shell commands, such as building a Dockerfile (which might require you to remember certain incantations): Doing so allows us to build a Docker container using the command when at the command line. Now, you'll probably notice the line at the top. What's that all about? By convention, the command would have referred to a file target on disk. (This is how Makefiles were originally designed.) However, if we want the command to be independent of any files on disk, we declare it to be a phony target that doesn't actually exist on disk.",
    "tags": [
      "automation",
      "computation",
      "reproducibility"
    ],
    "pub_date": "2021-03-04",
    "type": "blog"
  },
  {
    "id": "blog-machine-learning-system-design",
    "url": "/blog/2021/3/1/machine-learning-system-design/",
    "title": "Machine learning system design",
    "summary": "In which I share some resources on MLOps and ML Engineering.",
    "body": "I think we're past the age of building fancy models. After about a decade or so of seeing where models could make an impact on decisions being made, I think we're entering a phase where operationalizing these models is more important than building bigger and more powerful models. The field of data science is evolving yet again, this time with a big offshoot branch with the name \"Machine Learning Engineering\". The key problem here is that most data scientists don't possess the software and engineering skills to build and operationalize machine learning systems for colleagues. There's a big opportunity here to make machine learning systems work. The key here is to design machine learning systems the way software engineers design software systems: modular, with clean contracts, minimizing entanglement, etc. etc. There are very few resources that I know of that can help us learn what needs to be done, but here are a few that I have encountered. 1. [Machine Learning Systems Design][mlsd] by Chip Huyen. 2. [Machine Learning Engineering][mle] by Andriy Burkov 3. [MLOps][mlops] by @visenger [mlsd]: https://stanford-cs329s.github.io/index.html [mle]: https://leanpub.com/MLE [mlops]: https://ml-ops.org",
    "tags": [
      "machine learning",
      "system design",
      "data science"
    ],
    "pub_date": "2021-03-01",
    "type": "blog"
  },
  {
    "id": "blog-configure-tmux-and-zsh-to-play-nicely",
    "url": "/blog/2021/2/28/configure-tmux-and-zsh-to-play-nicely/",
    "title": "Configure tmux and zsh to play nicely",
    "summary": "If you're having problems with , , and your PATH environment variable like I did, learn from my experience: the interwebs gave me a way to get everything to play nicely.",
    "body": "I have been encountering this problem where reloads my PATH environment variable incorrectly. As it turns out, the fix is documented in [a StackOverflow post][so]. [so]: https://stackoverflow.com/questions/13058578/how-to-prevent-tmux-from-filling-up-the-global-path-variable-with-duplicated-pat The crux of the problem is as follows: When loads, my is re-evaluated. By setting the following line in my configuration file: I can now prevent from being instantiated as a login shell. (If you're now wondering what in the world is a \"login shell\", fret not, I had to search online too. Here's the best answer I found, this time on [StackExchange][se].) [se]: https://unix.stackexchange.com/a/46856",
    "tags": [
      "shell",
      "programming"
    ],
    "pub_date": "2021-02-28",
    "type": "blog"
  },
  {
    "id": "blog-experience-with-m1-macbook-air",
    "url": "/blog/2021/1/27/experience-with-m1-macbook-air/",
    "title": "Experience with M1 MacBook Air",
    "summary": "Some of my early impressions using the M1 MacBook Air for data science purposes.",
    "body": "Though I've been a Mac user for close to 15 years now, it's the first time that I've had to experience an architecture change. Having upgraded from an old 12\" MacBook to a new 13\" MacBook Air, I wanted to quickly document some of my early experiences with the M1 MacBook Air. Initial Ordering The online shopping experience is something that I have to mention and give full credit to Apple. They've enabled customers to order a MacBook Air at noon and get it by 2:30 pm, at least in the Boston area. I think this was possible because the model I ordered, a 16GB RAM + 1TB SSD MacBook Air, was available and in-stock in their South Shore store. Good move, Apple; I was pleasantly delighted at being able to make this purchase. (Hopefully, I get a good four years of use out of it, as I did for my little MacBook!) Installing tooling My tools primarily involve the following: 1. Homebrew for system-wide packages. 2. Anaconda for the Python data science stack. 3. VSCode for software development. 4. Jupyter for notebook experimentation. I started by installing as much as I could of my stack under native (ARM) mode rather than emulated (x86) mode. Here's my report on what worked and what didn't. Homebrew As of 26 Jan 2021, Homebrew has partial support for the M1 ARM processors. Installing Homebrew was not difficult. Open up the native Terminal, type in the official Homebrew installation commands, and see it go to work. No problems encountered. As an experiment, I removed the ARM-compatible Homebrew installation and re-installed it in emulation mode. To do this, I opened up the Hyper terminal, which was not ported over to M1 yet, and ran the official Homebrew installation commands. Likewise, I encountered no problems. As I have been a bit lazy, I did not revert to ARM Homebrew. ARM Homebrew and x86 Homebrew get installed into two different directories. The ARM goes into , while the x86 version goes into the usual . Just keep this point in mind; you might find it handy in the future. Anaconda Following the best practices I outline in my knowledge base, I install Miniconda on my machine. After reading some blog posts online, I discovered that there's miniforge, a Miniconda distribution that comes with native ARM compatibility. As such, I started by installing that first. Soon, however, I found that there were some environments that I couldn't build. These included the one for this website, and anything involving JAX. Initially, I considered living with the situation and using a remote development machine, but I soon felt that nagging feeling that I was leaving a lot of awesome compute on the table by doing that. As such, I uninstalled Miniforge and installed Miniconda in emulation mode. As with Homebrew, I got Miniconda going by running the installer inside Hyper. After the installation was complete, I could build all of my conda environments with ease. Having installed Miniconda, I went to work benchmarking a test suite for [](https://elarkk.github.io/jax-unirep/), an RNN model that my intern Arkadij and I accelerated together. One of my projects uses JAX, which I could not install in native ARM mode. The exciting thing for me is that I saw the test suite run with no issues! Also, I noticed that the slow tests ran at approximately twice as fast on my computer than on GitHub Actions. This anecdotal benchmark is enough for me to gain confidence in using JAX on this laptop. If you want numbers, though: - GitHub Actions for jax-unirep tests: - 45 seconds for fast test - 3m 35 seconds for slow tests - M1 MacBook air (while running on battery power in emulation mode) - 43 seconds for fast test - 1m 25 seconds for slow tests Docker Docker has a technical preview for the M1 Mac, and it has only crashed once thus far. (I didn't have enough time to dig deep and figure out why.) Other than that, I was able to install the tech preview with no hassle and even use it to build a development container in VSCode for , in which I made a few pull requests into the library. VSCode To get VSCode, I had to install and use the Insiders build for ARM64. No big hiccups here, everything installed without issues. As I mentioned above, building development containers involved zero hiccups. Jupyter The Jupyter notebook server ran in x86 emulation mode with no issues as well. I installed the latest jupyter lab 3.0.5, which ran with no hiccups, was snappy in the browser, everything behaving in line with what I was accustomed to. Julia Language I also went ahead and installed Julia both from conda-forge (it is a bit dated, at v1.1.1) and from the official Julia language website. Installation went smooth; running had no hiccups. Overall two-day impressions As someone who's used a Mac exclusively since 2006, nothing substantial has changed on the surface. That's something worth mentioning: Apple does it again, making a technically complicated technological transition without letting an end-user know as much. Doing this is insanely remarkable! I've done calls, ",
    "tags": [
      "data science",
      "macbook",
      "macos",
      "arm",
      "m1"
    ],
    "pub_date": "2021-01-27",
    "type": "blog"
  },
  {
    "id": "blog-dispatch-rather-than-check-types",
    "url": "/blog/2021/1/24/dispatch-rather-than-check-types/",
    "title": "Dispatch rather than check types",
    "summary": "I finally figured out how multiple dispatch works. Come read on to see my learnings here :).",
    "body": "I finally grokked an old thing! The thing I've been trying to understand properly is \"how and when to do single/multiple dispatching\". It's an idea that is heavily baked into the Julia programming language, and one for which I rarely see the pattern being used in Python. But after reading Michael Chow's blog post on single dispatching, I finally realized how dispatching gets used. Let me explain with an example. Let's say you have a function that can process one column or multiple columns of the a dataframe. As a first pass, you might write it as follows: Here are the problems I see: 1. Notice how nested that function is. It definitely goes against the Zen of Python's recommendation that \"flat is better than nested\". 2. At the same time, the function silently fails for data types that are not explicitly supported -- also an anti-pattern that should not be encouraged. Here's the same function written instead as two functions that share the same name, leveraging the multipledispatch package by Matt Rocklin: Notice how now each function is: 1. Flatter 2. Self-documented because of the type annotations used as part of the call. 3. Easier to see when the function will fail. The only downside I can think of to the \"dispatch\" programming pattern is that one must know the concept of \"dispatching\" before one can understand the syntax. The one who is unfamiliar with this concept might also be thrown off by declaring the same function twice. That said, I think the benefits here are immense though!",
    "tags": [
      "programming",
      "coding"
    ],
    "pub_date": "2021-01-24",
    "type": "blog"
  },
  {
    "id": "blog-librofm-drm-free-audiobooks",
    "url": "/blog/2020/12/27/librofm-drm-free-audiobooks/",
    "title": "Libro.fm: DRM-free Audiobooks",
    "summary": "I think I finally found a proper audiobook store that I'm willing to support! Come check out Libro.fm, or read on to see why I like it.",
    "body": "As COVID-19 raged on, I decided it was time to pick up the habit of listening to Audiobooks. There were convergent factors that helped me make this decision. Firstly, many hours of holding a child get boring if the child is sleeping. Audiobooks help do the job of alleviating boredom, as it means I can learn hands-free. Secondly, the company I work for has a benefit that let me test-drive audiobooks via Audible.com. I went crazy and downloaded 5 audiobooks into my Audible catalogue (that was the limit per month, I think, but I'm not quite sure). The book \"How will you measure your life\", by Harvard Business School professor Clayton Christensen, was such a wonderful audiobook to listen to that I was hooked. Thirdly, I have known for a long time that my first exposure to ideas is best done audibly, with hands-on experience and text reinforcing what I have heard. If I were to start engaging with ideas in text form first, then I would end up getting lost pretty quickly. Since audiobooks get the job of \"entertaining and teaching while hands-free\" done, I decided to take the plunge. Deciding on Libro.fm over Audible I started out with Audible because it is part of the benefits package that my company, Novartis, offers to its employees. However, I very quickly became dissatisfied with the offering. In particular, I was acutely aware of the lock-in I was buying into by going with Audible. And being an open-source developer and data scientist, I have this heightened \"lock-in\" radar (much as some individuals I know have heightened \"BS\" radar) that I actively try to avoid. Audible reeked of lock-in, so I went on the search for a different audiobook provider. That is where Libro.fm comes in. It has a comparably large library of audiobooks, and the authors whose works I was most interested in hearing were all available up there. The parts of their service that caught my eye the most though were two-fold: 1. All of their audiobooks are DRM-free. This means I can download the MP3 version for myself and keep it for as long as I remember it's there. 2. They support local bookstores through their sales. (I chose a bookstore in Porter Square, in Cambridge.) So rather than line the pockets of executives at Amazon, I decided to support local bookstores through my purchases. It felt like the right thing to do overall. Other than that, their app is well-designed and easy to use. I'm a tad miffed that the two books I have chapters named \"Track 1\", \"Track 2\" etc. instead of the chapter names, but that's a minor thing that I will be giving them feedback on.",
    "tags": [
      "personal"
    ],
    "pub_date": "2020-12-27",
    "type": "blog"
  },
  {
    "id": "blog-moving-my-ci-pipelines-to-github-actions",
    "url": "/blog/2020/12/24/moving-my-ci-pipelines-to-github-actions/",
    "title": "Moving my CI pipelines to GitHub Actions",
    "summary": "I had a bit of an adventure today beginning my move off Travis CI and onto GitHub Actions. GitHub Actions is a wonderfully built tool, and I'm a fan. Come check it out!",
    "body": "Today, I began the long-awaited migration of my CI pipelines off from Travis CI and on to GitHub Actions. I started with my Network Analysis Made Simple tutorial repository as my test-bed, for the following reasons: 1. It has a sufficiently complex workflow for which parts can be parallelized. 2. It is something I have been actively curating and developing, so I am familiar with what I need in this CI pipeline. 3. I strive actively to apply \"good software practices\" to that repository, and it has a good starting point. In this blog post, I'm going to document what I did and why, as well as lessons learned from migrating a CI pipeline off one provider onto another. Engineering the pipeline for modularity The biggest architectural change I did here was to split up my build pipelines into logical \"jobs\". I defined three logical stages: 1. Building the environment 2. Building the website (which requires the built environment) 3. Building the LeanPub book (which also requires the built environment) Building the environment takes approximately 3-6 minutes (depending on the performance of the machine that runs the install steps). Building the website requires executing all of the notebooks from top to bottom. In authoring the notebooks, I try my best to ensure that they can be executed without errors in under 15 seconds, though 1 minute is the upper-bound acceptable. MkDocs is the site building engine, while gives me the ability to execute the notebooks as part of the build. (A little interjection here: I also put in a PR to to enable insertion of a Binder link at the top of a notebook, so that readers can directly open a Binder session with the notebook!) The site that gets built is then force-pushed into the branch of the repository. In total, this step takes about 5-7 minutes. Building the LeanPub book also requires executing all of the notebooks programmatically. I take advantage of the MkDocs configuration file to order the content. I used a Jupyter notebook to prototype the script that builds the content, though I also subsequently went lazy and decided to just execute the notebook from top-to-bottom directly in a programmatic fashion too. (I am not sure when I'll come to regret this move, but hey, at least there's in-line documentation (Markdown cells) that explains what I'm trying to accomplish.) The manuscript that is built gets force-pushed to the branch of the repository, and I do a curl call to the LeanPub API to trigger a new build of the book. In total, this step takes about 5-7 minutes as well. Experimenting with parallel builds Building the website and LeanPub book can run in parallel as soon as the environment is built. Hence, I took advantage of this fact on GitHub Actions, which: 1. Allows me to specify which jobs depend on each other, and 2. Automatically runs builds in parallel. I could write the pipeline such that it would build the environment once, package it up as an artifact, and re-use the environment in both of the downstream steps, thus allowing me to parallelize the two steps. Engineering the pipeline to be modular The logical steps involved in both of the pipelines are identical except for the deployment because in the case of Network Analysis Made Simple, \"testing\" equals to \"ensuring the notebooks execute from top to bottom\". (Effectively, I have no unit tests on the tutorial repo, just integration tests.) As such, I took this opportunity to engineer the pipelines to be modular and work on both systems. To start, I made the build steps into individual bash scripts that are stored in the directory. The scripts stored here are, in order of operation: 2. : used to build the environment for the tutorial, and package it up using . 3. : used to build the source files that power the LeanPub book. 4. : used to build the files that are hosted on GitHub Pages. In terms of workflow, the order of operations is: In GitHub Actions, I structured the pipelines as different \"jobs\". Because user has a action available, I can skip over the step, and leverage what he has instead. After building the environment with , which includes a step to build a tarball with the environment packaged up, I used the action to take the built tarball and upload it as an artifact of the build. This allowed me to share the built environment between the job that builds the environment and the jobs that builds+deploys the website and book. The two deployment steps were quite similar, with the only difference being the build script that is called. Otherwise, before the build script, I would use the action to download the built artifact from the previous job, and execute their respectively configured actions to deploy the websites. On the LeanPub push, I would also execute a command to a web API (available only to Pro users, which I gladly paid for) to trigger the publishing of a new version. Finally, I wanted to make sure that my code adhered to code standards. As such, I made an extra job that runs independently, in which",
    "tags": [
      "data science",
      "network analysis made simple",
      "tutorial",
      "continuous integration",
      "pipelines",
      "github actions"
    ],
    "pub_date": "2020-12-24",
    "type": "blog"
  },
  {
    "id": "blog-graphs-language-and-categories",
    "url": "/blog/2020/12/18/graphs-language-and-categories/",
    "title": "Graphs, language, and categories",
    "summary": "I had an epiphany over the week, connecting graphs, programming, and category theory. I might be wrong, but if you're intrigued, come on in and check it out.",
    "body": "Just as I began vacation, my brain started going into overdrive. (This is a quirk of my head.) One of the epiphanies I arrived at involved graphs, programming languages, and categories. Let's talk about multiple dispatch Multiple dispatch is a programming construct. It essentially allows us to write functions that: 1. Share the same name, but 2. Exhibit different behaviour based on the types of the arguments passed in. This idea is baked into the Julia programming language and is part of what makes Julia so composable. Python programmers don't have this luxury, partially because multiple dispatch isn't part of the language design and partially because of namespaces. That said, Matt Rocklin and a few others maintain the [][mdisp] Python package, which implements multiple dispatch in a pretty sane fashion. To borrow the example from the package, we might design a function called , but need it to operate differently depending on whether we put in integers or Python objects: [mdisp]: https://pypi.org/project/multipledispatch/ Is this a good idea? I used to wonder when I'd know enough to answer the question. I think I do now. But first, we have to take a detour to graphs. Let's talk about graphs Graphs are a data structure that allows us to model entities and their relationships. Entities are nodes, while relationships are edges. I usually teach a tutorial at PyCon, SciPy and ODSC titled \"[Network Analysis Made Simple][nams]\", in which we use [NetworkX][nx] as a tool to learn applied graph theory. It's been running since 2014, and I've learned a lot, made a lot of friends, and had new opportunities spring up through teaching it. [nams]: https://ericmjl.github.io/Network-Analysis-Made-Simple/ [nx]: https://networkx.org/ As a data model, graphs are handy for reasoning because for our logic to make sense, we must be extremely clear on our definitions. What exactly constitutes a node? What exactly constitutes a relationship? What connections are allowed to exist between nodes (or, surprise! categories of nodes)? These answers must be precise. This then brings us to categories. Let's talk about categories I think it was from [Eric Schles][ericschles] that I learned that there was this entire field of mathematics called \"category theory\". (This was likely ODSC 2017, while both of us were chatting in the speaker's lounge.) Also, I started reading the blog of [Tai Danae-Bradley][tdb], who trained as a category theorist at NYU and even has a book on it. From [one of her excellent blog posts][categorytheory], I learned the core of category theory is about: \"...collections of objects that can relate to each other via morphisms...\" [ericschles]: https://github.com/EricSchles [tdb]: https://www.math3ma.com [categorytheory]: https://www.math3ma.com/blog/what-is-category-theory-anyway Sounds familiar... it sounds like a graph! So... what about multiple dispatch? If you stepped back one moment and looked carefully at the example, you'll notice that we have a microcosm of category theory being brought to life in a programming language. There's a category of objects called objects, and there's a (sub-)category of objects called integers. There are relations between integers called add, and a similar relation between objects, also called add. Both have a relationship named add, but each add differs because the \"integers\" case is a particular case that needs to be handled differently from the \"objects\" case. You could construct a graph to represent it all, one that looks like this: If we think about it carefully, because integers are a sub-category of objects, the second case covers the child cross-type cases. So what gives, what's the epiphany? Multiple dispatch works because linguistically, we might need to reuse the same category of relations, as indicated by their shared name, for different object categories, for no other reason than doing so is the semantically correct thing to do for our problem. And doing so allows us to leverage composition to write cleaner and interoperable programs.",
    "tags": [
      "math",
      "graph theory",
      "category theory",
      "programming"
    ],
    "pub_date": "2020-12-18",
    "type": "blog"
  },
  {
    "id": "blog-building-a-personal-knowledge-graph-on-obsidian",
    "url": "/blog/2020/12/15/building-a-personal-knowledge-graph-on-obsidian/",
    "title": "Building a personal knowledge graph on Obsidian",
    "summary": "On a recent vacation, since we've got nowhere to go, I decided to re-examine how I made notes in my personal and professional lives. The result is a lot of lessons learned into effective notetaking, and me building yet another thing. Curious to see what I learned? Read on!",
    "body": "Amidst COVID-19, I decided to take a break from work. I was definitely in need of a bit of personal rejuvenation after hitting hard on the accelerator at work, mostly to get things wrapped up before our first baby girl was born. During this personal rejuvenation time, I stumbled upon Andy Matuschak's notes, which are famous amongst the \"personal knowledge management\" community. At the same time, I also saw the rise of Roam Research and Obsidian, and I was looking with envious eyes at those who had early access to Roam. I was a bit turned off by the subscription and lock-in that characterized Roam and was much more attracted to the Freemium + \"respect your data rights\" model embraced by Obsidian, so I dipped my toes in. The result is this learning journey building my own \"personal knowledge management\" system from which I've never turned back. Let me explain. For a long time, I've known that the way that I think is in a \"linked\" fashion. I rarely entertain a thought on its own in isolation. Instead, I will jump and hop from one idea to another in my brain. Sometimes, I joke with others that my mind is effectively a random graph: nodes are ideas, and edges are relationships between them. The unfortunate thing is that I don't have the right tools for externalizing the relationships that I uncover. As a practicing data scientist, as I continuously learn and discover new model classes that may be useful in my projects, I also see connections between models, their components, and their applications. My previous notetaking tool, Evernote, is better described as an archival tool. In there, I would dump and collect everything and anything that I thought might be of marginal value. After all, Evernote's mantra is to help us store everything and make it searchable to us. It's a great tool, and I still use it in that fashion. However, using it as a tool for developing and understanding ideas is a bit like treating a flea market as a museum. Yes, it's a collection of items, but a flea market has everything and anything in there, while a museum is intentionally curated. Intentional curation is the key to internalizing insights from the external knowledge we gain. All of those points above brings us to the problem of tooling. Evernote is a tool for collection, not curation and linking. Tools that help us actively curate a concrete representation of an idea -- a note -- and help us link them together will also help us intentionally curate these ideas together. Obsidian gives us the necessary tooling to be such a curator. How Obsidian Helps There are at least three features of Obsidian that help with this. Markdown files Firstly, Obsidian treats individual notes, implemented as Markdown files, as the unit on which linking happens. When you want to create a wiki-style link to another page, you use a double square bracket () and start typing the name of the note that you want to link. A pop-up display will show the note titles, which can help you if you have a fuzzy or approximate notion of the idea you're trying to link. If you use \"imperative tone\" ideas to name your note files, then linking becomes even more natural. Graph view Secondly, Obsidian has a graph view. This graph view is nothing more than a node-link diagram, likely implemented using , with note titles overlaid. Yes, it is fascinating to explore our ideas' connected structure, but the more significant value proposition is quickly identifying every note file that exists as an unlinked singleton. Why? Because one of the goals of linked notetaking is to search for linkable ideas and then explicitly connect them. The visual graph view also helps me surf through concepts; each time I trace a path between the thoughts I've recorded down, I reinforce the link in my unreliable internal store. Random notes Thirdly, Obsidian has a \"show me a random note\" feature. This feature helps me ruminate through ideas. It's the digital equivalent of me jumping through concepts in my head, except now I have a reliable external store for ideas (Obsidian) rather than an unreliable internal store (my brain). Surfacing up random notes is an extremely excellent way of hopping through my knowledge graph to surface up ideas in a spaced repetition fashion, allowing my mind to uncover and create connections between ideas that I might not have made otherwise. Plain text Apart from those three key features, one other notable feature, pun intended, makes me trust Obsidian over other notetaking services. That feature is Obsidian's choice to rely on simple Markdown files as the unit on which linking happens. This choice stands in contrast to Evernote's expanded XML syntax, which, though open, is not plain enough to have a low barrier to parsing. The choice also stands in contrast to Roam's and Notion's choice to link individual blocks of text, which requires additional tech that, almost by definition, requires specialized data structures - again, being not low enough of a barrier. Effectively, wi",
    "tags": [
      "notetaking",
      "productivity"
    ],
    "pub_date": "2020-12-15",
    "type": "blog"
  },
  {
    "id": "blog-disable-sleep-on-ubuntu",
    "url": "/blog/2020/10/18/disable-sleep-on-ubuntu/",
    "title": "Disable sleep on Ubuntu",
    "summary": "How do you prevent a headless Linux machine from going to sleep/suspend? I recently learned how to do so :).",
    "body": "I have a running Linux tower at home that contains a GPU. It runs headless, i.e. without keyboard, mouse, and monitor. However, the box sometimes goes to sleep, which means I'm unable to SSH (or [](https://mosh.org)) into the tower on our home network, or access a perpetually running Jupyter session that I have. Short of keeping a keyboard next to it, I thought there had to be a better way to keep the tower alive. Turns out, it's a simple command. Based on this StackOverflow post, to disable sleeping/suspending, we execute the following command at the terminal (after logging in): You'll get the following output: Now, let's say we wanted to re-enable sleep/suspend. In that case, we have the inverse command available: You'll get the following output: The / combo is what does the magic. Specifically, when masking, the command creates a symbolic link in for each of sleep, suspend, hybernate and hybrid-sleep, and points it to . Pointing to is a great way to redirect commands to oblivion, thus disabling sleep.",
    "tags": [
      "linux",
      "tips"
    ],
    "pub_date": "2020-10-18",
    "type": "blog"
  },
  {
    "id": "blog-fermi-estimation-and-bayesian-priors",
    "url": "/blog/2020/10/15/fermi-estimation-and-bayesian-priors/",
    "title": "Fermi estimation and Bayesian priors",
    "summary": "Enrico Fermi had a unique way of thinking that I think dovetails nicely with constructing principled priors. Curious? Read on!",
    "body": "Fermi estimation is named after the renowned physicist Enrico Fermi for his ability to give accurate order-of-magnitude estimates for certain quantities. From Wikipedia, Fermi estimation is described as such: In physics or engineering education, a Fermi problem, Fermi quiz, Fermi question, Fermi estimate, order-of-magnitude problem, order-of-magnitude estimate, or order estimation is an estimation problem designed to teach dimensional analysis or approximation of extreme scientific calculations, and such a problem is usually a back-of-the-envelope calculation. The estimation technique is named after physicist Enrico Fermi as he was known for his ability to make good approximate calculations with little or no actual data. Fermi problems typically involve making justified guesses about quantities and their variance or lower and upper bounds. What struck me is how this relates to thinking about prior distributions for parameter estimates. A common critique of Bayesian estimation models is that \"one can cook up priors to make the model say anything you want\". While true, it ignores the reality that most of us can use Fermi estimation to come up with justified and weakly-informative priors. Let me illustrate. In biology, say we had a quantity to estimate, such as the average size of a species of bacteria, which we could reasonably assume to be Gaussian-distributed. Since we know that most other bacteria are 0.1 to 10 microns in size, we might set the log(mean) with a $N(0, 0.5)$ prior, in the units of microns, which would put us in a good prior distribution range. The mean is at $10^0 = 1$ microns, which is smack in the middle. Three scale units out to the right, and we are at 30-ish microns; three scale units out to the left, and we are at about 0.03-ish microns. As a prior, it's uninformative enough for the problem that data can refine the distribution, and few would argue against this choice when framed the way above. Let's consider counterfactually ridiculous priors. It would be unreasonable if I placed scale = 10 as the prior. Why? In that case, we are spanning orders of magnitude that are practically unreasonable for the category of things we call bacteria. We would go down to as small as $10^{-10}$ microns, which is less than the size of a bond between atoms, or as large as $10^{10}$ microns, which is on the scale of distances between towns on a map. It would also be unreasonable if I placed scale = 0.1 as the prior. Why? Because it ignores the one order of magnitude above and below our mean, which may be relevant for the category of bacteria. As a rule of thumb, one order of magnitude above or below where our guess lies is a good span for weakly-informative priors. So as you can see, Fermi estimation is a pretty principled way to guess what priors should be used for a problem. It also helps us constrain those priors to be not-so-unreasonable. Yet another critique against Bayesian bites the dust.",
    "tags": [
      "bayesian",
      "data science",
      "statistics"
    ],
    "pub_date": "2020-10-15",
    "type": "blog"
  },
  {
    "id": "blog-why-giving-talks-is-important-for-writing",
    "url": "/blog/2020/10/6/why-giving-talks-is-important-for-writing/",
    "title": "Why giving talks is important for writing",
    "summary": "I recently arrived at an epiphany: Delivering talks on a topic is an important way to prepare for writing a paper. Intrigued? Read on to learn why.",
    "body": "I recently arrived at an epiphany. Delivering talks on a topic is an important way to prepare for writing a paper. We've been writing a paper at work, one that I'm very excited about relating machine learning to enzyme engineering. My collaborators and I have been planning for a while to write the paper, but the writing part has been a bit laborious without external feedback and input. At the same time, though, we've delivered talks internally at NIBR about the enzyme engineering project, and on each iteration, the preparation work for the talk has been instrumental in crafting the scientific story. After delivering the talk, the feedback we've received from effectively a committee of colleagues has helped shape the story further too. After two or three talks, it became quite clear to me what we needed to write. And once that was clear, the writing part was so much easier. It's as if the ideas crystallized out of the solvent mess of my head. One Saturday afternoon, though it was not officially a work day, I spent the time typing furiously at the keyboard, getting the results out onto the Word document that my colleagues and I were collaboratively writing on. The constant practice and rehearsal made the writing portion much easier. So why did talking help with the writing? Turns out, when delivering a talk, we are forced to linearize the spaghetti mess of a project that we've worked on, which exactly parallels what must happen when we engage in the activity of writing. We also get real-time feedback from reviewer-like individuals who will evaluate our work. Our audience's questions reveal very quickly where our scientific storytelling remains in need of clarification. The counterpoints or addendum pointers raised help us uncover the broader context in which we're addressing our audience. This real-time feedback can then help us further refine the story. That said, the prior practice and inspired writing does not necessarily imply that the written piece will pass peer review easily! During grad school, the early versions of the paper I wrote was the product of many talks delivered, but even that one was rejected at four places before my advisor and I made the call to refine the story further. Nonetheless, the prior accumulation of talk experience really accelerated the writing process, as through iterative presentation and refinement, I became more adept at explaining the core ideas of the paper to a broader audience. This helped make the writing the process feel akin to talking. I think this iterative strategy of trying to explain my science and refining it further parallels Feynman's method for learning something new. After a few rounds, we reach a tipping point, until finally the story is so clear that we can dump it to text easily. So to grad students, and data scientists more generally out there, if there's anything I learned, don't be afraid to talk about and explain your work. Get good at explaining it! Your written communication will become much easier.",
    "tags": [
      "data science",
      "communication",
      "writing",
      "speaking",
      "talks",
      "career",
      "grad school"
    ],
    "pub_date": "2020-10-06",
    "type": "blog"
  },
  {
    "id": "blog-moving-data-securely-and-quickly-with-croc",
    "url": "/blog/2020/10/1/moving-data-securely-and-quickly-with-croc/",
    "title": "Moving Data Securely and Quickly with `croc`",
    "summary": "I found a new free and open source tool for moving data between computers in a secure and fast fashion, called . I highly recommend it! Come read on to learn more.",
    "body": "I recently had to move a large file from a computer in one site at work to another computer in another site at work, to obtain a dataset for machine learning purposes. To do this, I usually would have used , but for whatever reason (didn't debug, no time), it was borked on my user account. We apparently pay for an Aspera license, but it was installed on a very old system that I was struggling/wrestling with. Thus, I looked back on my GitHub stars, knowing that someone must have made something that we could use - and indeed, I found [](https://github.com/schollz/croc). What's ? It's a file transfer system that sends files securely using end-to-end encryption, via a file transfer relay. What's a \"file transfer relay\"? Essentially, it's a go-between computer that is set up to relay connections to and from computers -- it does this one and only one job -- but cannot read the contents of anything that is passed through it. Using a relay in between two computers sounds kind of roundabout. Why would we want this in contrast to directly -ing, or standing up an HTTP server and using an HTTP URL to send the data? To understand more, I read 's creator's blog post describing the design advantages of . The key advantages are speed, security, and simplicity, all-in-one. Speed because the relay doubles the number of computers sending/receiving data. Security because of the use of Password-Authenticated Key Exchange between the two computers, and the ephemeral nature of the connection. And simplicity, because of the user-interface. Now, baked into 's configuration is the use of a public relay server that 's creator has set up, but one can set up their own relay server, and configure to use that relay server at runtime. To do this requires a one-time setup on a third computer or in a docker container. I did the former at work, setting up a temporary relay server on a Linux workstation, because I couldn't access the public relay server on our VPN (which is exactly what is supposed to happen, for security!). The 6GB file was transferred at a rate of about 50 MB/sec, which, for me, was fast enough and heartening to get on with life! To do this, we access the relay computer, which should be a computer visible on the intranet, download , and run as a relay process: Then, we point croc away from the default public relay when sending a file: Then on the receiving end: Being written in the Go programming language, it's fast and easily distributed on multiple platforms. Coupled with its ease-of-use, count me a fan!",
    "tags": [
      "tools",
      "tips",
      "tricks",
      "croc"
    ],
    "pub_date": "2020-10-01",
    "type": "blog"
  },
  {
    "id": "blog-tools-to-help-you-write-consistent-python-code",
    "url": "/blog/2020/9/30/tools-to-help-you-write-consistent-python-code/",
    "title": "Tools to help you write consistent Python code",
    "summary": "How do you write clean Python code in the year 2020? Check out this latest essay I wrote. Cross-posted from my essays collection.",
    "body": "As a data scientist, you might be feeling the need to level-up your code-writing skills. After all, if your hacked together script becomes the basis of production systems, imagine the headache :). Moreover, as time goes by, it seems to me that for data scientists to uncover the high risk, but nonetheless potentially high value and high impact outlets, nothing beats having software skills to flexibly express the custom logic that's needed. Hadley Wickham's sentiment that one can't do data science in a GUI seems to ring true, though I'd probably refine it by saying that one can't do highest value data science without programming skills! Mostly because of the community of [](https://github.com/ericmjl/pyjanitor) developers, who encounter a smattering of tooling that I might not have, I have had the privilege of learning from them a collection of tooling that I think can really help with code style. Especially if you transition your code from a Jupyter notebook to a source file! Equipped with the right tools, you can write consistent code without needing to memorize all of the rules. And that idea of externalizing the rules to a program is what these tools are all about! tl;dr: What tools are there? For the impatient, here's the summary: Code formatters i.e. those that explicitly change your code: - Black - isort Code linters i.e. those that don\u2019t change your code, but flag errors: - Flake8 - Interrogate - Darglint - pylint - mypy Some of these come with , which I strongly suggest you use! VSCode tooling VSCode tooling for those who are used to VSCode as your development environment: - Python extension - pylance: this is superbly fast. hats off, Microsoft. External CI services Tools that run as an external service for your continuous integration pipeline: - deepsource That\u2019s a lot of tools; how do I interact them? A full-fledged tutorial is probably not the realm of this tiny blog post, but I will begin by giving you the overview of how you interact with them in general. That should give you enough patterns to work with when you look at their docs. At the CLI The code formatters and linters are runnable at the CLI. You can consider this to be the lowest-level/primitive way of using them. For example, with , you would call on it at the command line at the root of your project repository: What pops out is Black telling you what files it formatted, or error messages telling you which files it couldn\u2019t format. Likewise for : What pops out is an entire collection of \"error codes\", which specify one and only one error that it caught. The error messages will tell you what\u2019s wrong, so correcting the error is a matter of doing the opposite. For example, might tell me that a line is too long at 89 characters, which is greater than the 79 that it is configured for, and so I would have to split up that line, or if it\u2019s a string then remove unnecessary words, to correct the error. Configuration Sometimes, two tools might come with defaults that conflict with one another. and are two; , being opinionated, will format lines to fit in 88 characters, but will by default look for 79 characters. (I\u2019m in favour of 79, because it allows me to look at my code in its entirety side-by-side on a 15\" MacBook Pro.) Each tool comes with its own configuration documentation page. The Python community has started gravitating towards a config file called [](https://snarky.ca/what-the-heck-is-pyproject-toml/), which is intended for Python language tools. If is supported by a tool, then you should be able to find a page in the documentation. A well-written docs page will give you a copy/paste-able config file with extensive comments that you can read to know what\u2019s going on. (Black itself leads the charge: its page on the file is very well-documented!) I think that the community-wide standardization is a great thing! The way this has happened is a bit slow, but at least the tool authors don\u2019t make it onerous to migrate from existing config files. Fits all of the right patterns: make doing the right thing easy! Inside VSCode If you use VSCode as your day-to-day integrated development environment (or IDE), there are two extensions that are indispensable for Python developers: the official Python extension and Pylance. The official Python extension adds support for: IntelliSense, linting, debugging, code navigation, code formatting, Jupyter notebook support, refactoring, variable explorer, test explorer, snippets, and more! Pylance, on the other hand, is: powered by Pyright, Microsoft's static type checking tool. Using Pyright, Pylance has the ability to supercharge your Python IntelliSense experience with rich type information, helping you write better code faster. With code linting enabled in VSCode, you get the benefits of errors showing up as you type. An example from the library, which contains some holdover code style issues from before the time I was proficient in these tools, can be seen in the screenshot below. Underneath the hood, for co",
    "tags": [
      "data science",
      "tips and tricks",
      "programming",
      "python",
      "command line tools",
      "code formatting",
      "code style"
    ],
    "pub_date": "2020-09-30",
    "type": "blog"
  },
  {
    "id": "blog-add-a-direct-binder-link-for-built-html-notebooks",
    "url": "/blog/2020/9/12/add-a-direct-binder-link-for-built-html-notebooks/",
    "title": "Add a direct Binder link for built HTML notebooks",
    "summary": "I recently figured out how to dynamically insert a Binder badge into HTML pages built from Jupyter notebooks, so that users can one-click directly open a Jupyter notebook in the correct environment without needing to navigate or build an environment from scratch. Come see how I figured this out!",
    "body": "I recently learned a thing! If you write tutorial material in Jupyter notebooks, it's possible to include a Binder link directly to a notebook, such that Binder opens up directly inside that notebook (rather than in the file browser). The \"vanilla\" link one usually gets from mybinder.org looks like this (using Network Analysis Made Simple as an example): More generally speaking, the anatomy of the URL is: Now, if you want to point directly to a file to be opened on Binder launch, you would instead do something like this: The general form of the URL looks now like the following: Here, the is a POSIX-style string relative to the repository root. A thing to note: all of the characters in the probably have to be replaced with , otherwise the URL might not work correctly! Using this pattern, I was able to insert a Launch Binder icon for all of my Jupyter notebooks in the Network Analysis Made Simple repository! That said, manually inserting that many Launch Binder icons by hand is not an easy-to-maintain thing. Is there a better way? You bet! Leveraging the Jupyter notebook Python API (the package) as well as the navigation section, I was able to write a few custom functions that inserted a correct Binder URL with the badge into the top of each notebook before building the HTML pages with . The code roughly looked like this: The full build script is located here. I thought this might be a useful addition to , which handles Jupyter notebook preprocessing for , so I also raised an issue up there to see if the maintainer of is interested in it!",
    "tags": [
      "jupyter",
      "notebooks",
      "data science",
      "education",
      "teaching",
      "binder"
    ],
    "pub_date": "2020-09-12",
    "type": "blog"
  },
  {
    "id": "blog-faster-iteration-over-dataframes",
    "url": "/blog/2020/9/7/faster-iteration-over-dataframes/",
    "title": "Faster iteration over dataframes",
    "summary": "If is slow, what then is the alternative? Read on to figure out how to make looping over dataframes 1000X faster :).",
    "body": "I can't claim credit for this one, as I found it here on LinkedIn. That said, in case it's helpful for you, here's the tip: We are usually taught to loop through a dataframe using : Turns out, the faster way to loop over rows in a dataframe is: According to the post, this is about on the order of 1000X faster. The main reason for the speedup is because the use of [](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.itertuples.html) leads to the construction of [](https://docs.python.org/3/library/collections.html#collections.namedtuple). By contrast, the use of [](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.iterrows.html) returns as a pandas Series, which is slower to construct on each loop iteration. Of course, one would usually try to vectorize as much as possible, but in the event that looping is unavoidable, this might be a good tip to keep in your back pocket!",
    "tags": [
      "data science",
      "pandas",
      "tricks",
      "tips",
      "productivity"
    ],
    "pub_date": "2020-09-07",
    "type": "blog"
  },
  {
    "id": "blog-pandera-data-validation-and-statistics",
    "url": "/blog/2020/8/30/pandera-data-validation-and-statistics/",
    "title": "Pandera, Data Validation, and Statistics",
    "summary": "I test-drove the Python package this month, and I like its usage paradigm. Come read on to learn how you can incorporate into your workflow!",
    "body": "I have test-driven the [](pandera.readthedocs.io/) package over the past few days, and I'm extremely happy that this package exists! Let me tell you why. Data testing \"Data testing\" is the colloquial term that I have used in place of the more formal term, \"schema validation\", or \"data validation\". Ever since I was burned once by CSV files with invalid column names, I put together a data testing tutorial to kickstart the idea, and have been keeping an eye on the space ever since. But let\u2019s step back a bit. What do we mean by schema/data validation? It\u2019s merely a fancy way of checking, \"are my data as I expect them to be?\" Here\u2019s an example. Imagine you have a data table, in which the semantic meaning of a row is a \"sample\" (i.e. one row = one sample), and columns are observations about that sample. In this data, there are definitely some things you may want to guarantee to be true of our data. For example, I may have numeric measurements of each sample, and I need to guarantee that they cannot be negative or zero. Hence, for those columns, I need \"greater than zero\" check. For another column, it may be that we expect that the values in there take one of two values. An example dataframe could be as follows: If I were implementing these checks in a test suite in vanilla Python, I would write them as follows: Having the checks in the test suite is nice, as it means if the checks are run as part of a CI pipeline, they\u2019ll continually be checked. However, they cannot be checked whenever I call on , which is the most important time I\u2019d need those checks to appear \u2014 whenever I\u2019m going to use that data loading function, i.e. at runtime, and have to depend on them being correct. To have those checks available at runtime, I\u2019d have to duplicate all of the checks into the function. And we know that duplication of code increases risk of stale code. Enter . Instead of writing the checks in two separate places, I can start by writing the following declarative schema in a Python module, that serves as a defined source of truth: Now, the flexibility with which this schema gets used is simply superb! Because I've declared it as a Python object in code, I can import it anywhere I want validation to happen. For example, I can validate the inputs to a function at runtime: And because the validation happens as soon as the function is executed, I can also validate the schema as part of an automatically-executed test suite that continuously runs and checks data alongside code. The data test suite becomes simpler to maintain, as it gets reduced to a simple execution test. Validating data at every step that it could go wrong is pretty important! Additionally, if there is a new data file that needs to be processed in the same way, when we pass in the dataframe version into the function, it is automatically validated by \u2019s . If a dataframe that fails , then will give us an error message straight away! These are just the basics. Pandera comes with even more: - Validating the /. - Automatically inferring a draft schema that you can use as a base. I'm excited to see how this goes. Congratulations to the package author Niels Bantilan, it's a package that has all of the right API ideas in place! Its design is very lightweight, which helps encourage fast iteration and turnaround on schema validation. Being one of those \"eagerly executable\" and \"interactively debuggable\" tools helps its case as well. The package focuses on doing only one thing well, and doesn't try to be a \"God package\" that does all sorts of things in one package. For a person of my tastes, this increases the attraction factor! Finally, it targets DataFrames, an idiomatic data structure in the Python data science world, which is always a plus. Having incorporated into a project just recently, I'm excited to see where its future is headed! Data Validation as Statistical Evaluation I can see that data validation actually has its roots in statistical evaluation of data. Humour me for a moment. Say we have a column that can only take two values, such as \"yes\" and \"no\". From a statistical viewpoint, that data are generated from Bernoulli distribution, with \"yes\"/1 and \"no\"/0 being the outcomes. If you get a \"maybe\" in that column, that value is outside of the support of the distribution. It\u2019s like trying to evaluate the likelihood of observing a 2 from a Bernoulli. It won\u2019t compute. So every time we run a check, we are effectively expressing a statistical check of some kind. The byline of the package, \"Statistical Data Validation for Pandas\", is even more apt once we consider this viewpoint! Conclusions I hope this post encourages you to give a test-drive! Its implementation of \"runtime data validation\" is very handy, allowing me to declare the assumptions I have about my data up-front, and catch any violations of those assumptions as early as possible.",
    "tags": [
      "data science",
      "statistics",
      "data validation",
      "data engineering",
      "pandera",
      "software tools",
      "tips and tricks"
    ],
    "pub_date": "2020-08-30",
    "type": "blog"
  },
  {
    "id": "blog-software-engineering-as-a-research-practice",
    "url": "/blog/2020/8/21/software-engineering-as-a-research-practice/",
    "title": "Software Engineering as a Research Practice",
    "summary": "Why do software skills matter for data scientists? We might have heard that it matters for our workflow, but what about for organizing knowledge? In this essay, I argue that practicing good software skills has those benefits and more.",
    "body": "At this year's SciPy 2020, a talk and proceedings paper caught my eye: \"Software Engineering as Research Method\". (Here are links to the paper and the talk.) In it, the authors, Mridul Seth and Sebastian Benthall, detailed the benefits that software skills bring to the academic and industrial research communities, both from the perspective of making scientific progress and from the perspective of pedagogy. I've been doing some reflection of my own on how software skills have helped me tremendously, as a data scientist. Here's my thoughts on them, using the example of random number generators and probability distributions. Generating Random Numbers In building scientific software, we are recognizing pragmatically useful categories around us, and formalizing them in a language. Let's look at a really simple example of drawing s and s from a Bernoulli distribution. Oppa Imperative Style If we were to write a Bernoulli draws generator in Python code, we might implement it as follows: This is purely imperative style programming. Without going too deep (pun intended) into definitions, by using mostly built-in primitives of the Python language, we are operating at a fairly low-level paradigm. Now, let's imagine a world where the Bernoulli distribution object does not exist. I have a collaborator who wants to build on top of Bernoulli distributions to make other things. They would have to copy/paste this imperative block of code into their programs. Oppa Functional Style What can we do to alleviate this, then? One thing we can try is to encapsulate it in a function! The implementation might look like this: But wait, what are the semantics of ? Is it the probability of obtaining class 1, or class 0? The behaviour isn't documented in the class, so let's add some of it in. Now, we can do a few things with this implementation. Firstly, we can write a test for it. For example, we can assert that numbers drawn from the Bernoulli are within the correct support: Secondly, we can compose it in bigger programs, such as a Binomial generator: Please excuse my lack of docstrings for a moment! Oppa Classy Style This is all good and such, but there is another thing we need to consider. A random number generator exists logically as a sub-category of things that a probability distribution can do. There are other things that we do with probability distributions, such as evaluating data against a probability distribution's density/mass function. After all, generating numbers isn't necessarily the only action we want to take with a probability distribution. The probability density function is unique property of a probability distribution; it's also an invariant property. As such, it is something we can instantiate once and forget about later. Perhaps using an object-oriented paradigm might be good here. Let's see this in action: Now, with this object, we've made using the category of Bernoulli distributions much easier! An object-oriented paradigm here also works well, because the way and have to be implemented for each probability distribution is different, but the inputs to those functions are similar across the entire category of things called \"probability distributions\". A reflection I want to first state up-front that this essay is not one championing object-oriented programming. Those who know me actually know that I much prefer functional programming. Rather, this essay is about how leveraging the natural programming paradigms and data structures for a problem or model class gives us a way to mastering that problem or model class. After all, building useful and verifiable models of the world is a core activity of the quantitative sciences. By exploring and structuring probability distributions in a logical fashion, my pedagogical exploration of probability distributions as a modelling tool is reinforced with the following main ideas: 1. Probability distributions generate random numbers according to a set of rules, 2. Probability distributions can evaluate data against its PDF And by structuring a probability distribution as an object with class methods, our mental model of the world of probability distributions is made much clearer. \"Research discipline\", you say? Yes, research discipline, and in both ways. Here\u2019s what I mean. Discipline (a la \"rigour\") in thought means thinking clearly about how the thing we are working with fits most naturally in relation to other things. Nudging ourselves to make better software makes us better thinkers about a problem class. Bringing this level of discipline gives us an opportunity to organize the little subject matter discipline that we inhabit. As a discipline (i.e. \"way of thought\"), well-written software also enables us to build on top of others' thoughtfully laid out tooling. Stability in underlying \"primitives\" matter Contrary to a generation of us that might think that \"choice in everything is the ultimate\", a stable base is what a field really needs in order for a discipline to flou",
    "tags": [
      "data science",
      "software engineering",
      "software skills"
    ],
    "pub_date": "2020-08-21",
    "type": "blog"
  },
  {
    "id": "blog-datasoftware-challenges-as-tools-for-hiring",
    "url": "/blog/2020/7/26/datasoftware-challenges-as-tools-for-hiring/",
    "title": "Data/Software Challenges as Tools for Hiring",
    "summary": "In this post, I detail some of my thoughts about the use of \"data challenges\" for hiring data scientists. Though I have only used it in hiring data science interns, I think some of the lessons I've learned from test-driving the process can apply more generally.",
    "body": "Some teams/hiring managers love them, and some candidates really detest them. Having been on both sides of the hiring equation, I can definitely see both perspectives. I wanted to write down some thoughts after using data/software challenges to help with the hiring of three interns at work. What are data/software challenges? Data/software challenges are take-home interview assignments that are given a specific time frame to be completed. In the case of the three interns that I helped with hiring, two were for data science roles, and one was for a software engineering role. In all three cases, we gave them a one week time frame to complete the assigned challenge. For the software challenge, the candidates were asked to build the core of a piece of typical software that we would build in our department, according to a pre-defined spec, i.e. pretty much in-line with how we operate, just on a mini scale. For the data challenge, the candidates were given a list of four data sets, and told to come up with a research question and produce an analysis that were delivered in the form of a Jupyter notebook. After the data and software challenges were turned in, we would review them, and invite the subset of them that \"passed\" the bar to deliver a presentation \"on-site\". (With COVID-19, \"on-site\" has dramatically changed, of course.) The candidate had to deliver a seminar presentation on their challenge to myself and other colleagues involved in hiring. The challenges were designed in such a way that a competent and well-equipped individual could complete the task in about 2-3 afternoons of work. We put in some thought to designing the challenges such that they could not be trivially solved by looking up StackOverflow answers alone. (Use of SO was encouraged, though, as long as they documented which SO post they referenced for the non-trivial pieces of the project.) What were you looking for? I can speak more for the data challenge, as I designed it. To start, my data challenge borrowed ideas from the legend of Van Halen\u2019s Brown M&Ms contract clauses: they contained some pretty exact requirements that seemed trivial, but if ignored, I would fail the candidate and not bother with their candidacy going forward. Being detail-oriented is a pre-requisite for doing data science work, and if the candidate did not follow instructions precisely, even after I factored in language ambiguities and barriers as confounding factors, I knew I would have problems working with them. In the interest of being able to reuse and evolve the data challenges, I\u2019m clearly not going to reveal what those requirements are here, but the principle stands, nonetheless! Apart from that, I was looking for a candidate\u2019s ability to write \"good\" code (refactored and documented at least), take care to make sure their notebooks are \"readable\", and be able to \"defend\" in their prose their choice of problem and the individual choices made in their analyses. As a bonus, I was also on the lookout for \"modern\" tooling. Having one or two grammatical or spelling warts was fine; we all make these mistakes once in a while, but if the grammatical issues interfered substantially with readability, then that was cause for concern. What qualities in a candidate were you trying to infer from the aforementioned things you were looking out for? Primarily, this list: - Meticulousness - Empathy - Logical rigor - Creativity Meticulousness is most evident from their ability to pass the \"Van Halen details\" test. Creativity would show in their problem choice and choice of methods. Logical rigor would show up in how they defended their choices. Empathy shows up in the way they communicate, both written and oral. I am confident that these are non-controversial qualities to desire in colleagues. This next statement will possibly be controversial, but I hold it to be true: taken holistically, one can tell from a presented pattern of good/bad behaviours whether a candidate can hit the quality bar for the points listed above. This is controversial because the desire for objective fairness in hiring means subjectively perceived patterns can be unfair. However, if there is sufficiently diverse perspectives agreeing on the same set of observed patterns presented, this cannot be ignored, regardless of whether the patterns present as positive or negative. Weren\u2019t the expectations a bit high for interns? Possibly, though I did set a high bar because I expect the interns I work with to deliver high quality work. I\u2019m ready to teach them things they don\u2019t know, but a baseline level of skill level and quality is necessary. In order to ensure that my expectations were not set unreasonably high, I calibrated them against colleagues\u2019 opinions; in particular, I asked them specifically whether I\u2019d be asking too much of the candidate at their level. Were the data challenges effective? Subjectively, I would claim yes, but because I ended up working with these interns, I might clearly be biased. If my s",
    "tags": [
      "data science",
      "hiring",
      "data challenge"
    ],
    "pub_date": "2020-07-26",
    "type": "blog"
  },
  {
    "id": "blog-jupyter-notebooks-as-scripts",
    "url": "/blog/2020/7/11/jupyter-notebooks-as-scripts/",
    "title": "Jupyter notebooks as scripts",
    "summary": "I like writing in notebooks, for the ability to quickly prototype. But can we treat Jupyter notebooks as scripts that we can execute? The answer is yes, and in this blog post, I'll show you a few of the simplest ways to do so.",
    "body": "If you're one of the types of programmers for whom a notebook interface helps with prototyping and scripting, it is possibly handy to treat notebooks as a script and execute them programmatically. There's multiple ways to do accomplish this. to Python script One way is to use to convert your script into a Python script on the fly, and then execute the Python script itself: Direct programmatic execution of a Jupyter notebook This other way also uses , but slightly differently in that we take advantage of 's execution capabilities: By default, there is a timeout of 30 seconds, so if you need to specify a longer timeout than the default, you can do so: is in seconds, so that will give you a 20 minute execution timeout. With that said, you should ideally be ensuring that anything programmatically executed should execute quickly. Use Papermill takes the notebook execution paradigm one level up, in that it allows you parameterize your notebooks. I haven't used it myself, so I won't provide a code sample, but I'll link it here nonetheless. How do I choose? There's really not much of a difference between the first two methods, so I'd encourage you to test-drive both and see which one you're more comfortable with. Personally, I would choose the first option. Even though it \"feels\" a bit more roundabout, it helps me because I don't have to remember the syntax (which can feel a bit clunky typing at the command line). That said, if you wrap everything inside a nice , this minor practical difference goes away. As for , I see its utility for teams who don't necessarily yet have the software chops to make well-structured Python packages and scripts. It's a good hack en route to good practices, but at the end of the day, I'd choose \"principled workflow over hacks\" whenever practically possible. In the long-run, it makes a ton of sense for data science teams to equip themselves with software skills! Do you have an example? Yes, indeed! For the Network Analysis Made Simple tutorial series that my co-author Mridul and I teach, I recently spent a bit of time figuring out how to convert our collection of Jupyter notebooks and markdown files, which get rendered in our official tutorial website as a [](https://www.mkdocs.org) site with a beautiful and functional [](https://squidfunk.github.io/mkdocs-material/) theme, into leanpub-flavoured markdown that we then publish as a book for offline viewing. In doing so, we could preserve a single source of truth for the content, and automatically publish to two locations simultaneously. I needed the interactivity of a Jupyter notebook to prototype what I wanted to get done. I also wanted to use the notebook as the \"gold standard\" source of truth, rather than a Python script, as this would facilitate modifying and debugging later (i.e. I could execute up to a certain point and go deep). Thus, in the Travis CI script, we first convert the notebook to a Python script, then execute it, and also build the website: Finally, we get Travis to publish everything to the branch, which we never edit.",
    "tags": [
      "jupyter",
      "jupyter notebook",
      "notebook",
      "data science"
    ],
    "pub_date": "2020-07-11",
    "type": "blog"
  },
  {
    "id": "blog-how-i-feel-about-hey",
    "url": "/blog/2020/6/29/how-i-feel-about-hey/",
    "title": "How I feel about Hey",
    "summary": "I test-drove Hey, a new email product launched by Basecamp recently, and I'm ready to pay. Read on to find out why!",
    "body": "Hey is a new email service by Basecamp. If you\u2019re a techie type, you\u2019ve probably heard of it via the Twitter world. I saw the demo by their CEO Jason Fried, and I was sold enough to write an email to their invite request address (iwant@hey.com). My likes Now that I\u2019ve been trying it out for about a week, I wanted to share about my favourite features about Hey email that makes me ready to pay up for it. Batched replies Hey gives us an interface that lets us set aside emails into a bucket that one can \"Reply Later\". With any email open, we can mark it as an email to \"Reply Later\", and assign it to the \"Reply Later\" stack. At a later time in the day, we can go into that stack, and hit the \"Focus & Reply\" button, which gives us another interface to go down the stack and respond to emails one by one. This makes so much more sense, because: 1. It\u2019s easier to give each email the attention that it needs, 2. Without having to pass through the \"Inbox\" and being distracted by something else we \"feel\" we need to respond to (but don\u2019t actually have to). The Screener The Screener is pretty rad. I can quickly screen emails into the appropriate buckets: The Imbox, The Feed, and The Papertrail. This helps reduce the amount of clutter that hits my eyes. The screening procedure is quite deterministic. There\u2019s no fancy machine learning that goes on that attempts to stochastically move emails unexpectedly. Emails from important people, or important newsletters, get sent to the Imbox. (I get to decide what is important!) Other emails are sent out of sight, out of mind, until I feel ready to look at them as a collection. Functional piles \"Functional piles\" is, by far, the biggest thing I\u2019ve come to like about Hey. I\u2019ve alluded to this above, but I thought I\u2019d detail this a bit more. Hey gives us a functionally broad definition of \"piles\" of email: The Screening The Imbox The Papertrail The Feed The Set Aside Pile The Reply Later Pile What\u2019s really cool about this functional definition is that: 1. The definition is \"functional\", in that it determines what I do with the email in a sane workflow. 2. The definition is broad enough that I don\u2019t have to think too hard about which pile it goes into. For example, \"The Screening\" pile is for all of the first-time emails in my Inbox, and all I have to decide is whether I might want to see that email later or not. As another example, the \"Reply Later\" pile lets me batch together emails that I might want to focus and reply to at one shot. The workflow feels very functional, and the user interaction design is definitely geared towards making sane the way we handle email. By contrast, in Gmail, I was paying for SaneBox to help me automatically triage my email for me, but even then, I couldn\u2019t leisurely scroll through them because it was a separate service with a separate interface that made it difficult to work with. Also, Gmail never gave us the ability to screen out emails, neither did it give us the ability to set aside special piles to \"reply to\" later. I know I could have used Gmail tags to handle this, but dragging and dropping an email to a generic tag qualitatively feels different compared to hitting a button to send it to a pile that is still visible but not in my face. Finally, whenever an email is read, it gets moved from the top of the Imbox into the \"read\" pile. This is another one of those subtle designs that makes usage such a joy. There\u2019s a feed of email below that I\u2019ve seen once, but only the truly unread ones are still above. Having ingrained email habits for over 20 years, this takes a bit of getting used to, but once one is used to Hey\u2019s design, one realizes how helpful it is for triaging and batching (i.e. \"pretty good\" email workflow). Forwarding in Any email service can do this, but yes, email forwarding is the way that we get emails ported over from our old addresses. Forwarding is the most straightforward (ahem!) way to get started with the service without needing to resort to emailing a ton of people. I still keep my old Gmail account around because I use it for the calendar scheduling story, and for Google Drive. It\u2019s hard to part with an address I\u2019ve had for a long time, but I\u2019ve done migration off digital services before, and I know it just takes a bit of patience to finish. Concluding thoughts Perhaps my situation has become typical of the kind of user that Hey is trying to help: lots of emails, a small fraction being important, the others I might need to know. Yet again, good design is shown to matter - Hey makes \"great workflow\" front-and-center, whereas most email services don\u2019t. This seemingly little workflow detail has a disproportionate impact on the quality of the service. Good design matters, and I\u2019m happy to pay for it!",
    "tags": [
      "reviews",
      "email",
      "general stuff"
    ],
    "pub_date": "2020-06-29",
    "type": "blog"
  },
  {
    "id": "blog-statistical-tests-are-just-fancy-model-comparisons",
    "url": "/blog/2020/6/28/statistical-tests-are-just-fancy-model-comparisons/",
    "title": "Statistical tests are just canned model comparisons",
    "summary": "I came to the epiphany today that \"statistical testing\" protocols are nothing more than canned model comparisons (that sometimes have convoluted interpretations). Come read why!",
    "body": "I reached another statistics epiphany today. Statistical tests? They are nothing more than a comparison between statistical models that has been made canned. What do I mean? Let's say we have data for Korean men from both the North and the South. We could write a generative model for men's height that looks something like the following: $$h^n \\sim N(\\mu^{n}, \\sigma^{n})$$ $$h^s \\sim N(\\mu^{s}, \\sigma^{s})$$ (The superscripts don't refer to powers, but \"north\" and \"south\"; I was just having difficulty getting Markdown + LaTeX subscripts parsed correctly.) In effect, we have one data generating model per country. Think back to how you might analyze Korean male height data using canned statistical procedures. You might choose the t-test. In performing the t-test, we make some assumptions: - The two populations have equal variance, and - We are only interested in comparing the means. Yes, there are variations on the t-test, such as t-test with unequal variance and the likes, but I wonder how many of us would really think deep about the assumptions underlying the procedure when faced with data and a \"canned protocol\"? Instead, we'd simply ask, \"is there a difference in the means?\" And then calculate some convoluted p-value. How could we go beyond canned procedures and tell if the two populations have a difference? Well, we could compare their means, but we could also add another comparison of the variance too. All of this comes very naturally from thinking about the data generating story. Let's see this in action. In performing Bayesian inference, we would obtain posterior distributions for both the $\\mu$ and $\\sigma$ parameters of both models. We could then compare the $\\mu$ and the $\\sigma$ parameters, and they would both be informative. For example, if $\\mu$s turned out to be different and their posteriors do not overlap, we would claim that the two populations differ in their average height, and explain it as being due to the nutritional difference arising from economic mismanagement. On the other hand, if the $\\sigma$ turned out to be different and their posterios didn't overlap, we would claim that the two populations differ in their variation in height. How would we explain this? For example, if the distribution of North Korean heights was tighter giving rise to a smaller $\\sigma$ value, we might explain this as being due to rationing and genetic homogeneity. In both comparisons, by thinking more precisely about the data generating process, we had access to a broader set of hypotheses. If we had defaulted to the t-test as a \"canned statistical procedure\", and not thought carefully about setting up a data generating model, we would be missing out on a rich world of inquiry into our data.",
    "tags": [
      "data science",
      "bayesian statistics",
      "hypothesis testing"
    ],
    "pub_date": "2020-06-28",
    "type": "blog"
  },
  {
    "id": "blog-whats-the-most-optimal-way-to-learn-bayesian-statistics",
    "url": "/blog/2020/6/15/whats-the-most-optimal-way-to-learn-bayesian-statistics/",
    "title": "What's the most optimal way to learn Bayesian statistics?",
    "summary": "I've been reflecting on the way I learned statistics, and I think I learned it in a flawed fashion. In response to this reflection, I've began reworking an introduction to Bayesian statistical inference that focuses on statistical story telling. Read on to find out more!",
    "body": "I've been reflecting on the way I learned statistics, and I think I learned it in a flawed fashion. Traditionally, statistics is taught in the format of performing hypothesis tests to infer whether there's a difference between groups, or to learn the parameters of some curve. Learning statistics in this direction leads to a ton of confusion, because we're taught the shortcut to the answer, rather than the first-principles way of thinking about a problem. We end up with the \"standard t-test\" and multiple confusing names for regression modelling, masquerading as canned procedures that can be used on any problem. (OK, that's a bit of a stretch, but please do tell me you were at least tempted to use the t-test in a situation where you just had to crank out an analysis...) After seeing the following tweet from Michael Betancourt... Unpopular opinion: regression models are much harder to use well than generative models of an actual data generating process and consequently should not be the first and only modeling techniques that many people are taught.&mdash; \\mathfrak{Michael &quot;El Muy Muy&quot; Betancourt} (@betanalpha) June 15, 2020 ...I realized that the only reason why [Markov Models and their variants][hmm] clicked for me was thinking through the data generating process. The only reason why hierarchical models clicked for me was stepping through the data generating process on a real problem and linking them to statistical parameters. Without thinking through the data generating process, none of those models made any sense. [hmm]: https://ericmjl.github.io/essays-on-data-science/machine-learning/markov-models/ In some sense, thinking through the data generating process is an extremely natural thing to do. It's like telling a story about how our data came into being, and we know that telling stories is exactly what humans are great at. Storytelling helps us reason about the world. There should be no reason why we don't use statistical storytelling to reason about our problems. Worrying first_ about the data generating process and then about the inferential procedure makes statistical inference less of a black box and more of a natural conclusion of statistical storytelling. We become less concerned with whether something is \"significant\", and instead more concerned with whether we \"got the model right\". To put this into concrete action, I've been working on an alternative introduction to probabilistic programming and Bayesian inference that is lighter on math than most introductions, involves a lot of verbal storytelling, and goes heavier than most introductions in its use of programming. Here we practice the skill of hypothesizing a data generating story and translating that into the language of probability distributions, which can then be translated into SciPy stats Python code. Stay tuned!",
    "tags": [
      "bayesian statistics",
      "bayesian",
      "data science",
      "statistics",
      "inference"
    ],
    "pub_date": "2020-06-15",
    "type": "blog"
  },
  {
    "id": "blog-pab-to-phd",
    "url": "/blog/2020/6/14/pab-to-phd/",
    "title": "P(A,B) to P(H,D)??",
    "summary": "What gives us the logical leap from a joint distribution $P(A, B)$ in abstract, to the more tangible $P(H, D)$ (for hypothesis and data? Here's an excerpt from a tutorial in the making about Bayesian statistics.",
    "body": "Bayes' rule looks like this: $$P(A|B) = \\frac{P(B|A)P(A)} {P(B)} $$ It is a natural result that comes straight from the rules of probability, being that the joint distribution of two random variables can be written in two equivalent ways: $$P(A, B) = P(A|B)P(B) = P(B|A)P(A)$$ Now, I have encountered in many books write, regarding the application of Bayes' rule to statistical modelling, something along the lines of the following: Now, there is an alternative interpretation of Bayes' rule, one that replaces the symbol \"A\" with \"Hypothesis\", and \"B\" with the \"Data\", such that we get: $$P(H|D) = \\frac{P(D|H)P(H)}{P(D)}$$ At first glance, nothing seems wrong about this statement, but I did remember having a lingering nagging feeling that there was a logical jump unexplained here. More specifically, that logical jump yielded the following question: Why are we allowed to take this interpretation? It took asking the question to a mathematician friend, Colin Carroll, to finally \"grok\" the idea. Let me try to explain. Spaces of models and data We have to think back to the fundamental idea of possible spaces. If we set up a Bernoulli probability distribution with parameter $p$, then the space of possible probability distributions that we could instantiate is infinite! This result should not surprise you: $p$ can take on any one of an infinite set of values between 0 and 1, each one giving a different instantiated Bernoulli. As such, a hypothesis is drawn from a (very large) space of possible s, or more abstractly, hypotheses, thereby giving us a $P(H)$. Moreover, consider our data. The Bernoulli data that came to us, which for example might be , were drawn from a near-infinite space of possible configurations of data. First off, there's no reason why we always have to have three 1s and two 0s in five draws; it could have been five 1s or five 0s. Secondly, the order of data (though it doesn't really matter in this case) for three 1s and two 0s might well have been different. As such, we have the $P(D)$ interpretation. As a modelling decision, we choose to say that our data and model are jointly distributed, thus we have the joint distribution between model and data, $P(H, D)$. And once we have the joint distribution between model and data, we can begin to reason about Bayes' rule on it.",
    "tags": [
      "bayesian",
      "data science",
      "statistical inference"
    ],
    "pub_date": "2020-06-14",
    "type": "blog"
  },
  {
    "id": "blog-with-what-symbols-do-you-write-thermodynamic-equations",
    "url": "/blog/2020/6/9/with-what-symbols-do-you-write-thermodynamic-equations/",
    "title": "With what symbols do you write thermodynamic equations?",
    "summary": "How in the world did \"inference\" come to mean \"prediction\" amongst deep learners? I have no idea, but I do have a rant.",
    "body": "A rant egged on by Vicki Boykis after reading her excellent essay on model deployment. /beginrant When I was in my first year of undergrad, I was in a first-year program in which the sciences were taught as an integrated unit. (The program, Science One, was quite influential in my learning journey in learning across disciplinary silos.) There, the lecturers would coordinate their use of terminology and symbols in equations. The reason? They recognized that having defined, precise, and consistent terminology reduced learner confusion, increased retention, and aided in us linking key concepts. In other words, because they knew that symbolism and choice of words mattered, they were able to more easily teach the things that mattered more. One example of this was in thermodynamics. Heat, energy, and entropy have quite specific definitions, and in certain pedagogical traditions, they have the same equation forms but use different symbols. When we learned thermodynamics, we avoided all of that confusion by simply adopting the physicist's set of symbols. Nobody argued about it, because though the form changed, the \"spirit\" of the matter was preserved. Well, fast forward 14 years later, it appears the machine learning world has yet to learn the same lessons taught in Science One. No big deal, after all, Science One only educated 80 students at a time. But I think it's worth writing about, because we might end up with a new generation of data scientists who can't tell the difference between inference and inference. Wait, what?! \"You're willing to die on the inference hill?!\" In some ways, not. You might want to keep doing what you do, but hear me out. I got reasonz, yo. The following items are key components to a statistical modelling/machine learning workflow: (a) input data, (b) model/structure, (c) parameters to learn, and (d) output data. One can frame statistical learning methods in terms of a \"direction\" w.r.t. the output data (d). \"Simulation\"/\"prediction\" refer to the forward direction of going from (a-c) to (d), while \"inference\" or \"learning\" refer to the backward direction of going from (a, c, d) to (b). We use distinct terms because they are distinct and, in fact, basically opposite subclasses of the larger thing of calculating 1 thing from the other 3. \"Inference\", then, has a specific definition in the quantitative sciences, rooted in its use in statistics. It refers to \"inferring\" the parameters of a model, given data. It is distinct from simple \"guessing\", in that there are criteria (maximum likelihood, for example) for evaluating how good individual inferred values are. The term \"inference\", however, has been co-opted incorrectly by deep learning practitioners to refer to the task of generating or predicting what an output should be, given known input data and model structure + parameters. We are starting to hear the term mis-used all the time: \"At inference time...\", \"When performing inference...\". How this came about, I do not know, but I have a hypothesis that the first few people who co-opted the term were genuinely trying to make some form of linguistic connection between the statistics and machine learning worlds. I shall not try to, ahem, infer their intents from observed behaviour here though. :) But imagine the poor statistician trying to decipher what the machine learner is trying to say when they read the term, \"at inference time...\" It's like the chemist staring at the physicist's heat equation and being ever so subtly thrown off by symbol mismatch! Likewise, the statistician stands to be tripped up by the completely opposite use of \"inference\" when reading a machine learning paper. So what's the big deal here? Languages evolve, don't they? Yes! But surely that's not the only criteria for letting words slip? If words and their precise meanings do not matter, we might as well ! (If you disagree with me, go ahead and decipher what I really mean by that, since my words didn't matter and I can evolve the language anyways...) Statistics and machine learning are extremely interrelated fields. Machine learning research built upon statistical foundations and has given statistics practitioners new model classes to work with and tooling to perform inference on models more easily (I'm thinking about things like automatic differentiation!). Both fields stand to learn a ton from one another, but also stand to lose a lot without precise and consistent vocabulary. If you accept the premise (and, if I may dare claim, very common mental model) of directionality w.r.t. the output data, then using the same term to refer to opposite-direction procedures is bewilderingly confusing! Especially for learners of both domains. What's the solution going forth? The unfortunate reality is that the co-opting of the term has already happened. The optimistic reality is that there are individuals who are merely \"accepting of the fact but don't necessarily like/agree with it\". Much like I push back on the use of \"aye eye\"",
    "tags": [
      "deep learning",
      "machine learning",
      "statistics",
      "rants"
    ],
    "pub_date": "2020-06-09",
    "type": "blog"
  },
  {
    "id": "blog-data-science-design-manual",
    "url": "/blog/2020/6/2/data-science-design-manual/",
    "title": "Data Science Design Manual",
    "summary": "There's a pile of free books released for everybody! One of them caught my eye: the Data Science Design Manual. Link in the full post!",
    "body": "During this extraordinary COVID-19 time, Springer did an extraordinary thing that I never expected: They released a whole bucketload of books for free online! One of them caught my eye: [\"The Data Science Design Manual\"][dsmanual]. Having browsed through the book PDF, I'm impressed by its coverage of the foundational topics that I think every data scientist should be equipped with: statistical inference, data wrangling, linear algebra, and machine learning. The author, Steven Skiena, also covers more in there. Go [grab the PDF][dsmanual] while it's still free! [dsmanual]: https://link.springer.com/book/10.1007%2F978-3-319-55444-0",
    "tags": [
      "data science",
      "ebooks"
    ],
    "pub_date": "2020-06-02",
    "type": "blog"
  },
  {
    "id": "blog-development-containers",
    "url": "/blog/2020/5/31/development-containers/",
    "title": "VSCode Development Containers",
    "summary": "A mini-advertisement, completely unpaid and totally unsolicited, for the use of remote \"development\" containers on VSCode, with a little pro-tip about pre-building containers sprinkled in for good measure!",
    "body": "It's been about four days since I first discovered the idea of \"development containers\", and I'm absolutely hooked. We know that \"containers\", such as Docker or Singularity containers, give us operating system-level \"virtualization\". Or in other words, it gives us completely isolated runtime environments. Development containers take this idea one step further, and provide us with isolated development environments. This actually can take down the time needed for a newcomer to the project to get up and running with software development. They don't have to configure anything. In VSCode, simply \"open a repository in a remote container\" (a command palette command), and ensure that they have Docker running locally. Boom! Start developing. I added dev containers to , my personal website, my Essays collection, and also started using Dockerhub to prebuild the containers so that a newcomer just has to pull the container and go! This stands in contrast to building the container locally, which is a power drain (if on battery) and can take time. I plan to do more of this at work, so I can onboard interns and new colleagues onto a project much faster. I also got running remote development containers, i.e. containers that are running on my slightly more powerful remote GPU tower, but the VSCode client is running locally on my puny little 12\" MacBook. The future is looking bright! :)",
    "tags": [
      "docker",
      "containers",
      "software engineering",
      "skills"
    ],
    "pub_date": "2020-05-31",
    "type": "blog"
  },
  {
    "id": "blog-easy-matplotlib-animations",
    "url": "/blog/2020/5/26/easy-matplotlib-animations/",
    "title": "Easy `matplotlib` animations",
    "summary": "I recently stumbled upon a package that makes animations dead simple. Check it out!",
    "body": "Recently, [][celluloid] caught my eye: it's a package that lets you create animations easily! [celluloid]: https://github.com/jwkvam/celluloid If you need a dead-simple example to convince you to check it out, here's one lifted straight from the repository: But seriously though, if you use the workhorse Python drawing package for anything, this package can be considered to be one of those \"great tricks to have\" in your bag!",
    "tags": [
      "matplotlib",
      "python",
      "data visualization",
      "animation"
    ],
    "pub_date": "2020-05-26",
    "type": "blog"
  },
  {
    "id": "blog-what-are-lambda-expressions-in-python",
    "url": "/blog/2020/5/17/what-are-lambda-expressions-in-python/",
    "title": "What are lambda expressions in Python?",
    "summary": "In this blog post, I'll show you what lambda expressions are, and where we might use them in a data science setting.",
    "body": "Inspired by a conversation I had with a colleague who is learning Python, I wanted to write down an explainer of what \"lambda expressions\" are in Python. You might have seen lambda expressions in someone else's Python code, which looks like such: This, actually, is the equivalent of writing a function that we might name : Here's an explainer of the anatomy of the lambda function. - tells Python that we're constructing a function. - The signature of the function, meaning, the arguments the function takes in, is given by everything between and . In our example, the signature of the function is , meaning the function only takes in a single argument, . - The stuff the function returns is everything after the . In our case, it's the boolean result of . So a lambda function basically equivalent to a Python function. The key difference here is that it is considered \"anonymous\", in that we have not given it an explicit name. Let me explain. When we use the following pattern: the function has a name, given by . However, when we do a lambda function: this function doesn't have a name. Hence, the term \"anonymous\". But what's the use of a lambda function if all it does is nothing more than be \"anonymous\"? Well, one place I have used lambda functions is when I determine that a function that I want to implement is a simple one-liner that can get slotted in anywhere. For example, in , when transforming a column to see whether it's even: That would be less verbose than: RealPython has a great article which also details the appropriate uses of lambda expressions; definitely check it out!",
    "tags": [
      "python",
      "programming",
      "lambda expression"
    ],
    "pub_date": "2020-05-17",
    "type": "blog"
  },
  {
    "id": "blog-use-pyprojroot-and-pythons-pathlib-to-manage-your-data-paths",
    "url": "/blog/2020/4/21/use-pyprojroot-and-pythons-pathlib-to-manage-your-data-paths/",
    "title": "Use pyprojroot and Python\u2019s pathlib to manage your data paths!",
    "summary": "Are file paths driving you crazy in your data science project repository? Come read how you can use and Python\u2019s to help make life a tad easier. You\u2019ll love this new tool in your toolkit!",
    "body": "If you adopt a proper organizational structure for your data projects, then each project gets its own directory (i.e. a clean and isolated \"workspace\") and its own isolated analysis environment (e.g. a environment). In that workspace, your directory structure might look like this: As such, your notebook are all going to be in a different directory from your data. This is one way that keeps the mind sane: you might have subdirectories in the directory that you use to organize the notebooks further, yet you have multiple notebooks that use the same file, leading to brittle path linking. After all in one notebook, you might do: But in another notebook that lives in a different directory, to link to the dataset, you might have to do: The potential for confusion is just immense here. A better way is to provide one authoritative path to a particular dataset that you can use. For example: But even that is a bit tricky: if you move the notebook for whatever good reason, the path to the data might break. It\u2019s still brittle. We need a better way to resolve paths. Enter . Written by my fellow PyData conference doppleganger Daniel Chen, it provides a function that will resolve to your project root directory (hence the package name). The original was written in R (), and it\u2019s a wonderful tool for data scientists. Let\u2019s see it in action: And voila! No fragile relative paths, and no perpetually long chains of ! Just nice and clean resolution to your project root. How does it work? What does underneath the hood is recursively climb the file tree until it finds one of a set of pre-specified files that are commonly found in a project\u2019s root directory. For example, is a common one. For Python packages, is another. If your project doesn\u2019t \"fit\" any of the conventions assumed, or if you have a fancier structure, you can always add a to your project root, and configure the keyword argument so that only looks for that one authoritative file: And what exactly is the function returning? Well, it\u2019s returning a object, which has some seriously clever patching to allow it to work with the operator to represent paths in native Python code! Now, let us all toast to cleaner path resolution in our data projects!",
    "tags": [
      "data science",
      "pathlib",
      "python",
      "packages",
      "tools"
    ],
    "pub_date": "2020-04-21",
    "type": "blog"
  },
  {
    "id": "blog-introducing-a-new-essay-on-markov-models",
    "url": "/blog/2020/4/12/introducing-a-new-essay-on-markov-models/",
    "title": "Introducing a new essay on Markov models",
    "summary": "Here's the motivation behind my latest essay explaining Markov models and their variants, as well as how to perform Bayesian inference on them.",
    "body": "Two weeks ago on a Monday afternoon, I hacked on a work project with a colleague Zachary Barry (who was in the same program as I was in grad school). In that project, we wanted to build an autoregressive hidden Markov model that could model motion. (I cannot go further than that, given that this touches front-line workplace work.) This turned out to be both 1. non-trivial, because of the high degree of problem-specificity involved, and 2. difficult to grok, because of the lack of explanatory papers and teaching material out there that was oriented towards programming-types. Not all was lost, though, as the last time I touched Markov models was in 2014, when I first encountered it in a computational biology class at MIT. Leveraging that, my familiarity with PyMC3, and Zach's prior background, we hacked together a prototype that he could take forth (or that we would at least schedule another hack session for). That second difficulty, though, stuck with me. Since I couldn't find a satisfactory explainer on Markov models for programmers, I decided I'd write one. The result is the latest essay on Markov models, which I have just posted to my Essays collection. This is a culmination of two days of continuous writing plus two weeks of review by gracious colleagues and collaborators who have generously given their time to help me improve it. In this essay, we cover what Markov models are, interleaving prose, equations, code, and figures to help communicate to programmer-types what exactly Markov models are, particularly the mathematics behind them but presented with more code than equations. It's also a preview of the kind of writing I'd like to focus on going forth. I'm launching a monthly newsletter curating programmer-friendly data science tools, tips and techniques at Tinyletter. And if you'd like to support more programmer-oriented data science learning material, please consider pledging me a cup of coffee on Patreon!",
    "tags": [
      "data science",
      "bayesian",
      "markov models"
    ],
    "pub_date": "2020-04-12",
    "type": "blog"
  },
  {
    "id": "blog-resources-for-learning-python-during-covid-19",
    "url": "/blog/2020/3/25/resources-for-learning-python-during-covid-19/",
    "title": "Resources for learning Python during COVID-19",
    "summary": "During the COVID-19 outbreak period, you might find yourself with a good chunk of time to pick up Python. It's an incredibly productive language to learn! At the same time, the wealth of resources out there can be intimidating. Here's my opinionated list of resources in 2020 that could be handy for you. And if you have others, be sure to share!",
    "body": "With COVID-19 on hand, you might have some time to go deeper into Python programming. I'd like to recommend some resources for you, in case you wish to brush up, learn, or go deeper. DataCamp If you're a complete beginner, I recommend going with DataCamp. I say this with a full up-front disclosure that I'm a DataCamp instructor, and if you make it to the Network Analysis courses, then I benefit from you. (If you don't, then I don't.) The reasons why I recommend DataCamp is: - They have a hosted compute environment that you use, thus freeing you from the potentially tricky task of navigating how best to install Python on your local system, and letting you focus on learning Python. - The exercises are designed in such a way to be bite-sized, so you can pace yourself through the curriculum. A little practice every day goes a very long way to picking it up. This I know because I also had to design the network analysis curricula the same way. - There are clearly delineated specializations for both R and Python programmers, and psychologically, it can be very rewarding to navigate a pre-defined path for a learner, thus easier to keep up the learning. (This is also a very smart business strategy that ed-tech firms use.) - Beyond just the beginner-oriented courses, they go all the way up to the core PyData stack through to deep learning. - Finally, there are example projects that they recommend to you when you've passed the pre-requisite courses, thus giving you more practice. The reason why you wouldn't want to go with DataCamp is because you have to pay for it, and maybe because you don't trust the words of someone who has a conflict of interest. (I don't blame you, I'd take the same position.) You might need to discuss with your management if your team is going to sponsor your learning. Coursera Coursera has a wide range of offerings for learning Python, and they are completely free with certification if your organization sponsors it (my current employer Novartis includes this as a perk). (Otherwise, it's just free - also hard to beat.) Just on the basis of price I would not hesitate to recommend it. That said, there's a lot of courses to choose from. - The well-reviewed ones, that also come as a series, generally are from the University of Michigan. - JHU also provides some neat ones, such as \"Python for Genomic Data Science\". - As a prior (i.e. having not seen the course content), I would avoid the ones by IBM and other big firms. Prior experience has told me they're likely not hands-on (though I might be wrong). One of the bigger issues here is that you might need to learn how to set up your own Python programming environment. For this, consult your friendly neighborhood parsertongue speaker (i.e. a colleague who knows Python) to help you. Think Python Think Python is an online book by Allen Downey, a professor of computer science and more at the Olin College of Engineering in Needham, MA. Allen is also a fellow Python community educator, and has generously let me test-drive my deep learning tutorials at his classes. In this book, Allen leverages the friendliness of the Python programming language to teach you basic computing concepts. Allen is incredibly generous, and has made the book freely available online, though you can buy it via Amazon or O'Reilly Media. He's got other titles for those who already know Python: - Think Stats: Statistics taught using computation. - Think Bayes: Bayesian statistics (YAY!) taught using computation. (I used this book to get brushed up on Bayesian inference when in grad school.) - Think Complexity: A computation-oriented introduction to data science. This book actually kickstarted half of my doctoral thesis. Automate the Boring Stuff with Python This is a book by a Python friend of mine, Al Swiegart, whom I met at the annual PyCon USA. In this book, Al teaches you how to use Python code to automate all the boring, repetitive stuff that you encounter in your day-to-day work on a computer. It's this kind of project, which directly impacts your day-to-day, that can keep your motivation levels high while learning. Al is also incredibly generous, and has made the book freely available online, and sometimes gives away the physical book for free. But as it's one of his income sources, I'd encourage you to buy the book (just as I did, even though I don't really need to read it anymore). e2eML by Brandon Rohrer This is an online course created by Brandon Rohrer, who is a data scientist at iRobot (the maker of those fancy robot vacuums!). Brandon is pretty active on social media, and is a fellow education enthusiast. (I sometimes wish my current role could include formal classroom teaching as part of my professional goals.) With this online course, he brings you through one opinionated path to getting good with machine learning and data science. As a pre-requisite, you should know how to set up your own Python programming environment, as there's no hosted computing environment for yo",
    "tags": [
      "covid-19",
      "python",
      "learning",
      "data science"
    ],
    "pub_date": "2020-03-25",
    "type": "blog"
  },
  {
    "id": "blog-what-can-data-scientists-do-during-covid-19",
    "url": "/blog/2020/3/15/what-can-data-scientists-do-during-covid-19/",
    "title": "What can data scientists do during COVID-19?",
    "summary": "What can data scientists do during the COVID-19 outbreak? Many things, but the most important is probably encouraging local action to stop the spread of the virus. Our number crunching and communication skills can be put to good use locally here. That's one way we can help outside of the usual recommendations. That, and don't sneeze at the grocery store!",
    "body": "A friend in Austria pinged me with the following question: I was wondering if you have any ideas of how I (and other link-minded) people could help with combating this threat. Do you know how a Data Scientist can contribute to our global efforts? My response is below. The tl;dr version is: Your number-crunching and communication skills, paired with independent re-analyses of publications, can help you with local mitigation efforts. The usual measures are your best bet: stay at home, interact with people virtually if you need to do meetings, and minimize number of trips to the grocery store. That said, I'm sure you pinged me for a different answer :) Here's what I can think of. The overarching theme is to use your skills as a data scientist, particularly number-crunching and communication skills, to help re-emphasize and re-communicate the good practices that the epidemiology experts have been saying all along, the only difference being we're local to our communities, and can advocate among our own community. Firstly, independently replicating others' analyses is a good place to start. This doesn't necessarily mean ML models - it can mean an ODE model for case counts, or Bayesian models for estimation of growth rate, etc. Doing this will take some deep diving into the epidemiology literature, as epidemiologists have given much more thought to how to properly count cases and the likes than we have. Once you've done the first, extend those models to show how certain interventions that have been strongly recommended - such as the social distancing one I am emphasizing as well - can impact the spread of the virus. I trust the recommendation from epi experts to practice social distancing because I was once adjacent to the epi world (with my flu research), but I'm not sure others share the same level of trust. The lack of trust is probably partially due to the distance most of us have from a professional epidemiologist. It helps for one to hear it from someone one knows personally, especially someone one can trust who also has the necessary number-crunching and communication skills. Thirdly, produce communications of some kind (posters, blog posts, infographics) that help you communicate to others in your local community. This, I think, is something we can already do without doing any independent re-analyses, but the message among your local community might be more effective if you have already done your homework by doing independent analysis replication. (There's just something about being prepared to answer questions that helps when preparing communication pieces.) Some caveats when doing this. Firstly, if you find something in your analyses that disagrees with the experts, be ready to hold your own view with a grain of salt. There may be a key assumption you missed. Dig into the literature, ping your local epidemiology experts for help, and discuss with other data-oriented people. A good prior is, \"I'm wrong, they're probably right, I might need to re-look what I've done.\" Secondly, making dashboards/notebooks and publishing them is fun, but in these times, I would call them cheap fun if it isn't paired with local action. (To know why I call it cheap fun, see this article on #vizresponsibly. There's harder but more satisfying work to be done actually helping to influence local action than publishing something to the web and tweeting about it... unless doing that tweeting thing actually helps with influencing local action. Use good judgment here, and don't go for cheap fame. I trust you won't. From my time next to the epi world, the best thing you can do is, of course, encouraging local action: social distancing, leaving masks for medical personnel (so don't go raiding shelves), being prepared with 2 weeks of food supplies, and don't go to the hospital unless you have medical symptoms (like fevers), so you don't contribute to overwhelming the medical system. And while in isolation, don't forget to take care of your mental health. Nerds like myself can deal with the isolation, but others might need some social interaction. Audio/video call someone if you need to. One of the nice things of the internet age is that it's easier to stay connected than ever before. Finally, stay kind to the local grocery staff. They're probably under a lot of mental and physical stress with people flowing through their workplace. Don't sneeze in their vicinity! They're keeping the shelves stocked for the good of everybody.",
    "tags": [
      "epidemiology",
      "data science",
      "covid-19",
      "flattenthecurve"
    ],
    "pub_date": "2020-03-15",
    "type": "blog"
  },
  {
    "id": "blog-one-weird-trick-to-speed-up-your-tensorflow-model-100x",
    "url": "/blog/2020/2/13/one-weird-trick-to-speed-up-your-tensorflow-model-100x/",
    "title": "One Weird Trick to Speed Up Your TensorFlow Model 100X...",
    "summary": "Have you found your deep learning model's forward pass to be excruciatingly slow? Come read on to learn how we made a deep learning model 100X faster!",
    "body": "You\u2019ve got a TensorFlow 1.13 model on hand, written by an academic group, and you know it\u2019s going to be useful, because you\u2019ve already done some preliminary experiments with it. The model takes in protein sequences, and returns vectors, one per protein sequence. The vectors turn out to be excellent descriptors for downstream supervised learning, to predict protein activity from sequence data. The only problem? It\u2019s slow. Excruciatingly slow. Not just on CPU. It\u2019s slow on your local GPU as well. You can\u2019t run this locally without waiting for hours to process the thousands of protein sequences that you have on hand. Slow? What now? What do we do, then? Install a newer GPU with faster processors and more RAM? \"Lift and shift\" onto cloud GPUs? I\u2019d argue that all of those are evil choices, because premature optimization is the root of all evil (Donald Knuth). Moving the model onto a GPU without knowing the root cause of the model\u2019s slowness could be construed as a form of premature optimization, because we are prematurely moving to more powerful hardware without first finding out whether it can be improved on existing ones. Moving the model onto a cloud GPU, renting another person\u2019s computer while not maximizing pushing the model to its extremes locally, is probably even worse, because we end up with more variables to debug. By the aforementioned premises and Knuth\u2019s logical conjugation, those options are, therefore, evil choices. Rather provocatively, I\u2019d like to suggest that one right option when faced with a very slow deep learning model is to make it fast on a single CPU. If we make the model fast locally on a single CPU, then we might have a good chance of dissecting the source of the model\u2019s slowness, and thus make it really fast on different hardware. This is exactly what we did with the UniRep model. Our protein engineering intern in Basel, Arkadij, had already done the groundwork testing the model on our high performance computing cluster. It was slow in our hands, and so we decided it was time to really dig into the model. Reimplementation Because the model is an RNN model that processes sequential data, we need to know how the model processes one window of sequence first. In the deep learning literature, this is known as an \"RNN cell\". We isolated that part of the RNN, and carefully translated each line of code into NumPy. Because it processes one \"window\" or \"step\" at a time, we call it . Because the RNN cell is \"scanned\" over a single sequence, we then defined the function that processes a single sequence. To do this, we took advantage of the submodule\u2019s function, which literally \"scans\" the RNN cell over the entire sequence the way TensorFlow\u2019s RNN cell would be scanned. This basically is a vectorized for-loop. Thanks to Stephan Hoyer pointing out my conceptual error here; is a compiled for-loop with known number of iterations. Together, that defined the function call that processes a single sequence. This, incidentally, gives us the \"batch-\" or \"sample-wise\" RNN function, . Finally, we need a function that processes multiple sequences that are the same length. Here, we need the function to be vectorized, so that we do not incur a Python loop penalty for processing sequences one at a time. This forms the layer, which, semantically speaking, takes in a batch of sequences and returns a batch of outputs. Testing So, does the reimplementation work in the same way as the original? Absolutely. We passed an attrappe (Google it, it\u2019s German) sequence through the model, and checked that the reps were identical to the original - which they were. But don\u2019t take my word for it, check out the draft preprint we wrote to document this! Meanwhile, in case you don't want to click over and read our hard work, here's the figure linked from the source. (It's only off by a bit because we added a delta to make the two representations visible.) Besides that, we also added tests and elaborate docstrings for the functions in there, so anybody reading the model source code can know what the semantic meaning of each of the tensors\u2019 axes are. Having wrestled through undocumented academic code, knowing what input tensor dimensions were supposed to be were one of the hardest things to grok from looking at a relatively poorly undocumented model. By carefully reimplementing the deep learning model in pure JAX/NumPy, we were able to achieve approximately 100X speedup over the original implementation on a single CPU. Given JAX\u2019s automatic compilation to GPU and TPU, the speed improvements we could obtain might be even better, though we have yet to try it out. Again, don't take my word at it - check out the draft preprint if you're curious! The exact figure, though is below. I suspect that by reimplementing the model in NumPy, we basically eliminated the TensorFlow graph compilation overhead, which was the original source of slowness. By using some of JAX\u2019s tricks, including and , we could take advantage of vectorized looping that t",
    "tags": [
      "deep learning",
      "optimization",
      "software development",
      "data science"
    ],
    "pub_date": "2020-02-13",
    "type": "blog"
  },
  {
    "id": "blog-create-your-own-auto-publishing-slides-with-reveal-md-and-travis-ci",
    "url": "/blog/2020/1/18/create-your-own-auto-publishing-slides-with-reveal-md-and-travis-ci/",
    "title": "Create your own auto-publishing slides with reveal-md and Travis CI",
    "summary": "[Hugo Bowne-Anderson][hugo], a long-time collaborator and friend in the PyData community, asked in passing about whether I would write a blog post detailing how to set up Travis CI to continually publish my Markdown-sourced slides to the web. I gladly obliged, and I hope you find it useful! [hugo]: https://hugobowne.github.io/",
    "body": "For my PyData Ann Arbor Meetup talk, I decided to use [](https://github.com/webpro/reveal-md) and a Markdown file to generate my slides. Here, I'd like to write about how I used and Travis CI to continually publish my slides as I updated them, thus making them accessible to everybody on the web. What is ? is nothing more than a live web server for that converts Markdown files into slides that can be hosted via a web server, static site, or PDF. Why ? I much prefer to use Markdown to write my slides, as doing so comes with one big benefit: I am focused on the content that I want to deliver, and not the less important details that are easy to screw up (animations, positioning, etc.). Constraining what's accessible to me forces me to be extremely clear and succinct on what I'm trying to communicate. And if I really desired anything fancier, I could weave in some HTML with no issue. Create your own auto-publishing slides! Let\u2019s walk through the steps needed to make this a reality for you! Create a new repository On GitHub, create a new repository that has a nice and informative name. (For now, we\u2019ll just refer to that repository as for convenience.) Get setup locally To get setup, you need to make sure that is on your . I choose to use environments to manage my packages, so I have a slightly convoluted way of doing this, by using to install (which installs the node package manager), followed by using the node package manager to install . We first start by preparing an specification file that can use to build your environment: Now, we can execute the installation commands. To learn more about hacks that can improve your efficiency, I have a blog post that you can reference. Write your slides Before we go onto the automation, it\u2019s important that you get a feel for the workflow so you know what\u2019s being automated. Let\u2019s write a simple Markdown file that has two slides: Save it as . The filename isn\u2019t special, it\u2019s just convenient to remember. To see more of what can do, check out the RevealJS GitHub repository! Preview your slides To serve it up, run the following command at your terminal: Your browser should now pop open, and your slides will be there! made simple, thanks to . Continue editing your slides Now, you can continue editing your slides, keeping in mind the following pointers. Firstly, (three of them) denotes horizontal slide transition, while (four of them) denotes vertical slide transitions. Use this to organize your content. Secondly, to progressively reveal pointers on a slide, you need to add the following HTML comment right after the element. For example, to show bullet points progressively: Aside: fancier items If you need fancier things, you can weave in HTML at your own convenience. For example, I embedded an HTML table to organize [](https://github.com/ColCarroll/imcmc) logos that I had previously compiled. Get Travis CI setup You\u2019ll now want to create a , which commands Travis to do things. It\u2019s generally nothing more than a collection of bash commands that are executed in order. An example Travis configuration file from my data science testing talk looks like this: You\u2019ll notice that the commands we want Travis to execute are basically the same as those we executed manually. The only difference now is that we command to build a static site under the directory , which we then command Travis to push to GitHub pages. In the section, we are specifying to Travis that we want all of the content under the directory to be pushed to the branch of our repository. (We have not yet connected Travis to our repo; that will happen next, so sit tight!) Notice also the environment variable: we need to declare that as well. The is an authentication token that GitHub will recognize when Travis CI pushes the directory to . Because underneath the hood we are using syntax in the YAML file, when we declare the , we do it without the symbol, but when we need to grab it from the environment, we include the symbol, just as in regular plain old . Get a GitHub deploy token Under your repository settings, generate a deploy/\"personal access\" token. Exact docs are here, so in the spirit of \"don\u2019t repeat yourself\" and \"learn to read the docs\", I will encourage you to read them. Once you have generated a deploy key, copy them somewhere - you will need it later. (Tip: also make sure you don\u2019t accidentally save it to disk!) Connect Travis CI to your repository On Travis CI, connect your Travis CI account to GitHub, and then enable Travis to look for changes on the repository. Generally, this is done by going to your user settings, and searching for \"Legacy Services Integration\", then toggling the checkbox to enable it on your repository. Once that is done, go into the Travis CI settings for the repository. Navigate to the \"Environment Variables\" section, and declare the there. Be sure to keep it hidden from the output! Turn on GitHub pages To turn on GitHub pages, we are going to stick to a pretty sane and widely-used set of p",
    "tags": [
      "data science",
      "presentation skills",
      "continuous integration"
    ],
    "pub_date": "2020-01-18",
    "type": "blog"
  },
  {
    "id": "blog-pydata-ann-arbor-meetup-testing-for-data-science",
    "url": "/blog/2020/1/16/pydata-ann-arbor-meetup-testing-for-data-science/",
    "title": "PyData Ann Arbor Meetup: Testing for Data Science",
    "summary": "I went to Ann Arbor, MI, to give a talk on testing for data science, and I wanted to share a bit about what Ann Arbor, the PyData community in SE Michigan, and the food here was like.",
    "body": "I had the privilege of being invited to deliver a talk at the PyData Ann Arbor meetup this January, held at TD Ameritrade. My hosts, Sean Law, Rose Putler and Ben Zaitlen were very welcoming and inviting, and I enjoyed my time in Ann Arbor (or A2, as the locals seem to call it). The Talk The talk I delivered was on testing for data scientists. (Slides are available here), and the YouTube video is up too. The topic stemmed from a long-standing problem that I had seen: untested code that I depended on slowing later analyses down because I did not have the confidence that it would behave correctly outside of the original situations I used it in. To communicate this point, I used two examples from work I had done before: one being the use of an automagic testing system, Hypothesis, to ferret out bugs in my code for me, and the other being the use of tests on our data schema to make the creation and caching of views robust and dependable. The Community The PyData Ann Arbor community has some very dedicated members. There was someone who drove a whole 1.5 hours across the US-Canada border from Windsor, ON to listen in on the talk; another came by from a town 53 minutes away. Sean plays the role that Ned Batchelder has done for the Boston Python User Group, and has really fostered a wonderful community here. I was really honored by the dedication that they possessed. There were also a bunch of people I had never met in person, with whom I had interacted with online, with whom I finally got a chance to interact with in-person. It was great to meet some of them, including Bradley Dice (U of M PhD student) and Kyle Eaton (a UX engineer at Superconductive Health who is closely affiliated with the Great Expectations project). The Food Ann Arbor, according to Sean, has quite the foodie scene. It is quite true, even as a land-locked midwestern university town. On the first afternoon here, I went to this restaurant called Hola Seoul, which served Korean-Mexican fusion food. On my second day here, I had Panera for breakfast (not knowing anything better), skipped lunch, but went out with the PyData organizers to Slurping Turtle for some very delicious sushi, fried chicken, and spicy miso ramen. And on my third day here, I decided to enjoy: smoked salmon and avocado omelette at Cafe Zola (it was expensive\u2026 but worth it), and Sava\u2019s for a late lunch with Ben (NVIDIA) and Logan (TD Ameritrade). Career Chat with Sean A few hours before the talk, I had a free-ranging chat with Sean about the problems we tackle in our respective roles. Here\u2019s a smattering of thinking and talking points from our conversation. We share an \"internal consulting\" role at work, where both our teams\u2019 missions are to spearhead new initiatives. His team is explicitly tasked with feeling out what\u2019s upcoming in the next 5 years, POC-ing it, and seeing how it best fits in TD\u2019s systems. It\u2019s a very similar type of role that I\u2019m in. I shared with him the frustrations I had battling what I saw was unnecessary vendor tech. We both seemed to agree that front-liners and decision-makers not operating in the same circles was an important reason for this phenomena. I also learned a few lessons from Sean that I think I need to emphasize more at work: 1. A relentless focus on generating wins for other teams. 2. Strategically cycling between quick wins for credibility, and parlaying that into longer-term (but riskier) wins for the organization. 3. Coffee talk tours throughout the company to spread good ideas. 4. Minimizing surprises on colleagues (especially nasty ones; pleasant surprises are ok though). 5. Strategically holding back on \"just doing everything\", and instead letting colleagues co-create, to generate a sense of ownership over the final product. The Miscellaneous I was picked up by a limo to and from the hotel. While extremely comfortable, I was definitely pleasantly surprised, to the point of being a little bit not used to it. Delta Airlines was half-empty on the way over, but really full on the way back. Either way, my impressions are that without the WiFi access for free, they can\u2019t beat JetBlue for me. That said, PyData Ann Arbor paid for the flight and lodging, so I won\u2019t complain, it was still a comfortable flight nonetheless. This was my first time ever being in Michigan!",
    "tags": [
      "pydata",
      "data science",
      "testing",
      "software skills"
    ],
    "pub_date": "2020-01-16",
    "type": "blog"
  },
  {
    "id": "blog-honk-on-you-fools-insecure-in-your-miniscule-private-parts",
    "url": "/blog/2020/1/12/honk-on-you-fools-insecure-in-your-miniscule-private-parts/",
    "title": "\"Honk on, you fools insecure in your miniscule private parts.\"",
    "summary": "In Massachusetts, how are cyclists supposed to use the road? I always had a few questions about my own knowledge, until I finally decided to re-check the rules of the road for cyclists and motorists. Turns out, I wasn\u2019t wrong all this while.",
    "body": "A pattern I have noticed: the drivers who honk the most at cyclists are the ones who have the largest vehicles. Based on my anecdotal counting (highly inaccurate, probably exaggerated, but captures the effect), 1 in 15 pickup truck drivers will honk at me for cycling on the road in Quincy, and will shout out their window that I should ride on the sidewalk, followed by a gas pedal run right after. Supremely insecure ignoramus is the name I would give them. Usually, out of pure frustration, I just give them the metaphorical finger back by ignoring them. Based on my memory from my driving exam, I vaguely remembered a few pointers about the rules of the road pertaining to cyclist-motorist interactions. I decided to check them out. From the Massachusetts government website, the rules of the road document Chapter 4 (page 108), a selection of rules pertaining to the cyclist-driver interactions and cyclist road usage. As a cyclist: - You can use the full lane (emphasis in original) anywhere, anytime, and on any street (except limited access or express state highways where signs specifically prohibiting bicycles have been posted), even if there is a bike lane (emphasis mine). - You can keep to the right when passing a motor vehicle moving in the travel lane and you can move to the front of an intersection at stop lights. As a motorist: - Do Not Squeeze Bicycles in a Narrow Lane: If a lane is too narrow to pass a bicycle at a safe distance, be PATIENT (emphasis original) until you can safely use an adjacent lane or WAIT (emphasis original) until it is safe to pass in the lane you share. (Chap. 89, Sec. 2) You should stay at least three feet away when passing. (emphasis mine) - Be aware that bicyclists Do Not Always Have to Signal Turns! Bicyclists must signal their intent by either hand to stop or turn. However, the signal does not have to be continuous or be made at all if both hands are needed for the bicycle\u2019s safe operation. (Chap. 85, Sec. 11B). Turns out, I was not wrong about riding on the road! So there, according to Massachusetts\u2019 government-endorsed rules of the road, as a cyclist, I am entitled to use the entire lane whenever I see fit, even in the presence of a bike lane, and am under zero obligation to use the sidewalk simply for a motorist\u2019s convenience. I choose to ride on the right side just to make things easier for drivers (and as an occasional driver myself, I much appreciate it when cyclists do so; the Golden Rule is pretty good here). But where the rules are quite clear, I definitely do not appreciate being honked and shouted at for riding on a road that I am rightfully allowed to use. Addendum To be clear, I\u2019m already thankful it\u2019s only about 1 in 15 pickup-truck drivers and about 1 in 50 regular drivers who honk at me. At the same time, I\u2019m looking for ideas to make the ignoramuses aware of the rules of the road. Would be happy to chat on Mastodon or Twitter if you have ideas.",
    "tags": [
      "cycling",
      "rants"
    ],
    "pub_date": "2020-01-12",
    "type": "blog"
  },
  {
    "id": "blog-fastapi-flask-like-generator-of-web-apis",
    "url": "/blog/2020/1/7/fastapi-flask-like-generator-of-web-apis/",
    "title": "FastAPI: Flask-like generator of web APIs",
    "summary": "Have you heard of FastAPI? I didn't until a recent project at work, and I finally got a chance to test-drive it. Come read on to learn more about why it's fast!",
    "body": "I test-drove FastAPI yesterday, and true to it's name, it's fast! And when the developers say fast, they mean it in at least two ways: 1. It's fast to execute. 2. It's fast to develop an API. I'd like to write about the second point. Prior to FastAPI, my development framework of choice to build a web API would have been Flask (though I will admit, I have only developed web apps, not web APIs before). My familiarity with Flask turned out to be extremely helpful for learning FastAPI: its own API tracks very closely with Flask! Many of the same idioms, including instantiating a object and using decorators with string interpolation in the URIs, carry over. This is the first way that FastAPI makes API development fast: by using existing idioms. Prior to FastAPI, I did have reservations about developing a web API using Flask because I wasn't sure how to provide sufficient documentation to API users/testers on how to use it. (Have you seen Swagger APIs without example usage documentation? Frustrating!) Turns out, FastAPI parses your docstrings to provide API docs. This means you can provide an example usage inside the routing function, and on the endpoint, all they will render on the API docs! So in summary, FastAPI is a very productive tool to use for developing APIs. I highly recommend you check it out once you hit the appropriate opportunity!",
    "tags": [
      "software development",
      "web",
      "data science"
    ],
    "pub_date": "2020-01-07",
    "type": "blog"
  },
  {
    "id": "blog-build-your-digital-profile-as-a-data-scientist",
    "url": "/blog/2020/1/4/build-your-digital-profile-as-a-data-scientist/",
    "title": "Build your digital profile as a data scientist",
    "summary": "I have received questions from others on how to build a digital profile for career development. Everybody\u2019s going to have a unique path, and what I think I can offer are observations on what I think have been helpful for myself and others who have enjoyed similar successes in getting started. My hope is that these pointers, which come from self-reflection over the past two years of work, help you.",
    "body": "I have received questions from others on how to build a digital profile for career development. Everybody\u2019s going to have a unique path, and what I think I can offer are observations on what I think have been helpful for myself and others who have enjoyed similar successes in getting started. My hope is that these pointers, which come from self-reflection over the past two years of work, help you. The maybe-timeless and cross-industry principles First off, here are some things that I think are timeless principles in the job search, and are probably true for a large swathe of professional roles. Firstly, nobody owes me their time or a job; I have a lot of leeway to make it as easy as possible for others to say, \"Yes, I would like to have you on board\". Naturally, this statement ignores structural privileges and inequalities that exist in society, so I would be realistic in what I can accomplish. That said, it's usually a good prior to assume that everybody is busy, and that you need to properly advertise what you can offer. In line with the same idea, I start with the presumption that nobody has the spare time to think deeply about a candidate when there are many other things in one's mind. The corollary is that it is on me to frame how I want others to view me. Data science-specific things Now, for a data science role, being able to demonstrate the following can only help, not hurt, your application and candidacy: 1. Ability to use your skills to solve real problems of value. 2. Good coding practices. 3. Good storytelling and communication ability. 4. Proficiency with building \"products\" that aid in decision-making. 5. Ability to work collaboratively with others. In data science, \"value\" usually means saving money. Saving time = saving money, in case that was not clear. Hence, automation is valuable. Good coding practices are important: I have an essay collection on them, if you need a resource to get started. Communication skills are universally important in professional roles, and data science is no different. What does enhance communication is the ability to build interactive tools that guide a busy decision-maker towards ethical and profitable choices (hopefully in that order). Good value judgment is needed here! Where are prospective hiring teams going to look? This comes from candidates for (I think) 3 candidate searches at work that I was involved in. The order in which I was looking was: 1. Resume 2. LinkedIn 3. GitHub 4. Personal website 5. Google Scholar 6. Old/current research group website The resume is what you turn in. Make sure it's clean, readable, and that it concisely captures exactly what you're looking to communicate to the hiring team: how you're going to be a good fit for the role. When I looked at LinkedIn, I was, quite interestingly, looking at their social network. Who might they plausibly know? At least for the team I'm on, I know credentials and certifications matter less than evidence of projects done, which brings me to the next place... GitHub. I was looking for evidence of candidates' ability to code. A well fleshed-out GitHub profile with publicly browsable repositories and a contribution record that is mostly your own makes it so much easier to see your coding style. I also looked for evidence of familiarity with packages, continuous integration tooling, good version control, and collaborations with other package developers. Your projects that demonstrate the data science skills above should be prominently featured on your profile page. Project types that, in contemporary times, communicate these skills well include: 1. Data products that you've built 2. Teaching material that you've made 3. Contributions you've made to other repositories, in particular pull requests and issues politely raised. I looked at Google Scholar as well to get a flavour for a candidate's prior research work. It's an indication of one's domain expertise, and possibly an indicator of the kinds of problems one will gravitate towards. (This last point has been at least true for myself; however, for one jumping from, say, biological data science to flight data science, this will be much less relevant.) The diversity of one's collaborators also helps paint a picture: did you specialize in work with one other person all of your academic career, or did you work in large teams, or did you work mostly solo? (Don't put a value judgment so quickly: each has their own strengths.) A candidate's old research group is something I would check only out of curiosity, just to know more. The kitchen sink of tips Tip #1: if you put your source code on GitHub, always include in the README why the repository exists, and a guide to how to use the repository. It is a marker of \"sociable working style\": in other words, you're able to think of how others are going to interact with things that you've created. (Using others' tools happens all the time at work!) Tip #2: If you put a notebook up in your repository, be sure to make",
    "tags": [
      "career development",
      "job hunt",
      "data science"
    ],
    "pub_date": "2020-01-04",
    "type": "blog"
  },
  {
    "id": "blog-on-automating-principled-statistical-analyses",
    "url": "/blog/2020/1/2/on-automating-principled-statistical-analyses/",
    "title": "On automating principled statistical analyses",
    "summary": "How automated statistical procedures can still be useful.",
    "body": "I\u2019ve been known to rant against the t-test, because I see it as a canned statistical test that most scientists \"just\" reach for. From a statistical viewpoint, reaching for the t-test by default is unprincipled because our data may not necessarily fulfill the Gaussian-distributed assumptions of the t-test. That isn\u2019t to say, though, that I\u2019m against automated statistical analyses. If there\u2019s a data generating process that will need continual analysis, and we are aware that these processes can be broadly standardized enough that we can use a single statistical model across multiple groups and/or samples, then we might be able to automate the analysis method used. An example from my line of work is standardized high throughput (and/or large-scale) measurements with the same randomized experimental structure. If the high throughput measurement assay stays the same from project to project, and is a standardized assay measurement, then we should be able to use a single statistical model across all samples in the assay. I have done this with large-scale electrophysiology measurements, where we quantified electrophysiological curve decay constants as a function of molecule concentrations, and wrote a custom hierarchical Bayesian model for the data. In another project, my colleagues and I built a hierarchical Bayesian model for enzyme catalysis efficiency. In both cases, because we had confidence that the data generating process was constant over time, we could write a program through which we fed in standardized data and from which we obtained robust, regularized estimates of our quantities of interest. Counterfactually, if we had just picked some quantity and gone with the t-test (or worse, used t-test assumptions with multiple hypothesis correction), we would have likely made a number of errors in our automated analyses that would compound in our later decision-making steps. More pedestrian would have been the fact that I would not have been able to properly defend what we were doing in front of a properly-trained statistician who knows how to use likelihoods in appropriate situations. (Our data didn\u2019t necessarily have t-distributed likelihoods!) There\u2019s always this \"smell test\" that we can do. The \"likelihood smell test\" is a good one. In conclusion, automating a principled statistical analysis is fine, as long as the data generating process is more or less constant. Reaching for a canned test by default is not. And friends, if you write an automated pipeline, don\u2019t forget to write tests!",
    "tags": [
      "data science",
      "statistics",
      "automation",
      "bayesian"
    ],
    "pub_date": "2020-01-02",
    "type": "blog"
  },
  {
    "id": "blog-serving-multiple-panel-apps-together",
    "url": "/blog/2019/12/26/serving-multiple-panel-apps-together/",
    "title": "Serving multiple Panel apps together",
    "summary": "How one can serve up multiple apps on the same Panel server.",
    "body": "I learned a new thing today! If I have a bunch of small dashboard-like utilities, , which uses the server behind the scenes, can serve up multiple files together from the same server. Here's an example. Assume I have the following directory structure: If I start a Panel server here using: If you have a bunch of Jupyter notebooks, the analogous command is: Then all of the apps will be served up, with a default Bokeh landing page provided to link to each of the apps. Doing so lets us build multiple little utilities that can help ourselves and our colleagues be more productive! For an example of this, check out the minimal panel app I built to record these ideas. (Source code is available here.)",
    "tags": [
      "data science",
      "dashboarding",
      "python"
    ],
    "pub_date": "2019-12-26",
    "type": "blog"
  },
  {
    "id": "blog-simplifying-uncertainty-responsibly",
    "url": "/blog/2019/12/19/simplifying-uncertainty-responsibly/",
    "title": "Simplifying Uncertainty Responsibly",
    "summary": "Some reflections on uncertainty after reading a blog post by Brandon Rohrer.",
    "body": "I read an article from Brandon Rohrer titled \"Oversimplify\". The article provoked some thoughts. As someone who strives to model uncertainty in a day-to-day setting, it\u2019s easy to misread Brandon\u2019s article as saying, \"discard your modelling of uncertainty\". It took me a few reads and a bit of thinking to realize that my misunderstanding would have been wrong. The gist of Brandon\u2019s article is to communicate the fact that most people don\u2019t like uncertainty, so we must simplify the communication of uncertainty. In a Bayesian setting, I think this corresponds mostly to \"minimize regret\" when making decisions. Here\u2019s a classic example: weather forecast says \"30% probability of precipitation\" (assume this is rain). How does this fit into Brandon\u2019s conception of simplifying the communication of uncertainty? Simply telling someone the probability of precipitation doesn\u2019t help. It\u2019s like stating an unhelpful fact - unhelpful if the audience doesn\u2019t have a thought framework for acting on that uncertainty. Any good card-carrying Bayesian who also knows how to minimize regret would say, \"there\u2019s a 30% probability of precipitation. Since getting wet is more miserable than carrying an umbrella on a cloudy day, take that umbrella, and throw in your waterproof jacket and boots while you\u2019re at it.\" By contrast, someone other consultant might take the same probabilities, and instead recommend, \"Oh, there\u2019s basically greater odds of not raining than raining, so wear your cotton jacket since it\u2019s cold, but don\u2019t bother about an umbrella.\" This consultant, we might say, is a definite risk-taker, but the consultant is also simplifying the communication of uncertainty by providing an actionable recommendation. The key question most people are seeking an answer to is not \"what do the data say\", but rather \"how should I act?\" Though the latter question is normative, it can be informed by quantitative reasoning. So to summarize: _most people don\u2019t like uncertainty; we can simplify the communication of uncertainty by providing an actionable recommendation based on that uncertainty.",
    "tags": [
      "bayesian",
      "data science",
      "uncertainty",
      "decision making"
    ],
    "pub_date": "2019-12-19",
    "type": "blog"
  },
  {
    "id": "blog-a-review-of-the-python-data-science-dashboarding-landscape-in-2019",
    "url": "/blog/2019/12/15/a-review-of-the-python-data-science-dashboarding-landscape-in-2019/",
    "title": "A Review of the Python Data Science Dashboarding Landscape in 2019",
    "summary": "My reflection on the Python dashboarding landscape. There's been a lot of activity, and I provide some recommendations on how we as data scientists can choose between tools.",
    "body": "This blog post is also available on my collection of essays. Introduction As Pythonista data scientists, we are spoiled for choice when it comes to developing front-ends for our data apps. We used to have to fiddle with HTML in Flask (or Plotly's Dash), but now, there are tools in which \"someone wrote the HTML/JS so I didn't have to\". Let me give a quick tour of the landscape of tools as I've experienced it in 2019. Beginnings: Voila Previously, I had test-driven Voila. The key advantage I saw back then was that in my workflow, once I had the makings of a UI present in the Jupyter notebook, and just needed a way to serve it up independent of having my end-users run a Jupyter server, then Voila helped solve that use case. By taking advantage of existing the ecosystem and adding on a way to run and serve the HTML output of a notebook, Voila solved that part of the dashboarding story quite nicely. In many respects, I regard Voila as the first proper dashboarding tool for Pythonistas. That said, development in a Jupyter notebook didn't necessarily foster best practices (such as refactoring and testing code). When my first project at work ended, and I didn't have a need for further dashboarding, I didn't touch Voila for a long time. Another player: Panel Later, Panel showed up. Panel's development model allowed a more modular app setup, including importing of plotting functions defined inside files that returned individual plots. Panel also allowed me to prototype in a notebook and see the output live before moving the dashboard code into a source file. At work, we based a one-stop shop dashboard for a project on Panel, and in my personal life, I also built a minimal panel app that I also deployed to Heroku. Panel was definitely developed targeting notebook and source file use cases in mind, and this shows through in its source development model. That said, panel apps could be slow to load, and without having a \"spinner\" solution in place (i.e. something to show the user that the app is \"doing something\" in the background), it sometimes made apps feel slow even though the slowness was not Panel's fault really. (My colleagues and I pulled out all the tricks in our bag to speed things up.) Additionally, any errors that show up don't get surfaced to the app's UI, where developer eyeballs are on - instead, they get buried in the browser's JavaScript console or in the Python terminal where the app is being served. When deployed, this makes it difficult to see where errors show up and debug errors. Enter Streamlit Now, Streamlit comes along, and some of its initial demos are pretty rad. In order to test-drive it, I put together this little tutorial on the Beta probability distribution for my colleagues. Streamlit definitely solves some of the pain points that I've observed with Panel and Voila. The most important one that I see is that errors are captured by Streamlit and bubbled up to the UI, where our eyeballs are going to be when developing the app. For me, this is a very sensible decision to make, for two reasons: Firstly, it makes debugging interactions that much easier. Instead of needing to have two interfaces open, the error message shows up right where the interaction fails, in the same browser window as the UI elements. Secondly, it makes it possible for us to use the error messages as a UI \"hack\" to inform users where their inputs (e.g. free text) might be invalid, thereby giving them informative error messages. (Try it out in the Beta distribution app: it'll give you an error message right below if you try to type something that cant be converted into a float!) The other key thing that Streamlit provides as a UI nice-ity is the ability to signal to end-users that a computation is happening. Streamlit does this in three ways, two of which always come for free. Firstly, if something is \"running\", then in the top-right hand corner of the page, the \"Running\" spinner will animate. Secondly, anything that is re-rendering will automatically be greyed out. Finally, we can use a special context manager to provide a custom message on the front-end: So all-in-all, Streamlit seems to have a solution of some kind for the friction points that I have observed with Panel and Voila. Besides that, Streamlit, I think, uses a procedural paradigm, rather than a callback paradigm, for app construction. We just have to think of the app as a linear sequence of actions that happen from top to bottom. State is never really an issue, because every code change and interaction re-runs the source file from top to bottom, from scratch. When building quick apps, this paradigm really simplifies things compared to a callback-based paradigm. Finally, Streamlit also provides a convenient way to add text to the UI by automatically parsing as Markdown any raw strings unassigned to a variable in a file and rendering them as HTML. This opens the door to treating a file as a literate programming document, hosted by a Python-based server in the ba",
    "tags": [
      "dashboarding",
      "python",
      "data science",
      "data visualization",
      "software development"
    ],
    "pub_date": "2019-12-15",
    "type": "blog"
  },
  {
    "id": "blog-principled-git-based-workflow-in-collaborative-data-science-projects",
    "url": "/blog/2019/11/9/principled-git-based-workflow-in-collaborative-data-science-projects/",
    "title": "Principled Git-based Workflow in Collaborative Data Science Projects",
    "summary": "A short synopsis of a recent essay I wrote, on how to use GitFlow in data science projects.",
    "body": "Having worked with GitFlow on a data science project and coming to a few epiphanies with it, I decided to share some of my thoughts in an essay. One of my thoughts here is that most data scientists aren't resistant to using GitFlow (and more generally, just being more intentional about what gets worked on) because it's a bad idea, but because there's a lack of incentives to do so. In there, I try to address this concern. And because GitFlow does require knowledge of Git, it can trigger an, \"Oh no, one more thing to learn!\" response. These things do take time to learn, yes, but I see it also as an investment of time with a future payoff. Apart from that, I hope you enjoy the essay; writing it was also a great opportunity for me to pick up more advanced features of , a package that extends Markdown syntax with other really cool features.",
    "tags": [
      "data science",
      "git",
      "workflow"
    ],
    "pub_date": "2019-11-09",
    "type": "blog"
  },
  {
    "id": "blog-reimplementing-and-testing-deep-learning-models",
    "url": "/blog/2019/10/31/reimplementing-and-testing-deep-learning-models/",
    "title": "Reimplementing and Testing Deep Learning Models",
    "summary": "I share my thoughts on why re-implementing deep learning models from scratch can be a very valuable activity.",
    "body": "Note: this blog post is cross-posted on my personal essay collection on the practice of data science. At work, most deep learners I have encountered have a tendency to take deep learning models and treat them as black boxes that we should be able to wrangle. While I see this as a pragmatic first step to testing and proving out the value of a newly-developed deep learning model, I think that stopping there and not investing the time into understanding the nitty-gritty of the model leaves us in a poor position to know that model's (1) applicability domain (i.e. where the model should be used), (2) computational and statistical performance limitations, and (3) possible engineering barriers to getting the model performant in a \"production\" setting. As such, with deep learning models, I'm actually a fan of investing the time to re-implement the model in a tensor framework that we all know and love, NumPy (and by extension, JAX). Benefits of re-implementing deep learning models Doing a model re-implementation from a deep learning framework into NumPy code actually has some benefits for the time being invested. Developing familiarity with deep learning frameworks Firstly, doing so forces us to know the translation/mapping from deep learning tensor libraries into NumPy. One of the issues I have had with deep learning libraries (PyTorch and Tensorflow being the main culprits here) is that their API copies something like 90% of NumPy API without making easily accessible the design considerations discussed when deciding to deviate. (By contrast, CuPy has an explicit API policy that is well-documented and front-and-center on the docs, while JAX strives to replicate the NumPy API.) My gripes with tensor library APIs aside, though, translating a model by hand from one API to another forces growth in familiarity with both APIs, much as translating between two languages forces growth in familiarity with both languages. Developing a mechanistic understanding of the model It is one thing to describe a deep neural network as being \"like the brain cell connections\". It is another thing to know that the math operations underneath the hood are nothing more than dot products (or tensor operations, more generally). Re-implementing a deep learning model requires combing over every line of code, which forces us to identify each math operation used. No longer can we hide behind an unhelpfully vague abstraction. Developing an ability to test and sanity-check the model If we follow the workflow (that I will describe below) for reimplementing the model, (or as the reader should now see, translating the model between APIs) we will develop confidence in the correctness of the model. This is because the workflow I am going to propose involves proper basic software engineering workflow: writing documentation for the model, testing it, and modularizing it into its logical components. Doing each of these requires a mechanistic understanding of how the model works, and hence forms a useful way of building intuition behind the model as well as correctness of the model. Reimplementing models is not a waste of time By contrast, it is a highly beneficial practice for gaining a deeper understanding into the inner workings of a deep neural network. The only price we pay is in person-hours, yet under the assumption that the model is of strong commercial interest, that price can only be considered an investment, and not a waste. A proposed workflow for reimplementing deep learning models I will now propose a workflow for re-implementing deep learning models. Identify a coding partner Pair programming is a productive way of teaching and learning. Hence, I would start by identifying a coding partner who has the requisite skillset and shared incentive to go deep on the model. Doing so helps a few ways. Firstly, we have real-time peer review on our code, making it easier for us to catch mistakes that show up. Secondly, working together at the same time means that both myself and my colleague will learn something about the neural network that we are re-implementing. Pick out the \"forward\" step of the model The \"forward\" pass of the model is where the structure of the model is defined: basically the mathematical operations that transform the input data into the output observations. A few keywords to look out for are the and class methods. For models that involve an autoencoder, somewhat more seasoned programmers might create a class method called and , which themselves reference another model that would have a or defined. Re-implementing the part of the model is usually a good way of building a map of the equations that are being used to transform the input data into the output data. Inspect the shapes of the weights While the equations give the model structure, the weights and biases, or the parameters, are the part of the model that are optimized. (In Bayesian statistics, we would usually presume a model structure, i.e. the set of equations used alongside",
    "tags": [
      "data science",
      "deep learning",
      "testing",
      "pair programming",
      "code review"
    ],
    "pub_date": "2019-10-31",
    "type": "blog"
  },
  {
    "id": "blog-code-review-in-data-science",
    "url": "/blog/2019/10/30/code-review-in-data-science/",
    "title": "Code review in data science",
    "summary": "My thoughts on how to do code review in data science projects effectively.",
    "body": "This blog post is cross-posted in my essays collection. The practice of code review is extremely beneficial to the practice of software engineering. I believe it has its place in data science as well. What code review is Code review is the process by which a contributor's newly committed code is reviewed by one or more teammate(s). During the review process, the teammate(s) are tasked with ensuring that they - understand the code and are able to follow the logic, - find potential flaws in the newly contributed code, - identify poorly documented code and confusing use of variable names, - raise constructive questions and provide constructive feedback on the codebase. If you've done the practice of scientific research before, it is essentially identical to peer review, except with code being the thing being reviewed instead. What code review isn't Code review is not the time for a senior person to slam the contributions of a junior person, nor vice versa. Why data scientists should do code review The first reason is to ensure that project knowledge is shared amongst teammates. By doing this, we ensure that in case the original code creator needs to be offline for whatever reason, others on the team cover for that person and pick up the analysis. When N people review the code, N+1 people know what went on. (It does not necessarily have to be N == number of people on the team.) In the context of notebooks, this is even more important. An analysis is complex, and involves multiple modelling decisions and assumptions. Raising these questions, and pointing out where those assumptions should be documented (particularly in the notebook) is a good way of ensuring that N+1 people know those implicit assumptions that go into the model. The second reason is that even so-called \"senior\" data scientists are humans, and will make mistakes. With my interns and less-experienced colleagues, I will invite them to constructively raise queries about my code where it looks confusing to them. Sometimes, their lack of experience gives me an opportunity to explain and share design considerations during the code review process, but at other times, they are correct, and I have made a mistake in my code that should be rectified. What code review can be Code review can become a very productive time of learning for all parties. What it takes is the willingness to listen to the critique provided, and the willingness to raise issues on the codebase in a constructive fashion. How code review happens Code review happens usually in the context of a pull request to merge contributed code into the master branch. The major version control system hosting platforms (GitHub, BitBucket, GitLab) all provide an interface to show the \"diff\" (i.e. newly contributed or deleted code) and comment directly on the code, in context. As such, code review can happen entirely asynchronously, across time zones, and without needing much in-person interaction. Of course, being able to sync up either via a video call, or by meeting up in person, has numerous advantages by allowing non-verbal communication to take place. This helps with building trust between teammates, and hence doing even \"virtual\" in-person reviews can be a way of being inclusive towards remote colleagues. Parting words If your firm is set up to use a version control system, then you probably have the facilities to do code review available. I hope this essay encourages you to give it a try.",
    "tags": [
      "essays",
      "data science",
      "workflow",
      "good practices"
    ],
    "pub_date": "2019-10-30",
    "type": "blog"
  },
  {
    "id": "blog-ai-will-not-solve-medicine",
    "url": "/blog/2019/10/29/ai-will-not-solve-medicine/",
    "title": "\"AI will not solve medicine\"",
    "summary": "A mini-rant on why I think hyped-up \"AI\" has a very long way to go, and that medicine is much too complex for us to solve at one shot.",
    "body": "Those who think \"AI will solve medicine\" are delusional. I say this as a practitioner of machine learning in drug discovery and development. First things first, \"AI\" is an overused term. We should stop using it, especially in medicinal research. Now, my thoughts are more sanguine. The real value proposition of machine learning models in drug development is to navigate chemical, sequence, pathway, and knowledge space faster and smarter than we might otherwise do so without machine learning methods. It\u2019s a modeling tool, and nothing more than that. It\u2019s a tool for helping the human collective make better decisions than without it, but it\u2019s also a double-edged sword. We can use the tool and then constrain our thinking because we have that tool, because we want to continue using that tool. Or we can use the tool to our advantage and liberate our mind to think of other things. This thought was sparked off by an email that I was on at work. A molecule was approved for continued investigation (not even \"go for safety trials\"!), and 63 people were on that email. Imagine the number of people who are involved in getting a molecule past all research-phase checkpoints and all 3 clinical trial checkpoints. Hint: Many people are involved. As I combed through the names on that email, the number of machine learners was vastly outnumbered by the number of colleagues who toiled daily at the bench, wrangling with even more uncertainty than that we have at our computers. We machine learners work in service of them, delivering insights and prioritized directions, just as they toil to generate the data that our data-hungry models need. It\u2019s a symbiotic relationship. What do all of those 63 people work on? Some make the molecules. Others design the assays to test the molecules in. Yet others design the assays to find the target to then develop the assay for. It\u2019s many layers of human creativity in the loop. I can\u2019t automate the entirety of their work with my software tools, but I can augment them. I mean, yeah, I can find a new potential target, but ultimately it's a molecular biologist who develops the assay, especially if that assay has never existed before. There are others who professionally manage the progress of the project. There\u2019s sufficient complexity at the bench and in the silicon chips that we can\u2019t each keep track of the big picture. Someone has to do that, and keep everybody focused. And then there\u2019s the handful of us who deal with numbers and mainly just numbers. Yes, it\u2019s a handful. I counted them on my fingers. We do have an outsized impact compared to our numbers, but that\u2019s because we can get computers to do our repetitive work for us. At the bench, robots are harder to work with. Having been at the bench before and failing badly at it, I can very much empathize with how tedious the work is. It\u2019s expensive to collect that data, so the onus is on us computation types to get help navigate \"data space\" more smartly.",
    "tags": [
      "data science",
      "drug development",
      "artificial intelligence",
      "medicine"
    ],
    "pub_date": "2019-10-29",
    "type": "blog"
  },
  {
    "id": "blog-caching-long-running-function-results",
    "url": "/blog/2019/10/18/caching-long-running-function-results/",
    "title": "Caching Long-Running Function Results",
    "summary": ", a really nifty tool for caching function results, is really useful and easy-to-use! Come read why.",
    "body": "I found this nifty tool for caching the results of long-running functions: [](https://pypi.org/project/cachier/). This is useful when we\u2019re building, say, Python applications for which quick interactions are necessary, or for caching the results of a long database query. How do we use it? Basically it\u2019s nothing more than a decorator! Let\u2019s imagine I have a long-running function as below. Turns out, if you have a need to cache the result in a lightweight fashion, you can simply add : The result is stored in your home directory, so the cache is accessible to you. One nice thing also offers is the ability to set a time duration after which the cache goes stale. This can be useful in situations where you know that you need to refresh the cache, such as a database query that may go stale because of new data added into it. This is done by specifying the keyword argument: If you need to reset the cache manually, you can always do: There are other advanced features that provides, and so I\u2019d encourage you to go and take a look at it!",
    "tags": [
      "python",
      "tips",
      "optimization",
      "packages"
    ],
    "pub_date": "2019-10-18",
    "type": "blog"
  },
  {
    "id": "blog-multiple-coin-flips-vs-one-coin-flip-generalized",
    "url": "/blog/2019/10/5/multiple-coin-flips-vs-one-coin-flip-generalized/",
    "title": "Multiple Coin-Flips vs. One Coin Flip Generalized?",
    "summary": "Do we use multiple examples to highlight the same point, or do we take one example and layer on complexity? It all depends.",
    "body": "Do people learn better by: - Generalizing from one example explained well, or by - Having multiple case studies that highlight the same point? I think both are needed, but I am also torn sometimes by whether it\u2019s more effective to communicate using the former or the latter. Case in point: In teaching Bayesian statistics, the coin flip is a particular case of the Beta-Binomial model. However, the Beta-Binomial model can be taken from its most elementary form (estimation on one group) through to its most sophisticated form (hierarchically estimating ). I guess if the goal is to show how broadly applicable a given model class (i.e. the beta-binomial model) is, a teacher would elect to jump between multiple examples that are apparently distinct. However, if the goal is build depth (i.e. going from single group to multiple group estimation), sticking with one example (e.g. of baseball players, classically) would be the better strategy. Both are needed, just at different times, I think. Thinking through this example, I think, gives me a first-principles way of deciding which approach to go for.",
    "tags": [
      "teaching",
      "bayesian",
      "statistics"
    ],
    "pub_date": "2019-10-05",
    "type": "blog"
  },
  {
    "id": "blog-jupyter-server-with-https-on-personal-server",
    "url": "/blog/2019/10/5/jupyter-server-with-https-on-personal-server/",
    "title": "Jupyter Server with HTTPS on Personal Server",
    "summary": "Some notes on how to serve up an HTTPS-enabled Jupyter server.",
    "body": "Recording this for myself, since I did it once and probably don't have the brain bandwidth to remember this through repetition. I have known how to run a \"public\" Jupyter server (password-protected, naturally), but one thing I've struggled with was getting HTTPS working. Turns out, the instructions aren't that bad on Jupyter's docs. I just was ignorant in the past, and didn't know enough about Linux to get this working right. The key here is creating a certificate, and making sure file permissions are set correctly. First off, go to the Certbot page. Select the type of website you're running and operating system. For Jupyter, I chose \"None of the Above\" and \"Ubuntu 18.04 LTS (bionic)\" (even though I'm technically on Ubuntu 19). (Here's a shortcut link to the instructions if you're in the same situation.) On my system (Ubuntu-based), I used the following commands to install : Follow the instructions. will install into a protected directory. In my case, it was . Here, a problem will show up. That directory above is not accessible by a Jupyter server run under a user other than . But a desired property of running Jupyter servers is that we don't have to use to run it. How can we solve this? Basically, by making sure that the certificate is readable by a non- user. What I did, then, was to copy the files that were created by into a location under my home directory. For security by obscurity, I'm naturally not revealing its identity. Then, I changed ownership of those files to my username: Finally, I went into my Jupyter config (, this is well-known), and edited the two lines that specified the \"certfile\" and the \"keyfile\": If this helps you, leave me a note in the comments below. :)",
    "tags": [
      "jupyter",
      "dataops",
      "devops",
      "data science"
    ],
    "pub_date": "2019-10-05",
    "type": "blog"
  },
  {
    "id": "blog-dokku-building-an-internal-heroku-at-work",
    "url": "/blog/2019/9/7/dokku-building-an-internal-heroku-at-work/",
    "title": "Dokku: Building an internal Heroku at work",
    "summary": "Some of the benefits of using Dokku as a PaaS solution for data scientists. It's free, open source, and when paired with the right tools, enables data scientists to focus on making their data app prototypes.",
    "body": "At work, we don\u2019t have a service that has the simplicity of Heroku. Part of it is that we\u2019re still behind what\u2019s available for free in my FOSS life (both commercial and FOSS offerings), and cybersecurity tends to be a gatekeeper against adoption of new things, which is a reality I have to face at work. BUT! I am unwilling to simply bow down to this secnario. \"There\u2019s got to be a better way.\" What does that mean? It means if we want a Heroku-like thing internally, we have to hack together workarounds. Enter Dokku! What is it? It\u2019s a FOSS implementation of the functionality that Heroku provides. It\u2019s only slightly more involved than Heroku, and gives us a really nice taste of what\u2019s possible with Heroku. Dokku claims to be the \"smallest PaaS implementation you\u2019ve ever seen\", and I fully believe it. The maintainers have done a wonderful thing, making the installation process as simple and clean as possible. I\u2019ve successfully installed it on a bare DigitalOcean droplet and on my home Linux tower. I\u2019ve also successfully installed it in EC2 instances at work, albeit needing a few minor modifications to the script they provide. Why would I want to use Dokku? Taking Dokku on my DigitalOcean droplet as an example, what it effectively provides is a self-hosted Heroku. This means you can get 95% of the convenience that Heroku offers, except done in-house. This can be handy if you\u2019ve got cybersecurity standing in the way of awesome convenience, or if finance isn\u2019t willing to shell out the moolah. What can we do with Dokku? Here\u2019s a few neat things that we can do. 1. We can provision a database to run on the same compute node as the app, and then link them together. If your compute node is \"beefy\" enough (RAM/CPU/storage-wise) to handle both the database and the app (and I mean, I\u2019m confident that most disposable apps aren\u2019t going to be at a large scale), then it can be pretty handy, because it means we save on latency. 2. We can deploy apps using either Heroku buildpacks (which look for Procfiles) or using Dockerfiles. Docker containers can be easier to maintain if we have a large and/or complex environment, in my opinion, as we can reuse the existing environment spec, but Procfiles are much nicer for smaller projects. This fits with the paradigm of \"declaring what you need\", rather than \"programming what you need\". 3. Because Dokku is managing everything through isolated Docker containers, we can actually enter into a Docker container and muck around to debug, without worrying about breaking the broader system. I realize now how neat it is to have containerization, but without a unified front-end interface to manage the containers, networking interfaces, and environment variables, it\u2019s tough to keep everything straight. Dokku provides that front-end interface. What are you deploying right now? On my DigitalOcean box, which I use for personal projects, I have deployed both the \"Getting Started\" Ruby app that Heroku provides as well as a minimal app showcasing a minimal dashboard using Panel. The easy part was getting Dokku up and running. The hard part, though, was getting URLs and DNSs right. It took some debugging to get that work correctly. In particular, Dokku uses a concept called virtual hosts () to route from the Dokku host to individual containers. For example, to get up and running correctly, I had to ensure that was routed to my DigitalOcean box. How have we used this at work? At work, I just finished prototyping the use of Dokku on EC2. In particular, I was able to deploy both Dockerfile-based and Procfile-based projects. Once again, getting a domain was the most troublesome part of this project; spinning up an EC2 instance and configuring it became easy using a simple Bash script which we executed on each test spin-up machine. What changes between Heroku and Dokku? The biggest thing I found is that I need to at least have SSH-access to the compute box that is running Dokku. This is because what we would usually configure on Heroku\u2019s web interface (e.g. environment variables), we would instead configure using \u2019s command-line interface via SSH. Hence, not being afraid of the CLI is important. What\u2019s your verdict? If you know Heroku, Dokku gets you 95% of the convenience you\u2019re used to, plus quite a bit more flexibility to customize it to your own compute environment.",
    "tags": [
      "data apps",
      "data science",
      "devops",
      "deployment"
    ],
    "pub_date": "2019-09-07",
    "type": "blog"
  },
  {
    "id": "blog-how-to-be-a-great-code-sprinter",
    "url": "/blog/2019/7/29/how-to-be-a-great-code-sprinter/",
    "title": "How to be a great code sprinter",
    "summary": "Post-PyCon, I detail some of my thoughts on how one can productively participate in a code sprint as a participant.",
    "body": "This blog post is the second in a series of two on participating in code sprints. The first one is here. In this post, I will write about how a sprinter themselves can also help contribute to a positive sprint experience all-ways. Read the docs to understand the scope of the project As a sprinter, we may often have preconceived notions about what a project is about. It helps to have an accurate view on what a project is and isn\u2019t about. This is oftentimes best accomplished by reading the documentation of that project, assuming the docs are well-written. Doing so can help you better align what you think should be done on the project with what the package maintainer sees as priorities for the project. Be ready to make documentation contributions of any scale Documentation is oftentimes the hardest thing for a package maintainer to write, because it often entails slowing down to a beginner\u2019s speed (an unnatural speed at this point), while knowing one\u2019s own blind spots on where a beginner would stumble (also challenging to do). If you are newcomer sprinter, by focusing on the sections of the docs that pertain to \"processes\" (e.g. getting development environment setup) and slowly working through them and documenting what\u2019s missing, that can go a long way to helping other newcomers get set up as well. Anything that the maintainer leaves out may need to be made explicitly clear - and you can help make it clear! Package maintainers, and prior contributors, are human. That means that there inadvertently may be errors in language that may have been inserted into the package. Any small patch that fixes the docs, including even small typographical errors, can be very helpful to improving documentation quality. Don\u2019t be afraid to ask questions... You will find that asking questions can really accelerate your own progress on the project. This is important for getting unstuck, wherever you might be stuck. ...but also try keep your questions to the non-obvious things. That said, asking the too-simple questions that can be answered by a Google query is likely going to steal time and attention away from other sprinters who might have more substantial questions on hand. A pet peeve of mine is asking questions that can be answered in the docs. Asking these questions of the maintainer doesn\u2019t reflect positively on you, the sprinter. Whether or not you intended, what often gets received/communicated to the maintainer is carelessness and a lack of attention to detail, the opposite of both being generally good qualities to possess and project. There\u2019s a pretty broad balance point between the two, so don\u2019t feel inhibited by fear of not hitting a precise balance between looking for docs and asking questions. For any feature requests, try to be ready with a proposed implementation This one I find very important. Having a proposed implementation on hand for a thing that you think should be in the library goes a long way to helping the package maintainer (or other contributors) see what exactly you\u2019re trying to accomplish with that feature. Having a sketch on-hand makes it much easier for the package maintainer to say \"yes\" to the new feature, and having written the documentation and a proposed suite of tests for that new feature makes it even easier. If you aren\u2019t able to propose an implementation, then raising an inquiry rather than a request makes a world of difference in how a package maintainer perceives the communication of the issue at hand. As an example: - Request: \"Package X should be able to do Y.\" - Inquiry: \"Is it within scope for package X to be able to do Y?\" or \"Has this feature Y been considered before?\" The latter are more thoughtful, and communicates much less a sense of entitlement on the part of the sprinter\u2019s request. We\u2019re all building mental maps of each others\u2019 knowledge When two colleagues meet for the first time, we have to build a mental model of each others\u2019 strengths. At a sprint, the package maintainer has to multiply this by however many people are sprinting. If they are making an effort to map your skills against theirs, they may be very verbose, asking lots of questions to clarify what you do and don\u2019t know. It pays to be patient here. If they don\u2019t have the bandwidth to do so (and this is a charitable description for some maintainers), then they may be glossing over detail. Rather than being stuck, it pays to interrupt them gently and clarify. (Taking notes is a very good way of communicating that you\u2019re treating this process seriously too!) Give your sprint leader sufficient context As mentioned above, the sprint leader will oftentimes be context switching from person to person. It\u2019s mentally exhausting, so spoon-feeding a bit more context (such as the thing you\u2019re working on), and condensing your question to the essentials and asking it very precisely can go a long way to helping your sprint leader help you better.",
    "tags": [
      "software development",
      "sprint",
      "code sprint",
      "open source",
      "community",
      "community development"
    ],
    "pub_date": "2019-07-29",
    "type": "blog"
  },
  {
    "id": "blog-pyviz-panel-apps",
    "url": "/blog/2019/7/26/pyviz-panel-apps/",
    "title": "PyViz Panel Apps",
    "summary": "The key things I learned building my first Panel app: prototype in the notebook, use on the thing to serve up, test locally, and use Heroku!",
    "body": "I finally learned how to build and serve apps with Panel! Here are the key ideas: 1. Prototype the app inside a Jupyter notebook. That gives the real-time feedback on whether your apps/widgets are working or not. 2. The most important thing is that the final thing you package together is now a object. 2. Use Panel\u2019s command to test the app locally. It\u2019s actually quite magical - the serve command can actually parse a Jupyter notebook and serve it up on a local web server. 3. When you\u2019ve confirmed that everything is working properly locally, Heroku is a great deployment option. Using the default Python buildpack and a file, one can easily specify the exact Python environment for deployment. As a pedagogical implementation, I put up a minimal panel app on GitHub, and also served it up on Heroku. Come check it out! I hope it\u2019s useful for you.",
    "tags": [
      "data science",
      "data products",
      "app",
      "deployment"
    ],
    "pub_date": "2019-07-26",
    "type": "blog"
  },
  {
    "id": "blog-t-distributed-likelihoods-are-kind-of-neat",
    "url": "/blog/2019/7/23/t-distributed-likelihoods-are-kind-of-neat/",
    "title": "T-distributed likelihoods are kind of neat",
    "summary": "It\u2019s mainly because they are the generalization of two distributions, the Cauchy and Gaussian. Come learn more!",
    "body": "The Student\u2019s T distribution is the generalization of the Gaussian and Cauchy distributions. How so? Basically by use of its \"degrees of freedom\" ($df$) parameter. If we plot the probability density functions of the T distribution with varying degrees of freedom, and compare them to the Cauchy and Gaussian distributions, we get the following: [](./t-distributions.webp) Notice that when $df=1$, the T distribution is identical to the Cauchy distribution, and that as $df$ increases, it gradually becomes more and more like the Normal distribution. At $df=30$, we can consider it to be approximately enough Gaussian. On its own, this is already quite useful; when placed in the context of a hierarchical Bayesian model, that\u2019s when it gets even more interesting! In a hierarchical Bayesian model, we are using samples to estimate group-level parameters, but constraining group parameters to vary mostly like each other, unless evidence in the data suggests otherwise. If we allow the $df$ parameter to vary, then if some groups look more Cauchy while other groups look more Gaussian, this can be flexibly captured in the model.",
    "tags": [
      "data science",
      "statistics",
      "distributions"
    ],
    "pub_date": "2019-07-23",
    "type": "blog"
  },
  {
    "id": "blog-how-to-lead-a-great-code-sprint",
    "url": "/blog/2019/7/21/how-to-lead-a-great-code-sprint/",
    "title": "How to lead a great code sprint",
    "summary": "How can we, as sprint leaders, lead an effective sprint? I share some thoughts, having led a 2nd sprint at SciPy.",
    "body": "This blog post is the first in a series of two blog posts on participating in code sprints, and is the culmination of two other blog posts I\u2019ve written on leading a sprint. In this post, I\u2019ll be writing it from the perspective of what a sprint participant might appreciate from a sprint leader. Write good docs Documentation scales you, the package maintainer. Good docs let others get going without needing your intervention, while bad docs create more confusion. Write good docs ahead-of-time on: - The purpose and scope of the project. - How to get setup for contributing - What contributors should look out for when contributing, including: - Code style - Function scope - Documentation requirements - Testing requirements - How contributors can contribute without necessarily providing code Require documentation as a first contribution This may not necessarily apply to all projects, but for small-ish enough projects, this might be highly relevant. Requiring documentation contributions as the first contribution has a few nice side effects for newcomer contributors: 1. This enforces familiarity with the project before making contributions. 2. It\u2019s a very egalitarian way to kick-off the sprints, reducing the probability of sprinter anxiety from falling behind. 3. This reduces the burden of new contributions for first-time sprinters: docs do not break code! 4. Apart from being non-intimidating, it can sometimes give rise to repetitive tasks that newcomer sprinters with which newcomers can practice git workflow. Make clear who should sprint with your project Mismatched expectations breed frustration; hence, making clear what pre-requisite knowledge participants should have can go a long way to reducing frustrations later on. Drawing on my experience, I would want participants to at the minimum be me of the following: 1. users who have frustrations with the library, and would like to make contributions, or 2. Individuals who wish to make a documentation contribution and don\u2019t mind doing a fine-toothed pass over the docs to figure out what is unclear in the docs. Defining your so-called \"sprint audience\" ahead-of-time can go a long way to making the sprint productive and friendly. Communicate priorities of the project Sprinters want to know that their contributions are going to be valued. Though it is easy to say, \"come talk with me before you embark on a task\", the reality is that you, the sprint lead, are likely going to be extremely overbooked. One way to get around this, I think, is to have a high-level list of priorities for the sprint, which can help sprinters better strategize which tasks to tackle. Communicate this on a whiteboard, large sticky notes, or online Wiki page that you can direct people to. Have a publicly-viewable \"file lock\" Merge conflicts will inadvertently show up if multiple people are contributing to the same file simultaneously. It helps to have a publicly-viewable \"file lock\" on, say, a whiteboard or large sticky notes, so that we know who is working on what file. This helps prevent you, the sprint lead, from accidentally getting two people to work on the same file, and then having to resolve merge conflicts later. In the sprints, I frequently approved two people working on the same notebook; resolving merge conflicts in the notebook JSON later proved to be a big pain! This lesson was one learned hard. Encourage contribution of life-like examples If there are new package users in the crowd who want to get familiar with the package, then encouraging them to contribute life-like examples is a great way to have them make a contribution! This has some nice side effects: 1. In creating the example, they may find limitations in the package that could form the substrate of future contributions. 2. By using the library in the creation of an example, they become users of the project themselves. Celebrate every contribution This one is particularly important for first-time contributors. Oftentimes, they have never done standard Gitflow, and that is intimidating enough. So it doesn\u2019t matter if the contribution is nothing more than deleting an unnecessary plural or correcting a broken URL. We should celebrate that contribution, because they have now learned how to make a contribution (regardless of type), and can repeat the unfamiliar Gitflow pattern until they have it muscle memorized. At the SciPy sprints, for the project, once a contributor\u2019s PR finished building and passed all checks, I brought my iPad over to their table to let them hit the Big Green Button on GitHub. This is one touch I am quite confident our sprinters loved! Recognize and assign non-code tasks While code contributions are useful, I think a great way to encourage them to help out would be to have them help with non-code contributions. A few examples include: - Debugging others\u2019 setups - Triaging/tagging issues on your issue tracker - Talking with sprinters to help them prioritize - Social media sprinting for the project This is bec",
    "tags": [
      "software development",
      "sprint",
      "code sprint",
      "open source",
      "community",
      "community development"
    ],
    "pub_date": "2019-07-21",
    "type": "blog"
  },
  {
    "id": "blog-scipy-2019-post-conference",
    "url": "/blog/2019/7/15/scipy-2019-post-conference/",
    "title": "SciPy 2019 Post-Conference",
    "summary": "Some thoughts post SciPy 2019 - it was overall a productive and fun conference!",
    "body": "It\u2019s my last day in Austin, TX, having finished a long week of conferencing at SciPy 2019. This trip was very fruitful and productive! At the same time, I\u2019m ready for a quieter change - meeting and talking with people does take a drain on my brain, and I have a mildly strong preference for quiet time over interaction time. Tutorials I participated in the tutorials as an instructor for three tutorials, which I think have become my \"data science toolkit\": Bayesian statistical modeling, network analysis, and deep learning. Of the three, the one I had the most fun teaching was the deep learning one. The goal of that tutorial was to peel back a layer behind the frameworks and see what\u2019s going on. To reinforce this and make it all concrete, we live coded a deep learning framework prototype, and it worked! (I didn\u2019t plan for it, and so I was quite nervous while doing it, but we pulled it off as a class, and I think it reinforced the point about revealing what goes on underneath a framework. I also had a lot of fun teaching the Bayesian statistical modeling tutorial, which I had co-created with Hugo Bowne-Anderson, and as always, my personal \"evergreen\" tutorial on Network Analysis always brings me joy, especially when we reach the end and talk about graphs and matrices. I think the material connecting linear algebra to graph concepts is one that the crowd enjoys, and I might emphasize it more going forth at the SciPy tutorials. Talks This year, I delivered a talk on . Excluding lightning talks, this is probably the first time I\u2019ve started my slides one day before having to deliver it (yikes!). Granted, I\u2019ve had the outline in my head for a long time now, I guess having to do the talk was good impetus to actually get it done. Apart from that, there\u2019s a rich selection of talks at SciPy from which I think we can screen at work over lunches (Data Science YouTube). I particularly like the talk on Optuna, a framework for hyperparameter optimization, and I think I\u2019ll be using this tool going forwards. Sprints I did a sprint on with my colleague Zach Barry. This sprint, we had about 20+ sprinters join us, the vast majority of them being first-time sprinters. One thing that stuck for me, this time round, is how even first-timers have different degrees of experience. Some know while most others don\u2019t; most don\u2019t have any prior experience with Gitflow. I had an interaction that led me to realize it\u2019s very important to state meaningfully what \"beginner\" means in concrete terms. For example, a \"beginner\" contributor is probably a user, may or may not have used before, probably doesn\u2019t know GitFlow. A common prerequisite quality amongst contributors would probably be that they would have the patience to 1. Read the documentation, 2. Attempt at least one pass digesting the documentation, and 3. Ask questions regarding the intent behind something before asking for a change. In terms of the things accomplished at this sprint, contributions mainly revolved around: - Improving language in the docs, - New functions, and - New example notebooks. In addition to sprinting, special thanks goes to Felipe Fernandes, who helped me get up onto conda-forge! SciPy is really the place where we can get to meet people and get things done. Career advice learned While at SciPy, I had a chance to talk with Eric Jones, CEO of Enthought. Having described my current role at work, he mentioned how having a team like the one I\u2019m on parked inside IT gives us a very unique position to connect data science work across the organization to the consumers of our data products. When I raised to him my frustrations regarding our infatuation with vendors when FOSS alternatives clearly exist, his advice in return was essentially this: Focus on leveling-up your colleagues skills and knowledge, keep pushing the education piece at work, and don\u2019t worry about the money that gets spent on tooling. Having thought about this, I agree. Over time, we should let the results speak. At the same time, I want to help create the environment that I would like to work in: where my colleagues use the same tooling stack, are hacker-types, aren\u2019t afraid to dig deep into the \"computer stuff\" and into the biology/chemistry, and have the necessary skill + desire to design machine learning systems to systematically accelerate discovery science.",
    "tags": [
      "conference",
      "scipy2019",
      "data science"
    ],
    "pub_date": "2019-07-15",
    "type": "blog"
  },
  {
    "id": "blog-order-of-magnitude-is-more-than-accurate-enough",
    "url": "/blog/2019/7/7/order-of-magnitude-is-more-than-accurate-enough/",
    "title": "Order of magnitude is more than accurate enough",
    "summary": "Or, how thinking less precisely and more accurately can be a helpful thing in data science.",
    "body": "When I was in Science One at UBC in 2006, our Physics professor, Mark Halpern, said a quotable statement that has stuck for many years. Order of magnitude is more than accurate enough. Mark Halpern At the time, that statement rocked the class, myself included. We were classically taught that significant digits are significant, and that we have to keep track of them. But Mark\u2019s quote seemed to throw all of that caution and precision in Physics into the wind. Did what we learn in Physics lab class not matter? Turns out, there was one highly instructive activity that still hasn\u2019t left my mind. We were asked, during a recitation, to estimate how many days the city of Vancouver could be powered for if we took a piece of chalk and converted its entire mass into energy. This clearly required estimation of chalk mass and Vancouver daily energy consumption, both of which we had no way of accurately knowing. Regardless, I took it upon myself to carry significant digits in our calculation, while my recitation partner, Charles Au, was fully convinced that this wasn\u2019t necessary, and so did all calculations order-of-magnitude. We debated and agreed upon what assumptions we needed to arrive at a solution, and then proceeded to do the same calculations, one with significant digits, the other without. We reached the same conclusion. More precisely, I remember obtaining a result along the lines of $6.2 \\cdot 10^3$ days, while Charles obtained $10^4$ days. On an order of magnitude, more or less equivalent. In retrospect, I shouldn\u2019t have been so surprised. Mark is an astrophysicist, and at that scale, 1 or 2 significant digits might not carry the most importance; rather, getting into the right ballpark might be more important. At the same time, the recitation activity was a powerful first-hand experience of that last point: getting into the right ballpark first. At the same time, I was also missing a second perspective, which then explains my surprise at Mark\u2019s quote. Now that I\u2019ve gone the route of more statistics-oriented work, I see a similar theme showing up. John Tukey said something along these lines: Far better an approximate answer to the right question, which is often vague, than an exact answer to the wrong question, which can always be made precise. John Tukey The connection to order of magnitude estimates should be quite clear here. If we\u2019re on an order of magnitude correct on the right questions, we can always refine the answer further. If we\u2019re precisely answering the wrong question, God help us. What does this mean for a data scientist? For one, it means that means approximate methods are usually good enough practically to get ourselves into the right ballpark; we can use pragmatic considerations to decide whether we need a more complicated model or not. It also means that when we\u2019re building data pipelines, minimum viable products, which help us test whether we\u2019re answering the right question, matter more than the fanciest deep learning model. So yes, to mash those two quotes together: >Order of magnitude estimates on the right question are more useful than precise quantifications on the wrong question. Mashup",
    "tags": [
      "data science",
      "estimation",
      "statistics"
    ],
    "pub_date": "2019-07-07",
    "type": "blog"
  },
  {
    "id": "blog-scipy-2019-pre-conference",
    "url": "/blog/2019/7/7/scipy-2019-pre-conference/",
    "title": "SciPy 2019 Pre-Conference",
    "summary": "Looking forward to another fun year at the SciPy conference!",
    "body": "For the 6th year running, I\u2019m out at UT Austin for SciPy 2019! It\u2019s one of my favorite conferences to attend, because the latest in data science tooling is well featured in the conference program, and I get to meet in-person a lot of the GitHub usernames that I interact with online. I will be involved in three tutorials this year, which I think have become my data science toolkit: Bayesian stats, network science, and deep learning. Really excited to share my knowledge; my hope is that at least a few more people find the practical experience I\u2019ve gained over the years useful, and that they can put it to good use in their own work too. This year is also the first year I\u2019ve submitted a talk on , which is a package that I have developed with others for cleaning data, also excited to share this with the broader SciPy community! I\u2019m also looking forward to meeting the conference scholarship recipients. Together with Scott Collis and Celia Cintas, we\u2019ve been managing the FinAid process for the past three years, and each year it heartens me to see the scholarship recipients in person. Finally, this year\u2019s SciPy is quite unique for me, as it is the first year that I\u2019ll be here with colleagues at work! (In prior years, I came alone, and did networking on my own.) I hope they all have as much of a fun time as I have at SciPy!",
    "tags": [
      "conferences",
      "scipy2019"
    ],
    "pub_date": "2019-07-07",
    "type": "blog"
  },
  {
    "id": "blog-bone-marrow-donations",
    "url": "/blog/2019/6/30/bone-marrow-donations/",
    "title": "Bone Marrow Donations",
    "summary": "After hearing that a friend was diagnosed with leukemia, I detailed a reminder to myself and all of us to register as bone marrow donors.",
    "body": "A friend of mine just reached out to me, saying that he\u2019s been diagnosed with leukemia. Thankfully, he\u2019s not subject to the abysmal state of US healthcare (as he lives in a place where healthcare coverage is great), and so he\u2019s on treatment, progressing, and hopefully has a great shot at beating this cancer. He definitely knows how to speak to a data scientist: using data. The odds of a match for a patient who needs a bone marrow transplant are 500:1. That means on average, only about 1 donor in 500 will be a match. On the other hand, under certain assumptions, every 500 donors who registers will mean one life, on average, can be saved. I did some digging myself: According to the US Health Resources and Services Administration, nearly every single minority ethnic group is underrepresented in donor registry databases. As things turn out, signing up to be a donor is quite lightweight. Genetic information - specifically, only Human Leukocyte Antigen (HLA) type - is needed, and that can be obtained in a non-invasive fashion. If a match is found, the donor still has the option to withdraw if they have any objections. As such, the process is completely voluntary for the donor. There are two types of donations possible: peripheral blood stem cells (PBSC) and bone marrow, with PBSC donations being lightweight and bone marrow donations being more involved. Digging a bit deeper, it seems like the only sacrifice a donor has to make is that of time and some discomfort. I\u2019m putting this blog post up as a reminder to myself to register, and to encourage others to do so as well. If you\u2019re in the United States, Be The Match is the organization to get in touch with; if you\u2019re from my home country of Canada, the Canadian Blood Services manages the process.",
    "tags": [
      "personal",
      "charity",
      "leukemia",
      "donations",
      "blood donation",
      "community service"
    ],
    "pub_date": "2019-06-30",
    "type": "blog"
  },
  {
    "id": "blog-graphs-and-matrices",
    "url": "/blog/2019/6/15/graphs-and-matrices/",
    "title": "Graphs and Matrices",
    "summary": "The connection between graphs and linear algebra is profound and deep. I learned that lesson yet again. Come read more about it!",
    "body": "Once again, I\u2019m reminded through my research how neat and useful it is to be able to think of matrices as graphs and vice-versa. I was constructing a symmetric square matrix of values, in which multiple cells of the matrix were empty (i.e. no values present). (Thankfully, the diagonal is guaranteed dense.) From this matrix, I wanted the largest set of rows/columns that formed a symmetric, densely populated square matrix of values, subject to a second constraint that the set of rows/columns also maximally intersected with another set of items. Having thought about the requirements of the problem, my prior experience with graphs reminded me that every graph has a corresponding adjacency matrix, and that finding the densest symmetric subset of entries in the matrix was equivalent to finding cliques in a graph! My intern and I proceeded to convert the matrix into its graph representation, and a few API calls in later, we found the matrix we needed. The key takeaway from this experience? Finding the right representation for a problem, we can computationally solve them quickly by using the appropriate APIs!",
    "tags": [
      "graphs",
      "network science",
      "data science"
    ],
    "pub_date": "2019-06-15",
    "type": "blog"
  },
  {
    "id": "blog-mobile-working-on-the-ipad",
    "url": "/blog/2019/6/14/mobile-working-on-the-ipad/",
    "title": "Mobile Working on the iPad",
    "summary": "How I use my iPad as a mobile working station, and the pros and cons compared to a regular laptop.",
    "body": "A few years ago, I test-drove mobile work using my thesis as a case study, basically challenging myself with the question: how much of my main thesis paper could I write on iOS (specifically, an iPad Mini)? Back then, iOS turned out to be a superb tool for the writing phase (getting ideas into a text editor), and a horrible one for final formatting before submitting a paper to a journal (inflexible). Now, don\u2019t get me wrong, though - I would still use it as part of my workflow if I were to do it again! Fast-forward a few years, I now do more programming in a data science context than I do formal writing, and the tooling for software development and data analysis on iOS has improved greatly. I thought I\u2019d challenge myself with an experiment: how much of development and analytics could I do on an iPad, especially the Mini? This time round, armed with an iPad Pro (11\"), I decided to test again how much one can do on iOS, once again. Software Development I develop [](https://pyjanitor.readthedocs.io) as a tool that I use in my daily work, and as part of my open source software portfolio. When I\u2019m on my MacBook or Pro, or on my Linux desktop at home, I usually work with VSCode for it\u2019s integrated terminal, superb syntax highlighting, git integration, code completion with Kite, and more. Moving to iOS, VSCode is not available, and that immediately means to rely on Terminal-based tools to get my work done. I ponied up for Blink Shell, and found it to pay off immediately. Having enabled remote access on my Linux tower at home, I was thrilled to learn that Blink supports [](https://mosh.org/), and when paired with [](https://github.com/tmux/tmux), it is a superb solution for maintaining persistent shells across spotty internet conditions. A while ago, I also configured with syntax highlighting. As things turned out, syntax highlighting has the biggest effect on my productivity compared to other text editor enhancements (e.g. code completion, git integration, etc.). After I mastered most of 's shortcut keys, I found I could be productive at coding in just itself. Even though missing out on the usual assistive tools meant I was coding somewhat slower, the pace was still acceptable; moreover, relying less on those tools helped me develop a muscle memory for certain API calls. I also found myself becoming more effective because the single window idioms of iOS meant I was focusing on the programming task at hand, rather than getting distracted while looking at docs in a web browser (a surprisingly common happening for me!). Data Analysis For data analysis, Jupyter notebooks are the tool of my choice, for their interactive nature, and the ability to weave a narrative throughout the computation. Jupyter Lab is awesome for this task, but it\u2019s poorly supported on mobile Safari. To use Jupyter notebooks in iOS, the best offering at the moment is Juno, with its ability to connect to a Jupyter server accessible through an IP address or URL. This does require payment, though, and I gladly ponied up for that as well. I run a Jupyter server on my Linux tower at home. Because it has a GPU installed on it, when I am accessing the machine through Juno, my iPad suddenly has access to a full-fledged, fully-configured GPU as part of the compute environment! Coupled with the responsiveness of Juno, this makes for a fairly compelling setup to do Python programming on an iPad. Pros and Cons of iPad-based Development Cons Overall, the experience has been positive, but there have been some challenges, which I would like to detail here. Remote server required: Firstly, because we are essentially using the iPad as a thin client to a remote server, one must either pay for a remote development server in the cloud, or go through the hassle of setting up a development machine that one can SSH into. This may turn off individuals who either are loathe to rent a computer, or don\u2019t have the necessary experience to setup a remote server on their own. iOS Multi-Windowing: It\u2019s sometimes non-trivial to check up source code or function signatures (API docs, really) on the sidebar browser window in iOS. Unlike macOS, in which I have a number of shortcut keys that will let me launch and/or switch between apps, the lack of this capability on iOS means I find myself slowed down because I have to use a bunch of swiping gestures to get to where I need to be. ( seems to be the only exception, for it activates the app switcher, but the number of apps in the app switcher remembers is limited.) Pros Even with the issues detailed above, there\u2019s still much to love about doing mobile development work on an iOS device. iOS Speed: On the latest hardware, iOS is speedy. Well, even that is a bit of an understatement. I rarely get lags while typing in Blink and Juno, and even when I do, I can usually pin it down to network latency more than RAM issues. Focus: This is the biggest win. Because iOS makes it difficult to switch contexts, this is actually an upside for work tha",
    "tags": [
      "productivity",
      "mobile"
    ],
    "pub_date": "2019-06-14",
    "type": "blog"
  },
  {
    "id": "blog-reasoning-about-shapes-and-probability-distributions",
    "url": "/blog/2019/5/29/reasoning-about-shapes-and-probability-distributions/",
    "title": "Reasoning about Shapes and Probability Distributions",
    "summary": "I learned a ton hacking on PyMC4 with the TensorFlow Probability team in Montreal this year, particularly about probability distributions and semantic issues that we can run into with tensor shapes. If you\u2019re curious, read on!",
    "body": "I\u2019m here with the PyMC4 dev team and Tensorflow Probability developers Rif, Brian and Chris in Google Montreal, and have found the time thus far to be an amazing learning opportunity. Prior to this summit, it never dawned on me how interfacing tensors with probability distributions could be such a minefield of overloaded ideas and terminology. Yet, communicating clearly about tensors is important, because if problems can be cast into a tensor-space operation, vectorization can help speed up many operations that we wish to handle. I wanted to share a bit about something new about tensors that I learned here: the different types of shapes involved in a probabilistic programming language. Let\u2019s start by thinking about a few questions involving the most venerable distribution of them all: the Gaussian, also known as the Normal distribution. Let\u2019s start by thinking about a single draw from a standard Gaussian. Drawing one number from the standard Gaussian yields a scalar. In tensor space, a scalar is a rank 0 tensor, and this colloquially means that there\u2019s no dimensions involved. If we drew out the distribution, and drew out the process of drawing numbers from the distribution, it might look like the following: The distribution we are drawing from is on the left, and a draw is represented by a line (on the same numerical axis as the probability distribution), and the event shape, batch shape and sample shape shown to their right, followed by a \"plain English\" description. Over the course of this blog post, the shape concepts will be disambiguated; sit tight and enjoy the ride! What if we were to draw two numbers from this one Gaussian? We could use a vector with two slots to represent those draws. This might look like the following: However, the elementary event of drawing a single number did not fundamentally change when we drew two numbers, as we merely repeated the same event to draw two. With my hands waving in the air, I will claim that this holds true even with K samples drawn from the distribution. Now, what if I had a second Gaussian, say, with a different mean and/or variance? If I were to draw one number from the first Gaussian alongside one number from the second Gaussian, and then concatenate them into a vector, we can represent this as us drawing numbers from independent Gaussians. The illustration below should help clarify how this is different from the first. In this case, we may argue that per distribution, the elementary shape of the event did not change. However, since we have a batch of two distributions, this contributes to the final shape of the tensor. Again, with much waving of my hands in the air, this should extend to more than two distributions. Now, what if we had a multivariate Gaussian, with two variates? This makes for a very interesting case! The elementary event drawn from this multivariate Gaussian is a two-element vector, not a scalar, which means that its shape is apparently identical to the case where we have a single pair of numbers drawn from a batch of two independent Gaussians! This looks like the following: This is interesting, because a single draw from a bivariate Gaussian has the same overall shape as two draws from one Gaussian, which also has the same shape as one draw from a batch of two Gaussians. Yet, these apparently same-shaped draws are shaped differently semantically! In particular, the two independent Gaussians individually have elementary event shapes that are scalar, but when drawn as a batch of two, that is when their shape of forms. On the other hand, the multivariate Gaussian cannot have its two numbers drawn independent of one another (unless this is the special case of diagonal-only covariance - in which case, this is equivalent to independent Gaussians). Hence, the elementary event shape is not scalar, but vector (or more generally, same rank tensor as the mean tensor), but the batch has only a single distribution, hence it has a scalar batch shape. To summarize, here are the various kinds of shapes, defined: Event shape: The atomic shape of a single event/observation from the distribution (or batch of distributions of the same family). Batch shape: The atomic shape of a single sample of observations from one or more distributions of the same family. As an example, we can\u2019t have a batch of a Gaussian and a Gamma distribution together, but we can have a batch of more than one Gaussians. Sample shape: The shape of a bunch of samples drawn from the distributions. And finally, here\u2019s the full spread of possibilities, using one or two draws, uni- or bi-variate Gaussians, and one or two batches of distributions as an illustration. Special thanks goes to fellow PyMC devs, Ravin Kumar, Brandon Willard, Colin Carroll, and Peadar Coyle, who provided feedback on the figure over a late-night tea/dinner/bar session at the end of Day 2. Why Shapes Matter: Broadcasting Why do these different shapes matter? Well, it matters most when we are thinking about broadcasti",
    "tags": [
      "bayesian",
      "probabilistic programming",
      "tensors",
      "data science",
      "probability distributions"
    ],
    "pub_date": "2019-05-29",
    "type": "blog"
  },
  {
    "id": "blog-pycon-2019-sprints",
    "url": "/blog/2019/5/11/pycon-2019-sprints/",
    "title": "PyCon 2019 Sprints",
    "summary": "Together with my colleague Zach Barry, we led a sprint! It was really fun, looking forward to more in the future!",
    "body": "This year was the first year that I decided to lead a sprint! The sprint I led was for , a package that I developed with my colleague, Zach Barry, and a remote collaborator in NYC, Sam Zuckerman (whom I've never met in person!). This being the first sprint I've ever led, I think I was lucky to stumble upon a few ideas that made for a productive, collaborative, and most importantly, fun sprint. pyjanitor? I'm going to deliver a talk on later in the year, so I'll save the details for that talk. The short description of is that if you have one-liners that are difficult to remember, they should become a function in ; if you have a 10-liner that you always copy/paste from another source, they should become a function in . Sprints? Code sprints are a part of PyCon, and it's basically one to four days of intense and focused software development on a single project. Project sprint leads first pitch their projects at the \"Sprintros\", where they indicate what days they will be sprinting, and at what times. The next day, we indicate on which rooms our projects will be sprinting. Volunteers from the conference, who would like to make an open source project contribution, then identify projects they would like to come sprint with. In some senses, there's no way for a sprint leader to know how popular their sprint will be a priori. We have to be prepared to handle a range of scenarios from sprinting with just one other person to sprinting with a crowd. Structure Preparation In preparation for the sprint, I absorbed many lessons learned over the years of sprinting on others' projects. The most obvious one was to ensure that every sprinter had something to do right from the get-go. Having a task from the get-go keeps sprinters, especially newcomers, engaged right from the beginning. This motivated the requirement to make a doc fix before making a code fix. (You can read more below on how we made this happen.) I wrote out this requirement in a number of places, and by the time the sprint day rolled by, this rolled off pretty naturally. The second thing that I did to prep was to triage existing issues and label them as being beginner- or intermediate-friendly, and whether they were doc, infrastructure, or code contributions. Those two things were the my highest priority preparation for the sprint, and I think that helped a ton. Doc Fixes This sprint, I gave the structure some thought, and settled on the following: Before making a code contribution, I required a docs contribution. Docs contributions could be of any scale: - A typographical, grammatical, or spelling error. - A docstring that was unclear. - Installation/setup instructions that are unclear. - A sentence/phrase/word choice that didn't make sense. - New example/tutorial notebooks using the library. I think this worked well for the following reasons: 1. New contributors must read the docs before developing on the project, and hence become familiar with the project. 2. There's always something that can be done better in the docs, and hence, there is something that can be immediately acted on. 3. The task is a pain point personally uncovered by the contributor, and hence the contributor has the full context of the problem. 3. The docs don't break the code/tests, and hence doc contributions are a great way make a contribution without wrestling with more complex testing. 4. Starting everybody who has never worked on on docs is an egalitarian way of on-boarding every newcomer, beginner and experienced individuals alike. Nobody gets special treatment. For each individual's contribution, I asked them to first raise an issue on the GitHub issue tracker describing the contribution that they would like to make, and then clearly indicate in the comments that they would like to work on it. Then, they would go through the process of doing the documentation fix, from forking the repository, cloning it locally, creating a new branch, making edits, committing, pushing, and PR-ing. If two people accidentally ended up working on the same docs issue, I would assess the effort of the later one, and if it was substantial enough, I would allow them to consider it done, and move onto a different issue. Going forth, as the group of contributors expands, I will enforce this \"docs-first\" requirement only for newcomer sprinters, and request experienced ones to help manage the process. Code Contributions Once the docs contributions were done, sprinters were free to either continue with more docs contributions, or provide a code contribution. Code contributions could be of one of the following: 1. New function contributions. 2. Cleaner implementations of existing functions. 4. Restructuring of existing functions. 3. Identification of functions to deprecate (very important!) 4. Infrastructural changes to docs, build system, and more. The process for doing this was identical to docs: raise an issue, claim it, and then make a new branch with edits, and finally PR it. My Role For both days, we had ",
    "tags": [
      "pycon",
      "software development",
      "sprint",
      "open source"
    ],
    "pub_date": "2019-05-11",
    "type": "blog"
  },
  {
    "id": "blog-context-switching",
    "url": "/blog/2019/5/10/context-switching/",
    "title": "Context Switching",
    "summary": "Some realizations on how disruptive context switching can be. My first thoughts on how to deal with it during a sprint...",
    "body": "Context switching is hard. I noticed this when I was at the PyCon sprints, where I was bouncing from sprinter to sprinter, trying to give them each the necessary attention to get their chosen sprint tasks done. After a while, it took a toll on my brain, and I started finding it hard to concentrate on the next problem. Under such circumstances, when one is context switching often (most hopefully out of one's own volition), how do we communicate that we need some ramp-up time, and how can others help us help them? I think one practical thing that can be done is to frequently communicate on each context switch that context ramp-up is needed. In the future, when I switch contexts, first thing I'm going to ask is something along the lines of, \"What context do I need to help me help you?\" Or, if I'm lost, I can clearly communicate what I'm missing - if it's context that I'm missing - by stating, \"I think I'm missing some context. Can you bring me up to speed?\" At least while sprinting, sprinters can definitely help me help them by providing the necessary context up-front. Perhaps this applies more generally as well: when we're asking someone for help, we may be able to help them out by asking them, \"What context from me would help you get up-to-speed here?\"",
    "tags": [
      "programming",
      "coding",
      "data science"
    ],
    "pub_date": "2019-05-10",
    "type": "blog"
  },
  {
    "id": "blog-pycon-2019-tutorial-and-conference-days",
    "url": "/blog/2019/5/10/pycon-2019-tutorial-and-conference-days/",
    "title": "PyCon 2019 Tutorial and Conference Days",
    "summary": "My real-time thoughts on SciPy 2019\u2019s tutorial and conference days. It\u2019s been a pretty awesome experience thus far, though shouldering three tutorials was indeed a marathon for me, much as I love teaching!",
    "body": "It's just been days since I got back from PyCon, and I'm already looking forward to 2020! But I thought it'd be nice to continue the recap. This year at PyCon, I co-led two tutorials, one with Hugo Bowne-Anderson on Bayesian Data Science by Simulation, and the other with Mridul Seth on Network Analysis Made Simple. I always enjoy teaching with Hugo. He brought his giant sense of humour, both figuratively and literally, to this tutorial, and melded it with his deep grasp of the math behind Bayesian statistics, delivering a workshop that, by many points of feedback, is excellent. Having reviewed the tutorial feedback, we've got many ideas for our showcase of Part II at SciPy 2019! This year was the first year that Mridul and I swapped roles. In previous years, he was my TA, helping tutorial participants while I did the main lecturing. This year, the apprentice became the master, and a really good one indeed! Looking forward to seeing him shine more in subsequent tutorial iterations. During the conference days, I spent most of my time either helping out with Financial Aid, or at the Microsoft booth. As I have alluded to in multiple tweets, Microsoft's swag this year was the best of them all. Microelectronics kits in a blue lunch box from Adafruit, and getting set up with Azure. In fact, this website is now re-built with each push on Azure pipelines! Indeed, Steve Dowell from Microsoft told me that this year's best swag was probably getting setup with Azure, and I'm 100% onboard with that! (Fun fact, Steve told me that he's never been called by his Twitter handle () in real-life... until we met.) I also delivered a talk this year, which essentially amounted to a formal rant against canned statistical procedures. I had a ton of fun delivering this talk. The usual nerves still get to me, and I had to do a lot of talking-to-myself-style rehearsals to work off those nerves. For the first time, I also did office hours post-talk at an Open Space, where for one hour, we talked about all things Bayes. Happy to have met everybody who came by; I genuinely enjoyed the chat!",
    "tags": [
      "pycon 2019",
      "conferences",
      "data science"
    ],
    "pub_date": "2019-05-10",
    "type": "blog"
  },
  {
    "id": "blog-pycon-2019-pre-journey",
    "url": "/blog/2019/4/29/pycon-2019-pre-journey/",
    "title": "PyCon 2019 Pre-Journey",
    "summary": "I'm headed out to [PyCon 2019]! This year, I will be co-instructing two tutorials, one on network analysis and one on Bayesian statistics, and...",
    "body": "I'm headed out to [PyCon 2019]! This year, I will be co-instructing two tutorials, one on network analysis and one on Bayesian statistics, and delivering one talk on Bayesian statistics. [PyCon 2019]: https://us.pycon.org/2019 The first tutorial on [network analysis] is based on material that I first developed 5 years ago, and have continually updated. I've enjoyed teaching this tutorial because it represents a different way of thinking about data - in other words, relationally. This year, I will be a co-instructor for [Mridul], who has kindly agreed to step up and teach it this year at PyCon. The apprentice has exceeded the master! [network analysis]: https://us.pycon.org/2019/schedule/presentation/70/ [Mridul]: https://twitter.com/MridulSeth The second tutorial on [Bayesian statistics] is based on [material co-developed][bayesianmaterial] with [Hugo Bowne-Anderson][hugo]. Hugo is a mathematician by training, a pedagogy master, and data science aficionado. Like myself, he is a fan of Bayesian statistical modelling methods, and we first debuted the tutorial past year at SciPy. We're super excited for this one! [Bayesian statistics]: https://us.pycon.org/2019/schedule/presentation/77/ [bayesian_material]: https://github.com/ericmjl/bayesian-stats-modelling-tutorial [hugo]: https://twitter.com/hugobowne The talk that I will deliver is on [Bayesian statistical analysis of case/control tests][bayestalk]. In particular, I noticed a content gap in the data science talks, where case/control comparisons were limited to one case and one control. One epiphany I came to was that if we use Bayesian methods to analyze our data, there's no particular reason to limit ourselves to one case and one control; we can flexibly model multiple cases vs. one control, or even multiple cases vs multiple different controls in the same analysis, in a fashion that is flexible and principled. [bayestalk]: https://us.pycon.org/2019/schedule/presentation/174/ My final involvement with PyCon this year is as Financial Aid Chair. This is the first year that I'm leading the FinAid effort; during previous years, I had learned a ton from the previous chair Karan Goel. My co-chairs this year are Denise Williams and Jigyasa Grover; I'm looking forward to meeting them in 3D! All-in-all, I'm looking forward to another fun year at PyCon!",
    "tags": [
      "pycon",
      "python",
      "data science",
      "conferences"
    ],
    "pub_date": "2019-04-29",
    "type": "blog"
  },
  {
    "id": "blog-variance-explained",
    "url": "/blog/2019/3/24/variance-explained/",
    "title": "Variance Explained",
    "summary": "Have you heard of variance explained as a loss function and machine learning metric? Turns out it\u2019s quite useful and interpretable. I\u2019d like to share this new learning with you.",
    "body": "Variance explained, as a regression quality metric, is one that I have begun to like a lot, especially when used in place of a metric like the correlation coefficient (r2). Here's variance explained defined: $$1 - \\frac{var(y{true} - y{pred})}{var(y{true})}$$ Why do I like it? It\u2019s because this metric gives us a measure of the scale of the error in predictions relative to the scale of the data. The numerator in the fraction calculates the variance in the errors, in other words, the scale of the errors. The denominator in the fraction calculates the variance in the data, in other words, the scale of the data. By subtracting the fraction from 1, we get a number upper-bounded at 1 (best case), and unbounded towards negative infinity. Here's a few interesting scenarios. - If the scale of the errors is small relative to the scale of the data, then variance explained will be close to 1. - If the scale of the errors is about the same scale as the data, then the variance explained will be around 0. This essentially says our model is garbage. - If the scale of the errors is greater than the scale of the data, then the variance explained will be negative. This is also an indication of a garbage model. A thing that is really nice about variance explained is that it can be used to compare related machine learning tasks that have different unit scales, for which we want to compare how good one model performs across all of the tasks. Mean squared error makes this an apples-to-oranges comparison, because the unit scales of each machine learning task is different. On the other hand, variance explained is unit-less. Now, we know that single metrics can have failure points, as does the coefficient of correlation $r^2$, as shown in Ansecombe's quartet and the Datasaurus Dozen: Fig. 1: Ansecombe's quartet, taken from Wikipedia Fig. 2: Datasaurus Dozen, taken from Revolution Analytics One place where the variance explained can fail is if the predictions are systematically shifted off from the true values. Let's say prediction was shifted off by 2 units. $$var(y{true} - y_{pred}) = var([2, 2, ..., 2]) = 0$$ There's no variance in errors, even though they are systematically shifted off from the true prediction. Like $r^2$, variance explained will fail here. As usual, Ansecombe's quartet, as does The Datasaurus Dozen, gives us a pertinent reminder that visually inspecting your model predictions is always a good thing! h/t to my colleague, Clayton Springer, for sharing this with me.",
    "tags": [
      "data science",
      "machine learning"
    ],
    "pub_date": "2019-03-24",
    "type": "blog"
  },
  {
    "id": "blog-functools-partial",
    "url": "/blog/2019/3/22/functools-partial/",
    "title": "Functools Partial",
    "summary": "In praise of , and how I used it in a Flask/Bokeh app!",
    "body": "If you\u2019ve done Python programming for a while, I think it pays off to know some little tricks that can improve the readability of your code and decrease the amount of repetition that goes on. One such tool is . It took me a few years after my first introduction to before I finally understood why it was such a powerful tool. Essentially, what does is it wraps a function and sets a keyword argument to a constant. That\u2019s it. What do we mean? Here\u2019s a minimal example. Let\u2019s say we have a function , not written by me, but provided by someone else. In my code, let\u2019s say that I know that the value that takes on in my app is always the tuple . I now have a few options. The most obvious is assign the tuple to a variable, and pass that in on every function call: The other way I could do it is use and just set the keyword argument to equal to the tuple directly. Now, I can repeat the code above, but now only worrying about the keyword argument : And there you go, that\u2019s basically how works in a nutshell. Now, where have I used this in real life? The most common place I have used it is in Flask. I have built Flask apps where I need to dynamically keep my Bokeh version synced up between the Python and JS libraries that get called. To ensure that my HTML templates have a consistent Bokeh version, I use the following pattern: Now, because I always have pre-specified in , I never have to repeat it over every function call.",
    "tags": [
      "python",
      "hacks",
      "tips and tricks",
      "data science",
      "productivity",
      "coding"
    ],
    "pub_date": "2019-03-22",
    "type": "blog"
  },
  {
    "id": "blog-how-i-work",
    "url": "/blog/2019/3/20/how-i-work/",
    "title": "How I Work",
    "summary": "My tooling, routines, and techniques for getting things done and learning new things!",
    "body": "I was inspired to write this because of Will Wolf\u2019s interview with DeepLearning.AI, in which I found a ton of similarities between how both of us work. As such, I thought I\u2019d write down what I use at work to get things done. Tooling For a data scientist, I think tooling is of very high importance: mastery over our tools keeps us productive. Here\u2019s a sampling of what I use at work: - Compute: I have my own MacBook, but I prefer freeloading off my colleague\u2019s workstation, which is connected to our HPC compute cluster, allowing me to do parallelization with Dask! - Editors/IDEs: VSCode + Jupyter Lab (JLab). Lots of plugins for VSCode! - Terminal: iTerm, with my [](https://github.com/ericmjl/dotfiles) providing a high degree of customization. I also use the VSCode and JLab terminals where convenient. - General Purpose: Python, Dask, git - ML/Stats: , , - Data wrangling: , (a package I wrote to provide convenience APIs for data cleaning) - Data visualization: , , - App development: As you probably can see, I\u2019m a very Python-centric person! Daily/Weekly Routines Most of my work necessitates long stretches of thinking and hacking time. Without that, I\u2019m unable to get into \"the zone\" to do anything productive. Hence, I have a habit of packing meetings onto Mondays (a.k.a. \"Meeting Mondays\"). Backup times for meetings, which I prefer to not do, are 11 am and 1 pm, bookending lunch time so that I don\u2019t end up with a fragmented morning/afternoon. The only exceptions I make are for my two high-priority team meetings, for which I defer to the rest of the team. I\u2019m glad that my managers understand the need for long stretches of hacking time, and have stuck to Monday one-on-one meetings. Hence, almost every day from Tuesday through to Friday, I have long stretches of pre-allocated time for hacking. It\u2019s data science scheduling bliss! It also means I turn down a lot of \"can I meet you to chat\" invites - unless we can pack them on Monday! On Friday, I make a point to try to work remotely. It helps with sanity, particularly in the winter, when the commute gets harsh and I can\u2019t bike. Fridays also are the days on which I try to do my open source work. Pair Coding Pair coding with others on mutual projects has been a very productive endeavor, which I have written about before. Unlike weekly update meetings, I plan for pair coding on an as-needed basis. We have a pre-defined goal for what we want to accomplish, including a conceivably achievable goal and a stretch goal; achieving the easier one keeps us motivated. It follows the \"no agenda, no meeting\" rule of thumb by which I protect my time. I found that a good setup is really necessary for pair coding to be successful. A minimum is a dual-monitor setup, with one extra keyboard + mouse for my coding partner. One thing I didn\u2019t mention in my previous blog post was how knowledge transfer happens. Here\u2019s how I think it works. We have one in the \"driver\u2019s seat\", and the other in the observer role. Knowledge transfer generally happens from the more experienced person to the less experienced one, and the driver doesn\u2019t necessarily have to be the more experienced one. For example, when pair coding with my intern, I play the role of observer and may dictate code or outline what needs to be done, but I don\u2019t actively take over on my keyboard unless there\u2019s a situation that shows up that is irrelevant to the coding session goals. On the other hand, if there\u2019s a codebase I\u2019ve developed for which I need to play the tour guide role, I will be in the driver\u2019s seat, while the observer will help me catch peripheral errors that I\u2019m making. Learning New Things Pair coding has been one way I learn new things. For example, with my colleague Zach as the observer, we hacked together a simple dashboard project using Flask, Holoviews and Panel. I\u2019m not very mathematically-savvy, in that algebra is difficult for me to follow. (I\u2019m mildly algebra-blind, but getting better now.) Ironically, code, which is algebraic in nature too, but works with plain English names, works much better for me. Implementing algorithms and statistical methods using (for things that involve differential computing) and (for all things Bayesian) has served to be very educational. While implementing, I also impose some software abstractions on the math, and this also forces me to organize my knowledge, which also helps learning. Implementing things on the computer is also the perfect way to learn by teaching: The computer is the ultimately dumb student, as it will execute exactly as you tell it, mistakes included!",
    "tags": [
      "data science",
      "productivity"
    ],
    "pub_date": "2019-03-20",
    "type": "blog"
  },
  {
    "id": "blog-pair-coding-why-and-how-for-data-scientists",
    "url": "/blog/2019/3/1/pair-coding-why-and-how-for-data-scientists/",
    "title": "Pair Coding: Why and How for Data Scientists",
    "summary": "In this Q&A-style blog post, I detail how data scientists can begin to engage in pair coding as a more common practice in our day-to-day work, and why we should spend the time to do it as much as we can afford.",
    "body": "Introduction While at work, I've been experimenting with pair coding with other data science-oriented colleagues. My experiences tell me that this is something extremely valuable to do. I'd like to share here the \"why\" and the \"how\" on pair coding, but focused towards data scientists. What is pair coding? Pair coding is a form of programming where two people work together on a single code base together. It usually involves one person on the keyboard and another talking through the problem and observing for issues, such as syntax, logic, or code style. Occasionally, they may swap who is on the keyboard. In other words, one is the \"creator\", and the other is the \"critic\" (but in a positive, constructive fashion). What's your history with pair coding? I was inspired by a few places. Firstly, there are a wealth of blog posts detailing the potential benefits and pitfalls of pair coding, in a software developer's context. (A quick Google search will lead you to them.) Secondly, I had, at work, experimented with \"pair hacking\" sessions, which involved more than coding, including white-boarding a problem to get a feel for its scope, and it turned out to be pretty productive. Thirdly, I was inspired by a New Yorker article on Jeff and Sanjay, in which part of it chronicled how they worked as a pair to solve the toughest problems at Google. Now, because I'm not a software engineer by training, and because don't have extensive experience beforehand, and because there are no data-science-oriented resources for pair coding that I have read before (I'd love to read them if you know of any!), I've had to be adapt what I read for software development to a data science context. What are the potential benefits of pair coding? I can see at least the following benefits, if not more that I have yet to discover: 1. Instant peer review over data science logic and code. Because we are talking through a problem while coding it up, we can instantly check whether our logic is correct against each other. 2. Knowledge transfer. In my experience, I've had productive pair-coding sessions with another colleague who has a better grasp of the project than I do. Hence, I contribute & teach the technical component, while I also learn the broader project context better. 3. Building trust. We all know that the more closely you work with someone, the more rough corners get rubbed off. What pre-requisites do you see for a productive pair programming session? 1. A long, continuous, and uninterrupted time slot (at least 2-3 hours in length) to maintain continuity. 1. A defined goal or question that we are seeking to answer - keeps us focused on what needs to be done. 1. That goal should also be plausibly achievable within the 2-3 hour timeframe. 1. Large monitors for both parties to look at, or a code-sharing platform where both can see the code without needing to physically huddle. 1. A place where we can talk without feeling hindered. 1. No impromptu interruptions from other individuals. 1. Complementary and intersecting skillsets. 1. Open-minded individuals who are willing to learn. (Ego-free.) Where does pair coding differ for data scientists vs. software engineers? I think the differences at best are subtle, not necessarily overt. The biggest difference that I can think of might be in clarity. To the best of my knowledge, software engineers work with pretty well-defined requirements. The only hiccups that I can imagine that may occur are in unforeseen logic/code blockers. Data scientists, on the other hand, often are exploring and defining the requirements as things go along. In other words, we are working with more unknowns than a software engineer might. An example is a model I built with a colleague at work that involved groups of groups of samples. We weren't able to envision the final model right at the beginning, and code towards it. Rather, we built the model iteratively, starting with highly simplifying assumptions, discussing which ones to refine, and iteratively building the model as we went forward. Perhaps a related difference is that as data scientists, because of potentially greater uncertainty surrounding the final product, we may end up talking more about project direction than one would as a software engineer. But that's probably just a minor detail. Do you have any memorable quotes from the New Yorker article? Yes, a number of them. One on scaling things up. Alan Eustace became the head of the engineering team after Rosing left, in 2005. \"To solve problems at scale, paradoxically, you have to know the smallest details,\" Eustace said. Another on pair programming as an uncommon practice: \"I don\u2019t know why more people don\u2019t do it,\" Sanjay said, of programming with a partner. \"You need to find someone that you\u2019re gonna pair-program with who\u2019s compatible with your way of thinking, so that the two of you together are a complementary force,\" Jeff said.",
    "tags": [
      "data science",
      "programming",
      "best practices"
    ],
    "pub_date": "2019-03-01",
    "type": "blog"
  },
  {
    "id": "blog-minimum-viable-products-matter",
    "url": "/blog/2019/1/28/minimum-viable-products-matter/",
    "title": "Minimum Viable Products (MVPs) Matter",
    "summary": "I would like to encourage you to build more \"minimum viable products\" of your projects. Come learn why they\u2019re so valuable!",
    "body": "MVPs matter because they afford us at least two things: 1. Psychological safety 2. Credibility Psychological safety comes from knowing that we have at least a working prototype that we can deliver to whomever is going to consume our results. We aren't stuck in the land of imaginary ideas without something tangible for others to interact with. Credibility comes about because with the MVP on hand, others now can trust on our ability to execute on an idea. Prior to that, all that others have to go off are promises of \"a thing\". Build your MVPs. They're a good thing!",
    "tags": [
      "data science",
      "data products",
      "minimum viable products"
    ],
    "pub_date": "2019-01-28",
    "type": "blog"
  },
  {
    "id": "blog-advi-scalable-bayesian-inference",
    "url": "/blog/2019/1/21/advi-scalable-bayesian-inference/",
    "title": "ADVI: Scalable Bayesian Inference",
    "summary": "I've been exploring a Bayesian hierarchical 4-parameter dose response model at work. Initially, I used a few thousand samples for prototyping, but I've now scaled up to 400K+ samples. Fitting the model with NUTS would've taken a week, but ADVI did the job in just 2.5 hours. \ud83d\ude80 This experience has given me a new appreciation for ADVI, even in simpler models with large datasets. \ud83e\udde0",
    "body": "Introduction You never know when scalability becomes an issue. Indeed, scalability necessitates a whole different world of tooling. While at work, I've been playing with a model - a Bayesian hierarchical 4-parameter dose response model, to be specific. With this model, the overall goal (without going into proprietary specifics) was parameter learning - what's the 50th percentile concentration, what's the max, what's the minimum, etc.; what was also important was quantifying the uncertainty surrounding these parameters. Prototype Phase Originally, when I prototyped the model, I used just a few thousand samples, which was trivial to fit with NUTS. I also got the model specification (both the group-level and population-level priors) done using those same few thousand. At some point, I was qualitatively and quantitatively comfortable with the model specification. Qualitatively - the model structure reflected prior biochemical knowledge. Quantitatively, I saw good convergence when examining the sampling traces, as well as the shrinkage phenomena. Scaling Up Once I reached that point, I decided to scale up to the entire dataset: 400K+ samples, 3000+ groups. Fitting this model with NUTS with the full dataset would have taken a week, with no stopping time guarantees at the end of an overnight run - when I left the day before, I was still hoping for 5 days. However, switching over to ADVI (automatic differentiation variational inference) was a key enabler for this model: I was able to finish fitting the model with ADVI in just 2.5 hours, with similar uncertainty estimates (it'll never end up being identical, given random sampling). Thoughts I used to not appreciate that ADVI could be useful for simpler models; in the past, I used to think that ADVI was mainly useful in Bayesian neural network applications - in other words, with large parameter and large data models. With this example, I'm definitely more informed about what \"scale\" can mean: both in terms of number of parameters in a model, and in terms of number of samples that the model is fitted on. In this particular example, the model is simple, but the number of samples is so large that ADVI becomes a feasible alternative to NUTS MCMC sampling.",
    "tags": [
      "scalability",
      "bayesian model",
      "dose response",
      "parameter learning",
      "model specification",
      "convergence",
      "shrinkage",
      "large dataset",
      "nuts mcmc",
      "advi",
      "variational inference",
      "neural networks",
      "random sampling",
      "biochemistry",
      "data modeling"
    ],
    "pub_date": "2019-01-21",
    "type": "blog"
  },
  {
    "id": "blog-conda-hacks-for-data-science-efficiency",
    "url": "/blog/2018/12/25/conda-hacks-for-data-science-efficiency/",
    "title": "Conda hacks for data science efficiency",
    "summary": "The package manager has, over the years, become an integral part of my workflow. I use it to manage project environments, and have built a bunch of...",
    "body": "The package manager has, over the years, become an integral part of my workflow. I use it to manage project environments, and have built a bunch of very simple hacks around it that you can adopt too. I'd like to share them with you, alongside the rationale for using them. Hack #1: Set up your Why? It will save you a few keystrokes each time you want to do something with . For example, in my , I have the following: For more information on how to configure your , check the online documentation! Hack #2: Use one environment spec file per project This assumes that you have the habit of putting all files related to one project inside one folder, using subdirectories for finer-grained organization. Why? It will ensure that you have one version-controlled, authoritative specification for the packages that are associated with the project. This is good for (1) reproducibility, as you can send it to a colleague and have them reproduce the environment, and (2) will enable Hack #3, which I will showcase afterwards. A hack that I have related to this is that I use TextExpander shortcut to populate a starting environment spec file. Additionally, if I want to install a new package, rather than simply typing , I will add the package to the environment spec file, and then type , as more often than not, my default is to continue using the package I added. For more details on what the environment spec file is all about, read the online docs! Hack 3: use Written by Christine Doig, [](https://github.com/chdoig/conda-auto-env) is a bash hack that enables you to automatically activate an environment once you enter into a project directory, as long as an file already exists in the directory. If the environment does not already exist, then will automatically create one based on the file in your project directory. Why? If you have many projects that you are working on, then it will greatly reduce the amount of effort used to remember which project to activate. looks like this: To use it, you have two options. You can either copy/paste the whole original script into your , or you can put it in a file called , and source it from your . I recommend the latter, as it makes managing your easier: Hack 4: hijack bash aliases for commands I use aliases to save myself a few keystrokes whenever I'm at the terminal. This is a generalizable bash hack, but here it is as applied to commands. Anyways, these are the commands that I use most often, which I have found it useful to alias: Make sure your aliases don't clash with existing commands that you use! Then, source in your : Now, all of your defined aliases will be available in your bash shell. The idea/pattern, as I mentioned earlier, is generalizable beyond just bash commands. (I have aliased for , and aliased for - the epitome of laziness!) Conclusion I hope you found these bash and hacks to be useful. Hopefully they will help you become more productive and efficient!",
    "tags": [
      "data science",
      "conda",
      "hacks"
    ],
    "pub_date": "2018-12-25",
    "type": "blog"
  },
  {
    "id": "blog-gaussian-process-notes",
    "url": "/blog/2018/12/16/gaussian-process-notes/",
    "title": "Gaussian Process Notes",
    "summary": "Here are my notes from learning about Gaussian Processes. It's been a long intellectual journey; hope you find my notes useful.",
    "body": "I first learned GPs about two years back, and have been fascinated by the idea. I learned it through a video by David MacKay, and managed to grok it enough that I could put it to use in simple settings. That was reflected in my Flu Forecaster project, in which my GPs were trained only on individual latent spaces. Recently, though, I decided to seriously sit down and try to grok the math behind GPs (and other machine learning models). To do so, I worked through Nando de Freitas' YouTube videos on GPs. (Super thankful that he has opted to put these videos up online!) The product of this learning is two-fold. Firstly, I have added a GP notebook to my Bayesian analysis recipes repository. Secondly, I have also put together some hand-written notes on GPs. (For those who are curious, I first hand-wrote them on paper, then copied them into my iPad mini using a Wacom stylus. We don't have the budget at the moment for an iPad Pro!) They can be downloaded here. Some lessons learned: - Algebra is indeed a technology of sorts (to quote Jeremy Kun's book). Being less sloppy than I used to be gives me the opportunity to connect ideas on the page to ideas in my head, and express them more succinctly. - Grokking the math behind GPs at the minimum requires one thing: remembering, or else knowing how to derive, the formula for how to get the distribution parameters of a multivariate Gaussian conditioned on some of of its variables. - Once I grokked the math, implementing a GP using only NumPy was trivial; also, extending it to higher dimensions was similarly trivial!",
    "tags": [
      "data science",
      "bayesian"
    ],
    "pub_date": "2018-12-16",
    "type": "blog"
  },
  {
    "id": "blog-mathematical-intuition",
    "url": "/blog/2018/12/9/mathematical-intuition/",
    "title": "Mathematical Intuition",
    "summary": "Last week, I picked up Jeremy Kun's book, \"A Programmer's Introduction to Mathematics\". In it, I finally found an explanation for my frustrations...",
    "body": "Last week, I picked up Jeremy Kun's book, \"A Programmer's Introduction to Mathematics\". In it, I finally found an explanation for my frustrations when reading math papers: What programmers would consider \"sloppy\" notation is one symptom of the problem, but there there are other expectations on the reader that, for better or worse, decelerate the pace of reading. Unfortunately I have no solution here. Part of the power and expressiveness of mathematics is the ability for its practitioners to overload, redefine, and omit in a suggestive manner. Mathematicians also have thousands of years of \"legacy\" math that require backward compatibility. Enforcing a single specification for all of mathematics\u2014a suggestion I frequently hear from software engineers\u2014would be horrendously counterproductive. Reading just that paragraph explained, in such a lucid manner, how my frustrations reading mathematically-oriented papers, stemmed from mismatched expectations. I come into a paper thinking like a software engineer. Descriptive variable names (as encouraged by Python), which are standardized as well, with structured abstractions providing a hierarchy of logic between chunks of code... No, mathematicians are more like Shakespeare - or perhaps linguists - in that they will take a symbol and imbibe it with a subtly new meaning or interpretation inspired by a new field. That \"L\" you see in one field of math doesn't always exactly mean the same thing in another field. Biology vs. Math? The contrast is stark when compared against reading a biology paper. With a biology paper, if you know the key wet-bench experiment types (and there's not that many), you can essentially get the gist of a paper by reading the abstract and dissecting the figures, which, granted, are described and labelled with field-specific jargon, but are at least descriptive names. With a math-oriented paper, the equations are the star, and one has to really grok each element of the equations to know what they mean. It means taking the time to dissect each equation and ask what each symbol is, what each group of symbols means, and how those underlying ideas connect with one another and with other ideas. It's not unlike a biology paper, but requiring a different kind of patience, one that I wasn't trained in. Learning to Learn by Teaching As Jeremy Kun wrote in his book, programmers do have some sort of a leg-up when it comes to reading and understanding math. It's a bit more than what Kun wrote, I think - yes, many programming ideas have deep mathematical connections. But I think there's more. One thing we know from research into how people learn is that teaching someone something is an incredible way to learn that something. From my prior experience, the less background a student has in a material, the more demands are placed on the teacher's understanding of the material, as we work out how the multiple representations in our head to try to communicate it to them. As it turns out, we programmers have the ultimate dumb \"student\" available at our fingertips: Our computers! By implementing mathematical ideas in code, we are essentially \"teaching\" the computer to do something mathematical. Computers are not smart; they are programmed to do exactly what we input to them. If we get an idea wrong, our implementation of the math will likely be wrong. That fundamental law of computing shows up again: Garbage in, garbage out. Hierarchy of Ideas More than just that, when we programmers implement a mathematical idea in code, we can start putting our \"good software engineering\" ideas into place! It helps the math become stickier when we can see, through code, the hierarchy of concepts that are involved. An example, for me, comes from the deep learning world. I had an attempt dissecting two math-y deep learning papers last week. Skimming through the papers didn't do much good for my understanding of the paper. Neither did trying to read the paper like I do a biology paper. Sure, I could perhaps just read the ideas that the authors were describing in prose, but I had no intuition on which to base a proper critique of the idea's usefulness. It took implementing those papers in Python code, writing tests for them, and using abstractions that I had previously written, to come to a place where I felt like the ideas in the paper were a flexibly wieldable tool in my toolkit. Reinventing the wheel, such that we can learn the wheel, can in fact help us decompose the wheel so that we can do other new things with it. Human creativity is such a wonderful thing!",
    "tags": [
      "deep learning",
      "bayesian",
      "math",
      "data science"
    ],
    "pub_date": "2018-12-09",
    "type": "blog"
  },
  {
    "id": "blog-solving-problems-actionably",
    "url": "/blog/2018/11/13/solving-problems-actionably/",
    "title": "Solving Problems Actionably",
    "summary": "There's a quote by John Tukey that has been a recurrent theme at work. It's better to solve the right problem approximately than to solve the wrong...",
    "body": "There's a quote by John Tukey that has been a recurrent theme at work. It's better to solve the right problem approximately than to solve the wrong problem exactly. Continuing on the theme of quoting two Georges: All models are wrong, but some are more useful than others. H/T [Allen Downey][allen] for pointing out that our minds think alike. [allen]: http://www.allendowney.com/wp/ I have been working on a modelling effort for colleagues at work. There were two curves involved, and the second depended on the first one. In both cases, I started with a simple model, and made judgment calls along the way as to whether to continue improving the model, or to stop there because the current iteration of the model was sufficient enough to act on. With first curve, the first model was actionable for me. With the second curve, the first model I wrote clearly wasn't good enough to be actionable, so I spent lots more rounds of iteration on it. But wait, how does one determine \"actionability\"? Actionability For myself, it has generally meant that I'm confident enough in the results to take the next modelling step. My second curves depended on the first curves, and after double-checking multiple ways, I thought the first curve fits, though not perfect, were good enough when applied across a large number of samples that I could instead move on to the second curves. For others, particularly at my workplace, it generally means a scientist can make a decision about what next experiment to run. Insight's MVP Influence Going through Insight Data Science drilled into us an instinct for developing an MVP for our problem before going on to perfect it. I think that general model works well. My project's final modelling results will be the result of chains of modelling assumptions at every step. Documenting those steps clearly, and then being willing to revisit those assumptions, is going always a good thing.",
    "tags": [
      "data science",
      "insight data science"
    ],
    "pub_date": "2018-11-13",
    "type": "blog"
  },
  {
    "id": "blog-thoughts-on-black",
    "url": "/blog/2018/11/12/thoughts-on-black/",
    "title": "Thoughts on Black",
    "summary": "Having used Black for quite a while now, I have a hunch that it will continue to surpass its current popularity amongst projects.",
    "body": "Having used Black for quite a while now, I have a hunch that it will continue to surpass its current popularity amongst projects. It's one thing to be opinionated about things that matter for a project, but don't matter personally. Like code style. It's another thing to actually build a tool that, with one command, realizes those opinions in (milli)seconds. That's exactly what Black does. At the end of the day, it was, and still is, a tool that has a very good human API - that of convenience. By being opinionated about what code ought to look like, has very few configurable parameters. Its interface is very simple. Convenient. By automagically formatting every Python file in subdirectories (if not otherwise configured so), it makes code formatting quick and easy. Convenient. In particular, by being opinionated about conforming to community standards for code style with Python, ensures that formatted code is consistently formatted and thus easy to read. Convenient! Because of this, I highly recommend the use of for code formatting.",
    "tags": [
      "python",
      "code style"
    ],
    "pub_date": "2018-11-12",
    "type": "blog"
  },
  {
    "id": "blog-bayesian-modelling-is-hard-work",
    "url": "/blog/2018/11/7/bayesian-modelling-is-hard-work/",
    "title": "Bayesian Modelling is Hard Work!",
    "summary": "It's definitely not easy work; anybody trying to tell you that you can \"just apply this model and just be done with it\" is probably wrong.",
    "body": "It's definitely not easy work; anybody trying to tell you that you can \"just apply this model and just be done with it\" is probably wrong. Simple Models Let me clarify: I agree that doing the first half of the statement, \"just apply this model\", is a good starting point, but I disagree with the latter half, \"and just be done with it\". I have found that writing and fitting a very naive Bayesian model to the data I have is a very simple thing. But doing the right thing is not. Let's not be confused: I don't mean a Naive Bayes model, I mean naively writing down a Bayesian model that is structured very simply with the simplest of priors that you can think of. Write down the model, including any transformations that you may need on the variables, and then lazily put in a bunch of priors. For example, you might just start with Gaussians everywhere a parameter could take on negative to positive infinity values, or a bounded Half Gaussian if it can only take values above (or below) a certain value. You might assume Gaussian-distributed noise in the output. Let's still not be confused: Obviously this would not apply to a beta-bernoulli/binomial model! Doing the right thing, however, is where the tricky parts come in. To butcher and mash-up two quotes: All models are wrong, but some are useful (Box), yet some models are more wrong than others (modifying from Orwell). Critiquing Models When doing modeling, a series of questions comes up: - Do my naive assumptions about \"Gaussians everywhere\" hold? - Given that my output data are continuous, is there a better distribution that can describe the likelihood? - Is there are more principled prior for some of the variables? - Does my link function, which joins the input data to the output parameters, properly describe their relationship? - Instead of independent priors per group, would a group prior be justifiable? - Does my model yield posterior distributions that are within bounds of reasonable ranges, which come from my prior knowledge? If it does not, do I need to bound my priors instead of naively assuming the full support for those distributions? I am quite sure that this list is non-exhaustive, and probably only covers the bare minimum we have to think about. Doing these model critiques is not easy. Yet, if we are to work towards truthful and actionable conclusions, it is a necessity. We want to know ground truth, so that we can act on it accordingly, and hence take appropriate actions. Prior Experience I have experienced this modeling loop that Mike Betancourt describes (in his Principled Bayesian Workflow notebook) more than once. One involved count data, with a data scientist from TripAdvisor last year at the SciPy conference; another involved estimating cycle time distributions at work, and yet another involved a whole 4-parameter dose-response curve. In each scenario, model fitting and critique took hours at the minimum; I'd also note that with real world data, I didn't necessarily get to the \"win\" was looking for. With the count data, the TripAdvisor data scientist and I reached a point where after 5 rounds of tweaking his model, we had a model that fit the data, and described a data generating process that mimics closely to what we would expect given his process. It took us 5 rounds, and 3 hours of staring at his model and data, to get there! Yet with cycle time distributions from work, a task ostensibly much easier (\"just fit a distribution to the data\"), none of my distribution choices, which reflected what I thought would be the data generating process, gave me a \"good fit\" to the data. I checked by many means: K-S tests, visual inspection, etc. I ended up abandoning the fitting procedure, and used empirical distributions instead. With a 4-parameter dose-response curve, it took me 6 hours to go through 6 rounds of modeling to get to a point where I felt comfortable with the model. I started with a simplifying \"Gaussians everywhere\" assumption. Later, though, I hesitantly and tentatively putting in bound priors because I knew some posterior distributions were completely out of range under the naive assumptions of the first model, and were likely a result of insufficient range in the concentrations tested. Yet even that model remained unsatisfying: I was stuck with some compounds that didn't change the output regardless of concentration, and that data are fundamentally very hard to fit with a dose response curve. Thus I the next afternoon, I modeled the dose response relationship using a Gaussian Process instead. Neither model is completely satisfying to the degree that the count data model was, but both the GP and the dose-response curve are and will be roughly correct modeling choices (with the GP probably being more flexible), and importantly, both are actionable by the experimentalists. Thoughts As you probably can see, whenever we either (1) don't know ground truth, and/or (2) have messy, real world data that don't fit idealized assumptions about the data",
    "tags": [
      "bayesian",
      "data science",
      "statistics"
    ],
    "pub_date": "2018-11-07",
    "type": "blog"
  },
  {
    "id": "blog-more-dask-pre-scattering-data",
    "url": "/blog/2018/10/26/more-dask-pre-scattering-data/",
    "title": "More Dask: Pre-Scattering Data",
    "summary": "I learned a new thing about yesterday: pre-scattering data properly! Turns out, you can pre-scatter your data across worker nodes, and have them...",
    "body": "I learned a new thing about yesterday: pre-scattering data properly! Turns out, you can pre-scatter your data across worker nodes, and have them access that data later when submitting functions to the scheduler. How-To To do so, we first call on , pass in the data that I want to scatter across all nodes, ensure that broadcasting is turned on (if and only if I am sure that all worker nodes will need it), and finally assign it to a new variable. One key thing to remember here is to assign the result of to a variable. This becomes a pointer that you pass into other functions that are submitted via the interface. Because this point is not immediately clear from the docs, I put in a pull request (PR) to provide some just-in-time documentation, which just got merged this morning. By the way, not every PR has to be code - documentation help is always good! Once we've scattered the data across our worker nodes and obtained a pointer for the scattered data, we can parallel submit our function across worker nodes. Let's say we have a function, called , that takes in the variable and returns a number. The key characteristic of this function is that it takes anywhere from a few seconds to minutes to run, but I need it run many times (think hundreds to thousands of times). In serial, I would usually do this as a list comprehension: If done in parallel, I can now use the object to submit the function across all worker nodes. For clarity, let me switch to a instead: Because the does not have to worry about sending the large object across the network of cluster nodes, it is very fast to submit the functions to the scheduler, which then dispatches it to the worker nodes, which all know where is on their own \"virtual cluster\" memory. Advantages By pre-scattering, we invest a bit of time pre-allocating memory on worker nodes to hold data that are relatively expensive to transfer. This time investment reaps dividends later when we are working with functions that operate on the data. Cautions Not really disadvantages (as I can't think of any), just some things to note: 1. You need to know how much memory my data requires, and have to request for at least that amount of memory first per worker node at the the instantiation step. 2. Pre-scattering sometimes takes a bit of time, but I have not seen it take as much time as having the scheduler handle everything. Acknowledgments Special thanks goes to Matt Rocklin, who answered my question on StackOverflow, which in turn inspired this blog post.",
    "tags": [
      "dask",
      "parallel",
      "data science",
      "optimization",
      "gridengine"
    ],
    "pub_date": "2018-10-26",
    "type": "blog"
  },
  {
    "id": "blog-parallel-processing-with-dask-on-gridengine-clusters",
    "url": "/blog/2018/10/11/parallel-processing-with-dask-on-gridengine-clusters/",
    "title": "Parallel Processing with Dask on GridEngine Clusters",
    "summary": "I recently just figured out how to get this working... and it's awesome! :D Motivation If I'm developing an analysis in the Jupyter notebook, and I...",
    "body": "I recently just figured out how to get this working... and it's awesome! :D Motivation If I'm developing an analysis in the Jupyter notebook, and I have one semi-long-running function (e.g. takes dozens of seconds) that I need to run over tens to hundreds of thousands of similar inputs, it'll take ages for this to complete in serial. For a sense of scale, a function that takes ~20 seconds per call run serially over 10,000 similar inputs would take 200,000 seconds, which is 2 days of run-time (not including any other overhead). That's not feasible for interactive exploration of data. If I could somehow parallelize just the function over 500 compute nodes, we could take the time down to 7 minutes. GridEngine-based compute clusters are one of many options for parallelizing work. During grad school at MIT, and at work at Novartis, the primary compute cluster environment that I've encountered has been GridEngine-based. However, because they are designed for batch jobs, as a computational scientist, we have to jump out of whatever development environment we're currently using, and move to custom scripts. In order to do parallelism with traditional GridEngine systems, I would have to jump out of the notebook and start writing job submission scripts, which disrupts my workflow. I would be disrupting my thought process, and lose the interactivity that I might need to prototype my work faster. Enter Dask , alongside enables computational scientists like myself to take advantage of existing GridEngine setups to do interactive, parallel work. As long as I have a Jupyter notebook server running on a GridEngine-connected compute node, I can submit functions to the GridEngine cluster and collect back those results to do further processing, in a fraction of the time that it would take, thus enabling me to do my science faster than if I did everything single core/single node. In this blog post, I'd like to share an annotated, minimal setup for using Dask on a GridEngine cluster. (Because we use Dask, more complicated pipelines are possible as well - I would encourage you to read the Dask docs for more complex examples.) I will assume that you are working in a Jupyter notebook environment, and that the notebook you are working out of is hosted on a GridEngine-connected compute node, from which you are able to tasks. Don't worry, you won't be qsub-ing anything though! Setup To start, we need a cell that houses the following code block: Here, we are instantiating an object under the variable name . What stores is essentially a configuration for a block of worker nodes that you will be requesting. Under the hood, what is doing is submitting jobs to the GridEngine scheduler, which will block off a specified amount of compute resources (e.g. number of cores, amount of RAM, whether you want GPUs or not, etc.) for a pre-specified amount of time, on which Dask then starts a worker process to communicate with the head process coordinating tasks amongst workers. As such, you do need to know two pieces of information: 1. : The queue that jobs are to be submitted to. Usually, it is named something like , but you will need to obtain this through GridEngine. If you have the ability to view all jobs that are running, you can call at the command line to see what queues are being used. Otherwise, you might have to ping your system administrator for this information. 1. : You will also need to pre-estimate the wall clock time, in seconds, that you want the worker node to be alive for. It should be significantly longer than the expected time you think you will need, so that your function call doesn't timeout unexpectedly. I have defaulted to 1.5 million seconds, which is about 18 days of continual runtime. In practice, I usually kill those worker processes after just a few hours. Besides that, you also need to specify the resources that you need per worker process. In my example above, I'm asking for each worker process to use only 1GB of RAM, 1 core, and to use only 1 process per worker (i.e. no multiprocessing, I think). Finally, I can also specify extra environment setups that I will need. Because each worker process is a new process that has no knowledge of the parent process' environment, you might have to source some bash script, or activate a Python environment, or export some environment variable. This can be done under the keyword, which accepts a list of strings. Request for worker compute \"nodes\" I put \"nodes\" in quotation marks, because they are effectively logical nodes, rather than actual compute nodes. (Technically, I think a compute node minimally means one physical hardware unit with CPUs and RAM). In order to request for worker nodes to run your jobs, you need the next line of code: With this line, under the hood, will start submitting 500 jobs, each requesting 1GB of RAM and 1 core, populating my compute environment according to the instructions I provided under . At the end of this, I effectively have a 500-node cluster on ",
    "tags": [
      "parallel",
      "dask",
      "gridengine",
      "data science",
      "optimization"
    ],
    "pub_date": "2018-10-11",
    "type": "blog"
  },
  {
    "id": "blog-optimizing-block-sparse-matrix-creation-with-python",
    "url": "/blog/2018/9/4/optimizing-block-sparse-matrix-creation-with-python/",
    "title": "Optimizing Block Sparse Matrix Creation with Python",
    "summary": "Introduction At work, I recently encountered a neat problem. I'd like to share it with you all. One of my projects involves graphs; specifically, it...",
    "body": "Introduction At work, I recently encountered a neat problem. I'd like to share it with you all. One of my projects involves graphs; specifically, it involves taking individual graphs and turning them into one big graph. If you've taken my Network Analysis Made Simple workshops before, you'll have learned that graphs can be represented as a matrix, such as the one below: Because the matrix is so sparse, we can actually store it as a sparse matrix: [array([0, 0, 1, 2, 3, 3, 3, 4, 4, 5, 5, 5, 5, 6, 7, 7, 8, 8], dtype=int32), array([5, 7, 4, 7, 5, 6, 8, 1, 5, 0, 3, 4, 8, 3, 0, 2, 3, 5], dtype=int32), array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)] The most straightforward way of storing sparse matrices is in the COO (COOrdinate) format, which is also known as the \"triplet\" format, or the \"ijv\" format. (\"i\" is row, \"j\" is col, \"v\" is value) If we want to have two or more graphs stored together in a single matrix, which was what my projects required, then one way of representing them is as follows: Now, notice how there's 25 nodes in total (0 to 24), and that they form what we call a \"block diagonal\" format. In its \"dense\" form, we have to represent $25^2$ values inside the matrix. That's fine for small amounts of data, but if we have tens of thousands of graphs, that'll be impossible to deal with! You'll notice I used a function from , the function, which will create a block diagonal sparse matrix from an iterable of input matrices. is the function that I want to talk about in this post. Profiling performance I had noticed that when dealing with tens of thousands of graphs, was not performing up to scratch. Specifically, the time it needed would scale quadratically with the number of matrices provided. Let's take a look at some simulated data to illustrate this. Let's now define a function to profile the code. It is quite clear that the increase in time is super-linear, showing $O(n^2)$ scaling. (Out of impatience, I did not go beyond 50,000 graphs in this post, but at work, I did profile performance up to that many graphs. For reference, it took about 5 minutes to finish creating the scipy sparse matrix for 50K graphs. Optimizing performance I decided to take a stab at creating an optimized version of . Having profiled my code and discovering that sparse block diagonal matrix creation was a bottleneck, I implemented my own sparse block diagonal matrix creation routine using pure Python. Running it through the same profiling routine: I also happened to have listened in on a talk by Siu Kwan Lam during lunch, on , the JIT optimizer that he has been developing for the past 5 years now. Seeing as how the code I had written in was all numeric code, which is exactly what was designed for, I decided to try optimizing it with JIT. Notice the speed-up that JIT-ing the code provided! (Granted, that first run was a \"warm-up\" run; once JIT-compiled, everything is really fast!) My custom implementation only returns the (row, col, data) triplet. This is an intentional design choice - having profiled the code with and without calling a COO matrix creation routine, I found the JIT-optimized performance to be significantly better without creating the COO matrix routine. As I still have to create a sparse matrix, I ended up with the following design: You'll notice that the array creation step induces a consistent overhead on top of the sparse matrix triplet creation routine, but stays flat and trends the \"jit\" dots quite consistently. It intersects the \"custom\" dots at about $10^3$ graphs. Given the problem that I've been tackling, which involves $10^4$ to $10^6$ graphs at a time, it is an absolutely worthwhile improvement to JIT-compile the function. Conclusion This was simultaneously a fun and useful exercise in optimizing my code! A few things I would take away from this: - Profiling code for bottlenecks can be really handy, and can be especially useful if we have a hypothesis on how to optimize it. - can really speed up array-oriented Python computation. It lives up to the claims on its documentation. I hope you learned something new, and I hope you also enjoyed reading this post as much as I enjoyed writing it!",
    "tags": [
      "graph",
      "optimization",
      "numba",
      "python",
      "data science",
      "sparse matrix"
    ],
    "pub_date": "2018-09-04",
    "type": "blog"
  },
  {
    "id": "blog-3d-printed-wifi-access-qr-codes-part-2",
    "url": "/blog/2018/9/2/3d-printed-wifi-access-qr-codes-part-2/",
    "title": "3D Printed WiFi Access QR Codes: Part 2",
    "summary": "Part 2 of how to create 3D-printed QR codes!",
    "body": "Summary In this blog post, I'll detail how to create a 3D printable QR code model using Python. Recap In the previous blog post, I detailed how to use to create a QR code for a WiFi string. The most important parts were: 1. The WiFi string. It should have the following pattern: 2. The QR code creator. Creating a 3D Printed QR Code with Python Now, let's see how we can create 3D models with Python code. We will need a package called , and optionally to help us with some array processing. (It can be done entirely using built-in lists if needed.) Create QR Code Object To start, I first defined a convenience function that let me create and return a object that can be passed around and manipulated. Its use will become evident later. You'll also notice I'm using type hints inside the function. Create Text Representation Using the function, we can create a text representation of the QR code: This will give essentially a series of 1s and 0s. This is a string, though, not a array. Hence, we may have to convert this into a list of lists, or a array (as a user of the scientific Python stack, I prefer using arrays where possible, but in this case there is no real practical advantage to doing so because we are not doing linear algebra). Create Array Representation Let's now define a function that takes in the object and return an array version of the text rendering. With that, we can create an array version of our QR code above: Create 3D Model Now, we're ready to play with ! is a Python package that provides an interface to the OpenSCAD language. The OpenSCAD language allows a programmer to programmatically define 3D models using the language of geometry. This includes the creation of cubes and other 3D objects, as well as object manipulations, such as translation, coloring, and union-ing. For brevity, I'll not introduce you to more detail on what OpenSCAD is. Rather, I'll recommend two readings, to be read in order: 1. Beginner's Guide to OpenSCAD 1. SolidPython's Documentation Take a look at the code below for an example of how we create the 3D object. This will give the following OpenSCAD code, which I've truncated for brevity: What we've done here is take the 1s and created cubes where they are supposed to be, and leave the zeros empty. Then, we add a \"base plate\" so that everything stays nice and connected, and finally union all of the cubes with the base plate, so that we get one solid piece that is 3D printed. If you observe the output of the function , it will essentially be valid OpenSCAD text. With OpenSCAD text, you can paste it into OpenSCAD to render it: Following that, it can be exported as an STL file. The export process in OpenSCAD takes some time, but once done, it then has to be first converted into a file, which gives a 3D printer the necessary instructions to move its printhead around to print the QR code. In short, the flow is: Conclusions The key things to take away from this blog post are: 1. How to create a text representation of the QR code. 1. How to convert the text representation into an array. 1. How to create a 3D model of the QR code using the array. Now that you have an example of how to create an OpenSCAD file from Python using , I hope you'll go forth and make a ton of fun stuff!",
    "tags": [
      "3d printing",
      "python",
      "qr code"
    ],
    "pub_date": "2018-09-02",
    "type": "blog"
  },
  {
    "id": "blog-3d-printed-wifi-access-qr-codes-part-1",
    "url": "/blog/2018/9/1/3d-printed-wifi-access-qr-codes-part-1/",
    "title": "3D Printed WiFi Access QR Codes: Part 1",
    "summary": "Over the weekend, I embarked on a cool DIY project to create a 3D printed QR code for our guest WiFi network. I used Python and a few packages to generate the QR code. It was a fun, practical project that also satisfied my curiosity about QR codes. Stay tuned for more! \ud83d\ude04\ud83d\udc68\u200d\ud83d\udcbb\ud83d\udcf6",
    "body": "The Project Over the weekend, I embarked on a project to create a 3D printed QR code that guests at our house could scan to gain access to our guest wireless network. Why I decided to do this From the standpoint of practicality, sure, it's trivial to open up phone settings, find the WiFi network name, and give them the password, but... this has the coolness factor associated with it! Imagine scanning a 3D-printed QR code! Until this becomes commonplace, it's a cool thing to be able to do. Anyways, there's a ton of QR code generators out there on the web, and there's more than a handful of WiFi QR code generators out there - so why did I embark on this project? Partly it's borne out of security reasons - I am not giving my WiFi password up to some random website. Who knows whether they're actually storing the passwords? Another part of this is borne out of me wanting to scratch my itch surrounding QR codes. The last time I went to China (Xi'an and Shanghai, specifically), I saw QR codes everywhere. There surely had to be something good we could use this for at home that didn't involve just packing and storage. Getting Setup Ok, let's get started! To create QR codes, all you need are the following packages installed in your environment: 1. [] 2. [] If you want to do the 3D printing part, you'll need another package: 1. [] 2. [/] Finally, if you'd like to work with command line interfaces and Flask, you'll need: 1. [/] 2. [/] Encoding WiFi credentials in a QR code Let's start by creating a QR code for our WiFi guest network. Let's say that these are the security credentials for the network: - SSID (a.k.a. Network Name): - Password: - Security Type (one of WPA or WEP): Because QR codes are merely two-dimensional barcodes that encode a string that can be parsed by another program, in order to create a QR-code that is readable for accessing WiFi, we need a string that can be parsed. This string is structured as follows: So in our case, we would want a string that looks like: Now, we can code up our Python program do encode the QR code for us. I'll assume you're running Python 3.6 or later. With that block of code, you should get a QR code printed to your terminal, just like that! Let's say you wanted to do the simple thing, and just have a regular laser/inkjet printer make a printout of the QR code. To do so, you can save the QR code to disk as a PNG file: Conclusions And just like that, you've used Python to create a WiFi QR code! How easy was that? Along the way, I also used Kite in the Atom text editor while embarking on this project - this allowed me to view documentation and common usage patterns for the packages I imported. Now, if you remember that QR codes are just \"ASCII strings encoded in a 2D barcode\", then you'll know that you can pass any arbitrary string into the function. That means you can come up with any creative use of a short string that needs to be scanned to be useful! For example, you can create business cards with your LinkedIn profile URL embedded in the QR code, or use it to encode a serial number information on your possessions, or more! Stay tuned for the coming blog posts!",
    "tags": [
      "python",
      "qr codes",
      "wifi",
      "3d printing",
      "security",
      "coding",
      "technology",
      "diy",
      "networking",
      "home project",
      "guest network",
      "pyqrcode",
      "pypng",
      "solidpython",
      "numpy",
      "click",
      "flask"
    ],
    "pub_date": "2018-09-01",
    "type": "blog"
  },
  {
    "id": "blog-joint-conditional-and-marginal-probability-distributions",
    "url": "/blog/2018/8/7/joint-conditional-and-marginal-probability-distributions/",
    "title": "Joint, conditional, and marginal probability distributions",
    "summary": "A little cheat sheet to help you remember what joint, conditional, and marginal distributions are.",
    "body": "Joint probability, conditional probability, and marginal probability... These are three central terms when learning about probability, and they show up in Bayesian statistics as well. However... I never really could remember what they were, especially since we were usually taught them using formulas, rather than pictures. Well, for those who learn a bit better using pictures... if you know what a probability distribution is, then hopefully these help with remembering what these terms mean. (Clicking on the image will bring you to the original, hosted on GitHub.) [](https://github.com/ericmjl/distributions/raw/master/joint-conditional-marginal.pdf)",
    "tags": [
      "statistics",
      "probability",
      "bayesian",
      "data science"
    ],
    "pub_date": "2018-08-07",
    "type": "blog"
  },
  {
    "id": "blog-d-separation-in-causal-inference",
    "url": "/blog/2018/8/6/d-separation-in-causal-inference/",
    "title": "d-separation in causal inference",
    "summary": "Yesterday evening, I had an empty block of time during which I finally did a worked example of finding whether two nodes are \"d-separated\" in a...",
    "body": "Yesterday evening, I had an empty block of time during which I finally did a worked example of finding whether two nodes are \"d-separated\" in a causal graph. It was pretty instructive to implement the algorithm. It also reminded me yet again: there's this weird thing about me where I need programming to learn math! Anyways, if you're interested in seeing the implementation, it's available at GitHub.",
    "tags": [
      "causal inference",
      "bayesian",
      "data science"
    ],
    "pub_date": "2018-08-06",
    "type": "blog"
  },
  {
    "id": "blog-nxviz-05-released",
    "url": "/blog/2018/8/1/nxviz-05-released/",
    "title": "nxviz 0.5 released!",
    "summary": "A new version of nxviz is released! In this update, I have added a declarative interface for visualizing geographically-constrained graphs.",
    "body": "A new version of nxviz is released! In this update, I have added a declarative interface for visualizing geographically-constrained graphs. Here, nodes in a graph have their placement constrained by longitude and latitude. An example of how to use it is below: [](./carbon-original.webp) In the GeoPlot constructor API, the keyword arguments and specify which node metadata are to be used to place nodes on the x- and y- axes. By no means do I intend for GeoPlot to replace more sophisticated analysis methods; like , the interface is declarative; for me, the intent is to provide a very quick-and-dirty way for an end user to visualize graphs with spatially constrained nodes. Please enjoy!",
    "tags": [
      "nxviz",
      "visualization",
      "data science",
      "software",
      "open source"
    ],
    "pub_date": "2018-08-01",
    "type": "blog"
  },
  {
    "id": "blog-pyjanitor-0-3-released",
    "url": "/blog/2018/7/27/pyjanitor-0-3-released/",
    "title": "pyjanitor 0.3 released!",
    "summary": "A new release of [](https://github.com/ericmjl/pyjanitor) is out! Two new features that I have added in include: 1.",
    "body": "A new release of [](https://github.com/ericmjl/pyjanitor) is out! Two new features that I have added in include: 1. Concatenating column names into a single column, such that each item is separated by a delimiter. 1. Deconcatenating a column into multiple columns, separating on the basis of a delimiter. Both of these tasks come up frequently in data preparation. For example, concatenating a few columns together oftentimes lets us create an unique index based sample properties. On the other hand, deconcatenating columns into multiple columns can be useful when our index is used to store metadata. (This really shouldn't be happening, but... sometimes that's just how the world works right now...) Here's an example of how it works: [](./carbon-original.webp) To install , grab it from PyPI: The conda-forge build will be coming soon!",
    "tags": [
      "open source",
      "pyjanitor",
      "data science"
    ],
    "pub_date": "2018-07-27",
    "type": "blog"
  },
  {
    "id": "blog-scipy-2018",
    "url": "/blog/2018/7/26/scipy-2018/",
    "title": "SciPy 2018",
    "summary": "It's been about two weeks since SciPy 2018 ended, and I've finally found some breathing room to write about it.",
    "body": "It's been about two weeks since SciPy 2018 ended, and I've finally found some breathing room to write about it. SciPy 2018 is the 4th year I've made it to the conference, my first one being SciPy 2015 (not 2014, as I had originally incorrectly remembered). The conference has grown over the years that I've attended it! Organizing This year, I served again as the Financial Aid Co-Chair with Scott Collis and Celia Cintas. It brings me joy to be able to help bring others to the conference, much as I was a few years back when I was still a graduate student. Building upon last year's application process, where we implemented blinded reviews, this year, we improved the review process so that it was much less tedious and more user-friendly for reviewers, i.e. myself, Celia, Scott and our two committee reviewers, Kasia and Patrick. The review process can always be improved; we still have some work to do. One would be making the application less intimidating for under-represented individuals. Two might be reworking how we quantify how good our selections are; rather than some aggregate \"total\" score, it might be that we ought to optimize for a breadth of worthy contributions to the community. Finally, we definitely want to ensure that our focus is on FinAid's mission: to enable us to bring deserving and new people to the conference who otherwise might not have the resources! Tutorials I did two tutorials, one with Hugo and one with Mridul. The one with Mridul was on Network Analysis, and the one with Hugo was on Bayesian statistics. Over the years, I've developed muscle memory on Network Analysis, so it felt very natural to me. Bayesian statistics and probabilistic programming was a new topic for myself and Hugo; as such, I spent proportionally more time preparing for that tutorial instead. What was pleasantly surprising for me was that Bayesian statistics was gaining a ton of popularity, and this tutorial just happened to be there at the right time. I had a lot of one-on-one chats with tutorial participants after the tutorial and during the conference days, where we talked about the application of Bayesian methods to problems that they had encountered. There's a lot to do until people can generally communicate about data problems using Bayesian methods, but I think we're at an upwards-inflection point right now! Conference I missed the talks mostly because I was doing the Tejas Room track (sit in the Tejas room and chat with people). I nonetheless had a very fruitful and fun time doing so! To make up for lost time, I put together a [playlist][playlist] of things I'd like to catch up on later. [playlist]: https://www.youtube.com/playlist?list=PLW01hpWnEtbTORVE7o70mZqB7IVVjOr4E Sprints For the first time ever, I stayed on to sprint! However, I also simultaneously caught a conference bug, so I was basically knocked out for the second day of sprints. For this year's sprints, I implemented a declarative interface for geographic graph visualizations in nxviz, where node placement is prioritized according by geographic information. The intent here isn't to replace geospatial analysis packages, but rather to provide a quick, -like view into a graph's geographical structure. Once a user has a feel for the data, if nothing more is needed, they can use the graph as is; otherwise, they can move onto a different package.",
    "tags": [
      "scipy",
      "conferences",
      "python"
    ],
    "pub_date": "2018-07-26",
    "type": "blog"
  },
  {
    "id": "blog-bayesian-estimation-group-comparison-and-workflow",
    "url": "/blog/2018/7/16/bayesian-estimation-group-comparison-and-workflow/",
    "title": "Bayesian Estimation, Group Comparison, and Workflow",
    "summary": "Over the past year, having learned about Bayesian inference methods, I finally see how estimation, group comparison, and model checking build upon...",
    "body": "Over the past year, having learned about Bayesian inference methods, I finally see how estimation, group comparison, and model checking build upon each other into this really elegant framework for data analysis. Parameter Estimation The foundation of this is \"estimating a parameter\". In a typical situation, we are most concerned with the parameter of interest. It could be a population mean, or a population variance. If there's a mathematical function that links the input variables to the output (a.k.a. \"link function\"), then the parameters of the model are that function's parameters. The key point here is that the atomic activity of Bayesian analysis is the estimation of a parameter, and its associated uncertainty. Comparison of Groups Building on that, we can then estimate parameters for more than one group of things. As a first pass, we could assume that each of the groups are unrelated, and thus \"independently\" (I'm trying to avoid overloading this term) estimate parameters per group under this assumption. Alternatively, we could assume that the groups are related to one another, and thus use a \"hierarchical\" model to estimate parameters for each group. Once we've done that, what's left is the comparison of parameters between the groups. The simplest activity is to compare the posterior distributions' 95% highest posterior densities, and check to see if they overlap. Usually this is done for the mean, or for regression parameters, but the variance might also be important to check as well. Model Check Rounding off this framework is model checking: how do we test that the model is a good one? The bare minimum that we should do is simulate data from the model - it should generate samples whose distribution looks like the actual data itself. If it doesn't, then we have a problem, and need to go back and rework the model structure until it does. Could it be that we have a model that only fits the data on hand (overfitting)? Potentially so - and if this is the case, then our best check is to have an \"out-of-sample\" group. While a train/test/validation split might be useful, the truest test of a model is new data that has been collected. Thoughts These three major steps in Bayesian data analysis workflows did not come together until recently; they each seemed disconnected from the others. Perhaps this was just an artefact of how I was learning them. However, I think I've finally come to a unified realization: Estimation is necessary before we can do comparison, and model checking helps us build confidence in the estimation and comparison procedures that we use. Summary When doing Bayesian data analysis, the key steps that we're performing are: 1. Estimation 2. Comparison 3. Model Checking",
    "tags": [
      "bayesian",
      "statistics",
      "data science"
    ],
    "pub_date": "2018-07-16",
    "type": "blog"
  },
  {
    "id": "blog-ecdfs",
    "url": "/blog/2018/7/14/ecdfs/",
    "title": "ECDFs",
    "summary": "I detail why ECDFs are superior to histograms as a way of visualizing distributions. In short, they provide richer information than histograms do. Come learn about them!",
    "body": "In my two SciPy 2018 co-taught tutorials, I made the case that ECDFs provide richer information compared to histograms. My main points were: 1. We can more easily identify central tendency measures, in particular, the median, compared to a histogram. 1. We can much more easily identify other percentile values, compared to a histogram. 1. We become less susceptible to outliers arising from binning issues. 1. It is more difficult to hide multiple modes. 1. We can easily identify repeat values. What are ECDFs? ECDFs stand for the \"empirical cumulative distribution function\", and they map every data point in the dataset to a quantile, which is a number between 0 and 1 that indicates the cumulative fraction of data points smaller than that data point itself. To illustrate, let's take a look at the following plots. Let's compare the ECDF and the histogram for this data. Is the central tendency measure easily discoverable? We might say that there's some peak at just below the x-axis at just above zero, but is that the mode, median or mean? And what is its exact value? On the other hand, at least the median is easily discoverable on the ECDF: Draw a horizontal line from 0.5 on the y-axis until it crosses a data point, and then drop a line down to the x-axis to get the median value. Are percentiles easily discoverable? It's much clearer that the answer is \"yes\" for the ECDF, and \"no\" for the histogram. What is the value of the potential outlier? Difficult to tell on the histogram: it could be anywhere from 4 to 5 (high outlier) and maybe -3 to -4 on the low outlier. On the other hand, just drop a line down from the suspected outliers to the x-axis to read off their values. Is this a mixture distribution or is this a single Normal distribution? If you looked at the histogram, you might be tempted to think that the data are normally distributed with mean 0.5 and standard deviation about 2. However, if you look at the ECDF, it's clear that there are multiple modes, as shown by two or three sigmoidal-like curves. This should give us pause to see if there's a mixture distribution at play here. Are there repeat values? You can't tell in a histogram. However, it's evidently clear on the ECDF scatterplot that there's no repeat values -- they would show up on the plot as vertical stacks of dots. (Repeat-values might be important when working with, say, a zero- or X-inflated distribution.) Summary I hope this post showed you why ECDFs contain richer information than histograms. They're taught less commonly than histograms, so people will have a harder time interpreting them at first glance. However, a bit of guidance and orientation will bring out the rich information on the ECDFs. Credits I credit Justin Bois (Caltech) for teaching me about ECDFs, and Hugo Bowne-Anderson (DataCamp) for reinforcing the idea.",
    "tags": [
      "statistics",
      "visualization",
      "data science"
    ],
    "pub_date": "2018-07-14",
    "type": "blog"
  },
  {
    "id": "blog-git-tip-apply-a-patch",
    "url": "/blog/2018/6/17/git-tip-apply-a-patch/",
    "title": "Git Tip: Apply a Patch",
    "summary": "I learned a new thing this weekend: we apparently can apply a patch onto a branch/fork using . There's a few things to unpack here.",
    "body": "I learned a new thing this weekend: we apparently can apply a patch onto a branch/fork using . There's a few things to unpack here. First off, what's a ? The long story cut short is that a is nothing more than a plain text file that contains all information about between one commit and another. If you've ever used the command, you'll know that it will output a between the current state of a repository, and the last committed state. Let's take a look at an example. Say we have a file, called . In a real world example, this would be parallel to, say, a module that you've written. After a bunch of commits, I have a directory structure that looks like this: The contents of are as follows: Now, let's say I edit the text file by adding a new line and removing one line. If I looked at the \"diff\" between the current state of the file and the previous committed state of the file: While this may look intimidating at first, the key thing that one needs to look at is the and . The signals that there is an addition of one line, and the signals the removal of one line. Turns out, I can export this as a file. Now, let's simulate the scenario where I accidentally discarded those changes in the repository. A real-world analogue happened to me while contributing to CuPy: I had a really weird commit history, and couldn't remember how to rebase, so I exported the patch from my GitHub pull request (more on this later) and applied it following the same conceptual steps below. Now, the repository is in a \"cleaned\" state -- there are no changes made: Since I have saved the diff as a file, I can apply it onto my project: Looking at the diff again, I've recovered the changes that were lost! Don't forget to commit and push! How to export a patch from GitHub? I mentioned earlier that I had exported the patch file from GitHub and then applied it on a re-forked repository. How does one do that? It's not as hard as you think. Here's the commands below with comments.",
    "tags": [
      "git",
      "version control",
      "code snippets"
    ],
    "pub_date": "2018-06-17",
    "type": "blog"
  },
  {
    "id": "blog-my-latent-dissatisfaction-with-modern-ml",
    "url": "/blog/2018/6/5/my-latent-dissatisfaction-with-modern-ml/",
    "title": "My Latent Dissatisfaction with Modern ML",
    "summary": "It took reading Judea Pearl's \"The Book of Why\", and Jonas Peters' mini-course on causality, for me to finally figure out why I had this lingering...",
    "body": "It took reading Judea Pearl's \"The Book of Why\", and Jonas Peters' mini-course on causality, for me to finally figure out why I had this lingering dissatisfaction with modern machine learning. It's because modern machine learning (deep learning included) is most commonly used as a tool in the service of finding correlations, and is not concerned with understanding systems. Perhaps this is why Pearl writes of modern ML as basically being \"curve fitting\". I tend to believe he didn't write those words in a dismissive way, though I might be wrong about it. Regardless, I think there is an element of truth to that statement. Linear models seek a linear combination of correlations between input variables and their targets. Tree-based models essentially seek combinations of splits in the data, while deep learning models are just stacked compositions of linear models with nonlinear functions applied to their outputs. As Keras author Francois Chollet wrote, deep learning can be thought of as basically geometric transforms of data from one data manifold to another. (For convenience, I've divided the ML world into linear models, tree-based models, and deep learning models. Ensembles, like Random Forest, are just that: ensembles composed of these basic models.) Granted, curve fitting is actually very useful: much of image deep learning has found pragmatic use: image search, digital pathology, self-driving cars, and more. Yet, in none of these models is the notion of causality important. This is where these models are dissatisfying: they do not provide the tools to help us interrogate these questions in a structured fashion. I think it's reasonable to say that these models are essentially concerned with conditional probabilities. As written by Ferenc Husz\u00e1r, conditional probabilities are different from interventional probabilities (ok, I mutilated that term). Humans are innately wired to recognize and ask questions about causality; consider it part of our innate makeup. That is, of course, unless that has been drilled out of our minds by our life experiences. (I know of a person who insists that causes do not exist. An extreme Hume-ist, I guess? As I'm not a student of philosophy much, I'm happy to be corrected on this point.) As such, I believe that part of being human involves asking the question, \"Why?\" (and its natural extension, \"How?\"). Yet, modern ML is still stuck at the question of, \"What?\" To get at why and how, we test our understanding of a system by perturbing it (i.e. intervening in it), or asking about \"what if\" scenarios (i.e. thinking about counterfactuals). In the real world of biological research (which I'm embedded in), we call this \"experimentation\". Inherent in a causal view of the world is a causal model. In causal inference, these things are structured and expressed mathematically, and we are given formal procedures for describing an intervention and thinking about counterfactual scenarios. From what I've just learned (baby steps at the moment), these are the basic ingredients, and their mathematical mappings: - Causal model: a directed, acyclic graph - Variables: nodes in a graph - Relationships: structured causal model's equations (math transforms of incoming variables with a noise distribution added on top, embedded in each node) - Interventions: removal of edges in a graph (\"do-calculus\") - Counterfactuals: set causal model based on observation, then perform do-calculus. Having just learned this, I think there's a way out of this latent dissatisfaction that I have with modern ML. A neat thing about ML methods is that we can use them as tools to help us better identify the important latent factors buried inside our (observational) data, which we can use to construct a better model of our data generating process. Better yet, we can express the model in a structured and formal sense, which would expose our assumptions more explicitly for critique and reasoning. Conditioned on that, perhaps we may be able to write better causal models of the world!",
    "tags": [
      "data science",
      "machine learning",
      "deep learning",
      "causal inference",
      "graph theory",
      "probability"
    ],
    "pub_date": "2018-06-05",
    "type": "blog"
  },
  {
    "id": "blog-causal-modelling",
    "url": "/blog/2018/5/26/causal-modelling/",
    "title": "Causal Modelling",
    "summary": "Finally, I have finished Judea Pearl's latest work \"The Book of Why\"! Having read it, I have come to appreciate how much work had to go on in order...",
    "body": "Finally, I have finished Judea Pearl's latest work \"The Book of Why\"! Having read it, I have come to appreciate how much work had to go on in order to formalize the very intuitions that we have for causal reasoning into essentially a modelling language. \"The Book of Why\" is geared towards the layman reader. Thus, unlike a textbook, it does not contain \"simplest complex examples\" that a reader can walk through and do calculations by hand (or through simulation). Thankfully, there is a lecture series by Jonas Peters, organized by the Broad Institute and held at MIT, that are available freely online. From just viewing the first of the four lectures, I am thoroughly enjoying Jonas' explanations of the core ideas in causal modelling. Indeed, Jonas is a very talented lecturer! He builds up the ideas from simple examples, finally culminating in a \"simple complex example\" that we can simulate on a computer. Having just freshly read \"The Book of Why\" also helps immensely; it's also clear to me that people in the world of causal modelling are very much familiar with the same talking points. For those interested in learning more about causal modelling, I highly recommend both the book and the lecture series!",
    "tags": [
      "causal inference"
    ],
    "pub_date": "2018-05-26",
    "type": "blog"
  },
  {
    "id": "blog-model-baselines-are-important",
    "url": "/blog/2018/5/6/model-baselines-are-important/",
    "title": "Model Baselines Are Important",
    "summary": "For any problem that we think is machine learnable, having a sane baseline is really important. It is even more important to establish them early.",
    "body": "For any problem that we think is machine learnable, having a sane baseline is really important. It is even more important to establish them early. Today at ODSC, I had a chance to meet both Andreas Mueller and Randy Olson. Andreas leads [](http://scikit-learn.org/stable/) development, while Randy was the lead developer of TPOT, an AutoML tool. To both of them, I told a variation of the following story: I had spent about 1.5 months building and testing a graph convolutions neural network model to predict RNA cleavage by an enzyme. I was suffering from a generalization problem - this model class would never generalize beyond the training samples for my problem on hand, even though I saw the same model class perform admirably well for small molecules and proteins. Together with an engineer at NIBR, we brainstormed a baseline with some simple features, and threw a random forest model at it. Three minutes later, after implementing everything, we had a model that generalized and outperformed my implementation of graph CNNs. Three days later, we had an AutoML (TPOT) model that beat the random forest. After further discussion, we realize then that the work that we did is sufficiently publishable even without the fancy graph CNNs. I think there\u2019s a lesson in establishing baselines and MVPs early on!",
    "tags": [
      "machine learning",
      "data science",
      "deep learning",
      "automl"
    ],
    "pub_date": "2018-05-06",
    "type": "blog"
  },
  {
    "id": "blog-consolidate-your-scripts-using-click",
    "url": "/blog/2018/3/30/consolidate-your-scripts-using-click/",
    "title": "Consolidate your scripts using click",
    "summary": "Overview is amazing! It's a Python package that allows us to add a command-line interface (CLI) to our Python scripts easily.",
    "body": "Overview is amazing! It's a Python package that allows us to add a command-line interface (CLI) to our Python scripts easily. This blog post is a data scientist-oriented post on how we can use to build useful tools for ourselves. In this blog post, I want to focus on how we can better organize our scripts. I have found myself sometimes writing custom scripts to deal with custom data transforms. Having them refactored out into a library of modular functions can really help with maintenance. However, I still end up with multiple scripts that might not have a naturally logical organization... except for the fact that they are scripts that I run from time to time! Rather than have them scattered in multiple places, why not have them put together into a single file, with options that are callable from the command line instead? Template Here's a template for organizing all those messy scripts using . How to use Let's call this new meta-script , and make it executable. To execute it at the command line, we now a help command for free: We can also use just one script with varying commands to control the execution of what was originally two different files. Instead of versioning multiple files, we now only have to keep track of one file where all non-standard custom stuff goes! Details Here's what's going on under the hood. With the decorator , we have exposed the function from the command line as a \"group\" of commands that are callable from the command line. What this does is then \"wrap\" the function (somehow), such that now it can be used to decorate another function (in our case, and ) using the decorator syntax . Recap - Consolidate disparate Python scripts into a single file, wrapping them inside a callable function. - Use to expose them to the end-user (yourself, or others) at the command line.",
    "tags": [
      "programming",
      "code snippets",
      "scripting",
      "python",
      "data science"
    ],
    "pub_date": "2018-03-30",
    "type": "blog"
  },
  {
    "id": "blog-lessons-learned-and-reinforced-from-writing-my-own-deep-learning-package",
    "url": "/blog/2018/2/28/lessons-learned-and-reinforced-from-writing-my-own-deep-learning-package/",
    "title": "Lessons learned and reinforced from writing my own deep learning package",
    "summary": "At work, I\u2019ve been rolling my own deep learning package to experiment with graph convolutional neural networks.",
    "body": "At work, I\u2019ve been rolling my own deep learning package to experiment with graph convolutional neural networks. I did this because in graph-centric deep learning, an idea I picked up from this paper, the inputs, convolution kernels, and much more, are being actively developed, and the standard APIs don\u2019t fit with this kind of data. Here\u2019s lessons I learned (and reinforced) while doing this. is an amazing package I am using to write my neural networks. provides a way to automatically differentiate code. As long as I write the forward computation up to the loss function, will be able to differentiate the loss function w.r.t. all of the parameters used, thus providing the direction to move parameters to minimize the loss function. Deep learning is nothing more than chaining elementary differentiable functions Linear regression is nothing more than a dot product of features with weights and adding bias terms. Logistic regression just chains the logistic function on top of that. Anything deeper than that is what we might call a neural network. One interesting thing that I've begun to ponder is the shape of the loss function, and how it changes when I change model architecture, activation functions, and more. I can't speak intelligently about it right now, but from observing the training performance live (I update a plot of predictions vs. actual values at the end of training epochs), different combinations of activation functions seem to cause different behaviours of the outputs, and there's no first-principles reason why that I can think of. All-told, pretty interesting :). Defining a good API is hard work There are design choices that go into the API design. I first off wanted to build something familiar, so I chose to emulate the functional API of Keras and PyTorch and Chainer. I also wanted composability, in which I can define modules of layers and chain them together, so I opted to use Python objects and to take advantage of their method to achieve both goals. At the same time, imposes a constraint in that I need to have functions differentiable with respect to their first argument, an array of parameters. Thus, I had to make sure the weights and biases are made transparently available for to differentiate. As a positive side effect, it means I can actually inspect the parameters dictionary quite transparently. Optimizing for speed is a very hard thing Even though I'm doing my best already with matrix math (and hopefully getting better at mastering 3-dimensional and higher matrix algebra), in order to keep my API clean and compatible with (meaning no sparse arrays), I have opted to use lists of arrays. Graph convolutions have a connection to network propagation I will probably explore this a bit more deeply in another blog post, but yes, as I explore the math involved in doing graph convolutions, I'm noticing that there's a deep connection there. The short story is basically \"convolutions propagate information across nodes\" in almost exactly the same way as \"network propagation methods share information across nodes\", through the use of a kernel defined by the adjacency matrix of a graph. Ok, that's a lot of jargon, but I promise I will explore this topic at a later time. Open Sourcing I'm an avid open source fan. Lots of my work builds on it. However, because this \"neural networks on graphs\" work is developed on company time and for company use, this will very likely be the first software project that I send to Legal to evaluate whether I can open source/publish it or not -- I'll naturally have to make my strongest case for open sourcing the code base (e.g. ensuring no competitive intelligence is leaked), but eventually will still have to defer to them for a final decision.",
    "tags": [
      "data science",
      "deep learning",
      "message passing neural networks",
      "software engineering",
      "graph theory"
    ],
    "pub_date": "2018-02-28",
    "type": "blog"
  },
  {
    "id": "blog-joy-from-teaching",
    "url": "/blog/2018/2/26/joy-from-teaching/",
    "title": "Joy from teaching",
    "summary": "It always brings me joy to see others benefit from what I can offer. Thanks for sharing the fruits of your journey on LinkedIn, Umar! Also a big...",
    "body": "It always brings me joy to see others benefit from what I can offer. Thanks for sharing the fruits of your journey on LinkedIn, Umar! Also a big thanks to the others who have finished the course! I hope you have enjoyed the learning journey, and were able to find problems to apply your newly-gained knowledge! With big thanks to DataCamp as well, for the development of their platform, enabling us to do teaching even outside of the academy! (Special shout-out to Hugo Bowne-Anderson and Yashas Roy, with whom I've personally partnered with to make the content go live.)",
    "tags": [
      "teaching",
      "education",
      "datacamp"
    ],
    "pub_date": "2018-02-26",
    "type": "blog"
  },
  {
    "id": "blog-annotating-code-tests-and-selectively-running-tests",
    "url": "/blog/2018/2/25/annotating-code-tests-and-selectively-running-tests/",
    "title": "Annotating code tests and selectively running tests",
    "summary": "In this blog post, let me share with you how we can selectively run a test using 's machinery.",
    "body": "I just learned about a neat trick when using - the ability to \"mark\" tests with metadata, and the ability to selectively run groups of marked tests. Here's an example: What's really cool here is that I can selectively run slow tests or selectively run integration tests:",
    "tags": [
      "programming",
      "python",
      "testing",
      "software engineering"
    ],
    "pub_date": "2018-02-25",
    "type": "blog"
  },
  {
    "id": "blog-nxviz-first-pr-merged",
    "url": "/blog/2018/2/21/nxviz-first-pr-merged/",
    "title": "nxviz first PR merged!",
    "summary": "In celebration of the first pull request that the project has received!",
    "body": "With thanks to @KassnerNora, [](https://github.com/ericmjl/nxviz) has a new feature! Now, it is possible to specify the data to use to colour edges. Previously, this was available in the API but was not working as I had not the time to implement it. Now, with Nora's PR merged in, users can declare their edges to be coloured by some edge metadata key! In addition to that, Nora included a colour palette that may be useful for drawing other edge colours. The API remains a declarative API, giving users an easy way to specify how they want rational graph plots to be made. Thank you for the contribution, Nora!",
    "tags": [
      "software engineering",
      "data visualization",
      "nxviz",
      "graph theory",
      "data science"
    ],
    "pub_date": "2018-02-21",
    "type": "blog"
  },
  {
    "id": "blog-deep-learning-and-the-importance-of-a-good-teacher",
    "url": "/blog/2018/2/20/deep-learning-and-the-importance-of-a-good-teacher/",
    "title": "Deep Learning and the Importance of a Good Teacher",
    "summary": "Or: \"Why having a good teacher was so instrumental in my learning of deep learning.\"",
    "body": "I can\u2019t emphasize this enough - someone who teaches well can really open their student\u2019s minds. On my journey in to deep learning and now graph convolutions, David Duvenaud (currently a professor at the University of Toronto) simultaneously taught me, a newcomer to deep learning, the basics of deep learning and the mechanics behind the graph convolutional neural network he and his colleagues had just published. The key insight he passed on to me was that deep learning was nothing more than chaining differentiable functions together. Many times I\u2019d ask him, \"so does this mean I can do that operation?\", and the answer would usually be \"yeah, why not?\". Knowing this point has made me realize how flexible deep learning really is. Once I got under the hood of what deep learning really was, then I realized that actually, DL is all about chaining together math functions one after another. Best part is, we get to define what those math functions are! Knowing this has also helped me when I read new DL papers. It\u2019s now a lot easier to for me to tell when a research group has come up with something very different from the rest of the pack as opposed to advancing existing methods.",
    "tags": [
      "data science",
      "education",
      "teaching",
      "deep learning"
    ],
    "pub_date": "2018-02-20",
    "type": "blog"
  },
  {
    "id": "blog-data-scientists-need-to-write-good-apis",
    "url": "/blog/2018/2/13/data-scientists-need-to-write-good-apis/",
    "title": "Data scientists need to write good APIs",
    "summary": "In which I argue why data scientists need to know how to write good APIs, or more generally, have basic software development skills.",
    "body": "I had a breakthrough in my work today. This was not some scientific epiphany, but just breaking through a wall in my progress. Today's breakthrough was totally enabled by writing my class definitions in a way that made sense, and by writing class methods that enabled me to express my ideas in a literate fashion. Logical class definitions and methods, refactored functions... these should be reflexive habits, but unfortunately, this isn't always the case with data science. We get so caught up in writing the code to make that plot that we forget to refactor out so that the block of code isn't brittle. But that brittle code means that my future self will loathe my current self for not writing that code robustly. In other words, write good APIs.",
    "tags": [
      "software engineering",
      "data science"
    ],
    "pub_date": "2018-02-13",
    "type": "blog"
  },
  {
    "id": "blog-bayesian-inference-and-testing-sets",
    "url": "/blog/2018/2/7/bayesian-inference-and-testing-sets/",
    "title": "Bayesian Inference & Testing Sets",
    "summary": "Further thoughts on whether Bayesian models overfit.",
    "body": "This topic recently came up again on the PyMC3 discourse. I had an opportunity to further clarify what I was thinking about when I first uttered the train/test split comment at PyData NYC. After a little while, my thoughts for a layperson are a bit clearer, and I thought I'd re-iterate them here. 1. Model specification uncertainty: Did we get the conditional relationships correct? Did we specify enough of the explanatory variables? 1. Model parameter uncertainty: Given a model, can we quantify the uncertainty in the parameter values? These are different uncertainties to deal with. We must be clear: where we are pretty sure about the model spec, Bayesian inference is about quantifying the uncertainty in the parameter values. Under this paradigm, if we use more data, we get narrower posterior distributions, and if we use less data, we get wider posterior distributions. If we split the data, we're just feeding in fewer data points to the model; if we don't, then we're just feeding in more data points.",
    "tags": [
      "bayesian",
      "statistics",
      "data science"
    ],
    "pub_date": "2018-02-07",
    "type": "blog"
  },
  {
    "id": "blog-pycon-program-committee-review",
    "url": "/blog/2018/2/6/pycon-program-committee-review/",
    "title": "PyCon Program Committee Review",
    "summary": "My reflections on reviewing talk proposals for PyCon 2018.",
    "body": "This year, I participated in the PyCon 2018 program committee (ProgCom). Though it was my second time doing it, it nonetheless was an eye-opening experience. This was because in contrast to last year, when I was writing my thesis and thus couldn't follow through on both rounds of review, this year I was. I\u2019d like to write a bit about my thoughts on the process, with three-fold goals: 1. To document my experience reviewing this year's PyCon talks. 2. To bring a little bit more transparency to the process. (Proposal authors, understandably, might feel like the process is opaque.) 3. To indirectly encourage others to participate in the process by demystifying what goes on in the mind of a reviewer. Before I go on, though I do want to make a few points clear on what I will not be doing in this blog post. 1. I will not be commenting on any particular proposal. 2. I will not be mentioning specific reviewers' names and what they commented on specific proposals. 3. I will not be offering tips for making your proposal succeed next year - that will depend on what next year\u2019s program committee is looking for. 4. I will not be describing the review process in detail, as this will be dealt with by an \"official\" blog post from the PSF. 5. This blog post is definitely not an invitation to review your current round or next round proposal, as with a full-time job, I currently don't have the bandwidth for that. Ready to read on? Let's go! Stage 1: Scoring The process for selecting talks is a two-stage process. In the first stage, we are rating the talks qualitatively (ordinally) on six criteria (for which I will defer to the official PSF blog post for details when it comes out). At this stage, we are blinded to a speaker's name and prior experience, as we want to review only the content on its merits. I'd like to give my thoughts on a few of the criteria below. On \"code of conduct\", this criteria ensures that there's no disparaging of sub-communities. I generally ding'ed talks that carried any hints of negativity, as I'd like to see PyCon be a positive force in the community. A few proposals were easily misconstrued as being negative, even though they weren't in substance; we tried our best to communicate this concern to proposal authors. That said, most talks are technical in nature, thus this criteria was not really an issue. On \"completeness\", it's really as about seeing whether an author demonstrated the meticulousness in making it easy for us to review. Timings were super important to me, to gauge whether I thought the talk proposer had over- or under-proposed content. I also sought detail to evaluate the accuracy of content and connect the sub-points into the author's overall message. The additional detail made it easier for me to champion a talk in the later stages. A few authors, while in communication with the ProgCom, flat-out refused to add in details after I had messaged them requesting details, citing other conferences' practices. Unfortunately, each conference is going to be slightly different in how they operate, and that ongoing dialogue can indirectly influence reviewers' perception and therefore the proposal's score. We're all human, and if a talk proposer comes off as uncooperative, especially when we provide an ongoing opportunity to engage in dialogue, it just makes it tougher to justify their elevated presence as a speaker at a conference that emphasizes cooperation and community. On \"coherence\", this point somewhat overlaps with \"completeness\", in that the accuracy of content can help boost this criteria. A good talk would cover one sufficiently focused topic for 30 minutes (or 45 minutes if request in sufficient depth. I think that what constitutes \"sufficient\" depends on our state of knowledge. For example, in data science, I would view it as not sufficient to speak on \"how to do a data analysis\", as it is now quite clear that each analysis is quite different, and the generalizable principles are too vague to be of use for a listener. On the other hand, speaking about solving a particular (data) science problem can be illuminating with solid take-aways for an audience members. Other non-explicit criteria can affect the perception of a proposal. At this stage, we're doing an ongoing dialogue with proposal authors, up till the submission deadline. Thus, as mentioned above, an authors' cooperativeness can affect our perception of the proposal, particularly on the \"code of conduct\" criteria - it'll affect whether we can trust an author's ability to adhere to the code of conduct. Additionally, having good English grammar can affect how readable the proposal is. Finally, there is something qualitatively different about a proposal by an author who is deeply passionate about their topic and believes they have something important to say, in contrast to a proposal author who is merely trying to fulfill PyCon selection criteria. I want to hear from the former, not the latter. Stage 2: Selection At this stage, w",
    "tags": [
      "pycon",
      "conferences",
      "python"
    ],
    "pub_date": "2018-02-06",
    "type": "blog"
  },
  {
    "id": "blog-refactor-notebook-code",
    "url": "/blog/2018/1/29/refactor-notebook-code/",
    "title": "Refactor Notebook Code",
    "summary": "In which I argue why we should refactor code out of Jupyter notebooks as soon as possible.",
    "body": "Jupyter notebooks that are filled with complex analyses can get unwieldy. Refactoring repeated code out into functions placed in modules should be standard practice, but from the sampling of Jupyter notebooks I've seen, I don't think this is standard practice. When should code be refactored? As soon as we start copying/pasting it! Making sure I have self-contained functions ensures that lingering state in my notebook doesn't cause unexpected behaviour. (Side note: learning the \"functional\" programming mindset can be very useful here!) But won't this slow down my pace? Isn't it faster to just copy and paste the code, and tweak what I need? Yes, but a small speed hit is going to be traded for a massive bump in rigour. Just today, I saw the effects of \"lingering state\" in my notebooks causing my plots to display different things before and after refactoring. It's not a good sign for any analysis if this happens. In short, refactor your code.",
    "tags": [
      "jupyter",
      "data science",
      "software engineering"
    ],
    "pub_date": "2018-01-29",
    "type": "blog"
  },
  {
    "id": "blog-pymc3-docs-weibull-patches-merged",
    "url": "/blog/2018/1/18/pymc3-docs-weibull-patches-merged/",
    "title": "PyMC3 docs + Weibull patches merged!",
    "summary": "I made a few pull requests to PyMC3! Come read about them :).",
    "body": "I recently had a few PRs merged into the PyMC3 codebase. Really happy about it, and just like my previous bug fix, I thought I'd share a bit about how those PRs came about. The first PR was an update to the docs on when to specify precision and when to specify standard deviation. They're related, so only one has to be specified, but I sometimes am sloppy when reading the docs and didn't pick up on that. Thus, I added a few lines to make sure this was crystal clear to sloppy readers like me. The next PR was an update to the Mixture model docs, in which I added an example of the new API for specifying components of mixture models. It previously wasn't clear how to do this, as there were no examples provided, so I put in a documentation PR specifying examples. The final PR was a patch to the Weibull distribution. I wanted to play around with trying mixture Weibulls at work, but mixture Weibulls wouldn't work because it didn't have a mode specified. I checked on Wikipedia, and found that Weibull's mode is conditional on the value of its parameters, and thus put in a PR to make this happen. Trying it out on some simulated/toy data, it worked! Thus, the devs allowed it to be merged. A few lessons I've learned along the way: (1) Docs are an awesome place to start. In fact, I made a few formatting mistakes in my first and second PRs that gave an opportunity for another guy to fix! Nothing is too small to be made as a contribution. FWIW, my first contribution to open source software were documentation fixes for , and that was a superb learning journey! (2) Friendly maintainers are crucial. The PyMC dev team can basically be described as, \"generally super nice!\" From the online and in-person interactions I've had with them, there's little in the way of egos, they're always learning, always being generally helpful. If they weren't that way, I very likely would have second thoughts trying putting in a PR there. (3) Open source lets me fix bugs I find. This lets me work at the pace that I need to, without having to wait for commercial vendors to provide update patches. If the patch that I find turns out to be useful for others, then the work I did can possibly save a ton of people's time as well. Win-win scenario!",
    "tags": [
      "bayesian",
      "software development",
      "open source",
      "data science"
    ],
    "pub_date": "2018-01-18",
    "type": "blog"
  },
  {
    "id": "blog-offline-time",
    "url": "/blog/2018/1/10/offline-time/",
    "title": "Offline Time",
    "summary": "I came to realize how important digital offline time is.",
    "body": "I was locked out of my work computer due to password reasons. (It's not human error - something about corporate management tools locked me out. Okay, well, that's human error too.) That said, I inadvertently gained a full two hours of offline time on Monday. Those two hours turned out to be pretty productive. I spent some time sketching out my work projects, trying to make better sense of how the project could fit in a disease area researcher's workflow, and figuring out derivative analyses that could enhance the value to them. This was something I probably wouldn't be able to accomplish if I had the regular distractions of my computer nearby. Seems like Cal Newport's \"digital distraction de-cluttering\" is a good thing to do. I must do more of it.",
    "tags": [
      "work",
      "digital decluttering",
      "productivity"
    ],
    "pub_date": "2018-01-10",
    "type": "blog"
  },
  {
    "id": "blog-bayesian-uncertainty-a-more-nuanced-view",
    "url": "/blog/2018/1/8/bayesian-uncertainty-a-more-nuanced-view/",
    "title": "Bayesian Uncertainty: A More Nuanced View",
    "summary": "When is uncertainty useful? A short thought.",
    "body": "The following thought hit my mind just last night. Bayesian inference requires the computation of uncertainty. Computing that uncertainty is computationally expensive compared to simply computing point estimates/summary statistics. But when exactly is uncertainty useful, and more importantly, actionable? That's something I've not really appreciated in the past. It's probably not productive to be dogmatic about always computing uncertainty if that uncertainty is not actionable.",
    "tags": [
      "bayesian",
      "data science",
      "statistics"
    ],
    "pub_date": "2018-01-08",
    "type": "blog"
  },
  {
    "id": "blog-visual-studio-code-a-new-microsoft",
    "url": "/blog/2017/12/13/visual-studio-code-a-new-microsoft/",
    "title": "Visual Studio Code: A New Microsoft?",
    "summary": "VSCode is, for me, proof of a new Microsoft, one that is no longer open-source hostile! Read on to see my mini-review of VSCode too.",
    "body": "During my week attending PyData NYC 2017, which was effectively a mini-mini-sabbatical from work, I got a chance to try out Visual Studio Code. Part of it was curiosity, having seen so many PyData participants using it; part of it was because of Steve Dowell, a core CPython contributor who works at Microsoft, who mentioned about the Python-friendly tools they added into VSCode. I think VSCode is representative of a new Microsoft. But first, let me describe what using it is like. User Interface First off, the UI is beautiful. It's impossible to repeat enough how important the UI is. With minimal configuration, I made it basically match Atom's UI, which I had grown used to. It has an integrated terminal, and the colours are... wow. That shade of green, blue and red are amazing, ever just so slightly muted compared to the Terminal or iTerm. The background shade of black matches well with the rest of VSCode, and the colour scheme is changeable to match that of Atom's. The design feels... just right. Wow! Git Integration Secondly, the integration with Git rivals Atom; in fact, there's a one-click \"sync\" button! It also has nice analog where I can add and commit all of the files simultaneously. Intellisense Thirdly, IntelliSense is just amazing! I like how I can use it to look up a function signature just by mousing over the function name. Open Source Finally, it\u2019s fully open source and back able, in the same vein as Atom, minus the bloat that comes from building on top of electron. Impressive stuff! Other Thoughts Now, on the new Microsoft. Only at the recent PyData NYC did I learn that Microsoft has hired almost half of the core CPython developers! Not only that, they are encouraged to continue their contributions into the CPython code base. In my view, that\u2019s a pretty awesome development! It means the Python programming language will continue to have a strong corporate backing while also enjoying community support. Its a sign of a healthy ecosystem, IMO, and also a sign of Microsoft\u2019s support for Open Source Software! I\u2019m more and more impressed by what Microsoft is doing for the Open Source community. I\u2019m hoping they\u2019ll continue up with this!!",
    "tags": [
      "software development",
      "tooling",
      "hacks",
      "productivity",
      "data science"
    ],
    "pub_date": "2017-12-13",
    "type": "blog"
  },
  {
    "id": "blog-pydata-nyc-2017-recap",
    "url": "/blog/2017/11/30/pydata-nyc-2017-recap/",
    "title": "PyData NYC 2017 Recap",
    "summary": "My notes from what I learned at PyData NYC 2017!",
    "body": "With that, we\u2019ve finished PyData NYC! Here's some of my highlights of the conference. Keynotes There were three keynotes, one each by Kerstin Kleese van Dam, Thomas Sargent, and Andrew Gelman. Interestingly enough, they didn't do what I would expect most academics to do -- give talks highlighting the accomplishments of their research groups. Rather, Kerstin gave a talk that highlighted the use of PyData tools at Brookhaven National Labs. Thomas Sargent gave a philosophical talk on what economic models really are (they're \"games\", in a mathematical sense), and I took back the importance of being able to implement models, otherwise, \"you're just bull\\\\\\\\\\*ing\". Andrew Gelman surprised me the most - he gave a wide-ranging talk about the problems we have in statistical analysis workflows. He emphasized that \"robustness checks\" are basically scams, because they're basically methods whose purpose is reassurance. He had a really cool example that highlighted that we need to understand our models by modifying our models, perhaps even using a graph of models to identify perturbations to our model that will help us understand our model. He also peppered his talk with anecdotes about how he made mistakes in his analysis workflows. I took home a different philosophy of data analysis: when we evaluate how \"good\" a model is, the operative question is, \"compared against what?\" Talks The talks were, for me, the highlight of the conference. A lot of good learning material around. Here's the talks from which I learned actionable new material. Analyzing NBA Foul Calls using Python This talk by the prolific PyMC blogger Austin Rochford is one that I really enjoyed. The take-home that I got from him was towards the end of his talk, in which I picked up three ways to diagnose probabilistic programming models. The first was the use of residuals - which I now know can be used for classification problems as well as regression problems. The second was the use of the energy plots in PyMC3, where if the \"energy transition\" and \"marginal energy distribution\" plots match up (especially in the tails), then we know that the NUTS sampler did a great job. The third was the use of the Gelman-Rubin statistic to measure the in-chain vs. between-chain variation; measures close to 1 are generally considered good. Check out the talk slides here. -compatible model stacking This talk was a great one because it shows how to use model stacking (also known as \"ensembling\", a technique commonly used in Kaggle competitions) to enable better predictions. Conceptually, model stacking works like this: I train a set of model individually on a problem, and use the predictions from those models as features for a meta-model. The meta-model should perform, at worst, on par with the best model inside the ensemble, but may also perform better. This idea was first explored in Polley and van der Laan's work, available online. Civis Analytics has released their implementation of model stacking in their GitHub repository, and it's available on PyPI. The best part of it? They didn't try inventing a new API, they kept a -compatible API. Kudos to them! Check out the repository here. Stream Processing with Dask Matthew Rocklin, as usual, gave an entertaining and informative talk on the use of Streamz, a lightweight library he built, to explore the use of Dask for streaming applications. The examples he gave were amazing showcases of the library's capabilities. Given the right project, I'd love to try this out! Check out his slides here. Asynchronous Python: A Gentle Introduction This talk, delivered by James Cropcho, defined what asynchronous programming was all about. For me, things finally clicked at the end when I asked him for an example of how asynchronous programming would be done in data analytics workflows -- to which he responded, \"If you're querying for data and then performing a calculation, then async is a good idea.\" The idea behind this is as such: web queries written serially are often \"blocking\", meaning we can't do anything while we wait for the web query to return. If we want to do a calculation on the returned data point, we have to wait for it to return first. On the other hand, if written asynchronously, we could potentially do a calculation on the previous data point while waiting for the current result to return, shaving the total time off potentially by some considerable fraction. Turning PyMC3 into This talk was by Nicole Carlson, and she did a tremendously great job delivering this talk. In it, she walked through how to wrap a PyMC3 model inside a estimator, including details on how to implement the , , and methods. The code in her repository provides a great base example to copy from. One thing I can't emphasize enough is that from a user experience standpoint, it's super important to follow idioms that people are used to. What Nicole did in this talk is to show how we can provide such idioms to end-users, rather than inventing a slightly modif",
    "tags": [
      "pydata",
      "conferences",
      "python"
    ],
    "pub_date": "2017-11-30",
    "type": "blog"
  },
  {
    "id": "blog-bayesian-learning-and-overfitting",
    "url": "/blog/2017/11/16/bayesian-learning-and-overfitting/",
    "title": "Bayesian Learning and Overfitting",
    "summary": "Yesterday, after I did my Boston Bayesians dry run talk, there was a point raised that I had only heard of once before: Bayesian learning methods...",
    "body": "Yesterday, after I did my Boston Bayesians dry run talk, there was a point raised that I had only heard of once before: Bayesian learning methods don't overfit. Which means we're allowed to use all the data on hand. The point holds for simple Bayesian networks, and for more complicated deep neural nets. Though I believe it, I wasn't 100% convinced of this myself, so I decided to check it up. I managed to get my hands on Radford Neal's book, Bayesian Learning for Neural Networks, and found the following quotable paragraphs: It is a common belief, however, that restricting the complexity of the models used for such tasks is a good thing, not just because of the obvious computational savings from using a simple model, but also because it is felt that too complex a model will overfit the training data, and perform poorly when applied to new cases. This belief is certainly justified if the model parameters are estimated by maximum likelihood. I will argue here that concern about overfitting is not a good reason to limit complexity in a Bayesian context. A few paragraphs later, after explaining the frequentist procedure: From a Bayesian perspective, adjusting the complexity of the model based on the amount of training data makes no sense. A Bayesian defines a model, selects a prior, collects data, computes the posterior, and then makes predictions. There is no provision in the Bayesian framework for changing the model or the prior depending on how much data was collected. If the model and prior are correct for a thousand observations, they are correct for ten observations as well (though the impact of using an incorrect prior might be more serious with fewer observations). In practice, we might sometimes switch to a simpler model if it turns out that we have little data, and we feel that we will consequently derive little benefit from using a complex, computationally expensive model, but this would be a concession to practicality, rather than a theoretically desirable procedure. Finally, in the following section after describing how neural networks are built: In a Bayesian model of this type, the role of the hyperparameters controlling the priors for weights is roughly analogous to the role of a weight decay constant in conventional training. With Bayesian training, values for these hyperparameters (more precisely, a distribution of values) can be found without the need for a validation set. This seems to dovetail well with the following convoluted intuition that I've had: if I fit a Bayesian model on the \"training\" set of the data, then update it with the \"test\" set, it's equivalent to just training with the whole dataset. (This comes from the idea of \"exchangability\" in i.i.d. samples, where the order of when a sample comes in doesn't matter.) With wide priors, if I fit with a smaller dataset, my posterior distribution will be wider than if I fit with the entire dataset. So... where possible, just train with the entire dataset. That said, I've not had sufficient grounding in Bayesian stats (after all, still a newcomer) to justify this. I certainly have more reading/learning to do here. Looks like something neat to explore in the short-term.",
    "tags": [],
    "pub_date": "2017-11-16",
    "type": "blog"
  },
  {
    "id": "blog-the-value-of-thinking-simply",
    "url": "/blog/2017/11/14/the-value-of-thinking-simply/",
    "title": "The Value of Thinking Simply",
    "summary": "Some thoughts I have had on \"thinking simply\" about a problem.",
    "body": "Einstein has a famous quote that most people don't hear about. It can scarcely be denied that the supreme goal of all theory is to make the irreducible basic elements as simple and as few as possible without having to surrender the adequate representation of a single datum of experience. Albert Einstein It instead, most people hear the misquote: Everything should be made as simple as possible, but no simpler. Misquoted version Though a misquote, it's still a fair (though lopsided -- missing a sufficient translation of the latter half) simplification of the original. In my work, I'm reminded of this point. I can choose to go for the complex fancy thing, but if I don't start from first principles, or start with simplistic approximations, I will struggle to have a sufficiently firm grasp on a problem to start tackling it. And therein lies the key, I think, in making progress on creative, intellectual work. The past week, I've noticed myself not wasting time on mindless coding (which usually amounts to re-running code with tweaks), and instead devoting more time to strategic thinking. As an activity, strategic thinking isn't just sitting there and thinking. For me, it involves writing and re-writing what I'm thinking, drawing and re-drawing what I'm seeing, and arranging and composing the pieces that are floating in my mind. During that time of writing, drawing, arranging and composing, I'm questioning myself, \"What if I didn't have this piece?\". Soon enough, the \"simplest complex version\" (SCV) of whatever I'm working on begins to emerge -- but it never really is the final version! I go back and prototype it in code, and then get stuck on something, and realize I left something out in that SCV, and re-draw the entire SCV from scratch. Here's my misquote, then, offered up: Sufficiently simple, and only necessarily complex. A further mutated version.",
    "tags": [
      "quotes",
      "musings",
      "thoughts"
    ],
    "pub_date": "2017-11-14",
    "type": "blog"
  },
  {
    "id": "blog-boston-bayesians-talk-an-attempt-at-demystifying-bayesian-deep-learning",
    "url": "/blog/2017/11/3/boston-bayesians-talk-an-attempt-at-demystifying-bayesian-deep-learning/",
    "title": "Boston Bayesians Talk: An Attempt at Demystifying Bayesian Deep Learning",
    "summary": "I'm about to give a talk on Bayesian deep learning!",
    "body": "It's confirmed! I will be rehearsing my PyData NYC talk at Boston Bayesians, held at McKinsey's office. This time round, I've challenged myself with making the slides without using PowerPoint or Keynote, and I think I've successfully done it! Check them out: - GitHub repository - Online slides Side note, I'm starting to really love what we can do with the web!",
    "tags": [
      "talks",
      "data science",
      "bayesian"
    ],
    "pub_date": "2017-11-03",
    "type": "blog"
  },
  {
    "id": "blog-always-check-your-data",
    "url": "/blog/2017/10/31/always-check-your-data/",
    "title": "Always Check Your Data",
    "summary": "Today, I learned a hard lesson about data checking when I spent 1.5 hours trying to fit a Poisson likelihood to negative values \ud83e\udd26\u200d\u2642\ufe0f. Always sanity check your data for basic stats like bounds, central tendency, and spread. Lesson learned! \ud83d\ude05",
    "body": "True story, just happened today. I was trying to fit a Poisson likelihood to estimate event cycle times (in discreet weeks). For certain columns, everything went perfectly fine. Yet for other columns, I was getting negative infinity\u2019s likelihoods, and was banging my head over this problem for over an hour and a half. As things turned out, those columns that gave me negative infinity likelihood initializations were doing so because of negative values in the data. Try fitting a Poisson likelihood, which only has positive support, on that! This lost hour and a half was a good lesson in data checking/testing: always be sure to sanity check basic stats associated with the data - bounds (min/max), central tendency (mean/median/mode) and spread (variance, quartile range) - always check!",
    "tags": [
      "bayesian",
      "data analysis",
      "poisson likelihood",
      "statistics",
      "data checking",
      "sanity check",
      "data testing",
      "negative infinity",
      "event cycle times",
      "data bounds",
      "central tendency",
      "data spread",
      "variance",
      "quartile range",
      "data science",
      "lessons learned"
    ],
    "pub_date": "2017-10-31",
    "type": "blog"
  },
  {
    "id": "blog-random-forests-a-good-default-model",
    "url": "/blog/2017/10/27/random-forests-a-good-default-model/",
    "title": "Random Forests: A Good Default Model?",
    "summary": "Why I think random forests are a great baseline machine learning model.",
    "body": "I've been giving this some thought, and wanted to go out on a limb to put forth this idea: I think Random Forests (RF) are a good \"baseline\" model to try, after establishing a \"random\" baseline case. (Clarification: I'm using RF as a shorthand for \"forest-based ML algorithms\", including XGBoost etc.) Before I go on, let me first provide some setup. Let's say we have a two-class classification problem. Assume everything is balanced. One \"dumb baseline\"\" case is a coin flip. The other \"dumb baseline\" is predicting everything to be one class. Once we have these established, we can go to a \"baseline\" machine learning model. Usually, people might say, \"go do logistic regression (LR)\" as your first baseline model for classification problems. It sure is a principled choice! Logistic regression is geared towards classification problems, makes only linear assumptions about the data, and identifies directional effects as well. From a practical perspective, it's also very fast to train. But I've found myself more and more being oriented towards using RFs as my baseline model instead of logistic regression. Here are my reasons: 1. Practically speaking, any modern computer can train a RF model with ~1000+ trees in not much more time than it would need for an LR model. 1. By using RFs, we do not make linearity assumptions about the data. 1. Additionally, we don't have to scale the data (one less thing to do). 1. RFs will automatically learn non-linear interaction terms in the data, which is not possible without further feature engineering in LR. 1. As such, the out-of-the-box performance using large RFs with default settings is often very good, making for a much more intellectually interesting challenge in trying to beat that classifier. 1. With , it's a one-liner change to swap out LR for RF. The API is what matters, and as such, drop-in replacements are easily implemented! Just to be clear, I'm not advocating for throwing away logistic regression altogether. There are moments where interpretability is needed, and is more easily done by using LR. In those cases, LR can be the \"baseline model\", or even just back-filled in after training the baseline RF model for comparison. Random Forests were the darling of the machine learning world before neural networks came along, and even now, remain the tool-of-choice for colleagues in the cheminformatics world. Given how easy they are to use now, why not just start with them?",
    "tags": [
      "data science",
      "machine learning",
      "random forest"
    ],
    "pub_date": "2017-10-27",
    "type": "blog"
  },
  {
    "id": "blog-network-propagation",
    "url": "/blog/2017/10/22/network-propagation/",
    "title": "Network Propagation",
    "summary": "A mini-tutorial on network propagation, and how the math behind it works.",
    "body": "Introduction I recently read a [review paper on network propagation][network]. It looks like a very useful thing in data science, particularly where networks are involved, and I wanted to share it with everybody. [network]: http://www.nature.com/nrg/journal/vaop/ncurrent/abs/nrg.2017.38.html Before I go on, I will start off by assuming we have a dataset modelled as a \"network\", or a \"graph\", in which the nodes are entities in the graph, and edges are relationships between those entities. In the context of my work, I encounter biological networks of many kinds - gene-gene interactions, protein-protein interactions, protein-RNA interactions, evolutionary networks, and more. What kind of problems can network propagation solve? The problem class generally follows this logic: I start with some nodes of interest, and I'm most interested in finding other nodes of interest based on these \"seed\" nodes. Knowing that path-based methods can often end up over-prioritizing highly connected nodes, we need a different principled method for finding these nodes. One way would be to take a random walk on the graph, and find out how often we land on a particular set of nodes on the graph. This random walk is what we call \"network propagation\". How network propagation works Network propagation follows this intuition: Imagine nodes contain information, and edges dictate which nodes information can be shared with. Network propagation shares the information with other nodes, according to the following rules: 1. I can only share by copying to my neighbours in the graph, 2. I must share everything that I have with my neighbours, and 3. Exchanges in the graph happen simultaneously. An animation of how this works is shown below. If you take my DataCamp course, you'll learn that networks can be represented as matrices on a computer. The adjacency matrix can be a 1/0 (i.e. binary) matrix that describes how nodes (rows and columns) are connected, and nodes can have their own vector of information (i.e. whether they are \"seed\" nodes or not). As it turns out, network propagation has a very nice matrix representation: How to implement network propagation So, how do we do network propagation with NetworkX, and Python? Let's take a look at code. Firstly, how do you convert a graph to a matrix? This basically corresponds to the following graph: Great! Now, let's mark out some \"seed\" nodes. These are the \"nodes\" of interest. To represent this, we use a binary vector - each slot in the vector represents one node in the graph. Let's start by highlighting two nodes, the second and third one: The is done for convenience, so that each axis is aligned appropriately for the following matrix multiplication: After the end of one round, nodes 2 and 3 have no information held on them, but their neighbours do. Let's do two rounds of propagation, rather than just one. After two rounds of information, information is shared back with the original nodes, but not necessarily evenly - it all depends on the connectivity of each node. It'll get really verbose doing 100 rounds; imagine doing . Instead of doing that, a recursive function may be better. At the same time, with many rounds of network propagation, we will end up with really large numbers. Therefore, it will be helpful to also normalize the result of matrix multiplication. On a previous blog post, I've alluded to the fact that recursive functions are pretty darn useful! Result So, let's take a look at the result of a few rounds of propagation. The first thing that I'd like to highlight is that the information propagation process converges on a final result fairly quickly. The second thing I'd like to highlight is that the purple nodes (there are two lines!), which weren't highlighted at the start, ended up having the highest propagation scores, followed by the red node. For those who are familiar with Markov chains, I believe network propagation has strong parallels with that. Conclusion Network propagation is conceptually simple, and easy to implement in Python.",
    "tags": [
      "network science",
      "graph theory",
      "programming",
      "python",
      "code snippets",
      "data science"
    ],
    "pub_date": "2017-10-22",
    "type": "blog"
  },
  {
    "id": "blog-pypy-impressive",
    "url": "/blog/2017/10/11/pypy-impressive/",
    "title": "PyPy: Impressive!",
    "summary": "I profiled PyPy against CPython when running NumPy code, and found it to be very competitive! Read on more to find out why.",
    "body": "A few years on after trying out PyPy for the first time and wrestling with it, I still find it to be pretty awesome. Now that PyPy officially supports , I'm going to profile a few simple statistical simulation tasks: - Computing the mean of a number of random number draws. - Simulating many coin flips I'll profile each of the tasks four ways: - Pure Python implementation running from the CPython and PyPy interpreters - implementation running from the CPython and PyPy interpreters. So, how do PyPy and CPython fare? Let's show the results up front first. [](./profile.webp) Click on the image to view a higher resolution chart. The raw recorded measurements can be found on Google Sheets. Here's a description of what's happening: - (top-left): PyPy is approx. 10X faster than CPython at computing the mean of 10 million random numbers. - (top-right): When both are running , the speed is identical. - (bottom-left): When simulating coin flips, PyPy with a custom function is about 3X faster than CPython. - (bottom-right): When using instead, there is a bottleneck, and PyPy fails badly compared to CPython. It's pretty clear that when PyPy is dealing with \"pure\" data (i.e. not having to pass data between Python and C), PyPy runs very, very fast, and, at least in the scenarios tested here, it performs faster than the CPython interpreter. This is consistent with my previous observations, and probably explains why PyPy is very good for code that is very repetitive; the JIT tracer really speeds things up. That last plot (bottom-right) is a big curiosity. Using the code below, I measured the random number generation is actually just as fast as it should be using CPython, but that PyPy failed badly when I was passing in a array to the object (from the standard library). I'm not sure what is happening behind-the-scenes, but I have reached out to the PyPy developers to ask what's going on, and will update this post at a later date. UPDATE: I heard back from the PyPy devs on BitBucket, and this is indeed explainable by data transfer between the C-to-PyPy interface. It's probably parallel to the latency that arises from transferring data between the CPU and GPU, or between compute nodes. So, what does this mean? It means that for pure Python code, PyPy can be a very powerful way to accelerate your code. One example I can imagine is agent-based simulations using Python objects. Another example that comes to mind is running a web server that only ever deals with strings, floats and JSONs (in contrast to matrix-heavy scientific computing). Now, for those who are curious, here's the source code for the pure Python implementation of the mean of random numbers. And here's the source code for the implementation of the mean of random numbers. Next, here's the source code for coin flips in pure Python: And finally, source code for coin flips using :",
    "tags": [
      "python",
      "programming",
      "optimization"
    ],
    "pub_date": "2017-10-11",
    "type": "blog"
  },
  {
    "id": "blog-recursive-programming-and-dags",
    "url": "/blog/2017/10/10/recursive-programming-and-dags/",
    "title": "Recursive Programming and DAGs",
    "summary": "As a data scientist, I routinely find programming skills to be very important. This blog post shows an example of why knowing our data structures and algorithms can really help!",
    "body": "Over the past few days, I've found myself using recursive programming to implement a \"model specification\" system with inheritance for deep learning. The goal here is to enable reproducible computational experiments for particular deep learning hyperparameter sets. Reproducibility is something I learned from the Software/Data Carpentry initiative, thus I wanted to ensure that my own work was reproducible, even if it's not (because of corporate reasons) open-able, because it's the right thing to do. So, how do these \"model spec\" files work? I call them \"experiment profiles\", and they specify a bunch of things: model architecture, training parameters, and data tasks. These experiment profiles are stored in YAML files on disk. A profile essentially looks like the following (dummy examples provided, naturally): In this YAML file, the key-value pairs essentially match the API of the tooling I've built on top of Keras' API to make myself more productive. (From the example, it should be clear that we're dealing with only feed-forward neural networks and nothing else more complicated.) The key here (pun unintended) is that I have a key-value pair that specifies another experiment profile that I can inherit from. Let's call the above example . Let's say I want to run another computational experiment that uses the optimizer instead of plain vanilla . Instead of re-specifying the entire YAML file, by implementing an inheritance scheme, I can re-specify only the optimizer and optimizer_options. Finally, let's say I find out that 20 epochs (inherited from ) is too much for Adam - after all, Adam is one of the most efficient gradient descent algorithms out there - and I want to change it to 3 epochs instead. I can do the following: Okay, so specifying YAML files with inheritance is all good, but how do I ensure that I get the entire parameter set out correctly, without writing verbose code? This is where the power of recursive programming comes in. Using recursion, I can solve this problem with a single function that calls itself on one condition, and returns a result on another condition. That's a recursive function in its essence. The core of this problem is traversing the inheritance path, from to to . Once I have the inheritance path specified, loading the YAML files as a dictionary becomes the easy part. How would this look like in code? Let's take a look at an implementation. The most important part of the function is in the / block. If I have reached the \"root\" of the inheritance path, (that is, I have hit which has no parent), then I return the traversed. Otherwise, I return into the function call again, but with an updated list, and a different to read. It's a bit like doing a loop, but in my opinion, a bit more elegant aesthetically. Once I've gotten the path list, I can finally load the parameters using a single function that calls on . This is the equivalent of traversing a Directed Acyclic Graph (DAG), or in some special cases, a tree data structure, but in a way where we don't have to know the entire tree structure ahead of time. The goal is to reach the root from any node: Also, because we only have one pointer in each YAML file to its parent, we have effectively created a \"Linked List\" that we can use to trace a path back to the \"root\" node, along the way collecting the information that we need together. By using this method of traversal, we only need to know the neighbors, and at some point (however long it takes), we will reach the root. If you were wondering why linked lists, trees and other data structures might be useful as a data scientist, I hope this illustrates on productive example!",
    "tags": [
      "programming",
      "code snippets"
    ],
    "pub_date": "2017-10-10",
    "type": "blog"
  },
  {
    "id": "blog-pydata-nyc-2017",
    "url": "/blog/2017/10/10/pydata-nyc-2017/",
    "title": "PyData NYC 2017",
    "summary": "Things I prospectively hope to learn at PyData NYC 2017!",
    "body": "I'm seriously looking forward to PyData NYC this year -- there's a great lineup of talks that I'm particularly looking forward to hearing! The theme for my set of must-see talks this year is \"Bayesian machine learning\" - there's much for me to learn! The first is by my fellow Boston Bayesian Colin Caroll with his talk titled Two views on regression with PyMC3 and scikit-learn. Colin is a mathematician at heart, even though he does software engineering for living now, and I can't wait to hear about regularization strategies! The second is by Nicole Carlson, with her talk titled Turning PyMC3 into scikit-learn. Nicole's talk is of interest to me because I've implemented models in PyMC3 before, and now would like to know how to make them reusable! The third talk is by Chaya Stern, with her talk titled Bayesian inference in computational chemistry. Super relevant to my work at Novartis! The fourth is by my fellow Boston Pythonista Joe Jevnik, who will be speaking on the first day about his journey into deep learning on some really cool time-series data. He works at Quantopian, BUT the spoiler here is that his talk is NOT about financial data! (I've heard his talk outline already.) The fifth is a tutorial by Jacob Schrieber, with his talk titled pomegranate: fast and flexible probabilistic modeling in python. 's API models after the 's API; with the API being the user-facing interface, and being the de facto go-to library for machine learning, I'd be interested to see how much more adds to the ecosystem, particularly w.r.t. Bayesian models. There are a swathe of other good talks that I'm expecting to be able to catch online later on. Matt Rocklin, who is the lead developer of Dask, has done a ton of work on speeding Python up through parallelism. His talk will be on the use of Cython & Dask to speed up GeoPandas. Also, Thomas Caswell, one of the [](http://matplotlib.org/) lead devs who helped guide my first foray into open source contributions, is giving a tutorial on developing interactive figures in matplotlib. Highly recommended if you're into the visualization world! Finally, the always-interesting, always entertaining en zyme will be speaking on an interesting topic. Looking forward to being at the conference, and meeting old and new friends there!",
    "tags": [
      "pydata",
      "conferences",
      "python"
    ],
    "pub_date": "2017-10-10",
    "type": "blog"
  },
  {
    "id": "blog-a-data-scientists-guide-to-environment-variables",
    "url": "/blog/2017/10/7/a-data-scientists-guide-to-environment-variables/",
    "title": "A Data Scientist's Guide to Environment Variables",
    "summary": "Environment variables might seem mysterious, but hopefully, after reading this blog post, you'll no longer feel that way!",
    "body": "You might have encountered a piece of software asking you for permission to modify your variable, or another program's installation instructions cryptically telling you that you have to \"set your variable correctly\". As a data scientist, you might encounter other environment variable issues when interacting with your compute stack (particularly if you don't have full control over it, like I do). This post is meant to demystify what an environment variable is, and how it gets used in a data science context. What Is An Environment Variable? First off, let me explain what an environment variable is, by going in-depth into the environment variable. I'd encourage you to execute the commands here inside your bash terminal (with appropriate modifications -- read the text to figure out what I'm doing!). When you log into your computer system, say, your local computer\u2019s terminal or your remote server via SSH, your bash interpreter needs to know where to look for particular programs, such as (the text editor), or (your version control software), or your Python executable. This is controlled by your PATH variable. It specifies the paths to folders where your executable programs are found. By historical convention, command line programs, such as , , and , are found in the directory . By historical convention, the folder is for software binaries, which is why they are named . These are the ones that are bundled with your operating system, and as such, need special permissions to upgrade. Try it out in your terminal: Other programs are installed (for whatever reason) into instead. is one example: Yet other programs might be installed in other special directories: How does your Bash terminal figure out where to go to look for stuff? It uses the environment variable. It looks something like this: The most important thing to remember about the variable is that it is \"colon-delimited\". That is, each directory path is separated by the next using a \"colon\" () character. The order in which your bash terminal is looking for programs goes from left to right: - - - On my particular computer, when I type in , my bash interpreter will look inside the directory first. It'll find that doesn't exist in , and so it'll move to the next directory, . Since my exists under , it'll execute the program from there. You can see, then, that this is simultaneously super flexible for customizing your compute environment, yet also potentially super frustrating if a program modified your variable without you knowing. Wait, you can actually modify your variable? Yep, and there's a few ways to do this. How To Modify the variable Using a Bash Session The first way is transient, or temporary, and only occurs for your particular bash session. You can make a folder have higher priority than the existing paths by \"pre-pending\" it to the variable: Or I can make it have a lower priority than existing paths by \"appending\" it to the variable: The reason this is temporary is because I only export it during my current bash session. or File If I wanted to make my changes somewhat more permanent, then I would include inside my or file. (I recommend using the file.) The / file lives inside your home directory (your environment variable specifies this), and is a file that your bash interpreter will execute first load. It will execute all of the commands inside there. This means, you can change your PATH variable by simply putting inside your : Data Science and the environment variable Now, how is this relevant to data scientists? Well, if you're a data scientist, chances are that you use Python, and that your Python interpreter comes from the Anaconda Python distribution (a seriously awesome thing, go get it!). What the Anaconda Python installer does is prioritize the folder in the environment variable. You might have other Python interpreters installed on your system (that is, Apple ships its own). However, this modification ensures that each time you type into your Bash terminal, ou execute the Python interpreter shipped with the Anaconda Python distribution. In my case, after installing the Anaconda Python distribution, my looks like: Even better, what conda environments do is prepend the path to the conda environment binaries folder while the environment is activated. For example, with my blog, I keep it in an environment named . Thus... Notice how the bash terminal now preferentially picks the Python inside the higher-priority environment. If you've gotten to this point, then you'll hopefully realize there's a few important concepts listed here. Let's recap them: - is an environment variable stored as a plain text string used by the bash interpreter to figure out where to find executable programs. - is colon-delimited; higher priority directories are to the left of the string, while lower priority directories are to the right of the string. - can be modified by prepending or appending directories to the environment variable. It can be done transiently inside a ",
    "tags": [
      "data science",
      "bash",
      "environment variables",
      "programming"
    ],
    "pub_date": "2017-10-07",
    "type": "blog"
  },
  {
    "id": "blog-new-habits",
    "url": "/blog/2017/10/3/new-habits/",
    "title": "New Habits",
    "summary": "Ever since \"going corporate\", it's meant picking up more new productivity/coding habits. Here's a sampling of what I've learned.",
    "body": "Ever since \"going corporate\", it's meant picking up more new productivity/coding habits. Here's a sampling of what I've learned. (1) Living by my calendar Basically, the \"work calendar\" defines everything about the day. I've had to make sure that if I am not going to be pencilled in for a meeting, I have to block out time on the calendar first. Also, sending invites to people + rooms -- the latter being the newest habit I've had to pick up. Finally, setting informative titles for calendar events - if I want to have coffee or lunch with X, I can't just write \"Coffee with X\" - it literally shows up as \"Coffee with X\" on X's calendar, which is super awkward, as if they're having coffee with themselves! Something more informative, like, \"Eric <> X coffee\" really helps the other person, who might be super busy and thus only glances at their calendar once in a while. (2) Flagging emails and applying rules Emails fly everywhere. It gets super overwhelming after a while. If there's stuff that needs to be followed-up on, it stays in my Inbox until it's done. It also gets flagged, which automatically creates a Todo on my task list. (If this sounds like Outlook - yes, it's Outlook. On macOS. With no \"email snoozing\" feature...) (3) Hacking through legacy code and shared compute resources Working on a compute cluster with a code base that's built for legacy versions of programming languages is super frustrating! Thankfully I know enough about the differences between Python 2 and Python 3 to hack my way through. Shared compute resources means using , but not everybody sets up with the same set of assumptions as others. Some create virtual environments inside a module, others append to , and getting the right combinations in a modular way is really tricky. It means I have some really painful one-off hacks to make stuff work. Documentation is paramount - without putting in docs, I'll never remember what I did... (4) Adapting to others' coding styles Not everybody is a Python programmer, and not everybody is a Pythonic programmer. The usual Python idioms that I'm used to (whether functional or object-oriented) sometimes get thrown out in favour of some other style (globals, anybody?), and I have to adapt to figure out what's going on. Thankfully my colleagues are open to me modifying their code, as long as I can demonstrate that the new version works fine, and I've been working hard to bring in Pythonic code style. (5) Performance reviews Gotta start getting used to this. I had a taste of it while volunteering as part of Tang Hall's student leadership, but now it's for real.",
    "tags": [],
    "pub_date": "2017-10-03",
    "type": "blog"
  },
  {
    "id": "blog-visualize-large-datasets-by-sampling",
    "url": "/blog/2017/9/14/visualize-large-datasets-by-sampling/",
    "title": "Visualize Large Datasets by Sampling",
    "summary": "Just a little tip, putting it here for myself and others in case it helps. Sometimes, you need to visualize a large dataset, but it takes a ton of...",
    "body": "Just a little tip, putting it here for myself and others in case it helps. Sometimes, you need to visualize a large dataset, but it takes a ton of time to render it or compute the necessary transforms. If your samples are statistically sampled independently of one another (i.e. basically not timeseries), and the goals are to do some statistical visualizations, then it's basically valid to visualize a downsampled set of the dataset. I recently encountered this point at work. After running a clustering analysis, I wanted to see a pair plot of the distribution of features in each cluster. However, with cluster sizes ranging from 200-2 million, rendering times were unreasonably long (making things non-interactive) for the large sized clusters. I thus decided to downsample the large clusters to a maximum of 2,000 data points. Instantly, render times improved, and I could start interacting with my data again. Little things matter!",
    "tags": [],
    "pub_date": "2017-09-14",
    "type": "blog"
  },
  {
    "id": "blog-nano-text-editor-hacks",
    "url": "/blog/2017/9/11/nano-text-editor-hacks/",
    "title": "nano text editor hacks",
    "summary": "Much as I've embraced the Atom text editor, there are times when the GUI isn't accessible to us, and we are forced to use a Terminal-based text...",
    "body": "Much as I've embraced the Atom text editor, there are times when the GUI isn't accessible to us, and we are forced to use a Terminal-based text editor. Now, I'm not one of those crazy types who use emacs or vim - those are the real seasoned pros. (I still don't know how to exit vim, btw.) As such, my terminal editor of choice remains the venerable . Here's some hacks that I recently figured out, to make text editing much easier in . (1) Syntax highlighting This is such a big one! Syntax highlighting seriously helps a ton. If you're on a Mac, make sure you install 's version of - you can look at my [dotfiles][dotfilesL41] or run the command: Then, edit your file to look something like this: Next time you use (from your user account), syntax highlighting should be enabled! You can find a sample [.nanorc] file on my GitHub [dotfiles] repository [dotfilesL41]: https://github.com/ericmjl/dotfiles/blob/master/install.sh#L41 [dotfiles]: https://github.com/ericmjl/dotfiles/ [.nanorc]: https://github.com/ericmjl/dotfiles/blob/master/.nanorc-mac (2) Keyboard Shortcuts Here's a laundry list of keyboard shortcuts I've muscle-memorized: : quits. There will be a prompt to save the file if it's been modified. I usually end up doing . scrolls down a page scrolls up a page searches the document for a term that you type in (think \"where\") cuts the line pastes a cut line (i.e. on macOS keyboards) starts a \"select\" cursor. You can use arrow keys to expand or shrink the selection, which can then be cut and pasted. cancels any commands that are 'active'. * activates the \"save file\" dialogue - lets you save your state without quitting. (3) Persistence , being not as fancy as or , means it doesn't have the concept of sessions. Doesn't matter - use [] to persist! []: https://github.com/tmux/tmux/wiki All-in-all, the biggest one that aids in writing on a terminal editor is syntax highlighting. I wrote this blog post in , and being able to visually see different parts of my text highlighted according to their meaning has made writing much easier.",
    "tags": [
      "programming",
      "nano",
      "text editor",
      "coding snippets"
    ],
    "pub_date": "2017-09-11",
    "type": "blog"
  },
  {
    "id": "blog-what-would-be-useful-for-aspiring-data-scientists-to-know",
    "url": "/blog/2017/8/31/what-would-be-useful-for-aspiring-data-scientists-to-know/",
    "title": "What would be useful for aspiring data scientists to know?",
    "summary": "I detail a number of topics that I think might be useful for aspiring data scientists to know.",
    "body": "I originally titled this post, \"What you need to know to become a data scientist\", but I backed off from having such an authoritative post title for I wanted to keep things opinionated without being pompous :). Data Science (DS) is a hot field, and I'm going to be starting my new role doing DS at Novartis in September. As an aside, what makes me most happy about this role is that I'm going to do DS in the context of the life sciences (one of the \"hard sciences\")! Now that I have secured a role, some people have come to ask me questions about how I made the transition into DS and into the industry in general. I hope to provide answers to those questions in this blog post, and that you, the reader, find it useful. I will structure this blog post into two sections: 1. What do I need to know and how do I go about it? 1. What do I need to do? Ready? Here we go :) First off, let's talk about what I think you, an aspiring data scientist, needs to know, and how to go about learning it. Topic 1: Statistical Learning Statistical learning methods are going to top the list. From the standpoint of \"topics to learn\", there's a laundry list one can write - all of the ML methods in , neural networks, statistical inference methods and more. It's also very tempting to go through that laundry list of terms, learn how they work underneath, and call it a day there. I think that's all good, but only if that material is learned while in the service of picking up the meta-skill of statistical thinking. This includes: 1. Thinking about data as being sampled from a generative model parameterized by probability distributions (my Bayesian fox tail is revealed!), 1. Identifying biases in the data and figuring out how to use sampling methods to help correct those biases (e.g. bootstrap resampling, downsampling), and 1. Figuring out when your data are garbage enough that you shouldn't proceed with inference and instead think about experimental design. That meta-skill of statistical thinking can only come with practice. Some only need a few months, some need a few years. (I needed about a year's worth of self-directed study during graduate school to pick it up.) Having a project that involves this is going to be key! A good introduction to statistical thinking for data science can be found in a SciPy 2015 talk by Chris Fonnesbeck, and working through the two-part computational statistics tutorial by him and Allen Downey (Part 1, Part 2) helped me a ton. Recommendation & Personal Story: Nothing beats practice. This means finding ways to apply statistical learning methods to projects that you already work on, or else coming up with new projects to try. I did this in graduate school: my main thesis project was not a machine learning-based project. However, I found a great PLoS Computational Biology paper implementing Random Forests to identify viral hosts from protein sequence, and it was close enough in research topic that I spent two afternoons re-implementing it using , and presenting it during our lab's Journal Club session. I then realized the same logic could be applied to predicting drug resistance from protein sequence, and re-implemented a few other HIV drug resistance papers before finally learning and applying a fancier deep learning-based method that had been developed at Harvard to the same problem. Topic 2: Software Engineering Software engineering (SE), to the best of my observation, is about three main things: (a) learning how to abstract and organize ideas in a way that is logical and humanly accessible, (b) writing good code that is well-tested and documented, and (c) being familiar with the ever-evolving ecosystem of packages. SE is important for a data scientist, because models that are making predictions often are put into production systems and used beyond just the DS themselves. Now, I don't think a data scientist has to be a seasoned software engineer, as most companies have SE teams that a data scientist can interface with. However, having some experience building a software product can be very helpful for lubricating the interaction between DS and SE teams. Having a logical structure to your code, writing basic tests for it, and providing sufficiently detailed documentation, are all things that SE types will very much appreciate, and it'll make life much easier for them when coming to code deployment and helping with maintenance. (Aside: I strongly believe a DS should take primary responsibility for maintenance, and not the SE team, and only rely on the SE team as a fallback, say, when people are sick or on vacation.) Recommendation & Personal Story: Again, nothing beats practice here. Working on your own projects, whether work-related or not, will help you get a feel for these things. I learned my software engineering concepts from participating in open source contributions. The first was a contribution to documentation, where I first got to use Git (a version control system) and Travis CI (a continuous integration",
    "tags": [
      "career development",
      "data science",
      "job hunt"
    ],
    "pub_date": "2017-08-31",
    "type": "blog"
  },
  {
    "id": "blog-reading-and-writing-docs-the-overlooked-programming-skill",
    "url": "/blog/2017/8/24/reading-and-writing-docs-the-overlooked-programming-skill/",
    "title": "Reading & Writing Docs: The Overlooked Programming Skill?",
    "summary": "I recently read a blog article by DataCamp's CTO (Dieter) on how to scale their projects and their engineering team - it's a [great read][scaling]!...",
    "body": "I recently read a blog article by DataCamp's CTO (Dieter) on how to scale their projects and their engineering team - it's a [great read][scaling]! In the article, Dieter states that the only way to scale an engineering team is to have well-written docs. I can see the benefits to doing it this way - we minimize the number of channels that any coder needs to use to find out information; the docs should be the place where the intent and technical detail of the code are simultaneously documented alongside usage examples. [scaling]: https://medium.com/datacamp/technical-vision-part-1-5f016c163340 Thus, in the final weeks up to starting my new job at Novartis as a Data Scientist, I decided to make sure I have the practice of writing, reading and publishing docs as good as muscle memory. I can already envision cases where, while conducting and building analyses, I end up writing a bunch of generally-useful functions that should be documented as well. What I write may eventually need to be used by someone else, including my future self; keeping track of how exactly a function is intended to be used is going to be very useful. I think reading and writing docs is an overlooked skill in programming. It's probably because this isn't a test of \"creative capacity\" (i.e. can you build something new), which is the \"sexy\" thing. It's more a test of \"maintenance capacity\" - and this is given less value and importance in the coding world. But it's incredibly important - many basic problems can be solved by reading the docs... but also, so many problems can be avoided by writing really good docs! The onus is on both parties - package maintainers and developers - to write and read good docs. But writing good docs is a tough job! I absolutely agree with this. There are different styles through which developers read docs - some prefer examples, while others just want to see function definitions - and it's very difficult to cater to every style. I personally think starting off with the style one's most comfortable with, and then gradually accepting community contributions, is the right way to go. One package that I maintain, [][nxviz], used to not have any docs written apart from that single file in the README. Thanks to my friend [Remi Picone][remi], I was able to learn how to configure Sphinx to get my docs working through copying [his example repository][remirepo]. Through that, I configured Sphinx to build docs on my nxviz project - and finally got it going! You can find it on [RTFD][nvdocs]. [nxviz]: http://github.com/ericmjl/nxviz [remi]: https://www.linkedin.com/in/rempic/ [remirepo]: https://github.com/rempic/Image-Features-Extraction [nvdocs]: http://nxviz.readthedocs.io/en/latest/ Learning this was really fun - looking forward to putting up more docs!",
    "tags": [],
    "pub_date": "2017-08-24",
    "type": "blog"
  },
  {
    "id": "blog-next-steps",
    "url": "/blog/2017/8/10/next-steps/",
    "title": "Next Steps",
    "summary": "Signed and done! I will be joining the Novartis Institutes for Biomedical Research (NIBR) in September, as part of the Scientific Data Analysis...",
    "body": "Signed and done! I will be joining the Novartis Institutes for Biomedical Research (NIBR) in September, as part of the Scientific Data Analysis (SDA) team under Novartis Informatics (NX). NIBR is the research arm of Novartis, and the SDA team is essentially a \"Data Special Ops\" team inside NIBR. The nature of the position involves both internal consulting and the development of new initiatives across teams. The nature of the role I'm being hired into is in statistical learning, which is a general direction I've been moving towards during my time in grad school. I picked up and implemented a number of useful and interesting deep learning algorithms back then, and over the past half a year, have finally gotten in underneath the hood of graph & image convolutions, variational autoencoders and gaussian processes. It's really fun stuff, at its core, and to me, it's even more fun translating biological and chemical data problems into that language, and back. After a summer learning lots and networking with industry professionals and fellow Fellows at Insight, I'm ready for a bit more structure in my life. Looking forward to starting there!",
    "tags": [
      "job hunt",
      "data science",
      "insight data science"
    ],
    "pub_date": "2017-08-10",
    "type": "blog"
  },
  {
    "id": "blog-open-source-software",
    "url": "/blog/2017/8/2/open-source-software/",
    "title": "Open Source Software",
    "summary": "Open source software is awesome, and I've just been thoroughly convinced of why. Today, I put in a [PR][pm] to PyMC3.",
    "body": "Open source software is awesome, and I've just been thoroughly convinced of why. Today, I put in a [PR][pm] to PyMC3. This was a bug fix related to the PyMC3 multinomial distribution's random variates generator, which uses 's multinomial under the hood, which arose from floating point precision errors. I first encountered this bug last week, when I started trying out the use of PyMC3 on my GPU tower. GPU stuff is tricky. One of the issues relates to floating point precision. I'm not well-versed enough on this to write intelligently about the underlying causes, but one thing I learned is that GPUs prefer 32-bit floating point precision (), while modern CPUs can handle 64-bit (). (I'm sure this will change in the future.) In the vast majority of \"large number\" computations, it's no big deal, but when we deal with small numbers (decimals in the thousandths range and smaller), addition errors can crop up. This was the exact problem I was facing. I had some numbers crunching on the GPU in space. Then, I had to pass them back to 's multinomial, which implicitly converts everything to . Because multinomial takes in a list of s (probabilities) that must sum to one, I was getting issues with my list of s summing to just infinitesimally (in computation land) greater than one. I dug around on-and-off for about a week to look for a solution, but none came. Instead, I had to rely on a small hack that I didn't like, adding a 1 millionth value to the sum and renormalizing probabilities... but that felt hacky and unprincipled. The fix was inspired by someone else's problems that was discussed on 's repository. The trick was to convert the numbers from to first and re-compute the probabilities in precision. I implemented that locally, and everything worked! I quickly ran two of the most relevant tests in the test suite, and they both passed. So I pushed up to GitHub and submitted a PR on this (after checking in with the lead devs on their issue tracker) - and it was just merged tonight! [pm]: https://github.com/pymc-devs/pymc3/pull/2470 If PyMC3's and 's code bases were not open source, with issues discussed openly, I would not have been able to figure out a possible fix to the issues with the help of other people. Also, I wouldn't have been able to patch the codebase locally first to see if it solved my own problems. I also wouldn't have access to the test suite to check that nothing was broken. All-in-all, working with an open source codebase was instrumental to getting this fix implemented. Big shout-out to the PyMC devs I interacted with on this - Colin & Junpeng. Thank you for being so encouraging and helpful!",
    "tags": [
      "data science",
      "open source",
      "gpu",
      "bayesian",
      "statistics"
    ],
    "pub_date": "2017-08-02",
    "type": "blog"
  },
  {
    "id": "blog-bayesian-neural-networks",
    "url": "/blog/2017/7/22/bayesian-neural-networks/",
    "title": "Bayesian Neural Networks",
    "summary": "Sharing some thoughts on a Bayesian neural network that I just implemented.",
    "body": "During this week, while us Insight Fellows begin going out to interview with other companies, my \"side hustle\" has been working on my Bayesian Analysis Recipes repository. Two particularly interesting problems I've wanted to write my own implementation for are multinomial classification and Bayesian deep learning. I finally got both of them done today, after about 2-3 days of hacking on them. Multinomial classification (notebook here) is the problem where we try to classify an item as being one of multiple classes. This is the natural extension to binary classification (done by logistic regression). To do this, I took the forest cover dataset and used PyMC3 to implement multinomial logistic regression. Seeing how to do it with PyMC3 was the most important aspect of this; actual accuracy wasn't much of a concern for me. However, having seen the classification report (at the bottom of the notebook), and having read that the dataset was originally classified using neural networks, I immediately had the thought of doing a Bayesian neural network for multi-class classification, having seen it implemented for binary classification on the PyMC3 website. Bayesian neural networks are not hard to intuit - basically, we place priors on the weights, rather than learning point estimates. In doing so, we are able to propagate uncertainty forward to predictions. Speaking as a non-expert in the field, I think the tricky part is the sampling algorithms needed. One thing nice about the field of Bayesian deep learning is the use of variational inference to approximate the true distribution of predictions with a mathematically more tractable one (e.g. a Gaussian). In doing so, we gain a fast way towards approximately learning the uncertainty in predictions - essentially we trade a little bit of accuracy for a lot of speed. For complex models like neural nets, this can be very valuable, as the number of parameters to learn grows very, very quickly with model complexity, so anything fast can make iteration easier. Starting with the code from Thomas Wiecki's website, I hacked together a few utility functions and boiled down the example to its essentials. Feed-forward neural nets aren't difficult to write - just a bunch of matrix ops and we're done. The notebook is available as well. One nice little feature is that by going with a deep neural network, we have additional predictive accuracy! Moving forward, I'd like to improve on that notebook a bit more, by somehow implementing/developing a visualization for multiclass classification uncertainty which is the thing we gain from going Bayesian. Hopefully I'll be able to get to that next week - it's shaping up to look quite hectic! As a side note, I found a bug with the multinomial distribution implementation in PyMC3, and am working with one of the core developers to get it fixed in PyMC3's master branch. (Thanks a ton, Junpeng, if you ever get to read this! ) In the meantime, I simply took his patch, modified mine a little bit, and used the patched up PyMC3 for my own purposes. This is why I think open source is amazing - I can literally patch the source code to get it to do what I need correctly! Wherever I work next has to be supportive of things like this, and have to allow re-release of generally/broadly useful code that I touch - it is the right thing to do!",
    "tags": [
      "bayesian",
      "deep learning",
      "data science"
    ],
    "pub_date": "2017-07-22",
    "type": "blog"
  },
  {
    "id": "blog-lessons-learned-during-insight",
    "url": "/blog/2017/7/17/lessons-learned-during-insight/",
    "title": "Lessons Learned During Insight",
    "summary": "(a) Solving healthcare goes beyond solving the science underlying it. At its core, healthcare delivery is essentially a human problem.",
    "body": "(a) Solving healthcare goes beyond solving the science underlying it. At its core, healthcare delivery is essentially a human problem. Even what we choose to optimize for is a hard problem. Do we optimize for changing human behaviour, or do we optimize for more precise treatments? (b) Healthcare is complex The biggest thing preventing a \"solving of healthcare\" is misaligned incentives. (c) I like scientific data Regardless of the lesson that healthcare needs to be solved with more than science, I still found myself naturally much more engaged with companies that were dealing with scientific data as part of their data science problems. Teams that were dealing with other types of data: insurance claims, financial, marketing, platform product analytics, click streams... these were much less engaging. I know my best fit now, though I won\u2019t rule out other teams. (d) People can change the equation. I met with some people whose intellect and grasp of knowledge I really admire! Additionally, passion is infectious. It helps to work with colleagues who energize one another, rather than drain each others\u2019 energy. (e) Some Insight alumni are awesome And I want to be like them when I help with mentoring for the next batch. Perhaps if I get a chance to interview others, I\u2019d like to be able to model how I interview after the alumni mentors. Biggest shout-out to George Leung, who works for Vectra, tailored his mentoring session by first asking me about my Insight project, which involved Gaussian processes and variational auto-encoders (VAEs). George asked me first about what VAEs were, and then asked me to solve a Bayes problem on the board. I could tell he was building his questions on-the-fly. The other shout-out goes to Ramsey Kamar, who went through the \"Big 4\" questions: tell me about yourself, what\u2019s your previous accomplishments, how did you face a conflict, and what\u2019s your biggest weakness. His feedback to me was direct, positive, and most importantly, always encouraging. (f) Humanities tools are needed On reflection, I think that if we\u2019re going to solve the \"human\" portion of healthcare, we\u2019re going to need tools from the humanities - the tools that let us qualitatively and quantitatively study human behaviour. While data science can provide a quantitative path towards a solution, the qualitative side of it will remain as important as ever.",
    "tags": [],
    "pub_date": "2017-07-17",
    "type": "blog"
  },
  {
    "id": "blog-insight-week-7",
    "url": "/blog/2017/7/15/insight-week-7/",
    "title": "Insight Week 7",
    "summary": "Aaand with that, week 7 of Insight is done! I had a short week because of SciPy 2017, and I'm thankful that I got a chance to head out there - had...",
    "body": "Aaand with that, week 7 of Insight is done! I had a short week because of SciPy 2017, and I'm thankful that I got a chance to head out there - had the opportunity to reconnect with many friends from the SciPy community. The two days of Week 7 that I experienced were probably the weirdest week 7 any Fellow has experienced to date. Because I had missed a demo on account of SciPy, and because the company didn't want to just watch the pre-recorded demo video, I made a trek up to Cambridge to demo on-site. What initially was a 30 minute session turned out to be a 1.5 hr demo. I have two more demo obligations to fulfill next week. Other than that, it's going to be mostly interview preparation with other fellows, and more data and coding challenges, and more studying of topics that we're not familiar with. I am trying to brush up on SQL more, as I can see it being a useful tool to have to query data out of databases. Now that we're done with Week 7, we're going to be alumni soon. As such, I've began thinking about how I could give back as an alumni. Some ideas have come to mind, inspired by what others have done. Firstly, I think I can help standardize future Fellows' coding environments by providing a set of annotated instructions for installing the Anaconda distribution of Python. Perhaps even an evening workshop on the first Thursday might be useful. Secondly, I've come to recognize that the biggest bottleneck for Fellows' projects is the web deployment and design portion. Model training to obtain an MVP is fairly fast - one of 's models is often good enough. However, most of us didn't know HTML and Bootstrap CSS, and the deadline makes it stressful enough to pick this up on-the-fly. (The stress is probably compounded by the fact that the web app/blog post is not the most intellectually interesting portion of the project.) Perhaps a workshop at the end of Week 2 or beginning of Week 3 might be good. Thirdly, I see this trend where a lot more projects are going to start using deep learning. I think putting a workshop together with, say, Jigar, might be a useful thing to have. Finally, my interview simulator questions have become famous for being a 'hybrid' between stats, ML and CS. It's very much in the same vein as what I got when I interviewed with Verily. Until we get hired, we are allowed (and one might even say, expected) to continue coming into the office to help each other prepare for upcoming interviews. We're all looking forward to getting hired and solving data problems! With this post, I think I'll end the regular blog post series here. Hope this post series was an informative insight into Insight! Next one I'll post is going to be a summary of lessons learned from my time as an Insight Health Data Fellow.",
    "tags": [
      "insight data science",
      "data science",
      "job hunt"
    ],
    "pub_date": "2017-07-15",
    "type": "blog"
  },
  {
    "id": "blog-scipy-2017",
    "url": "/blog/2017/7/12/scipy-2017/",
    "title": "SciPy 2017",
    "summary": "I just finished from SciPy 2017! This was a fruitful conference, and I'm glad I managed to make it. Monday was the first day.",
    "body": "I just finished from SciPy 2017! This was a fruitful conference, and I'm glad I managed to make it. Monday was the first day. I wanted to get a better feel for the Jupyter widgets ecosystem, and as such I sat in on the corresponding tutorial. That happened to be the only tutorial I sat in live. Nonetheless, one nice thing about the tutorials is that they are live recorded, and so we can watch the ones we missed on our own time when back home. These are the ones I hope to catch, partly out of interest, partly from recommendations by other conference attendees who sat in them: - Numba - Holoviews - Dask - scikit-image Looking at the list, I kind of realize now how much of a Continuum Analytics fanboy I've become... On the second day, I delivered my own Network Analysis Made Simple. I collected some feedback right at the end of the tutorial, and it looked like they were overall very positive. Many liked the whiteboard illustrations that I added on. When delivering this at PyCon, I think it would benefit from having a whiteboard of sorts. The third day was the start of the conference talks. There's many, many great talks out there! I also had the opportunity to connect with new people over breakfast, lunch, coffee and dinner. I tried hosting \"office hours\", like Matt Davis did last year, but I think I announced it a bit too late. All-in-all, I think it was great to attend SciPy 2017 this year. I'm happy to have not broken the chain of attendance. Looking forward to serving on next year's organizing committee again, and I hope to have a new tutorial in the works!",
    "tags": [
      "scipy",
      "conferences",
      "python"
    ],
    "pub_date": "2017-07-12",
    "type": "blog"
  },
  {
    "id": "blog-insight-week-6",
    "url": "/blog/2017/7/8/insight-week-6/",
    "title": "Insight Week 6",
    "summary": "We had a short week this week because of the long July 4th weekend (Happy Birthday, America!). Wednesday was my second demo day, this time at MGH.",
    "body": "We had a short week this week because of the long July 4th weekend (Happy Birthday, America!). Wednesday was my second demo day, this time at MGH. There were 8 of us demoing at MGH's Clinical DS team, and I really enjoyed the interaction with them. The team asked of me two technical questions about Flu Forecaster, both of which were analogous to other questions I had heard before. After the demo, we hung out with the team and chatted a bit about their latest projects. In the afternoon, I focused on doing the data challenge and leetcode exercises; in the evening, I (at the last minute) signed up for back-to-back behavioral and ML interview practice sessions. It was good to chat with the alumni helping with the sessions, as I learned much more about their thought process. In the future, I'll probably be called on to interview other people, and I will definitely draw on my experiences here. On Thursday we had more prep. I helped with mock interviewing by being an observer for Xi and an interviewer for Angela. The role-playing with Angela was an interesting one for me. I tried playing the role of a conversational but technically-competent interviewer. Also asked questions genuinely out of curiosity too. I think that combined with Angela's outgoing personality kept the conversation enjoyable for all three of our spectators. In the late afternoon, an NYC session alum came by and gave us a session on data challenges. The exercise he gave was quite neat - basically, given one categorical output column and a slew of other feature columns, train the best model that has the highest accuracy score. Oh, the twist? Do it in 25 minutes. The key point from this exercise was to have us get prepared for an on-site data challenge. The on-site data challenge mainly helps the hiring team check that we have the necessary coding chops to work with the team. It also lets them see how we perform under time constraints. The most important thing is to deliver a model with some form of results. Iterating fast is very important. Thus, it helps to push out fast one model that works. On Friday, we did another round of the interview simulator. I thought it was better run this time round. The mutual feedback from one another is very helpful. I was tasked with a stats question, which I melded into a hybrid stats + CS question, thus modelling what I had received when I was interviewed at Verily. FWIW, the question I asked was to define bootstrap resampling (sampling with replacement), implement it using the Python standard library, and discuss the scenarios where it becomes a useful thing. If tasked with a similar one for the next time, I will probably ask about writing a function to sample from a Bernoulli distribution using only the Python standard library. It's useful to know how to implement these statistical draws when it's not easy or impossible to use other libraries. (I had to do it when trying out the PyPy interpreter a few years back, and didn't want to mess with installing for PyPy.) I liked a few of the other questions asked as well - for example, the knapsack problem posed by Steve: Given a set of produce items, each with their own value and weight (in Kg), and a knapsack that can only carry a maximum weight of produce, find the set of produce that will maximize value at the market. That afternoon, we slowed things down a bit. Regardless of how much we benefit from them, the interview simulators nonetheless are tiring. But that's the key point - interviews are day-long, exhausting endeavours that test stamina and ability to switch between contexts (both technical and social). The simulator aims to simulate that. Looking forward to next week. For me it'll be a short one, because I'll be at SciPy 2017 to lead a Network Analysis tutorial. Also hoping to represent Insight well!",
    "tags": [
      "insight data science",
      "data science",
      "job hunt"
    ],
    "pub_date": "2017-07-08",
    "type": "blog"
  },
  {
    "id": "blog-insight-week-5",
    "url": "/blog/2017/7/1/insight-week-5/",
    "title": "Insight Week 5",
    "summary": "First off, Happy Canada Day! Week 5 is primarily focused on interview prep as a bunch of us go out for our demos.",
    "body": "First off, Happy Canada Day! Week 5 is primarily focused on interview prep as a bunch of us go out for our demos. We kicked off Monday with an interview prep field day. The main areas of focus for us were CS fundamentals, machine learning, SQL, and behavioral interviewing. I found SQL to be my weakest point, and I'll definitely be focusing a lot of efforts on there. I had a chance to explain gradient descent and regularization using algebra - something I never thought I would do! On Tuesday, Fellows began going outside for demos. My first demo will be at Boston Health Economics this Thursday, followed by (in no particular order) MGH, Biogen, Merck, OM1, and Immuneering. Definitely looking forward to presenting Flu Forecaster to them! On the side, we also started thinking through computer science fundamentals problems, and doing data analytics challenges. CS fundamentals are what you think it would be, covering data structures and algorithms. I found myself to be particularly fond of recursion, and implemented a recursive algorithm for something that could be solved in linear time without recursion. It was good to see my biases, and to try my hand at implementing the same thing in fundamentally two different styles. In the evening, Nick (one of the fellows) gave us a run through on SQL. It was very useful to have his perspective, which was basically that most of the problems we will encounter involve some degree of nested searches, and that we have to work backwards from what we want. I also had a good perspective from my alumni mentor on how to approach describing my thesis to interviewers. On Wednesday, the interview prep continued with more coding challenges, demo trips, and fellow-led workshops. Together with Jeff and Jigar, we led a deep learning fundamentals workshop, in which we went through how deep learning works for feed forward neural networks and convolutions neural networks. Thursday came my first demo, which was at Boston Health Economics. Overall, I thought the demo session went well, and that Catherine, our host, kept engaged with the presentations. I very much appreciate her intellect. Additionally, I took the approach of \"free styling it\" (of course conditioned on having previously rehearsed it enough times), which resulted in a demo presentation that was overall smoother than what I had previously delivered Apart from that, we continued our interview prep. This involved more CS fundamentals for me, getting more practice with common algorithms, and finishing the coding exercises that Ivan gave us. On Friday, we did an interview simulator, in which we practiced interviewing one another. This gave me a better view into the thought process that an interviewer might be going through, particularly when conducting a technical interview. From prior experience interviewing, I remembered that my most pleasant interviews were with individuals who kept the atmosphere positive, encouraging, and provided hints along the way. Thus, I tried to conduct the mock interviews in the same way. In the afternoon, I gave a very short workshop on how to write Pythonic code, which covered [](https://www.python.org/dev/peps/pep-0008/#introduction) (which is now check-able using [](https://github.com/PyCQA/pycodestyle)). It was fun seeing everybody go, \"Whoa! Atom can do that?!\" and then promptly going ahead to clean up their code according to the linter's recommendations. Interspersed throughout the week, I made an effort to summarize my thesis work a bit more. I think I have a few ways/hooks to explain it to a 'recruiter without a technical background', a 'computer scientist without biology background', and a 'biologist without a computing background'. Making it concise with a good \"hook\" was the hardest part, but I think I have something good now.",
    "tags": [
      "insight data science",
      "data science",
      "job hunt"
    ],
    "pub_date": "2017-07-01",
    "type": "blog"
  },
  {
    "id": "blog-using-bokeh-in-fluforecaster",
    "url": "/blog/2017/6/30/using-bokeh-in-fluforecaster/",
    "title": "Using Bokeh in FluForecaster",
    "summary": "In this blog post, I will show how Bokeh featured in my Insight project, FluForecaster.",
    "body": "Motivation As a Health Data Fellow at Insight, we spend about 3 weeks executing on a data project, which we demo at companies that are hiring. I built FluForecaster, which was a project aimed at forecasting influenza sequence evolution using deep learning. My choice of project was strategically aligned with my goals on a few levels. Firstly, I wanted to make sure my project showcased deep learning, as it's currently one of the hottest skills to have. Secondly, I had components of the code base written in separate Jupyter notebooks prior to Insight, meaning, I could execute on it quickly within the three weeks we had. Thirdly, I had intended to join Insight primarily with the goal of networking with the Insight community, and that basically meant 'being a blessing' to others on their journey too - if I could execute fast and well on my own stuff, then there'd be time to be a team player with other Fellows in the session, and help them get their projects across the finish line. Each of us had to demo a \"final product\". Initially, I was thinking about a \"forecasting dashboard\", but one of our program directors, Ivan, suggested that I include more background information. As such, I decided to make the dashboard an interactive blog post instead. Thus, with FluForecaster being a web-first project, I finally had a project in which I could use Bokeh as part of the front-end. Applying Bokeh Bokeh was used mainly for displaying three data panels in the browser. Firstly, I wanted to show how flu vaccine efficacy rarely crossed the 60% threshold over the years. Secondly, I wanted to show a breakdown of the number of sequences collected per year (as used in my dataset). Thirdly, I wanted to show a visual display of influenza evolution. For yearly vaccine effectiveness, it was essentially a line and scatter chart, with the Y-axis constrained between 0 and 100%. I added a hover tooltip to enable my readers to see the exact value of vaccine effectiveness as measured by the US CDC. To show the number of sequences per year in the dataset, the same kind of chart was deployed. Bokeh magic became really evident later when I wanted to show sequence evolution in 3 dimensions. Because 3D charts are generally a poor choice for a flat screen, I opted to show pairs of dimensions at a time. A nice side-effect of this is that because my was shared amongst each of the three pairs of coordinates, panning and selection was automatically linked for free. Usage Pros and Cons Bokeh's API is very powerful, in that it supplies many plotting primitive objects (glyphs, particularly), and that makes it a big plus for users who are experienced with the library, who are creating complex interactive charts. Most of my fellow Fellows at Insight ended up using the interface, and I did too. I think the interface provides the best balance between ease-of-use and flexibility. If you take a look at the code here, you'll notice that there's often a bit of boilerplate that gets repeated with variation, such as in the configuration of custom hover tools. I think this is the tradeoff we make for configurability... or I might just be not writing code most efficiently. :) There were times where I was tempted to just use the ' declarative interface instead. It's a lot more easy to use. However, I did have some time on hand, and wanted to get familiar with the interface, because there's a possibility that I might want to make wrappers for other visualizations that can lend themselves to a declarative API. Embedding Visualizations I built my interactive blog post using a combination of Flask, hand-crafted HTML, Bootstrap CSS & JS, and Bokeh - which took care of the bulk of visuals. I drew static figures using Illustrator. Embedding the necessary Bokeh components wasn't difficult. Very good documentation is available on the Bokeh docs. The key insight that I had learned was that I could have the passed into my Flask app functions' statements, and embed them using Jinja2 templating syntax. An example can be found here. Basically, returns a and a object, which are essentially just strings. To embed them in the templates, we use the syntax and . That is very important: it tells the Jinja2 templating engine that it's safe to render those pieces of Javascript and HTML. Conclusion Through the project, I became a lot more familiar with the Bokeh plotting library. Now I feel a bit torn! I've contributed to both the Bokeh and projects, and I love them both! I've also come to deeply respect the lead developers of both projects, having interacted with them many times. If I were to make a comment on \"where to use what\" based on my experience, it'd probably still be the conservative view of \" for papers, for the web\"... but I'm sure that will be outdated soon. Who knows how the Python plotting landscape will evolve - it's exciting times ahead, and at least for now, I'm happy for the experience driving a dataviz project with Bokeh!",
    "tags": [
      "bokeh",
      "data science",
      "dashboarding"
    ],
    "pub_date": "2017-06-30",
    "type": "blog"
  },
  {
    "id": "blog-insight-week-4",
    "url": "/blog/2017/6/24/insight-week-4/",
    "title": "Insight Week 4",
    "summary": "Week 4 has been all about demos. Polishing our demos, picking companies that we want to demo at (and possibly interview at later on).",
    "body": "Week 4 has been all about demos. Polishing our demos, picking companies that we want to demo at (and possibly interview at later on). Every morning, we practice our demos, 10 minutes per person, with the goal of keeping our demo to under 5 minutes to leave time for Q&A. I've found that the act of rehearsing our demos makes it much easier to pick out where I need improvement. For example, I tended to have trouble with explaining the validation portion smoothly, even though I knew what I was doing there. A tool that seems useful, especially for short demos, is to write out exactly what I want to say, and that definitely helped. On the type of work that I'm interested in, here's some things I've become much clearer on. Firstly, the factors I'm considering for a company. The ideal combination is: a company that deeply values the hard sciences (in my case life sciences), and is solving very tough technical problems that requires growth in and mastery of deep technical topics, on a team that encourages experimentation, personal growth, and open source contributions on company time. We'd have to be at the innovation boundary of very powerful techniques. This is important for me, because I believe that 5-10 years down the road, I would have mastery over very foundational and broadly applicable tools with the appropriate experience applying them to real-world problems, which I could leverage to solve more cool and interesting problems. It's also a good defence against being pigeon-holed into a particular domains or tasks - autonomy in problem selection and definition is very important to me, so most of my choices aim to maximize that over money. Secondly, I've effectively ruled out companies that are dealing with non hard-science data, e.g. insurance claims, marketing & advertising, finance, and business data. Having applied computation to the life sciences over grad school, and being trained in the life sciences for over 10 years, I'm not ready to give up that background knowledge to work on other problems. I also believe that investing in the hard sciences means investing in the next wave of real-world innovation, and I'd like to ride that wave. Thirdly, within the next 5 years, I see myself growing as a technical person, rather than a management person. People issues, particularly conflict resolution, make it difficult to focus on being a good craftsman, and I much more enjoy craftsmanship than management. Now, on the companies that have come by... Most are using open data science tools in their toolkit, and this mostly means Python and R, Spark and a few other big DB tools. Some are still using SAS (.................) and didn't show a trend towards open data science languages, and effectively ruled themselves out of contention. (Using legacy tools signals a lack of forward-thinking and a desire to favour the status quo over pushing boundaries.) Some have given us words of wisdom. One guy basically said that healthcare has messed up (he used stronger language) incentives. Another said that to solve healthcare we need to first solve human behaviour. All very interesting points that are well-taken on my side. A non-healthcare company told us that if we're not paying for a service, then we're the product. In our session, it was basically the pharma research arms that piqued my interest the most, aside from one hospital's internal startup team. The gap in interest between #4 and #5 (for me, at least) was really big, and the gap of interest from #5 to the rest was even larger. Anyways, week 5 begins soon, and we pivot over into interview prep. Looking forward to learning lots, particularly doing deep dives on my weak spots!",
    "tags": [
      "insight data science",
      "data science",
      "job hunt"
    ],
    "pub_date": "2017-06-24",
    "type": "blog"
  },
  {
    "id": "blog-insight-week-3",
    "url": "/blog/2017/6/17/insight-week-3/",
    "title": "Insight Week 3",
    "summary": "This week was a week of polishing our final products and getting them in shape for our demos. Pushing a product to final production really involves...",
    "body": "This week was a week of polishing our final products and getting them in shape for our demos. Pushing a product to final production really involves a lot of nitty-gritty tweaking. In this blog post, I'll detail some of what I had to work on. My [final product][fluforecaster] a hybrid web dashboard + blog post. Behind the dashboard is a fairly complex set of computations, which currently are run through a Jupyter notebook. The front-end, therefore, only renders the predicted flu sequences returned from the Jupyter notebooks. As part of my forecasts, I want to show the uncertainty surrounding the predictions, and how they're associated with individual forecasted sequences. This requires computing a convex hull surrounding a point cloud, and plotting it. I spent about 3-4 hours on Tuesday figuring out the code to make this part of the visualization, which I consider integral to communicating the project. [fluforecaster]: https://fluforecaster.herokuapp.com Another important thing is the user experience (UX) when interacting with my hybrid blog post + dashboard. Unlike this blog or one written in Medium (the blog of choice for Insight), I have interactive elements in the post, which meant I had to hand-craft the HTML for the page. In plotting the figures on the page, there are a set of functions in the backend that are run before the page is rendered. These compute the necessary JS for interactive web plots. They have to run fast enough, otherwise Heroku will timeout. Introducing code to plot the bounding boxes above slowed the loading time of the page beyond the 30 second limit Heroku imposed. As such, I had to carefully profile my code (mostly manually, with timing statements printed to console) to isolate the slow part, rewrite the implementation for speed, and re-deploy to Heroku. This took another good 3-4 hours, all to shave off dozens of seconds. The things we do with our lives! Throughout the week, a lot of other Fellows were getting their web demos set up. A lot of questions regarding Bokeh and Flask were flying around. Because of the discussion, I think I have a much better grasp over the programming model involved in making Bokeh work with Flask. Basically there's a bunch of plotting computation that is needed to get the JavaScript computer by Bokeh, and then through Jinja2 templating and HTML divs, we can put the final plot in the HTML canvas. A few more rounds of practice and I should be able to commit it to memory. The final part is in getting the presentation overall looking polished and understandable. This involves many tasks, from tweaking the text to making static figures and more. I have spent time with column layouts and configuring modals to get my page content looking overall fresh and yet also informative. Requires a lot of thought!",
    "tags": [
      "insight data science",
      "data science",
      "job hunt"
    ],
    "pub_date": "2017-06-17",
    "type": "blog"
  },
  {
    "id": "blog-insight-week-2",
    "url": "/blog/2017/6/10/insight-week-2/",
    "title": "Insight Week 2",
    "summary": "This week has been intense, mostly because I knew in advance that I'd be spending two days wearing a fancy hat, funky robe, and yellow sash.",
    "body": "This week has been intense, mostly because I knew in advance that I'd be spending two days wearing a fancy hat, funky robe, and yellow sash. Because I was missing two days of Insight, I had to get my Minimum Viable Product (MVP) out by Wednesday - thankfully, I did! If you've followed my blog post series on Insight (this is the 2nd post), my project is forecasting influenza sequences. This week, I hacked out my MVP and deployed it to Heroku as a hybrid HTML report + dashboard. I also picked up and incorporated a few new things along the way. The first is the use of tooltips on my Bokeh plots. Bokeh is really powerful, and in some of the exploratory analyses, I desired having tooltips as a UI element to help a reader (who might need some introduction) understand the nature of the problem and the data involved. The second is further mastery of Bootstrap CSS & JS. Now, Insight's Program Directors have told us clearly that we're not gunning to become front-end designers (and the likes). Keeping that in mind, I still think it's important to know at least one front-end framework well enough to produce pleasant-looking interactive tools or reports - knowing front-end elements potentiates us to communicate with front-end designers on final data products. For the MVP, I tried further experiments with the Grid layout and Modal JS. The key idea behind Grid layouts was easier to grasp - prioritize rows, then columns. With the Modal, stepping back for a moment, my goal was to display the science behind the project. However, it gets really technical. My audience is probably going to fall into one of two personas: the \"business person\" who just wants to see the final result and doesn't really care about the techniques, and the \"technical person\" who wants to dig deeper. I chose to use the Modal effect to satisfy both. The scientific methods are described at a high level on the main page, and the Modal element is used to show further information, graphics, and the likes. The third was deployment to Heroku itself! David Baumgold first showed me how to use Heroku at PyCon 2016, but I could never wrap my head around it at first. I think I didn't understand how \"deployment\" worked. A year later, stuff that DB taught me came to fruition, as I hacked on deploying a minimal Flask app to Heroku with my younger brother. That gave me enough of the Heroku-specific concepts to hack together the necessary and files to deploy Flu Forecaster to the web. For next week, these are my plans: - Solve the problem of \"average sequences per quarter\" being ~10 amino acids different from actual sequences. Two approaches I'm thinking of: - Use GPflow to train a Sparse Variational GP regressor on my dataset. This should allow me to scale up the forecasts beyond 67 quarters and into 700+ weeks, which is a greater time resolution and thus will give me greater sequence resolution in forecasts. - Try forecasting variance in addition to mean coordinates. This will give me a full probability distribution to sample from, which may allow me to stick with PyMC3 and quarterly forecasts. Still not sure which of the above two approaches are the better one, so I'll be sure to give each a shot.",
    "tags": [
      "insight data science",
      "data science",
      "job hunt"
    ],
    "pub_date": "2017-06-10",
    "type": "blog"
  },
  {
    "id": "blog-walk-the-stage-2017-edition",
    "url": "/blog/2017/6/10/walk-the-stage-2017-edition/",
    "title": "\"Walk the Stage\" 2017 Edition",
    "summary": "Finally put on a funny hat and an oversized robe, and topped it off with a yellow hood that only six of us received last Friday.",
    "body": "Finally put on a funny hat and an oversized robe, and topped it off with a yellow hood that only six of us received last Friday. Just six in the entire Institute this year - only six of us weirdos chose the SciDoc (ScD) degree! (I chose it because I like yellow over blue, and get to have a bit of fun confusing recruiters out there.) The day was too hot, and so I couldn't be bothered to dress up - who's going to see what I'm wearing underneath the robes anyways?! Anyways, overall a good feeling to be done. A little bittersweet because I'm leaving a time where I had a ton of fun learning new things, especially in my final two years of grad school, though I also think it's nice to have a change of environment and to have a new set of problems to solve. My hope is to continue being deeply engaged with the hard sciences, even outside of the academic ivory tower, just because it's a fun thing to do. Here's to hoping I can find a good match with a company out there.",
    "tags": [],
    "pub_date": "2017-06-10",
    "type": "blog"
  },
  {
    "id": "blog-insight-week-1",
    "url": "/blog/2017/6/2/insight-week-1/",
    "title": "Insight Week 1",
    "summary": "Insight's Week 1 is done! Here's some of my thoughts so far. Firstly, the Fellows at Insight is very fast at learning things.",
    "body": "Insight's Week 1 is done! Here's some of my thoughts so far. Firstly, the Fellows at Insight is very fast at learning things. Everybody is either a PhD or MD, some have done post-doctoral work, and even fewer have become professors, but everybody is interested in doing data stuff, and are very fast at picking up new things. I think at the same time, we're also good at thinking strategically upon being given feedback; once an idea sounds infeasible, new ideas come out of the pivot or even switch. Secondly, I see now the importance of developing a great data product. I think of a data product in terms of the input data, the transformation applied to the data, and the insight returned from the data. Think of it as a Python function: Most of the \"data products\" being developed are consumer-facing type projects that a user can interact with, but a small number of them, mine included, are \"dashboard-style\" products that can continually ingest continually updated data and return continually updated insights. Both are good ideas. Thirdly, I've become clear on the importance of first clearly defining the problem we want to solve, and then working backwards to define what we build, particularly for the minimum viable product (MVP). This way of thinking keeps us agile, and prevents us from being stuck in a rut. Fourthly, other fellows know lots of good stuff that I've been able to learn about. For example, in deep learning, there's been a few steps I wasn't sure about w.r.t. convolutional neural networks in autoencoders. One other fellow, a post-doc from UC Berkeley, gave me the master-class run-through on what happens at the vector/matrix level with convolutional neural networks. Thus far, really nice. I've noticed we don't generally end up competing with one another, and the atmosphere is very collaborative. We're working with one another, talking with one another, building trust and the likes. I'm looking forward to the coming weeks!",
    "tags": [
      "insight data science",
      "data science",
      "job hunt"
    ],
    "pub_date": "2017-06-02",
    "type": "blog"
  },
  {
    "id": "blog-pycon-2017-highlights",
    "url": "/blog/2017/5/22/pycon-2017-highlights/",
    "title": "PyCon 2017 Highlights",
    "summary": "A selection of my favourite talks and tutorials from PyCon 2017!",
    "body": "Last post was about thoughts on past PyCons, having attended PyCon 2017. This post is on PyCon 2017's highlights for me. (1) Serving as part of the organizing committee. I had the privilege of serving on the FinAid committee this year, and spent a large fraction of time in the staff room preparing to disburse FinAid cheques. I have very vivid memories of how slow the line was when I was receiving my cheques back in the day, and so I wanted to make sure FinAid recipients could receive their reimbursements as fast as possible, without wasting time in line (when they could instead be listening on talks). (2) Teaching two tutorials. This year, I submitted two tutorial proposals, and both were accepted. In the three years that I've been teaching it, Network Analysis Made Simple has always been popular, and I think it's because it gives participants a different way of thinking about data, thus making it an intellectually stimulating topic. I also developed a new material on Best Testing Practices for Data Science. This one, in retrospect, was much fresher, and thus in need of more battle-testing and polish compared to Network Analysis. I have some ideas, including modifications to the workshop format, narrowing the target audience and more, to make it more useful for future iterations. (3) First talk at PyCon! I also gave a talk at PyCon on doing Bayesian Statistical Analysis with PyMC3! This was my first PyCon talk ever. It was so nice to have a tweet-commendation by PyMC3's creator Chris Fonnesbeck too: . @ericmjl is giving the #PyMC3 talk you really want to hear if you want to learn how to put it to practical use. Room 251. #PyCon2017&mdash; Chris Fonnesbeck (@fonnesbeck) May 21, 2017 It was also nice to have Thomas Wiecki's tweet-commendation too: Excellent #PyMC3 talks at #PyCon2017 by @fonnesbeck: https://t.co/iFbxjSz9C1 https://t.co/VhAJLpVQBR and @ericmjl: https://t.co/MnkyWRivXs&mdash; Thomas Wiecki (@twiecki) May 22, 2017 Beyond that, the attendees seemed to like the talk too on the Twitterverse! Great introduction to Bayesian data analysis with PyMC3 by @ericmjl at #PyCon2017 https://t.co/BjbR0tZ6AO&mdash; Matteo Visconti dOC (@MatteoVdOC) May 21, 2017 Awesome talk on Bayesian analysis by @ericmjl! Check out the material here: https://t.co/HAPVYjKUhM&mdash; Sarah Guido (@sarahguido) May 21, 2017 An excellent concrete introduction to Bayesian stats. I&#39;m really looking forward to working through the notebooks as well. #PyCon2017 https://t.co/ec230Ui3iC&mdash; Leland McInnes (@lelandmcinnes) May 21, 2017 Problems, code and explanation - nice! https://t.co/ulJEXfMAo0&mdash; AV Speech Processing (@AVSP) May 21, 2017 Great summary slide @ericmjl! pic.twitter.com/JbFV4qrzCl&mdash; William Farmer (@willzfarmer) May 21, 2017 Last technical talk of #PyCon2017 and my brain is full. Excellent introduction to #PyMc3 and Bayesian variable inference by @ericmjl&mdash; Justin Gosses (@JustinGosses) May 21, 2017 Great talk from @ericmjl on PyMC3 making everything clear with concrete examples. #pycon2017&mdash; Leland McInnes (@lelandmcinnes) May 21, 2017 .@ericmjl giving #bayesian intuition at #PyCon2017 ; as always, a great, inspiring speaker https://t.co/4qz9QeTArc&mdash; Hugo Bowne-Anderson (@hugobowne) May 21, 2017 Foregoing formula, @ericmjl wants you to walk away w/intuition about Bayesian stats: &quot;update beliefs having seen the evidence&quot; #pycon2017&mdash; Melissa @ #pycon2017 (@iffor) May 21, 2017 It's very heartening to see how many people want to move into Bayes-land! The talk also happened to be the last in the session and last of the day, so I think many people were tired by that point and wanted to go to the final keynote. Thus, the only question came from my friend Hugo, with whom I also worked on a course at DataCamp, who asked about \"how we might communicate these ideas to, say, a manager.\" My thoughts on that were to report not a single number (e.g. the mean), but also the range, and communicate how the lower and upper bound of the range would affect bottomline decisions, or open up new opportunities (though I probably could have expressed this sentiment better). (4) Feeding Guido van Rossum. Python's BDFL, Guido van Rossum, wandered into the staff office asking to see whether the speaker ready room was open, because he was hungry and was looking for some snacks. We initially suggested the main conference hall, but later I ran out and called him back, because we had some English biscuits in the staff room, and we engaged in a short chat. That's when I had my star-awed moment! Was tempted to get a photo, but I figured he'd probably be fed up with people asking for photo ops, so I decided against it, hoping to be considerate for him. When he finished the biscuit, he said goodbye, and left the staff office. Amazing how everybody else just went about their own business while he was in the room; speaks to the lack of ego that PyCon celebrities have, and that sets a great example for the rest of ",
    "tags": [
      "pycon",
      "conferences",
      "python"
    ],
    "pub_date": "2017-05-22",
    "type": "blog"
  },
  {
    "id": "blog-post-pycon-2017-thoughts",
    "url": "/blog/2017/5/21/post-pycon-2017-thoughts/",
    "title": "Post-PyCon 2017 Thoughts",
    "summary": "This year's PyCon 2017 is over! Well, for me at least, as I head back to Boston, a place I've had to call home for the past 6 years.",
    "body": "This year's PyCon 2017 is over! Well, for me at least, as I head back to Boston, a place I've had to call home for the past 6 years. I've noticed my Portland PyCons have felt different from my Montreal PyCons. In Montreal, I felt more like a taker, a newcomer, a beginner. In Portland, I felt more like someone who could finally give back to the community. If anything, I hope I've been able to encourage others to also give back to the community. In Montreal, with respect to the community, I felt like I had to slowly navigate a new landscape of networks with people. There, I met a bunch of people who first became my PyCon community mentors: Stuart Williams and Reuben Orduz, whose years of experience in the community and in life are way beyond mine, became long-distance friends with whom I would look forward to meeting with again at the next PyCon. Carol Willing, a fellow MIT alum whom I met at a SciPy conference, also likewise became a community mentor for me. They didn't have to do much: words of encouragement, encouraging us to contribute back while themselves leading the charge, and connecting people together. These two years in Portland, I've instead started to get involved with the internal organization of PyCon, volunteering a bit of my time on the Financial Aid committee. That's where I got to meet even more people in the community, and in person too! LVH and Ewa, a husband-and-wife team who have made many community contributions. Karan Goel, a software engineer at Google who led FinAid this year and whom I shadowed for taking on next year's FinAid chair role (I think we'll just share the duties again like this year). Kurt, PSF's treasurer who's been doing this for decades, and even at his age, still loves programming, and who loves black decaf coffee. Brandon Rhodes, who is a Python community celebrity for his eccentricity and entertaining talks, who gave me many words of encouragement as I rehearsed my PyCon talk. Ned Jackson Lovely, for whom no words other than \"positive energy radiating through everything he does\" can best describe him. I think the PyCon community has done the \"community building\" portion of coding really well, and I'm thankful to be able to be part of this community of people. At the end of the day, good code is about bringing a benefit to people. So at the end of the day, while programming is an act of making routine things efficient, it's ultimately still about people, not code in and of itself. Thank you, PyCon community, it's been really fun being a part of the community this far, and I'm looking forward to many more years too!",
    "tags": [
      "pycon",
      "conferences",
      "python"
    ],
    "pub_date": "2017-05-21",
    "type": "blog"
  },
  {
    "id": "blog-thesis-defence-video",
    "url": "/blog/2017/5/15/thesis-defence-video/",
    "title": "Thesis Defence Video!",
    "summary": "My thesis defence video is up on YouTube!",
    "body": "About two weeks after being done, my thesis defence video is up on YouTube! It can be found here: https://youtu.be/ePqhQusK-3Q?t=1m23s. My favourite parts are recollecting the thought of being scooped by someone else 4 years ago, saying that some people like doing sampling, and stating how the lessons from my first committee meeting have been passed on. Ahh, so many good memories!",
    "tags": [
      "grad school",
      "academia",
      "thesis",
      "defence"
    ],
    "pub_date": "2017-05-15",
    "type": "blog"
  },
  {
    "id": "blog-why-i-teach-coding-tutorials",
    "url": "/blog/2017/5/13/why-i-teach-coding-tutorials/",
    "title": "Why I Teach Coding Tutorials",
    "summary": "I'm very excited to be at PyCon! It's a bit of a personal challenge this year, as I'll be leading two tutorials, one on Network Analysis and one on...",
    "body": "I'm very excited to be at PyCon! It's a bit of a personal challenge this year, as I'll be leading two tutorials, one on Network Analysis and one on Data Testing. With a bit of time on hand, I've done a bit of introspection as to why I love doing these tutorials. I think I can boil it down to a few broad themes. Reason 1: Learning. When it comes to learning material, nothing beats having to teach it to someone else. This means I have to master the material in order to teach it responsibly to someone else. Reason 2: Reputation. Grounded on the foundation of having mastered the material I'm going to teach, getting out there helps me build a reputation for having both technical mastery and the ability to communicate the material out. Reason 3: Networking. By going to conferences where my tutorials are accepted, it's a great way to meet people and learn about the latest and greatest out there. My hope is wherever I end up working, I can continue this craftsmanship!",
    "tags": [
      "pycon",
      "conferences",
      "data science"
    ],
    "pub_date": "2017-05-13",
    "type": "blog"
  },
  {
    "id": "blog-pycon-2017-tutorials-and-talks-preview",
    "url": "/blog/2017/5/4/pycon-2017-tutorials-and-talks-preview/",
    "title": "PyCon 2017: Tutorials and Talk Preview!",
    "summary": "This year, I'll be at PyCon 2017 presenting two tutorials and one talk! I'm very excited to be attending! The first tutorial I will deliver is on...",
    "body": "This year, I'll be at PyCon 2017 presenting two tutorials and one talk! I'm very excited to be attending! The first tutorial I will deliver is on network analysis. The GitHub repository is [online][nams], and is the most mature of the three. This will be my 3rd year teaching the tutorial; I first developed the material in 2015, and have been refining it ever since. This year, I have great help from Mridul Seth, a student from India who has also been doing network analysis. [nams]: https://github.com/ericmjl/Network-Analysis-Made-Simple The second tutorial I will be leading is on testing practices for data science. The GitHub repository is [online][dtest], and will cover the use of automated tests for checking code and data integrity, as well as the use of visualization methods in EDA to sanity-check the data. The material is still in development right now, and I'm hoping to get good feedback from the Boston Python community when I dry-run it locally in the Boston area. [dtest]: https://github.com/ericmjl/data-testing-tutorial My talk will be on Bayesian statistical analysis using PyMC3. As usual, the materials are available online on [GitHub][bayes]. In it, I will cover the two most common types of statistical analysis problems - parameter estimation and comparison of treatment with controls, and demonstrate the process of reasoning through model building, implementing it in PyMC3, and interpreting the data. [bayes]: https://github.com/ericmjl/bayesian-stats-talk Really excited to be making three contributions back to the Python community. I've benefited much from the use of Python tools, and every PyCon I learn something new, so this is my little way of giving back!",
    "tags": [
      "pycon",
      "conferences",
      "python"
    ],
    "pub_date": "2017-05-04",
    "type": "blog"
  },
  {
    "id": "blog-managing-conda-environments",
    "url": "/blog/2017/5/3/managing-conda-environments/",
    "title": "Managing conda environments",
    "summary": "I recently got around to hacking a system for managing my environments better. Previously, my coding projects mostly relied on one master...",
    "body": "I recently got around to hacking a system for managing my environments better. Previously, my coding projects mostly relied on one master environment (with exceptions, e.g. development, or my Network Analysis Made Simple tutorial), but conflicts started cropping up. Thus, I decided to separate out my environments. However, keeping track of which environments go with which projects began getting tedious. I thus decided to automate some of the steps involved in maintaining environments, and keep everything centrally managed so my brain doesn't overload. It involves a bit of GitHub and a bit of bash scripting, but altogether gives a ton of flexibility and control over keeping my environments updated. I start by keeping a central repository of environment YAML specifications. Mine is kept here. Each YAML specification includes just the minimum set of packages that I need; manages the dependencies. For example, my environment specification for Bayesian statistical analyses looks as such: Now, I've not pinned specific versions here, because I like to keep up with the latest stable releases. However, if version pinning is desired, it's totally possible to pin specific packages to particular versions, using the same syntax as I did for . In each project repository, I have an script, that looks something like this: The key idea here is that I download only the relevant YAML file, export it as a generic file, and then run the command on it to keep the environment up-to-date. Now, here's the magic. I hacked Christine Doig's script to execute , and then auto-activate the environment. If my environment needs change, I can always update the environment YAML spec file (e.g. , or ) in the central repository, and use that to automatically update individual project environments.",
    "tags": [],
    "pub_date": "2017-05-03",
    "type": "blog"
  },
  {
    "id": "blog-defended",
    "url": "/blog/2017/4/28/defended/",
    "title": "Defended!",
    "summary": "It is done! I have defended my doctoral thesis, supervised by Prof. Jonathan A. Runstadler, advised by Prof. Mark Bathe and Prof.",
    "body": "It is done! I have defended my doctoral thesis, supervised by Prof. Jonathan A. Runstadler, advised by Prof. Mark Bathe and Prof. Jukka-Pekka Onnela, conducted as a student in the Department of Biological Engineering at MIT. Looking back, it's been an amazingly fun journey, and I'll be updating this space with a more detailed account of the thesis defence process. Stay tuned!",
    "tags": [],
    "pub_date": "2017-04-28",
    "type": "blog"
  },
  {
    "id": "blog-thesis-defence",
    "url": "/blog/2017/4/24/thesis-defence/",
    "title": "Thesis Defence!",
    "summary": "My thesis defence announcement has been made to the department! - Date: Thursday, April 27th, 2017 - Time: 2:00 PM - Location: 56-614 Identification...",
    "body": "My thesis defence announcement has been made to the department! - Date: Thursday, April 27th, 2017 - Time: 2:00 PM - Location: 56-614 Identification of Reassortant Influenza Viruses at Scale - Algorithm and Applications by Eric Ma Abstract Reassortment is a reticulate evolutionary process that results in genome shuffling; the most prominent virus known to reassort is the influenza A virus. Methods to identify reassortant influenza viruses do not scale well beyond hundreds of isolates at a time, because they rely on phylogenetic reconstruction, a computationally expensive method. This thus hampers our ability to test systematically whether reassortment is associated with host switching events. In this thesis, I use phylogenetic heuristics to develop a new reassortment detection algorithm capable of finding reassortant viruses in tens of thousands viral isolates. Together with colleagues, we then use the algorithm to test whether reassortment events are over-represented in host switching events and whether reassortment is an alternative 'transmission strategy' for viral persistence. Committee - Jonathan Runstadler, Assistant Professor of Biological Engineering and Comparative Medicine, MIT (Advisor) - Mark Bathe, Associate Professor of Biological Engineering, MIT (Committee Chair) - Jukka-Pekka Onnela, Assistant Professor of Biostatistics, Harvard School of Public Health",
    "tags": [
      "graduate school",
      "thesis defence"
    ],
    "pub_date": "2017-04-24",
    "type": "blog"
  },
  {
    "id": "blog-moving-on-from-mit",
    "url": "/blog/2017/4/21/moving-on-from-mit/",
    "title": "Moving on from MIT",
    "summary": "It's my final week before defending my thesis. The thesis has been submitted to my committee, and my slides, as usual, are being made until the last...",
    "body": "It's my final week before defending my thesis. The thesis has been submitted to my committee, and my slides, as usual, are being made until the last minute. I've had a number of conversations with a number of people, both in person and via email, and a few recurring themes start showing up. New Challenges Scientists are artists in some senses, computational scientists particularly, and I think I'm ready for a new challenge. Jobs People invariably ask, \"What's next?\" I tell them that I've ruled out a \"traditional post-doc\", and that I'm not wedded to the academic ivory tower, I'm wedded to my wife, and so if the timing doesn't work out for independent research fellows positions, I'm jumping out. Well, as things turned out, yes, I'm jumping out, and I'm looking forward to this new journey! Data Science A few months ago, I was pretty deflated with the job search. Interviews were slow to come, and I began to think that all the \"hype\" around life science DS was just that - hype - and that the demand wasn't there. A few months later, I'm proven wrong, and quite happily proven wrong too. Insight I signed the Insight Health Data Science Fellows contract, to join them for 7 weeks in the summer. Many were perplexed - isn't Insight all about getting a job? Not really. For me, it's about meeting like-minded individuals and being able to network with them. Interviews I've done interviews at a few places now, and the response has been very positive. No offers yet, but nonetheless these are all places where I can foresee myself being valued for what I can contribute, while having fun working with colleagues on new problems. Feelings The defence, at this point, has this magical effect of inducing anxiety at times, and feeling like \"just another thing to do\" at other times. I'm not sure what to make of this. Thesis My thesis, as it turns out, is super duper short. The departmental average range is on the order of 100-400 pages. Mine stands at about 80, including references, with double-spaced text, and sometimes having one figure/table on one page. I think I've broken some departmental records here... Audience Someone asked, \"Who is your defence audience going to be?\" I've decided it'll be for my committee, who have supported my intellectual journey through infectious disease, computation, and data science. In some ways it's my \"final performance\" during grad school, a way of saluting their support. As the clock winds down, I'm reminded of that phrase from Philippians: ... forgetting what lies behind and straining forward to what lies ahead... (Philippians 3:13-14) Laurels, yes, I have acquired indeed. MIT is no slouchy place to graduate from, and it's a privilege to have this degree. I've had the privilege and opportunity to remake my skillset while having fun along the way. But it's no time to rest on my laurels. It's time to start serving the world through the skillset I have been given. Onward!",
    "tags": [
      "graduate school",
      "job hunt",
      "insight data science",
      "data science",
      "mit"
    ],
    "pub_date": "2017-04-21",
    "type": "blog"
  },
  {
    "id": "blog-staying-nimble",
    "url": "/blog/2017/4/7/staying-nimble/",
    "title": "Staying Nimble",
    "summary": "As I wrap up grad school, one thought recurrently comes to mind: as we grow older, do we intrinsically lose mental agility, nimbleness and ability...",
    "body": "As I wrap up grad school, one thought recurrently comes to mind: as we grow older, do we intrinsically lose mental agility, nimbleness and ability to learn? Or is it because of external factors that cause one to become less adventurous, less curious, and hence, less able to learn new things? I find myself having flashes of fear crop up as I observe those who are older than me. They are stuck in between a hard place and a rock. Their skills may be on the wane (in terms of demand). They want to learn new things, but have to continue doing the old things to keep the ship afloat. I can imagine, it feels tough to be in that kind of position! Without the safety net to take a risk and learn something new, they may be stuck in a dying trade. Will I have to face the same fate? Is it avoidable, or even perhaps, surmountable? The even harder part is staying intellectually nimble, and not being stuck in particular ways of thought. Is that an intrinsic property of aging, or not? Just some questions I've been pondering...",
    "tags": [],
    "pub_date": "2017-04-07",
    "type": "blog"
  },
  {
    "id": "blog-thesis",
    "url": "/blog/2017/3/28/thesis/",
    "title": "Thesis",
    "summary": "Some short thoughts on how I used \"continual publishing\" to enable a single-source, multi-output, continually-updated thesis.",
    "body": "I've finally turned in a polished draft of my thesis (HTML or PDF) to my committee! My thesis topic is on the development of an algorithm to identify reassortant influenza viruses from large sequence databases, and its application to the study of influenza's evolution and ecology. Well, actually, it was last week when I finished it, but I've been doing the job hunt the past week that I've delayed on writing this blog post. Apart from the written summary of the work that I've been doing, I wanted to simultaneously write for PDFs and for the web, so I started assembling a software toolchain that compiles my raw markdown files, converts figures from PDF to JPG, and simultaneously builds the PDF and the HTML versions. A lot of Python packages, including [](https://github.com/mplewis/csvtomd), the pandoc-xnos series, and non-Python tools, including ImageMagick (https://www.imagemagick.org/script/index.php). Yes, I know I could have done most of this with Authorea, but being me, building things and doing reverse engineering is also kind of fun! (Especially for learning purposes.) I hope you enjoy my thesis!",
    "tags": [
      "thesis",
      "academia",
      "grad school"
    ],
    "pub_date": "2017-03-28",
    "type": "blog"
  },
  {
    "id": "blog-default-bayesian-models",
    "url": "/blog/2017/3/11/default-bayesian-models/",
    "title": "\"Default\" Bayesian Models",
    "summary": "As a positive distraction from my thesis writing, I've been thinking a bit about the statistical crisis in biomedical sciences and psychology...",
    "body": "As a positive distraction from my thesis writing, I've been thinking a bit about the statistical crisis in biomedical sciences and psychology research, and how it might be mitigated. A number of opponents of p-values and proponents of Bayesian inference have influenced my thinking around this issue. As such I have come to the conclusion that Bayesian estimation and inference should be more widely used, because it essentially comes with interpretable uncertainty built into the inference philosophy. I think one thing preventing adoption of Bayesian inference methods is their flexibility (read: complexity). How does one compose an model with little grounding in statistics? To address this problem, I've started putting together Jupyter notebooks showing common problems in the experimental sciences and a sensible default model that one can use for that kind of problem. For me, a recurrent (and very interesting) theme came up. The nature of probabilistic graphical models is such that if we are able to forward-simulate how the data may be generated, then given the data and a loss function, fitting the data is merely a matter of optimization. The core idea behind these notebooks, then, is that there are a small number of \"generic\" models of how data may be generated that can cover a large proportion of scenarios, particularly in scenarios where we don't have sufficiently good theory to forward-simulate the complex data-generating distribution underlying the data.",
    "tags": [],
    "pub_date": "2017-03-11",
    "type": "blog"
  },
  {
    "id": "blog-bostons-geospatial-data",
    "url": "/blog/2017/2/13/bostons-geospatial-data/",
    "title": "Boston's Geospatial Data",
    "summary": "I finally did it! The city of Boston recently released their data in the open. I wanted to have an excuse to play with geospatial data, so as a...",
    "body": "I finally did it! The city of Boston recently released their data in the open. I wanted to have an excuse to play with geospatial data, so as a distraction from thesis & proposal writing, I hunkered down for about a workday's worth of time and put together a bokeh app. In this web app, the end user can select from one of a number of geospatial datasets released, and visualize the distribution of those data points. There's no added metadata right now, but if I get fed up with thesis & proposal writing I might go back and add in metadata hover/tooltips. I hope you have fun cloning the repo and running it!",
    "tags": [],
    "pub_date": "2017-02-13",
    "type": "blog"
  },
  {
    "id": "blog-numba-my-first-attempt-at-being-serious-with-it",
    "url": "/blog/2017/2/8/numba-my-first-attempt-at-being-serious-with-it/",
    "title": "Numba: My first attempt at being serious with it",
    "summary": "This evening, I saw a Tweet about using , and I thought, it's about time I give it a proper shot. I had been solving some dynamic programming...",
    "body": "This evening, I saw a Tweet about using , and I thought, it's about time I give it a proper shot. I had been solving some dynamic programming problems just for fun, and I thought this would be a good test case for 's capabilities. The DP problem I was trying to solve was that of collecting apples on a grid. Here's how the problem is posed: I have a number of apples distributed randomly on a grid. I start at the top-left hand corner, and I'm only allowed to move downwards or to the right. Along the way, I pick up apples. What's the maximum number of apples I can pick up along the way? This is a classic 2-dimensional DP problem. I simulated some random integers: I then wrote out my solution, and wrapped it in two versions of the function call: one native and one numba-JIT'd. Here's the performance results: The slowest run took 4.27 times longer than the fastest. This could mean that an intermediate result is being cached. 10000 loops, best of 3: 99.7 \u00b5s per loop 10 loops, best of 3: 50.3 ms per loop Wow! Over 500-fold speedup! All obtained for free using the decorator.",
    "tags": [
      "numba",
      "open source",
      "data science",
      "optimization",
      "coding snippets"
    ],
    "pub_date": "2017-02-08",
    "type": "blog"
  },
  {
    "id": "blog-data-diagnostics-missingno",
    "url": "/blog/2017/2/6/data-diagnostics-missingno/",
    "title": "Data Diagnostics: missingno",
    "summary": "Sometimes, all that you need is a visual cue on whether the data you have on hand are complete or not.",
    "body": "Sometimes, all that you need is a visual cue on whether the data you have on hand are complete or not. Looking at a table can be dizzying at times, so I'm very glad I found this packaged called [](https://github.com/ResidentMario/missingno)! It provides a way to quickly visualize the \"nullity\" of your dataset. See an example below: [](./data-completeness.webp) It's built on top of , and takes in DataFrames, which means it plays very nicely with the rest of the PyData stack. I recently took it for a tour when I did a quick stats consult with Mia Lieberman (DCM); the above plot was made using her data, used with permission. Highly recommended package!",
    "tags": [],
    "pub_date": "2017-02-06",
    "type": "blog"
  },
  {
    "id": "blog-verily-interview",
    "url": "/blog/2017/1/25/verily-interview/",
    "title": "Verily Interview!",
    "summary": "I have finished my first complete job interview! I'm happy for the experience, no matter what the result is, as it's a very eye-opening one.",
    "body": "I have finished my first complete job interview! I'm happy for the experience, no matter what the result is, as it's a very eye-opening one. I've been in an ivory tower for quite a while, so going out and seeing what the experience like is worthwhile enough, even though it'd also be a very nice icing on the cake to have a job offer too. If I'm not mistaken, I'm bound by NDA not to write/speak about the specifics of the interview, but I probably can speak about it in general. Overall, they were very welcoming! I was lodged in a very nice hotel just 6 minutes away from Verily by foot, with breakfast included (yum!). 5 people served as my interviewers, and I had lunch with a more senior person to learn more about the company's history; turns out 2 of them share a Boston connection as well. There was a good mix of coding and biology, and after the interview, I immediately went back and coded up my answer to my second interviewer's questions... only to find out I had done it wrong! Oh well! What's done is done, and I now know the combinatorics functions in the Python standard library much better. I also went back and thought more about the other coding problem, asked by my final interviewer. It was a frequentist statistics question, and I had been very much in a Bayesian frame of mind when he asked the question, so I was tripped up by my own biases when that happened. (There are foundational philosophical differences between the Bayesian and frequentist statistical mind sets, but I won't go in that direction.) Three of my other interviewers are very interested in the biological question behind what I was working on, and so I had a much smoother time (I think) with them. One was a wet bench scientist, and the other two do computation work as part of the computational biology team. I also think that my time with them played to my strengths, which are translating the biological problems into code, rather than making algorithms efficient. Not that I'm ruling out learning how to write efficient algorithms, though - I'd definitely love to master that! I say that my time with them played to my strengths because I was able to show them the thought process of how I turned a biological problem into a computable problem, and was able to highlight where the limitations of what I did were. My third and fourth interviewers took a deep interest in my work, and I was really, really happy to share with them what I'd been doing. Overall, a very fun experience! Big thanks to the HR and Comp Bio teams there, it was an intense day, but it was also very fun. I'm now just waiting for my red-eye flight to go back to Boston to teach a Data Carpentry workshop at Tufts Medical. Hope I survive till noon tomorrow!",
    "tags": [],
    "pub_date": "2017-01-25",
    "type": "blog"
  },
  {
    "id": "blog-building-bokeh-apps",
    "url": "/blog/2017/1/17/building-bokeh-apps/",
    "title": "Building Bokeh Apps!",
    "summary": "I finally did it - I built a Bokeh server app for myself! All of last week, I brought my UROP student Vivian and a fellow church friend Lin to the...",
    "body": "I finally did it - I built a Bokeh server app for myself! All of last week, I brought my UROP student Vivian and a fellow church friend Lin to the \"Data Science with Python\" skill-building workshop series, hosted by the Harvard IACS. As I already knew most of what was going to be taught, I decided that it'd be a fun thing to try playing with the dataset that they used for the contest. (Having a contest was also extra motivation to try building something I've never done before.) The goal of the contest was \"to build a visualization of the UCI forest dataset that would help an analyst make decisions\". The UCI forest dataset basically asks us to predict forest cover type from cartographic variables. As I had always wanted to use Bokeh to build a data dashboard, I thought I should give it a shot with the UCI dataset. What I eventually built were two things. The first one was a linked scatter-plot dashboard to visualize how the quantitative variables varied with one another and how they grouped by forest cover type. Here, the user can select the two variables that they want to visualize together, using drop-down menus. [](./bokehscatter-full.webp) The second one was a bar chart showing the mutual information between the categorical variable values and the forest cover type. Here, the user can select from over 40+ variables, and their mutual information score will be displayed as one of the bars in the bar chart. [](./bokehselect-full.webp) As usual, I copied heavily from the examples and then modified it for my own use. Breaking the copied app helped me figure out where my cognitive blind spots were. Here's what I've learned. Firstly, I finally have wrapped my head around callback functions and their use in GUI applications. The idea behind callbacks is that when I click on a GUI element (e.g. button) or change the GUI element's state (e.g. selecting a checkbox, typing into a textbox), that function is called that \"does something\". What's that something? Well, it's any action I decide that needs to be taken. Secondly, I finally figured out how non-intimidating Bokeh server really is. It's like... Flask, except for data visualizations. And it takes care of most of the HTML hard lifting for the data visualization portion. Thirdly, I finally got 'linked plots' working, in which the axes scales or data selections are shared across plots. I noticed it works when using the mid-level \"plotting\" interface, but doesn't work when using the high-level \"charts\" interface. Definitely something to keep in mind. Fourthly, I saw how Bokeh server allows us to write custom Python code for callbacks. At least for me, it's much more user-friendly this way, as I find JS idioms to be a bit wonky. Maybe I'm just not used to them. Either way, being able to use custom Python means I'm able to do things like filtering Pandas DataFrames before pushing the data onto the plot. Finally, I got to use the VBar contribution I made to the Bokeh library last year! Though I eventually moved my bar plots to the \"charts\"-level API, my initial implementation was done using the \"plotting\"-level API. It brought me a modicum of satisfaction to use the glyph, something that I had contributed (of course, acknowledging the amount of hand-holding provided by Sarah Bird and Bryan Van De Ven, who are the lead developers of Bokeh). My code can be found online here.",
    "tags": [],
    "pub_date": "2017-01-17",
    "type": "blog"
  },
  {
    "id": "blog-on-learning-math",
    "url": "/blog/2017/1/5/on-learning-math/",
    "title": "On Learning Math",
    "summary": "Though I will admit to being somewhat algebra-blind (more on that later), I wasn't necessarily bad at math concepts.",
    "body": "Though I will admit to being somewhat algebra-blind (more on that later), I wasn't necessarily bad at math concepts. I did have one big problem with the way I was learning math, though - it always seemed to be more theoretical and less applied, as if solving puzzles for solving puzzles' sake was the best way to approach learning math. I'll grant that, yes, many mathematicians and statisticians I know, are one of those types where doing and learning math doesn't have to be tied to some applied goal, and for whom math is viewed as intrinsically fun. But... I'm not one of those types, and it was for this reason I nearly abandoned quantitative thinking in my undergrad. Math was boring after Science One, because I wasn't shown how it was applicable to real-world problems in upper-year courses. Yet, after this PhD, I'll have essentially developed an area of expertise in some hybrid of statistical evolutionary biology and deep learning for biochemistry. Makes me wonder whether the current mode of puzzle-/theory-driven math (and perhaps even CS) is actually driving off people who might not be interested in the intrinsic fun of math, and are more interested in the applied fun? Science One math was taught by Leah Keshet and Mark MacLean. The part I remember most vividly was learning about differential equations as applied to ecological problems and biochemical reaction kinetics. Along the way, we had to learn differentiation and integration... not for differentiation and integration's sake, but for figuring out useful properties of these two wildly different systems. Now that was amazing math. What do I mean by algebra-blind? I basically mean that I tend to confuse algebraic symbols, and can't hold them in my head without having some lookup-table/glossary for what they mean. The lookup-table of algebraic symbols to their meaning was a hack that I never mastered until I finished formal math education; yet it was extremely helpful to have when I was discussing math with my collaborators up at Harvard. Most of my score deductions in math tests came from confusing algebraic symbols with one another...",
    "tags": [
      "data science",
      "education",
      "graduate school"
    ],
    "pub_date": "2017-01-05",
    "type": "blog"
  },
  {
    "id": "blog-on-openness-equals-source-code-inspectability",
    "url": "/blog/2017/1/3/on-openness-equals-source-code-inspectability/",
    "title": "On \"Openness = Source Code Inspectability\"",
    "summary": "One tenet of open science is the notion of \"being able to inspect the source code\". It's a good, lofty goal, but it comes with a big assumption that...",
    "body": "One tenet of open science is the notion of \"being able to inspect the source code\". It's a good, lofty goal, but it comes with a big assumption that I think needs to be made clear. This assumption is that scientists who are reviewing papers that incorporate code are capable of accurately interpreting the source code in a reasonable amount of time, have a sufficient working knowledge of the language that the work is implemented in, while also possessing sufficient domain knowledge to review the paper. That middle point is the current sore point. In certain fields, such as artificial intelligence/machine learning and computational biology (both my pet fields), this is not an issue, as the substrate of scientific work is code. On the other hand, there's the scenario of experimental work that is analyzed with custom code. Here, the substrate of the scientific work is not primarily the code, it is the experiment. Reviewers who evaluate this type of work may not necessarily have the expertise to inspect the source code of the pipeline. I am not aware of journals whose editors make the extra effort to solicit a team of reviewers capable of covering both experimental design and code inspection. To be just a tad more pedantic, I'd insist that code inspection is important. Having myself reviewed a paper for PLoS Computational Biology, I was interested in the accuracy of the implementation (done in R), not simply whether I could re-run the code and obtain the same results (reproducibility). (Side note: though I'm a Python person, I'd be happy to review R code, though I would do so with the recognition that I'm not as well-versed in R idioms and coding patterns as I am with Python's. I'd be less-than-fully-qualified under the \"sufficient working knowledge\" criteria.) In summary, my claim here is that openness requires more scientists who are also technically qualified, in order to achieve the goal of reproducibility + veracity. Without the ability to inspect source code, we're only left with reproducibility, which is not good enough.",
    "tags": [
      "open science",
      "open source",
      "peer review"
    ],
    "pub_date": "2017-01-03",
    "type": "blog"
  },
  {
    "id": "blog-my-flask-app-building-sprint",
    "url": "/blog/2016/12/27/my-flask-app-building-sprint/",
    "title": "My Flask app-building sprint",
    "summary": "Overview This winter, I decided to embark on a coding project purely for fun. In preparation to build my own Raspberry Pi photo display, I wanted to...",
    "body": "Overview This winter, I decided to embark on a coding project purely for fun. In preparation to build my own Raspberry Pi photo display, I wanted to build an easily-installable, portable (across operating systems) and completely hackable stand-alone image displayer. This project ended up being an awesome way to get myself familiarized with a wide variety of concepts in web development, software packaging, and software distribution. I learned a ton, and I want to share the process behind it. The design goals were as follows: 1. It does one and only one thing well: run the app from any directory, and show the photos in that directory in a random order. 1. It has to be easily distributable. I chose to use as my distribution mechanism, partly because of familiarity, partly be cause it is sufficiently ubiquitous (with Python). 1. It should be completely hackable. My source code is up on [GitHub][imgdisplay]. Anybody can fork it, hack it, and redistribute it. Go for it - it's BSD-3 licensed! [imgdisplay]: https://github.com/ericmjl/imgdisplay The philosophical goals were pretty simple. Learn how to do the whole stack from scratch, and be free from commercial, closed-source software constraints by being free to build exactly what I need from reusable components. Writing the App Logic My choice of tools were as follows: - 3.5 - : great framework for web development in Python. Provides glue between Python and HTML. - : provides native GUI wrappers for each platform. - : awesome framework for command-line options. My thought process here was as such: write the user-facing interface using HTML, and write application logic in Python, and we get automatic cross-platform portability. Run the app from the command-line, which is the lowest-common denominator for running applications. I structured my app as follows: By most standards, this (at least in the eyes of pros) is probably a very, very simple Flask app. The app logic was the first part that I tackled. Let's start with the file . Here's my commentary on each of the sections. Section A: Flask boilerplate. Here, we instantiate a Flask instance called . The variable was later added on, because I later learned that Flask apps had to look within the project directory for the folder; this variable ensures that the correct template directory path is specified. Section B: Main application logic. This is Flask's \"hello world\" function expanded. What we're doing here is reading a list of files from the bash shell's current working directory. If there are images present, we tell choose one file, and tell Flask to render the template () passing in an to the keyword argument. If none, we pass in an error text message to the keyword argument. If this were a more complicated app, we would probably move to an MVC-like model, where the application logic would be in an importable module adjacent to the rendering code. Here, because the logic is simple enough, and only really amounts to three lines of Python, it's simple enough to not require placing it in a separate Python module. I think at this point, it's best to show how these will get rendered. Below is , the template that is being used. Flask uses templating - what this basically means is that we can insert Python-like code into other text-based files, allowing for passed in values to be substituted. For example, consider the block below: What this is essentially saying is: if the \"error\" keyword from is not a null value, fill in the value passed to the keyword (). What about the header? It's got something much more complicated in there, the function. What this is saying here is render the URL for () the static directory to load the CSS file , which allows us to use the static CSS file to style the user interface appropriately. If you inspect the HTML source after rendering, you will see that it maps to . What about the image? It's a bit complicated, so let me try my best to explain what's going on. This uses the function, which is Flask magic for saying, \"render the URL for a particular function\" (), while passing in the necessary keywords arguments to that function (). But... where did the function come from, and why is it's keyword arguments named as ? Well, that's the best segue into Section C. Section C: random_image...?! If by the end of my own explanation you don't get it, don't worry. The inner workings remain a bit of black magic to me still. Here's the code: This function is what is called in the template. The function takes in an image name keyword argument, which is then passed to Flask's function. Here, we are essentially saying, \"get the file from the directory , as an attachment (), and send it to Flask.\" Somehow, this provides the correct way to send the image file to the browser renderer. Side note: Figuring this out turned out to be the better of a whole day's worth of debugging and reading through the Flask package documentation, plus another half a day on Stack Overflow trying to figure out the right coding ",
    "tags": [
      "software engineering",
      "side projects",
      "flask",
      "python"
    ],
    "pub_date": "2016-12-27",
    "type": "blog"
  },
  {
    "id": "blog-how-to-make-your-python-scripts-callable-from-the-command-line",
    "url": "/blog/2016/12/24/how-to-make-your-python-scripts-callable-from-the-command-line/",
    "title": "How to make your Python scripts callable from the command-line",
    "summary": "So you've written this awesome Python module/script that does awesome stuff, but now you want to call it from the command line.",
    "body": "So you've written this awesome Python module/script that does awesome stuff, but now you want to call it from the command line. I'm going to show you how to make this happen. First off, create your Python script. Below, borrowing from the [Click][click] package documentation, I'm going to show a script that is just slightly more complex than a simple \"hello world\". [click]: http://click.pocoo.org/5/ Let's say this script is saved as in your project directory: What you'd want to put in your script is the following: The key magic point here is . What this portion is effectively saying is, \"attach the 'hello' function under the script to the console command . (I'm being playful here for clarity; it may make much more sense to name your console command the same name as the Python module you're distributing.) Now, if you open a new terminal session, you can run using the command, and use all of the command-line arguments that Click provides:",
    "tags": [],
    "pub_date": "2016-12-24",
    "type": "blog"
  },
  {
    "id": "blog-female-doctors-are-better-than-male-doctors-for-real",
    "url": "/blog/2016/12/20/female-doctors-are-better-than-male-doctors-for-real/",
    "title": "Female Doctors are Better than Male Doctors - For Real?",
    "summary": "For real. Read on for the statistical perspective on why. I saw this article published by JAMA, posted by a number of my friends on Facebook.",
    "body": "For real. Read on for the statistical perspective on why. I saw this article published by JAMA, posted by a number of my friends on Facebook. The claim here was that patients treated by women doctors show lower readmission rates than patients treated by men, controlling for as many other factors as can be plausibly controlled for. When I saw the clickbait-y headlines and posts by my mostly left-leaning friends basically going \"women are better than men\", my first instinct was \"go check the stats\". (I'm probably best labelled as centrist.) Indeed, I think I would have made Prof. Allen Downey proud - I didn't buy the p-values at face value. Rather, I was most interested in the effect size. If you check the main figure on the paper, it'll look like this: Peeking into the text for a bit more context for mortality (it's roughly the same magnitude difference elsewhere). Physician Sex and Patient Mortality The final sample for the analyses of 30-day mortality rates included 1 583 028 hospitalizations treated by 57 896 physicians. Overall 30-day mortality for the entire sample was 179 162 (11.32%). Patients cared for by female physicians had lower 30-day mortality than did patients treated by male physicians (10.82% vs 11.49%; risk difference [RD], \u20130.67%; 95% CI, \u20130.80% to \u20130.54%; P < .001; number needed to treat [NNT] to prevent 1 death, 149) after accounting for patient characteristics (Table 2). The difference in mortality persisted after adjustment for hospital fixed effects (female physicians, 10.91% vs male physicians, 11.46%; adjusted RD, \u20130.55%; 95% CI, \u20130.67% to \u20130.42%; P < .001; NNT, 182). Further adjusting for physician characteristics had a limited effect on these results (female physicians, 11.07% vs male physicians, 11.49%; adjusted RD, \u20130.43%; 95% CI \u20130.57% to \u20130.28%; P < .001; NNT, 233). If you look at just the difference between the two 'treatment' groups (male-treated vs. female-treated patients), there only seems to be a 0.42% difference in mortality rates after controlling for hospital patient characteristics, hospital fixed effects, physician characteristics, length of stay, use of care, discharge location, patient volume, and physicians\u2019 years of practice. Damn, that's a really small difference! The effect size (here improperly defined as ratio of larger to smaller) is only 1.03. Hardly an effect worth mentioning, isn't it? Wrong! That 0.42% difference occurred from a sample size of more than ~1.5 million hospitalizations. This is mortality that we're speaking of here, so in context, we're talking about ~6,600+ fewer deaths when women treat patients. 6,600+! Hospitalizations happen frequently and in large numbers, so a small percentage difference still means large numbers of people. If you happen to be pro-life, this is something that you really should care about. I still take issue with the article's emphasis on \"statistically significant or not\" method of reporting the results, but I think that's a flaw of the medical literature publishing culture, and something that the authors probably had to conform to in order to successfully publish the results. Nonetheless, if you even just skim the text of the paper, the data are consistent across all comparisons made, regardless of whether the results were \"statistically significant\" or not. That's pretty strong evidence, in my opinion.",
    "tags": [
      "data science",
      "statistics"
    ],
    "pub_date": "2016-12-20",
    "type": "blog"
  },
  {
    "id": "blog-experimenting-with-github-pages",
    "url": "/blog/2016/12/15/experimenting-with-github-pages/",
    "title": "Experimenting with GitHub Pages",
    "summary": "I've been playing a ton with GitHub pages recently, and so far, I've been impressed! My main use case has been in creating places where I can host...",
    "body": "I've been playing a ton with GitHub pages recently, and so far, I've been impressed! My main use case has been in creating places where I can host Reveal.js slides online, and showcasing some writing that I've put together that doesn't fit in my \"blog\" format. My [GitHub page][ghpage] showcases some of the best. [ghpage]: https://ericmjl.github.io/ Along the way, I've learned a ton. Here's some of the highlights. Firstly, I learned about standardizing project templates using [][cookiecutter], a Python package available on conda-forge and the cheese shop (PyPI). Initially, I tried hacking my own cookiecutter-ish hodge-podge of scripts, particularly for downloading the necessary components for [Reveal.js][reveal], but that didn't really work out, and I was eventually confused by my own logic. eliminated all of the confusion, and was really easy to operate. [reveal]: https://github.com/hakimel/reveal.js/ [cookiecutter]: https://github.com/audreyr/cookiecutter Secondly, because I was able to nail down a template for building Reveal.js slides, I managed to convert two of my previous talks (a [Journal Club][jc] on the Earth's virome, and the other my [Big Data Boston meetup talk][bdbos], which were written in Markdown and converted to PDF, into Reveal.js versions. I dug a bit into the SASS toolchain, and found that Reveal.js used SCSS files that were then compiled to CSS, and made a few minor tweaks that standardized the font sizing to something more in-line with my tastes. I also used the GitHub pages to host an \"HTML notes\" copy of the slides that the audience can use to follow along. [jc]: https://ericmjl.github.io/uncovering-earths-virome/ [bdbos]: https://ericmjl.github.io/big-data-boston-2016/ Thirdly, in the process of building the HTML notes, I created a CSS style that I ended up liking quite a lot, and it formed the basis of all of the pages that I ended up creating. The learning journey picking up the CSS specification was quite enjoyable, as things showed an immediate, one-to-one mapping between code and placement on the HTML canvas. This type of 'immediate', closed-loop feedback is important for learning stuff. When I had to go offline, thankfully there was [DevDocs][devdocs] where I could look up all of the styling options available to different types of HTML. [devdocs]: http://devdocs.io/ All in all, quite a fun experience! I have also put up my cookiecutter templates for [talks] and [writing] on GitHub, alongside a tutorial for the [slides][slides-tutorial], with the hope that it's useful for others! [talks]: https://github.com/ericmjl/cookiecutter-talk [writing]: https://github.com/ericmjl/cookiecutter-writing [slides-tutorial]: https://ericmjl.github.io/easy-talk-slides-and-notes/",
    "tags": [],
    "pub_date": "2016-12-15",
    "type": "blog"
  },
  {
    "id": "blog-scientific-inquiry-laymans-version",
    "url": "/blog/2016/12/8/scientific-inquiry-laymans-version/",
    "title": "Scientific Inquiry, Layman's Version",
    "summary": "Today on Facebook, I saw a link shared by the friend of a professor at Berkeley whose group I nearly joined 6 years ago.",
    "body": "Today on Facebook, I saw a link shared by the friend of a professor at Berkeley whose group I nearly joined 6 years ago. (Life would have been very different if I did.) It was a link to a [Bloomberg graphic][bloomberg], in which the article essentially outlined the scientific inquiry method in easy-to-follow, layman's terms. [bloomberg]: http://www.bloomberg.com/graphics/2015-whats-warming-the-world/ The presentation format was essentially as such: the graphic creator posed a series of hypotheses forward, and tested them using the data available on hand. \"Did X cause global warming? If it did, we would expect to see a correlation between levels of X and global average temperatures.\" A suite of plausible hypotheses (though nonetheless non-exhaustive) were presented: could it be CFCs? Ozones? Orbital changes? Volcanic activity? Finally, for each hypothesis, the data were charted to see if there was a visual correlation between the hypothesis variable and temperature. By a process of elimination, that's how the global scientific consensus has settled on greenhouse gases as the main driver of global warming. Could we be wrong? Definitely! The hypotheses, as I mentioned earlier, are non-exhaustive. Therefore, there still remains the low probability that the correct explanation has not been enumerated as a hypothesis put forth. That's okay! If there's another hypothesis to be tested, let it be tested by the same, consistent framework of evaluation. Regardless of the fact that the hypotheses are non-exhaustive, I would still consider it to be a low probability that newer hypotheses will overtake GHGs in credibility. Most of it stems from my (nonetheless non-exhaustive) reading of the scientific literature and layman scientific reporting. Additionally, as the apologist Ravi Zacharias has mentioned before, truth has to pass the test of coherence and correspondence: it must be part of a logically coherent whole, and it must have empirical evidence to support it. Without elaborating how, I will assert that GHGs as the causative factor of global warming passes those two tests. Anyways, props to the creative team at Bloomberg for putting [the article][bloomberg] together. Do check it out!",
    "tags": [
      "science"
    ],
    "pub_date": "2016-12-08",
    "type": "blog"
  },
  {
    "id": "blog-the-renaissance-researcher",
    "url": "/blog/2016/12/1/the-renaissance-researcher/",
    "title": "The Renaissance Researcher",
    "summary": "This is the computational biologist who knows how to design, execute, and analyze wet lab experiments and the generated data.",
    "body": "This is the computational biologist who knows how to design, execute, and analyze wet lab experiments and the generated data. This is the wet lab scientist who knows how to develop software that crunches the massive amounts of data that her experiment generates. Does this person exist? I'm going to make a bold statement, and then a bold prediction. This person does not exist. And this person will become huge in demand in the biosciences in the future. And yet this person will turn out to be a team. I think I'm qualified to make this statement. I've been on both sides of this dichotomy. I was a bench synthetic biologist, failed badly and went into computational evolutionary ecology, and then came back to the bench trying to marry together high throughout experimentation and deep learning. It's really, really tough to handle both at the same time. Part of it is a matter of context switching. Computation really requires continuous stretches of iterating between coding and thinking. Experiments, by contrast, represent a continuous disruption to focus time. It\u2019s impossible to juggle both in parallel. Moreover, both require month-long stretches in order to have momentum, during which one or the other gets lost. If I code, I lose momentum with my experiments. If I run experiments, I lose momentum with my code. There\u2019s either a cognitive or energy cost to juggling both at a high performance level. For me, it's a temperament issue. I learned during my three computation years developing algorithms that the quick turnover time of computation, to go from hypothesis to falsifying results, was something that I absolutely suited my temperament. I absolutely dreaded, and had no patience for the prospect of a failed hypothesis needing weeks to verify. I also work best in \"staggered sequential mode\", not \"parallelized mode\". Staggered sequential means as the results of an experiment become easier to forecast, I start something new. Parallelized mode means juggling things together even when the results aren't easily forecastable. There's a subtle distinction there. Parallelized mode doesn't work for me, because I get very easily confused between project details. With computation, that turnover time is often on the order of dozens of minutes max. With experiments, its weeks. With my temperament, I would much prefer the quick-turnover computational hypothesis testing rather than painstaking and slow rounds of hypothesis testing. Yet, as a computation type, I face the dilemma all the time: I have a hypothesis for which I have no data to test it with. This means going back into the lab, which means confronting the very pace that my temperament would prefer not to face. Alternatively, I can find people with whom I can collaborate. And that brings me to my point. I don't think there's such thing as the renaissance researcher. Rather, I think there is such a thing as a renaissance research team. Here, hypothesis generation and testing are a shared activity, and and the data generation and analysis are carried out by respective specialists. It's iterative, and it's undoubtedly going to be slower (at least initially) compared to going solo. But the rewards, I think, are going to be much greater once the team is in the state of \u2018flow\u2019. In some ways, it reminds me of the problem with finding the ideal \"data scientist\" - one who could wrangle big amounts of data, make plots, infer conclusions, create interactive dashboards, and communicate the results to business executives. Nonsense, this unicorn doesn\u2019t exist, and the industry quickly figured that out, thankfully, employing data science teams comprised of people with complementary skills.",
    "tags": [],
    "pub_date": "2016-12-01",
    "type": "blog"
  },
  {
    "id": "blog-vienna-and-imed-2016-reflections",
    "url": "/blog/2016/11/10/vienna-and-imed-2016-reflections/",
    "title": "Vienna & IMED 2016 Reflections",
    "summary": "This trip & conference was a productive one! I'm glad I went for it and didn't have to miss out on much (even though WebJet/Turkish Airlines (still...",
    "body": "This trip & conference was a productive one! I'm glad I went for it and didn't have to miss out on much (even though WebJet/Turkish Airlines (still can't sort out which) messed up my initial flight booking big-time). My biggest takeaways from IMED 2016 are as follows: 1. One Health is a big concept for disease surveillance, and I'm convinced it's the right way to do it. Sample not just humans reactively, but also animals proactively. 1. There's a project to \"whole genome sequence\" viruses from a whole variety of animals, called the Global Virome Project. It'll likely have a longer-term impact on disease forecasting than virus-specific surveillance networks. 1. [EcoHealth Alliance][eca] (NYC) and [Metabiota][metabiota] (SF) are doing cool stuff for disease surveillance. 1. Made some new friends at the conference. Global conferences are always fun. 1. Learned from Drew (of ISID) that conference locations should also coincide with off-peak tourist timing and with commitments over multiple years; it helps build the case for city sponsorship as well. 1. Hackathons are fun, though I remain convinced that their main utility is as a catalyst for launching new long-term collaboration efforts, not as a way to advance new ideas. 1. Receptions at fancy places (we did the Mayor's hall) are a great way to meet new people. Note to self, if I ever organize a conference myself. [eca]: http://www.ecohealthalliance.org/ [metabiota]: http://metabiota.com/ And then there's Vienna. Oh Vienna, what a beautiful old city you are. (Your donairs are awesome, btw.) I took 2 \"tourist days\" to walk around the city, and get a feel for it. I got to do some more iPhone photography, which was a nice change of pace. After finding my way to the Danube, I walked back and took the train to the Vienna city centre. The photos I took were mostly of the architecture, the parks, and activity there. Naturally, I couldn't resist touching up the photos on my computer. Here's an album of my favourites. I noticed that the public transit is amazingly advanced. Amazing, all round. That evening, though, was when I felt the stomach bug rumbling, and I realized I had food poisoning. I'm not sure where I got it, though, as it seemed too soon after trying out Schnitzel for the first time; I thought it might have been my body reacting to an overly-oily dinner. As such, I spent the second day stroking the cat and catching up on YouTube videos and eBook reading and sleeping in bed. The donair from a store nearby made for a good breakfast though! That marks the end of a really fun trip to Vienna. Hope to visit again! And I hope to see the others I met at IMED again!",
    "tags": [],
    "pub_date": "2016-11-10",
    "type": "blog"
  },
  {
    "id": "blog-book-thoughts-the-theory-that-would-not-die",
    "url": "/blog/2016/11/10/book-thoughts-the-theory-that-would-not-die/",
    "title": "Book Thoughts: The Theory That Would Not Die",
    "summary": "One of two eBooks I bought to read on this trip to Vienna was \"The Theory That Would Not Die\". It was a good read, and I definitely have come to...",
    "body": "One of two eBooks I bought to read on this trip to Vienna was \"The Theory That Would Not Die\". It was a good read, and I definitely have come to appreciate more the history behind Bayes' rule and its multiple deaths and resurrections. Three things stuck out for me. Firstly, I didn't realize how big of a genius Laplace was, and how prior to the two great wars, the epicentre of scientific inquiry was in France. Laplace basically touched a wide variety of foundational topics that later generations have built on, and his mastery of concepts as diverse as probability, calculus and the likes is amazing. I definitely can appreciate how he was able to connect many disciplines together. Secondly, many of the great breakthroughs of the past came from people dabbling in a problem for over a decade. Compare that to the expected constant output by professional scientists now. It seems to me that it is a truism that scientific inquiry can only really be conducted under certain circumstances, and funding crunches, KPIs and publication metrics, and distracted service to an academy aren't the ways to get it done. Finally, the role of the military was such a catalyst for new advances! Yet the military's role in research results in a paradox. We all want new breakthroughs to be open, freely accessible, and used for peaceful, civilian purposes. Yet the military provides the catalytic impetus for accelerating new technologies and ideas for practical use; not even capitalism has that force. The book highlighted how Bayes remained highly theoretical and impractical in the academic realm for decades after the military already started pushing through practical applications of it. All in all, good book, highly recommended to read!",
    "tags": [],
    "pub_date": "2016-11-10",
    "type": "blog"
  },
  {
    "id": "blog-why-phenotypic-interpretation-matters",
    "url": "/blog/2016/11/6/why-phenotypic-interpretation-matters/",
    "title": "Why Phenotypic Interpretation Matters",
    "summary": "At IMED 2016, I heard a lot of talks surrounding surveillance efforts. Most of it has been syndromic surveillance, because that's what's currently...",
    "body": "At IMED 2016, I heard a lot of talks surrounding surveillance efforts. Most of it has been syndromic surveillance, because that's what's currently collectable. For digital disease surveillance, search queries (a marker of \"interest\" in a disease) are used as a proxy for probable presence of a disease. These are good for real-time surveillance, but I think it's not good enough from the standpoint of accuracy and understanding mechanism. For accuracy, syndromic surveillance and search queries are nth-degree manifestations of underlying causes. Multiple different pathogens can present the same symptoms. Upticks in searches for a given symptom or pathogen may have multiple underlying causes, though \"hearing of the disease\" and \"being concerned about it\" may be related to certain diagnoses and local spread of information. Found data can help act as a very early sentinel to a problem, and can guide us by providing leads to follow-up, but for understanding mechanism, neither tell us how a new virus is going to be dangerous. In other words, while fast and rapid and local, syndromic surveillance and digital disease surveillance remains a reactive endeavour, because we can't mechanistically forecast risk from the data. On the other hand, genomic data is often collected with the intention of understanding pathogen dynamics, as we have well-developed phylogenetic tools for this. However, we still rely on single point mutations, identified in small-scale, one-off, ad-hoc, [insert-some-dashed-phrase-that-expresses-frustration] studies, and basically treat the limited amount of evidence as gospel. I am quite sure that is not the right way to do phenotypic interpretation of genomic data. If we're going to make disease surveillance a proactive endeavour, I am convinced that the disease surveillance world will need a paradigm shift, and investment in infrastructure. By predictive, this is what I mean: 1. Calculating what has a high probability of jumping into humans. 1. Phenotypically testing those viruses without using live viruses (concept of pseudotyping). 1. Using genomic information to predict phenotype. Here's how I see it happening, given everything I've learned at IMED 2016: (1) Sequence everything, in animals that interface with humans 60-70% of new infections are going to be zoonotic in nature. To know what might be coming up, use metagenomics to sequence animal species in geographic areas that humans, prior to the host switch event. To me, this is a no-brainer. Long-term serial sampling is going to be key. (2) Leverage pseudotyping technologies and genetic systems, and automation, to create systematic phenotyping platforms Doing so will allow us to phenotypically test components of a virus for a particular step in its life cycle, such as speed of entry into a cell, or resistance to a drug. With rapid DNA synthesis & assembly technologies, we should be able to experimentally measure a virus' epidemiologically-relevant phenotype within 24 hours of an outbreak. We should also be able to rapidly generate large-scale genotype-phenotype data for machine learning. What would be relevant phenotypes? I can think of a few, broken down basically to \"druggability\" and \"life cycle steps\": 1. For druggable proteins, measure drug resistance. It has high epidemiological relevance, and hence, value. 1. For viruses targetable by drugs and not alike, measure cellular entry capability. 1. For identified polymerase complexes, measure replication capacity. 1. For cellular release machinery, if identified, measure release. 1. If there's others already existing for which the case can be made for systematic phenotyping, scale the assay up. (3) Develop machine learning tools that allow for learning of structural properties that determine epi-relevant protein phenotype By doing so, we can explicitly model epistasis in a protein, and therefore account for epistatic interactions explicitly when developing learning algorithms that map genotype to phenotype. With this kind of technology coupled to the ability to sequence a virus within hours of sample collection, we'll be able to cut down the time for phenotypic interpretation from the order of a day (because of experimentation required) to seconds (because it's a computational prediction). By going Bayesian, we can even model uncertainty in final predictions, thereby aiding decision-making under uncertainty. In conjunction with the \"sequence everything\" approach, systematic phenotyping and deep learning will allow us to move to a \"predictive\" paradigm because it'll give us the ability to rationally predict the phenotypic risk of a virus before it even jumps into humans. I see this as an infrastructural project: requiring up-front commitment of time and money. Yet, we know that investments in infrastructure always pay off dividends in a myriad of ways, from time saved to lives saved. Something I learned from IMED 2016, however, is that epidemiology is an old discipline, and with old disciplines",
    "tags": [],
    "pub_date": "2016-11-06",
    "type": "blog"
  },
  {
    "id": "blog-imed-2016-talks",
    "url": "/blog/2016/11/5/imed-2016-talks/",
    "title": "IMED 2016 - Talks",
    "summary": "Today (Day 1) was the day that talks and posters really started. Hackathon participants missed the first day, in which there was a plenary session...",
    "body": "Today (Day 1) was the day that talks and posters really started. Hackathon participants missed the first day, in which there was a plenary session of \"high-level speakers\". A bit of a pity to miss it, but that's okay - the hackathon was more fun. After delivering the Animal Village hackathon pitch once more in the morning with my teammate Emily Iacobucci, I hung around to learn more about what people in the Emerging Infectious Diseases & Surveillance community are thinking about. I was most interested in what set of tools were being developed, and what problems they solved. Below are my summarized notes. Talks For the talks, I focused mostly on the experimental and data collection tools being developed. Also attended one talk on forecasting new emerging diseases - this is like the holy grail of infectious diseases - can we predict where new outbreaks are going to happen? Also in development are genetic tools to manipulate any virus. Key tools I took note of included: - Number of different viral hosts is a great predictor of spillover into humans. | [paper][sci-rep] - Genetic systems for [pseudotyping][pstype]. | [paper (paywall)][pstype-paper] - Syndromic and/or search surveillance software and participatory community surveillance by patients/lay people ([PODD][podd]/[Flu Near You][fny]) and physicians ([UpToDate][utd]) [sci-rep]: http://www.nature.com/articles/srep14830 [pstype]: https://en.wikipedia.org/wiki/Pseudotyping [pstype-paper]: https://www.ncbi.nlm.nih.gov/pubmed/25936665 [fny]: https://flunearyou.org/ [podd]: http://www.skollglobalthreats.org/tag/podd/ [utd]: http://www.uptodate.com/home Posters For the posters, I mainly caught three posters that dealt with data and modelling; most of the other posters were about some specific outbreak in some specific animal or some specific region. These are all good and such; it's just that my nature inclines me towards generalizable topics: - Validation of NA-Fluor assay for clinical use: most interested because of genotype-phenotype problem I'm tackling. - Data sharing by the [International Diseases Data Observatory][iddo]: an interesting one where I learned that data governance has to be done right to get people to share data. - Modelling of disease spread using airplane travel data, by [EcoHealth Alliance][eca] [iddo]: https://www.iddo.org/ [eca]: http://www.ecohealthalliance.org/ Update from Day 2 The talks that I picked to hear today were centred mostly on data and antimicrobial resistance (AMR). Highlights include the following: 1. European CDC has a network of AMR surveillance efforts that are going on. I think a lot of it has to be organically grown in individual nations, and can't be forced upon. 1. Data sharing ethics: Representatives from the Wellcome Trust, WHO, and European CDC gave an overview of the principles behind best open data sharing practices, issues and challenges currently faced, and ethics. 1. Principles for data sharing: 1. Using data to help communities be prepared for an outbreak. 1. Speed but not at the cost of issues of \"justice\" (e.g. not inadvertently marginalizing groups in interest of speed) 1. Identifying appropriate level of risk that society can bear. 1. Building relationships and confidence. 1. Moving focus away from ownership of data to providing a fair and just system for data recognition. Apart from this, I sat in on Eddy Rubin's talk on the global virome (more like \"stood in\", it was standing room only). I am convinced that taking a \"sequencing everything\" approach is likely going to be the key for predictive surveillance. The next step is whether we'll be able to interpret risk from the genome or not, but that'll be the focus of my next post.",
    "tags": [],
    "pub_date": "2016-11-05",
    "type": "blog"
  },
  {
    "id": "blog-imed-2016-hackathon",
    "url": "/blog/2016/11/4/imed-2016-hackathon/",
    "title": "IMED 2016 - Hackathon",
    "summary": "Yesterday and today, I was at the [IMED 2016 hackathon][imed], jointly organized by [Hacking Medicine at MIT][hackmed] and the [International...",
    "body": "Yesterday and today, I was at the [IMED 2016 hackathon][imed], jointly organized by [Hacking Medicine at MIT][hackmed] and the [International Society for Infectious Disease][isid]. [imed]: http://hackathon.isid.org/ [hackmed]: http://hackingmedicine.mit.edu/ [isid]: http://isid.org/ I arrived late for the hackathon because of a cancelled flight, and as such I had to book another flight at the last minute. Because I arrived late (not sure who to blame still: Turkish Airlines or Webjet), I missed out on the opening portion of the hackathon. I joined a team comprised of: - Emily Iacobucci, WU Vet School - Bonnie Qian, ETH Zurich - Andres Gaviria, Univ. South Carolina - Raphael Dachs, Vienna University of Economics and Business Our hackathon pitch was as such: To create a \"Reddit\" or \"Quora\" or \"Stack Overflow\" for animal workers, much akin to Plant Village. With such a platform, we can help communities help each other with questions about their animals. This also happens to be a great curated knowledge set that can be of great value to governments and veterinary medicine companies. This being my first hackathon, I learned a number of cool things. Firstly, there's a lot of interesting solutions that can be brainstormed out of a hackathon. For example, there was one about measuring the environmental impact of humanitarian aid, in the form of a CO2 calculator; it turned out to be the winning idea. (It also was the idea most closely related to the theme of the hackathon, which was climate change and infectious disease.) Secondly, I learned more about my own work style. Hackathons can be pressurizing scenarios. This can be a polarizing kind of situation to be in; some are not ready for it, others thrive under pressure. I learned something new about myself here - I don't thrive well under pressurizing situations where the goal isn't clear, and it's even harder to bring out my best when I have doubts about the majority's direction. On the flip side, when there's a clear goal forward, that's when my best qualities show up. The hackathon setting also got me thinking about the kinds of problems that are solved here, versus the kinds of problems solved over a longer period of time. A hackathon environment is especially attuned to forcing out new ideas that are mish-mashes of stuff that already exist, but contrary to my pre-conceived notions, it might not necessarily be the right scenario for solving problems. I think what a hackathon does (that is more impactful than solving problems) is bring people together that might not have necessarily worked with one another before. Very rapidly, we'll figure out what types we're capable of working together with, and what type's we're not. In contrast, a PhD-timeframe for solving a problem allows us to go much deeper, at a pace that doesn't force out new solutions, but rather at a pace that allows us to execute a series of \"personal sprints\" that let us move towards a \"truth\" about the world. The time scales are different, and hence the nature of the problem that we end up solving is much different. Anyways, we had a great time hacking together as a team, and I think the hackathon was a great time of bonding as well. Our pitch was one of the pitches that will be shown tomorrow morning, so as I conclude this jet-lagged night, it's time to go to bed so that I can be up in time for tomorrow's conference!",
    "tags": [],
    "pub_date": "2016-11-04",
    "type": "blog"
  },
  {
    "id": "blog-paywalls",
    "url": "/blog/2016/11/1/paywalls/",
    "title": "Paywalls",
    "summary": "What a twist! A paper that I contributed to is behind a Wiley paywall. [](./ecoletters.webp) (click on the image to enlarge it) Not the first time...",
    "body": "What a twist! A paper that I contributed to is behind a Wiley paywall. [](./ecoletters.webp) (click on the image to enlarge it) Not the first time it's happened to me, another paper that I helped out with also ended up behind a paywall - the one we wrote on the effect of mutations on influenza polymerase activity. All I wanted to do for this paper was reference one of the figures inside it. For these reasons, I think it's very, very important to upload pre-print versions of final papers to pre-print servers - BioRxiv, ArXiv, PeerJ pre-prints etc. Particularly, authors, just before submitting back to the journals after peer review, put an unformatted copy up on BioRxiv first. The content matters much, much more than the formatting (even if the formatting helps with reading).",
    "tags": [],
    "pub_date": "2016-11-01",
    "type": "blog"
  },
  {
    "id": "blog-pycon-2017",
    "url": "/blog/2016/10/29/pycon-2017/",
    "title": "PyCon 2017",
    "summary": "Registered for PyCon 2017, and tutorial & talk proposals submitted! PyCon remains my favourite annual conference.",
    "body": "Registered for PyCon 2017, and tutorial & talk proposals submitted! PyCon remains my favourite annual conference. What makes a conference great really is the people, and the PyCon community remains a great one to be at. Attendees generally know that being welcoming is valued higher than showing off technical prowess. The talks and tutorials are fully recorded and available online, meaning that everything presented there is also available for the rest of the world, for free. My tutorial proposal was on teaching network analysis. This will be my 3rd year showing Pythonistas how to use NetworkX as a tool for their modelling problems. My talk proposal was on the use of PyMC3 to do Bayesian statistical modelling. My goal is to show how one doesn't really need to know deep math to wield the tools necessary to do Bayesian statistical analysis, even though I will concede that having a strong grasp of the underlying mathematical theory is very helpful. Hoping to have a blast at next year's PyCon! The timing also happened to be just right, just before MIT's convocation ceremony, and (hopefully) a month after my thesis defence. PyCon 2017, here we come!",
    "tags": [
      "pycon",
      "conferences",
      "python"
    ],
    "pub_date": "2016-10-29",
    "type": "blog"
  },
  {
    "id": "blog-r21-grant-results-attempt-2",
    "url": "/blog/2016/10/28/r21-grant-results-attempt-2/",
    "title": "R21 Grant Results: Attempt 2",
    "summary": "Just got back reviewer comments for the 2nd attempt at an NIH R21 grant that I led the writing for. Reading the comments, I was at happy that the...",
    "body": "Just got back reviewer comments for the 2nd attempt at an NIH R21 grant that I led the writing for. Reading the comments, I was at happy that the reviewers did at least read through how we addressed the first round's comments - you can never tell, I've heard the horror stories of those that don't bother. We had a score of 44 this time round (according to my advisor), compared to our previous score of 60+ (can't remember off the top of my head), so it was a marked improvement. Lower scores are better. I have to admit that I don't know how the scores are calculated, though. When I tabulated up all of the values listed in the proposal, the original submission had a score of 49, and this second submission had a score of 48. Maybe there's some \"weighting\" or \"averaging\" that goes on, and final scores are adjusted based on the consensus of three reviewers, and not just some numerical summation of the three reviewers. Either way, I have to admit, getting grants is hard! We'll likely go ahead and submit again. 3rd time lucky, perhaps? (Or maybe not - I didn't get 3rd time lucky with my paper submissions.) Looking at the reviewer comments, it looks like everything is addressable. What we had proposed was the \"NIH\" version of my Broad Next10 proposals... actually, my BN10s were the \"risky visionary\" version of the NIH R21. I digress. The biggest reviewer concerns were: 1. Lack of clarity on proposed machine learning algorithm. 1. Lack of clarity on advantages of using protein graphs as inputs --> we gain visual interpretability. 1. Phenotype choice might be too complex, could try something more biochemically simpler. 1. Lacking example on how structural information would be interpreted; may need structural biologist as a collaborator. Reflecting on how hard it was to make the changes to the first proposal (mainly because of space constraints), now that we have to add in more content to address the reviewer's comments means we'll have to do more cutting, rewriting and summarizing. The space constraints are the biggest challenge here. Onward!",
    "tags": [],
    "pub_date": "2016-10-28",
    "type": "blog"
  },
  {
    "id": "blog-reinventing-statistical-language-in-science",
    "url": "/blog/2016/10/23/reinventing-statistical-language-in-science/",
    "title": "Reinventing Statistical Language in Science",
    "summary": "How we might make progress communicating statistics better.",
    "body": "There's the [p-value][538] vs. [significance][nature] problem. I think it's widely agreed that education and training have a big role to play in fixing the culture of p-hacking. I think apart from the math-y parts of the problem, scientists are also stuck with a language problem. [538]: http://fivethirtyeight.com/features/statisticians-found-one-thing-they-can-agree-on-its-time-to-stop-misusing-p-values/ [nature]: http://www.nature.com/news/scientific-method-statistical-errors-1.14700 For example, I think you'd find this familiar: a statistically poorly-trained scientist will ask \"whether the data show the hypothesis to be significant or not\". I won't dive into an analysis as to how misleading the phrasing is, but I hope that the error in logical thinking is evident. Data don't decide whether a hypothesis is significant or not. The nature of the scientific question, and future historians' evaluation of the question, define the \"significance\" of the hypothesis being tested. Less poorly informed scientists may ask, \"is there a significant difference between my treatment and control?\" That really depends on the magnitude of the difference, whether it's percentage/fractional/fold difference, or absolute difference, or odds ratios. Note here, p-values don't tell that. I'd point the reader to Allen Downey's [excellent set of material][allen] that introduce this concept. [allen]: https://sites.google.com/site/pyinference/home/scipy-2015 I'd assert here that for a scientist, the language that's used in describing the analyses and interpretations absolutely matter, and that taking away the \"language of p-values\" without providing something as a replacement will only result in the language of p-values returning. Here's my humble attempt at solving this problem, shamelessly borrowing ideas from [other][downey] [heroes][jvdp] in the PyData world. [downey]: http://www.allendowney.com/ [jvdp]: https://jakevdp.github.io/ (1) Instead of describing a \"significant difference\", describe the difference. Replace the following phrasing: our results showed a significant difference between treatment and control (INCORRECT) with: \"the treatment exhibited 220% the activity level of the control\". (CORRECT) (2) Instead of reporting a p-value, report the effect size. Following on from the statement, instead of saying: our results showed that the treatment exhibited 220% the activity level of the control (p < 0.05) (INCORRECT) replace it instead with: our results showed that the treatment exhibited 220% the activity level of the control (cohen's d: 2.5) (CORRECT) (3) Report uncertainty. Let's build on that statement with uncertainty. Instead of: our results showed that the treatment exhibited 220% the activity level of the control (cohen's d: 2.5) (INCOMPLETE) replace it instead with: our results showed that the treatment exhibited (220 \u00b1 37%) the activity level of the control (100 \u00b1 15%) (cohen's d: 2.5) (MORE COMPLETE) (4) Go Bayesian. With a generative model for the data (not necessarily a mechanistic one), uncertainty can be better represented in the figures and in the text. I'll show here just what the text would look like. Here's the old version: our results showed that the treatment exhibited (220 \u00b137%) the activity level of the control (100 \u00b1 15%) (cohen's d: 2.5) (GOOD) And here's the better version: our results showed that the treatment exhibited 220% (95% HPD: (183, 257)) the activity level of the control (100%, 95% HPD: (85, 115)) (cohen's d: 2.5) (BETTER) A quick thought on this final phrasing: The phrasing is inflexible for reporting distributions that may be more skewed than others, and assumes that the posterior distribution of the parameter is symmetrical.This is why I'd prefer reporting the 95% HPD, which if I'm not mistaken, is equivalent to the 95% confidence intervals when making certain prior assumptions in the Bayesian framework. Update 24 October 2016: Just after making this post live, I saw a [very helpful set of material][jbois] by Justin Bois on how to decide which error bars to report. The errorbars don't respect the bounds of the posterior distribution. [jbois]: http://bebi103.caltech.edu/2016/tutorials/t5acredibleregions.html If accompanied by figures that report the estimated posterior distribution of values, we can properly visualize whether two posterior distributions overlap, touch, or are separated. That should provide the \"statistical clue\" about whether the results can differ from \"statistical noise\" or not. (Note: it still doesn't tell us whether the results are \"significant\" or not!) What do you think about the need for language change?",
    "tags": [
      "statistics",
      "data science"
    ],
    "pub_date": "2016-10-23",
    "type": "blog"
  },
  {
    "id": "blog-3d-printing",
    "url": "/blog/2016/10/18/3d-printing/",
    "title": "3D Printing!",
    "summary": "Who would have thought that making stuff could be so fun? (Particularly when the tediousness of actually building it is removed!) I think that the...",
    "body": "Who would have thought that making stuff could be so fun? (Particularly when the tediousness of actually building it is removed!) I think that the 3D printer I just bought for the lab might really open up new doors for us. We have the MakerBot Replicator 2. MakerBot is one of the leading brands on the market, according to what I've read online. Their printers tend to be on the expensive side, costing upwards of $2000 for a good model. I was lucky that there was a store demo model on sale at Micro Center nearby campus, so I took a ride over and splurged $600 on it. I did previously have an M3D Micro for the lab, but that was awfully slow, the heater element was unreliable (resulting in poor prints), the quality of the model filling wasn't as good, and the build volumes were really small. Even though it was touted as a PLA printer, the prints also kept warping. I eventually returned it. MakerBot, by contrast, was fast, had a reliable heater element, had very strong high quality and high resolution prints, and could print much taller objects. So why did I splurge lab money on a printer? Well, there's a few reasons. Firstly, I've been inspired by the maker movement. I'm also thinking about how I could start my own research group or company. Given the spiralling costs of how to do research, mastering 3D printing now could pay off dividends later, as my group could custom 3D print stuff as we needed it, potentially saving time and money down the road. It'd also give us the freedom to think about what equipment we really need, and then just go ahead and make it. Secondly, one side project I did just for fun was to build a Raspberry Pi-based remote time-lapse camera to spy on seals on Monomoy and Nantucket (see my research). (The larger context is a \"seal infectious disease\" project led by my advisor Jon.) I worked on this during December 2015 to January 2016, sort of as a distraction while I was waiting for my paper's reviews to come back. As things turned out, my hacked together Tiki cams (as we called them) didn't perform too shabbily, and we could capture, with high resolution over a long period of time (8+ hours), the qualitative contact structure of the seals. But my hack was fragile and inflexible, and I wanted to print better housings for the case for this year's excursion. Thirdly, ever since the Ebola 2014 outbreak, and now with the current Zika outbreak, I've been reflecting on how to get labs up and running cheaply in resource-poor regions, thereby bringing molecular diagnostics to regions that don't have access to it. DIY-bio PCR machines, gel boxes, and RT-PCR kits are one avenue. I think 3D-printing has a role to play as well. Cheap PLA can be brought over with the 3D printer, and within a matter of minutes to hours, parts that are needed may be printed easily, say, extra tube racks where needed. I'd argue that there's a human side to this as well: objects carry meaning to people, and for those affected by the outbreak, a printer may provide a humanitarian role by allowing a response team to also print designs that are meaningful to the affected people group, perhaps bringing an added element of trust, comfort, and resilience to the population. (Of course, none of this replaces the human acts of empathy and self-sacrifice.) Finally, I really, really, really wanted to start playing with 3D printing. (Cue the \\sheepish look\\ here.) But we all know that's a quarternary reason, right, right? In my quest to figure out 3D printing, I've done some test objects. The MakerBot had a \"nut and bolt\" design that was my first print on the machine. I then used Tinkercad to design a single 50 mL tube holder, and I'm fiddling with designs that'll allow any number of these guys to be chained together. To commemorate, the purchase of the printer, I've also printed custom keyrings for each of my labmates. To explore the concept of \"appropriate tolerances\" for objects and the like, and I've printed out a custom business card holder for myself too. My colleague Kim also wants a custom 96-well plate holder for a magnetic bead system that she uses for sequencing, so I'll give that a shot. Later on, once I've finally mastered how to do the design work properly, I'll probably start designing the battery and Raspberry Pi cases for the TikiCam Version 2.",
    "tags": [
      "hobbies",
      "3d printing"
    ],
    "pub_date": "2016-10-18",
    "type": "blog"
  },
  {
    "id": "blog-low-cost-robotic-liquid-handlers",
    "url": "/blog/2016/10/16/low-cost-robotic-liquid-handlers/",
    "title": "Low-Cost Robotic Liquid Handlers!",
    "summary": "Just found out about [OpenTrons][opentrons], which is a company that produces open source robotic liquid handlers for less than $5000.",
    "body": "Just found out about [OpenTrons][opentrons], which is a company that produces open source robotic liquid handlers for less than $5000. In terms of research budgets, this is amazing! While some may contend that it may sometimes be faster for a bench scientist to conduct the experiments, I would respond by saying that freeing up the bench scientist from the nitty gritty work of moving small amounts of liquids from one container to another would allow them to focus on higher level thinking, which is what we're trained for. Definitely, conducting the experiments by hand is necessary while establishing a protocol, but once it's settled, having the protocol infinitely replicable by a robot can really free up a ton of time, and more importantly, free up mental energy. Other good stuff about the robot: - It's open source. Hardware + software. This means everything about the hardware and software is introspectable. - It's hackable. The hardware is based on a Raspberry Pi computer, which means I can, as needed, upgrade the Pi, the machine parts, or the software. [opentrons]: https://opentrons.com/robots",
    "tags": [],
    "pub_date": "2016-10-16",
    "type": "blog"
  },
  {
    "id": "blog-github-lab-notebooks-update",
    "url": "/blog/2016/10/12/github-lab-notebooks-update/",
    "title": "GitHub Lab Notebooks: Update",
    "summary": "It's been about 2 months since starting up with the GitHub lab notebooks for a project that I'm coordinating. Here's some thoughts on it.",
    "body": "It's been about 2 months since starting up with the GitHub lab notebooks for a project that I'm coordinating. Here's some thoughts on it. Firstly, version control systems are really powerful as a backup system! Already more than once I've been able to salvage work that was accidentally over-written. Secondly, GitHub offers some pretty neat settings that ensure the integrity of a lab notebook and provide for peer review of lab experimental reports. I have my GitHub repo's master branch on lockdown, meaning all new changes to it have to be pulled in from other forks, and the submitter cannot be the reviewer that approves it. With my UROP student now added as a collaborator on the repository, basically we have to approve each other's work before being allowed to merge into master, and can add requests for minor additions that will make the lab experimental report easier to follow. It's akin to signing off on a lab notebook, basically, meaning that the person responsible for the experiment and the person responsible for checking the experiment are both provisioned and tracked. Thirdly, we've already used the fact that the lab notebook is completely public and online, and therefore completely available 24/7, to check for mistakes made in previous reports or as-of-yet non-replicable results. Mistakes will always be made, and some results will turn out to be non-replicable, so it's okay to make them once or twice (but not repeatedly, of course); the more important part is that it's publicly recorded, and discussed either in the merging portion or as an issue. Because everything is digitally recorded, and everything is also distributed, in the event that, say, GitHub goes down, there's at least multiple backups on local computers and local computer backups to restore snapshots of the work. (We will lose all of the commenting history, of course, if GH goes down.) Because our lab reports use Markdown files with descriptive file names, it's actually possible to write scripts that automatically produce indexes of all experiments conducted for easier browsing. I'm looking forward to seeing what else can be built on top of the file system with version control.",
    "tags": [],
    "pub_date": "2016-10-12",
    "type": "blog"
  },
  {
    "id": "blog-collaborations-between-experimentalists-and-statisticians",
    "url": "/blog/2016/10/11/collaborations-between-experimentalists-and-statisticians/",
    "title": "Collaborations Between Experimentalists and Statisticians",
    "summary": "I just saw an awesome pair of blog posts by Roger Peng and Elizabeth Matsui, detailing how scientists and statisticians can work together more...",
    "body": "I just saw an awesome pair of blog posts by Roger Peng and Elizabeth Matsui, detailing how scientists and statisticians can work together more effectively. Highly recommended read! In the duration of my PhD, I've been one of two resident statisticians for our research group (it's a relative expertise thing - I know enough to be dangerous), and so the article really resonated with me. Here's my summary. Firstly, both ought to take the time to learn each others' core concepts and methods. For the experimentalist, learn some basics about distributions of a random variable and what 'inference' really means beyond p=0.05 (FYI, p<0.05 \u2260 inference). For the statistician, take the time to learn some of the basic (bio)chemistry, the experimental setup, and how difficult it is to collect data. Secondly, have some patience! Both sides will need some time to learn the ins-and-outs of the data collection, analysis, inference reasoning process and the likes. Thirdly, involve the statistician earlier on in a collaboration, not towards the end. The statistician can help make sense of the scientific question in terms of the data collected, and thus can help with the experimental design. Fourthly, and this point really matters for PIs, give the statistician sufficient support, particularly on the grants. 1% effort doesn't cut it. Even 5% is on the low side of reasonable. Something more like 10-20% effort makes more sense. Finally, a statistician's primary goal should be to advance the science through the proper deployment of statistics. A scientist's primary goal is to help identify the most important problems of a (sub-(sub-))field to solve. Together as collaborators, they can find the right experimental design towards this pair of goals. I'll add in a pointer of my own here - as a statistician, it really shouldn't be about helping your scientist collaborator find \"what's significant\", which (ahem) often means p<0.05. My view of inference has, over the past year, turned Bayesian, and I'm now increasingly confident in the notion that Bayesian-style reasoning is not only the most natural way to reason through the data, it's actually the right way to do inference on scientific data. Given priors, some perhaps uninformative, about the state of nature, our goal in science is to compute the probability distribution over some parameters of nature.",
    "tags": [],
    "pub_date": "2016-10-11",
    "type": "blog"
  },
  {
    "id": "blog-reproducible-pi-manifesto",
    "url": "/blog/2016/10/9/reproducible-pi-manifesto/",
    "title": "Reproducible PI Manifesto",
    "summary": "I found this set of slides by Lorena Barba: the Reproducible PI Manifesto. I love it, and I hope other PIs take it up!",
    "body": "I found this set of slides by Lorena Barba: the Reproducible PI Manifesto. I love it, and I hope other PIs take it up!",
    "tags": [
      "reproducibility",
      "data science"
    ],
    "pub_date": "2016-10-09",
    "type": "blog"
  },
  {
    "id": "blog-conference-finaid-packages",
    "url": "/blog/2016/10/8/conference-finaid-packages/",
    "title": "Conference FinAid Packages",
    "summary": "I've been thinking back on my experience being on the SciPy 2016 Financial Aid committee, having myself received a full scholarship to attend SciPy...",
    "body": "I've been thinking back on my experience being on the SciPy 2016 Financial Aid committee, having myself received a full scholarship to attend SciPy 2015. I had a chance to talk with two of the SciPy scholarship recipients. They belonged to a trio that we really wanted to fund, but because of budget constraints and a desire to help more excellent applicants attend the conference, we had to drop one (the more experienced guy) from the list. It still gets to me sometimes, having to be part of the decision-makers that makes these \"constrained decisions\". I can't imagine, for example, making life-and-death decisions instead. I later learned that the trio contacted their hosting organization, and using some leftover money that the other two were entitled to + some other leftover funds elsewhere, they were able to help the experienced guy come in to the conference. I'm not really good with words for emotions, so the only word that most accurately describes my feelings towards this is \"wow\". Relative to what I observe in modern North America, where I keep hearing about the constant fights for rights, this was a huge dose of self-sacrifice instead, and I was really moved. On another note, I also learned that SciPy has one of the most generous financial aid packages available in terms of total dollars awarded. I hope this can continue; having money to disburse to purposefully bring people together is an investment in people that will pay off many fold non-monetary dividends elsewhere.",
    "tags": [],
    "pub_date": "2016-10-08",
    "type": "blog"
  },
  {
    "id": "blog-boxplot-or-violin-plot",
    "url": "/blog/2016/9/27/boxplot-or-violin-plot/",
    "title": "Boxplot or Violin Plot?",
    "summary": "Which would you choose? Let's figure out which might be more useful.",
    "body": "The same data, different visualizations. Here's the box plot version: And here's the violin plot version. Which is more appropriate, and why? I think the Box Plot is more suited for clearly delineating ranges: (min, max), (IQR), and medians, whereas the Violin Plot is more suited for visualizing the distribution of data, including the possibility of bi-/multi-modality.",
    "tags": [
      "data visualization",
      "data science"
    ],
    "pub_date": "2016-09-27",
    "type": "blog"
  },
  {
    "id": "blog-paper-review",
    "url": "/blog/2016/9/22/paper-review/",
    "title": "Paper Review",
    "summary": "I recently reviewed a paper for PLOS Computational Biology. This was my first ever scientific paper review, and so I did my best to be constructive...",
    "body": "I recently reviewed a paper for PLOS Computational Biology. This was my first ever scientific paper review, and so I did my best to be constructive and helpful to the authors, just as other reviewers were for my first lead author publication. I had seen stinging reviews of my colleague's work, those that made it look like the reviewer had an agenda and didn't bother reading the paper properly. Hence, I was determined to have this review not just objectively, but also constructively. I also signed off on the review, having been convinced of the need for a more transparent review system. I'm not sure how much my single contribution will make, but I think it's still the right thing to do. Because of confidentiality reasons, I can't discuss what the specific topic of the paper was, who the authors were, and what affiliation they had. However, I think I am allowed to describe (in broad general terms) some of the more prominent episodes of thought that went through my mind as I did the review. (If the editor contacts me at a later time and says this post isn't allowed either, I'll be the first to take it private into my own reflections...) Essentially the manuscript was about a new algorithm as applied to biology. As I was writing the review, I was constantly reminded of some mistakes I had made early on in my PhD training w.r.t. computational work. The first was the need for simulation data, which was not provided. Another one was to avoid claims of computational efficiency unless I had conducted a formal analysis of my algorithm's order of complexity. The authors had made both of these mistakes, and I hope that my review pointed it out in a way that didn't \"sting\" (I've seen those, they hurt, my colleagues have been victims of those reviews before.) One of PLOS Computational Biology's requirements for their journal article is that a new software method yields insight into some biological problem. My evaluation of the paper was that it wasn't clear what the biological problem was, and I thought it might help the authors by suggesting a few questions that I thought could be answered by their algorithm. As it turns out, the 2nd reviewer also concurred. The 2nd reviewer had a sharper eye for statistical issues than I did. Looking back, I was clearly more focused on the algorithm and its utility; I had swept aside statistical issues, partly because of a lack of confidence. Thankfully the 2nd reviewer addressed the question of whether there was statistical evidence for confidence in the conclusions. Just from reading the reviewer's comments, I think I've learned more about how to approach the stats question. I was sad, though, that the editors ultimately decided on a rejection; I had only requested for a major revision, and had tried my best to specify what would be necessary and what would be \"good-to-have\". The authors' topic is close to my heart, and I genuinely enjoyed seeing that there's a competing algorithm that worked faster than my own. Looking through Reviewer 2's comments, it must be the case that Reviewer 2 (who remained anonymous) recommended a rejection. I hope the authors find a good place to put their paper; the rejection always will sting, but I hope that at least I was able to help them make their work better.",
    "tags": [],
    "pub_date": "2016-09-22",
    "type": "blog"
  },
  {
    "id": "blog-why-uncertainty-matters",
    "url": "/blog/2016/9/16/why-uncertainty-matters/",
    "title": "Why Uncertainty Matters",
    "summary": "Uncertainty is part and parcel of the process of doing science; in effect it's a currency of science. What scientists do is essentially...",
    "body": "Uncertainty is part and parcel of the process of doing science; in effect it's a currency of science. What scientists do is essentially quantitatively measure or qualitatively describe some property of the natural, physical world. Because of the limitations of our instrumentation, uncertainty in our measurements will crop up. Quantified uncertainty is especially important, because we can use it to identify which parts of the world we need to measure more, if the data are not useful enough for something practical. For example, does the uncertainty surrounding a predicted drug resistance value encompass a clinically-relevant cutoff (the cut-off problem)? If it does, perhaps we need to measure again to gain more precision in our estimated drug resistance value. Machine learning models are becoming important in science, but uncertainty isn't explicitly incorporated in most of the off-the-shelf tools that are available (e.g. ). This is a limitation of the current state-of-the-art tools, but is by no means a reason not to use them. That said, without uncertainty being modelled and propagated from measurement to data to prediction, we run into the \"cut-off\" problem with predicted values - what if a prediction had a large uncertainty around it, and encompassed an actionable cut-off value? And what if the action taken were incorrect on the basis of the false certainty surrounding a prediction? If we don't take the \"black-box\" ML approach, and instead decide to compose our own models that try to explain the world, the model prediction \u00b1 uncertainty may not overlap with our measurement \u00b1 uncertainty. This tells us that our model has missing parts that cannot explain the data fully, and the model needs to be extended. And so I think uncertainty matters. It matters because it tells us where we need more data. It matters because uncertainty can help inform whether or not we can be confident in taking action given the data. It matters because it can help tell us whether our models of the world are good enough at explaining the data collected. For these reasons, and many more, I think uncertainty really matters.",
    "tags": [
      "statistics",
      "bayesian",
      "data science"
    ],
    "pub_date": "2016-09-16",
    "type": "blog"
  },
  {
    "id": "blog-scientists-eliminate-link-rot",
    "url": "/blog/2016/9/8/scientists-eliminate-link-rot/",
    "title": "Scientists: Eliminate Link Rot!",
    "summary": "Reading up on Bayesian methods for analysis of high throughput measurement data, I saw this very promising paper by Johnson & Li.",
    "body": "Reading up on Bayesian methods for analysis of high throughput measurement data, I saw this very promising paper by Johnson & Li. In the abstract, there was a link. It looked awesome! There's software that accompanies this paper - I'm going to be able to try it out! I entered the URL into Chrome, and got a DNS error. The link was not active. Bummer. Scientists, it's time to archive your software properly. Put it on a digital repository that has a higher probability of being maintained long-term, and not on individual lab websites. Link rot is an issue that we have to deal with in the scientific literature. (Granted, Johnson & Li published this in 2007, in which the issue of storing data and software on repositories was not widely recognized. I think the reader should be forgiving in this case.) There's a ton of places we can put our work: - Figshare, - Zenodo, - DSpace, - institutional repositories. I did it with my own papers, and I'm encouraging my colleagues to do so as well. I'd encourage you to do so too.",
    "tags": [],
    "pub_date": "2016-09-08",
    "type": "blog"
  },
  {
    "id": "blog-hack-a-gel-imager",
    "url": "/blog/2016/9/6/hack-a-gel-imager/",
    "title": "Hack a Gel Imaging Enclosure",
    "summary": "Last week, following a power outage, the expensive gel doc machine & its computer failed. (It's also one floor up.) And so, we decided to just take...",
    "body": "Last week, following a power outage, the expensive gel doc machine & its computer failed. (It's also one floor up.) And so, we decided to just take a photo with our phones - and it turned out just fine. So, as things turn out, we don't need a $3000 gel imager to get an image of the gels. But if we think carefully about what a gel imager really is, it's a light box on the bottom, a camera on the top, and a box that shrouds the lighting to make imaging easier. If we have a computer that can send the images anywhere, even better. Guess what? All of that can be hacked together for less than $100. Feeling inspired by Google Cardboard, I took some cardboard boxes, and cut out five 24 cm2 square pieces out of old cardboard boxes, and taped them together in a \"+\" configuration. I then taped them together to form a 24 cm3 illuminator cover. On the middle square piece, I cut out a small square hole, big enough for an iPhone 6+ camera to peer into. This turns out to be about just the right size for the transilluminator that we have, with the right amount of elevation for the phone to capture the entire illuminator at one shot. Here's a few images of how it looks. And for those who are curious what the final, cropped gel image looks like: Bill of Materials: 1. Smart phone: An old Moto E with 5 megapixels rear camera should suffice. $70. 1. Cardboard box: Free; price may be higher if actually welded/laser-cut/3D-printed instead. Some thoughts: I believe in taking a hacker's approach to science. New labs, start-up companies, and labs in resource-poor regions need these alternatives to the \"premium\" products that are being sold in developed nations. The more we can use cheap, off-the-shelf components to do our science, the better.",
    "tags": [],
    "pub_date": "2016-09-06",
    "type": "blog"
  },
  {
    "id": "blog-github-lab-notebooks",
    "url": "/blog/2016/9/6/github-lab-notebooks/",
    "title": "GitHub Lab Notebooks",
    "summary": "A new UROP, Vivian Zhong, has joined the Runstadler lab! She is working with Islam and I on a project that is directly related to the broader...",
    "body": "A new UROP, Vivian Zhong, has joined the Runstadler lab! She is working with Islam and I on a project that is directly related to the broader problem of genotype-phenotype. For this project, I decided to try and experiment with GitHub and Markdown files as our lab notebook. Our repository is here. The reasoning is as such: 1. Open science! The GitHub repository is a publicly available repository. This was a design choice. We're holding ourselves to the standard that science funded by the public should be accessible to the public. Scooping? A previous advisor of mine told me to never be worried about being scooped, because the space of interesting problems is always larger than what we can see. Plus, we claim priority with the time-stamped changes recorded on GH. 1. Full record keeping. All history of changes committed into the repository are stored. I can recall each and every contribution that Vivian has made, which will make things much easier down the road if and when this gets written up as a paper. 1. Collaboration. Because our lab notebook is digital, everybody involved in the project can contribute to the lab notebook. I can keep track of progress remotely, and answer/raise questions whenever they come up. 1. Review. A nice feature on GitHub is that I can add line comments, meaning I can specifically pinpoint where I think there may be issues in record keeping or experimental design. The Pull Request model for working really helps here, because as a gatekeeper, I can maintain a specified standard for record keeping. 1. Flexibility & Search. We essentially use the file system as a database, meaning we're not limited by the linear nature of a traditional lab notebook. This means we can organize the notebook more logically, and search for whatever we need inside the repository. What are some of the downsides that I'm anticipating? 1. As with all digital lab notebooks, there's a gap between doing the experiment and recording it. Nothing really beats pen & paper for this. 1. Git allows for changing of history (e.g. squashing commits, editing messages). This isn't the most ideal, but at least it's not possible to change the content of changes. 1. It's a good thing we're not working with patient data. I'm quite sure HIPAA rules would prevent us from operating a publicly available lab notebook. We'll report back on how it goes at the end of the academic year!",
    "tags": [],
    "pub_date": "2016-09-06",
    "type": "blog"
  },
  {
    "id": "blog-precision-and-not-hypothesis-rejectionaccpetance",
    "url": "/blog/2016/8/18/precision-and-not-hypothesis-rejectionaccpetance/",
    "title": "Precision, and not Hypothesis Rejection/Acceptance!",
    "summary": "I've recently been considering the problem of when to stop measuring something, especially in the biological sciences.",
    "body": "I've recently been considering the problem of when to stop measuring something, especially in the biological sciences. Turns out, John Kruschke has given this a ton of thought, in the psychological sciences. I listened to a talk, freely available on YouTube, about how to decide when to stop collecting experimental data. It's a very good talk, in which Kruschke states why precision should be the goal of measurement, and not acceptance or rejection of some hypothesis. The main problems with stopping data collection when some rejection or acceptance criteria has been fulfilled is that the reported measured parameter estimates will have a tendency to be biased away from some true value. Yet, \"collecting data till I get some p < 0.05\" pretty much sums up the mentality of a large swathe of experimental biologists. I think the more rational way to approach scientific measurement is to treat it the way that the physicists do. They're out to measure some property of nature, in an \"absolute\" sense, and then quantify the uncertainty surrounding that measurement. For biologists, oftentimes, we're out to measure some property relative to some control, and so the uncertainty surrounding the computed difference should be calculated. Kruschke advocates for stopping measurements when we reach a precision that is better than some Region Of Practical Equivalence (ROPE, he's good with acronyms). There's no free lunch here - we have to define ahead of time what the width of that region looks like. He also acknowledges that there are scenarios where the best precision of our measurements, or the natural variation/noise in the population, preclude the collected data from converging on the desired precision. Even then, the data collected are valuable: it's informative about the best precision that we can achieve. The focus on precision here is because, given an agnostic state towards nature, true parameter values cannot be known, and can at best only be estimated through measurement. Naturally, scientists should be interested in accuracy, and not just precision, and I think continued, repeated measurements are our best bet at getting there, assuming our measurements are set up right. A final thought: I'm a proponent of getting rid of publishing only \"significant\" (statistically or otherwise) studies, and I agree with Kruschke that scientists should be publishing studies that pose an unexplored and well-motivated question and report uncertainty in whatever their measurements are. Evaluating whether something is \"well-motivated\" is a tough thing to do, and it's difficult to use easy proxies to evaluate motivation, but I think it's the part that makes scientific inquiry interesting.",
    "tags": [],
    "pub_date": "2016-08-18",
    "type": "blog"
  },
  {
    "id": "blog-bayesian-estimation-greater-t-test",
    "url": "/blog/2016/8/18/bayesian-estimation-greater-t-test/",
    "title": "Bayesian Estimation > t-Test",
    "summary": "Recently found a very good article on the ArXiv about Bayesian estimation for difference in sample means.",
    "body": "Recently found a very good article on the ArXiv about Bayesian estimation for difference in sample means. Loved the fact that someone's done the work so that I don't have to! The paper essentially explores how a Bayesian estimation (BEST) is essentially much, much better than traditional Null Hypothesis Significance Testing (NHST), and the author provides a good number of convincing arguments for this. The author basically covers the most commonly-used case in the experimental sciences - inter-comparison between an experimental and control group. I'd recommend reading the paper (link). While skimming through it, I then found a PyMC3 recipe, as usual, provided by the awesome Thomas Wiecki and Chris Fonnesbeck. It makes me wonder... given how Bayesian inference is much more natural than Frequentist inference, why isn't this used more often?",
    "tags": [],
    "pub_date": "2016-08-18",
    "type": "blog"
  },
  {
    "id": "blog-the-problem-of-too-many-splits",
    "url": "/blog/2016/8/16/the-problem-of-too-many-splits/",
    "title": "The problem of too many splits?",
    "summary": "I recently followed an interesting Twitter conversation, in which one tweet struck me as surprising: @jakevdp @datahope It&#39;s not too late to use...",
    "body": "I recently followed an interesting Twitter conversation, in which one tweet struck me as surprising: @jakevdp @datahope It&#39;s not too late to use and improve common tools.Too many splits: Py2/Py3, pip/conda, CPython/pypy/jython/pyston&mdash; Gael Varoquaux (@GaelVaroquaux) August 16, 2016 (I'd recommend following the full Twitter conversation in order to get a full view of Gael Varoquaux's opinions; don't vilify or exalt a person on the basis of one tweet.) If I understand Gael's opinion correctly (and I'm open to being corrected here), he must be of the opinion that having a multitude of splits is confusing for the community, new users, and the likes. He must also be of the opinion that tools really should be consolidated such that there is, according to the Zen of Python, \"one and preferably only one way of doing things\". If those are his sentiments, I agree with the former, but not necessarily the latter. I'll outline why, but before I go on, let's quickly review what are all the options available to Pythonistas. Language + Interpreter CPython is the de-facto Python implementation that everybody loves (e.g. the implementation is ported to all major platforms) and hates (the GIL!). But PyPy exists, with its JIT compilation experiment, and then Jython, IronPython, and a multitude of smaller experimental Python implementations. And then there's Cython, which isn't really Python and isn't really C but a mish-mash of the two, to complicate things further! So the base interpreter and language (I've mixed the two here) seem to be suffering from confusing splits. Which do we use? Packaging Then there's and . installs stuff from PyPI, which is currently an open publishing platform for Python packages, while installs stuff from Anaconda cloud; conda-forge, which hosts many PyPI packages that are also built into packages, is currently a curated set, and is currently used more as a package redistributor to the three major platforms. only installs Python stuff; installs any binary, and is language- and platform-agnostic, and is very widely used by the PyData sub-community of Pythonistas. used to have problems with packages like and ; now it's less of an issue with the Wheel packaging system. admittedly can get confusing sometimes; I worked on for a bit at the PyCon 2016 sprints, and installed , Javascript's package manager. So I have, right now, a JS package manager inside the package manager which plays nicely with the Python-only package manager... Not to mention, if you're on Ubuntu, there's for package installation that is linux-specific, but cross-language. If you're on a Mac, then it's that is the equivalent platform-specific, language-agnostic tool. Data Visualization Ah, another confusing. There's the venerable . And then there's , , , , ... Oh my! Which do we go with?! Web Development Let's not get started on this one. Flask and Django; we have a running joke at Boston Python that Flask and Django developers shouldn't sit together at Project Nights! Py2/3 And then, of course, there's Python 2 and Python 3, one split that really created a big split in the community; there are some Python folks who see no need to go to Python 3 unless absolutely forced to (Armin Ronacher, I think, is one), and then there are those who fully advocate for moving on (Jake Vanderplas, for example, and myself). So, how do we go forward? Conda + Py3 + CPython? + Py2 + PyPy? Permit C-extensions? I won't claim to know what's the best, though I will state my preference for + Py3 + PyPy (oh, if only it could be this way!). Put more generally, \"whatever works, but while also being setup to evolve with a community\". As an example, I'm tooled up for working with the PyData crowd ( + Py3 + CPython), and I trust that the general direction of the PyData crowd will more or less be concordant with the broader Python community, so I don't worry so much about incompatibilities with the broader Python community. Moreover, I often use some web development tools for toy projects, and so I end up using anyways, and is engineered to play nicely with , so I'm happy there too. But why do I disagree with Gael that the state of things ought to be consolidated? I disagree because I see the splits as being indicative of different groups of people trying to actively solve a diverse multitude of problems. In other words, I see this diversification of tools as a suite of hypotheses that are being tested. Sure, works well, and is being actively developed, but PyData folks sometimes need to use tools from other languages because none currently exist in Python, and so is testing the hypothesis that Python can be used to make a better cross-language and cross-platform package manager. Sure, CPython is the most stable and performant interpreter, but PyPy (and other JIT interpreters) are testing the hypothesis that we can make a stable, language-compliant interpreter without all of the type checking overhead of the CPython implementation. In fact, I think I remem",
    "tags": [
      "python",
      "data science"
    ],
    "pub_date": "2016-08-16",
    "type": "blog"
  },
  {
    "id": "blog-variational-inference-with-pymc3-a-lightweight-demo",
    "url": "/blog/2016/8/6/variational-inference-with-pymc3-a-lightweight-demo/",
    "title": "Variational Inference With PyMC3: A Lightweight Demo",
    "summary": "Following on the heels of me recently attending Thomas Wiecki's Boston Bayesians talk, I decided to put up a notebook showing a lightweight example...",
    "body": "Following on the heels of me recently attending Thomas Wiecki's Boston Bayesians talk, I decided to put up a notebook showing a lightweight example of how to do Bayesian modelling with PyMC3. Comments are welcome! The notebook can be found here. Thomas Wiecki's blog post on Bayesian Deep Learning, which served as the original inspiration for my notebook example, can be found here. Happy reading!",
    "tags": [
      "python",
      "bayesian",
      "variational inference",
      "pymc3",
      "statistics",
      "data science"
    ],
    "pub_date": "2016-08-06",
    "type": "blog"
  },
  {
    "id": "blog-being-able-to-learn-anything-is-an-important-skill",
    "url": "/blog/2016/7/28/being-able-to-learn-anything-is-an-important-skill/",
    "title": "Being able to learn anything is an important skill",
    "summary": "Graduate school has taught me the importance of being able to learn almost anything, and to learn that thing as quickly as possible.",
    "body": "Graduate school has taught me the importance of being able to learn almost anything, and to learn that thing as quickly as possible. I think that's an important skill to have in life. (Ironically, it's taken me much of 5 years, now going on 6, to learn this point.) Forest White, a professor in the department, once counselled me with this statement (paraphrased): That's grad school for you! You come in thinking you'll leverage that one skill that you have, and you leave grad school with an entirely different skill set mastered.\"",
    "tags": [],
    "pub_date": "2016-07-28",
    "type": "blog"
  },
  {
    "id": "blog-notetaking-and-writing-using-atom",
    "url": "/blog/2016/7/27/notetaking-and-writing-using-atom/",
    "title": "Notetaking and writing using Atom",
    "summary": "I recently have been trying out the Atom text editor for writing. While its load times are slower than Sublime Text, it overall is a solid text...",
    "body": "I recently have been trying out the Atom text editor for writing. While its load times are slower than Sublime Text, it overall is a solid text editor. Being free doesn't hurt it either, and being open source and hackable means that (theoretically) I can also extend it the way Sublime Text is extendable too. Today, I focused on making progress with the Genomic Surveillance whitepaper I'm writing. (Back story - I'm writing this whitepaper to help consolidate ideas and in planning for next career steps.) In particular, I thought of test-driving a system that I cooked up in my head for powering through a section of text. This system looks as such: 1. I determine what information needs to be conveyed. 1. I spend a few hours creating a bullet-point summaries of each paper, but only w.r.t. the necessary information that needs to be conveyed. 1. I spend another few hours crafting the prose version of the text. 1. I then build the final PDF as necessary. The key difference in this experiment compared to my previous attempts is that I am now choosing to write a single section in a purposeful fashion, meaning that my reading is also directed towards a purpose and not merely for the sake of broadening my scope of knowledge (that requires a different type of reading strategy, IMO). On reflection, I think this helps me waste less time reading in-depth things that aren't actually going to be helpful. That skill of strategically and selectively identifying relevant literature to read is something we pick up during graduate school. Anyways, back to the writing process. I first wrote the bullet points on their own. [](./notetaking.webp) A few hours later, once I was done with that, I switched to a two-pane view, and kept the notes on the right and prose on the left. [](./writing.webp) A few hours later, I had a draft version written! As a side benefit, Atom's interface was quite gorgeous. I'm using the default theme, and the default colour scheme easily highlights citations, headers, bullet points/numbers and bold/italicized text. I also totally dig the gutter highlighting that shows me where things have been changed. This is definitely one of Atom's current strengths over Sublime Text. ST currently allows users or packages to set syntax and background colour together, but it's never really been obvious to me how to change font colours based on markdown status. The other benefit is that the file explorer on the left hand side has icons, which really helps navigation.",
    "tags": [],
    "pub_date": "2016-07-27",
    "type": "blog"
  },
  {
    "id": "blog-sparse-matrix-multiplication-in-python-3",
    "url": "/blog/2016/7/24/sparse-matrix-multiplication-in-python-3/",
    "title": "Sparse Matrix Multiplication in Python 3",
    "summary": "Sparse matrix multiplication shows up in many places, and in Python, it's often handy to use a sparse matrix representation for memory purposes.",
    "body": "Sparse matrix multiplication shows up in many places, and in Python, it's often handy to use a sparse matrix representation for memory purposes. One thing nice about the newest version of Python 3 is the operator, which takes two matrices and multiplies them. While has had the function for a while, I think can be a more expressive way of expressing the matrix multiplication operation. One hidden benefit of the operator: it handles matrices pretty well! Consider the following binary matrix in a dense format: We can convert that matrix to a sparse format: Let's say now that we want to multiply it against a random matrix. Let's first instantiate the random matrix: Cool! So, starting with the operation... Because doesn't natively recognize matrices, when we try to do the matrix multiplication, we get... Totally not what we wanted! On the other hand, if we used the operator... Exactly what we wanted! The reverse multiplication, in which we take a matrix and try to multiply it with a matrix, should fail: With the matrix being multiplied in , however, it doesn't work out to the correct answer... On the other hand, the matrix multiplier catches the error perfectly! I hope I've provided one more piece of evidence in favour of switching to Python 3!",
    "tags": [
      "data science",
      "python",
      "sparse matrix",
      "linear algebra"
    ],
    "pub_date": "2016-07-24",
    "type": "blog"
  },
  {
    "id": "blog-jupyter-lab",
    "url": "/blog/2016/7/19/jupyter-lab/",
    "title": "Jupyter Lab",
    "summary": "Jupyter Lab was announced at SciPy 2016! And oh my, this is an exciting development. It currently is a very alpha-stage project, and so it didn't...",
    "body": "Jupyter Lab was announced at SciPy 2016! And oh my, this is an exciting development. It currently is a very alpha-stage project, and so it didn't work on my Mac, but it worked beautifully on my Linux machine. Here's two screenshots. [](./jupyterlab-1.webp) [](./jupyterlab-2.webp) A few interesting things pop out: 1. The Terminal emulator respects my ! (I have custom colouring for the prompt specified there.) 1. I can actually run from inside the Terminal emulator. This is bordering on inception... 1. The layout in the 2nd screenshot just rocks. I can foresee how launching into an interface like this may prove to be handy for end users. 1. We have the Qt console handy as a scratch pad. No need to trouble the Jupyter devs to do scratchpad cells anymore! 1. Because I can have a Terminal emulator and a file browser nearby on the same screen, I don't have to go switching windows (read: switching contexts, i.e. bad idea) to find out the path of some file. 1. Because the developers enabled a command palette (in the style of Sublime Text and Atom), this makes it easier to keep my fingers on the keyboard. 1. Even amidst all of this, the notebook still lives on. I think the stuff that's going on with the Jupyter team is really amazing. Kudos to Brian Granger for making open source software central to his tenure package at Cal Poly, and for leading an awesome team on this project.",
    "tags": [],
    "pub_date": "2016-07-19",
    "type": "blog"
  },
  {
    "id": "blog-nxviz-a-networkx-visualization-package",
    "url": "/blog/2016/7/16/nxviz-a-networkx-visualization-package/",
    "title": "nxviz: A NetworkX Visualization Package",
    "summary": "I have been developing network visualization implementations for Python, mostly because I needed them.",
    "body": "I have been developing network visualization implementations for Python, mostly because I needed them. I also believe in open source principles, that broadly useful and applicable code should be made freely available with their implementations made transparent. Based on these principles, I have released Python implementations of circos and hive plots, with a ton of help from Justin Zabilansky (MIT, now AeroFarms) and Jon Charest (MGH, who, by the way, is an aspiring data scientist). At SciPy 2016, I had the opportunity to chat with our conference co-chair, Aric Hagberg, who also happens to be the lead maintainer of the NetworkX package. Specifically, I asked about contributing circos and hive plots into NetworkX. From our discussion, and from discussing with Jon, we have this idea to create an [](https://github.com/ericmjl/nxviz) package that provides a consistent API for creating network visualizations. Really excited about this! I also roped in a brilliant 1st-year UW undergrad Nelson Liu, who also has some network visualizations that he\u2019d like to implement for directed acyclic graphs (DAGs). (No pressure, of course!) Looking forward to developing this \"rational\" network visualization package!",
    "tags": [
      "nxviz",
      "data visualization",
      "data science",
      "software engineering"
    ],
    "pub_date": "2016-07-16",
    "type": "blog"
  },
  {
    "id": "blog-principles-of-network-visualization",
    "url": "/blog/2016/7/16/principles-of-network-visualization/",
    "title": "Principles of Network Visualization",
    "summary": "I use networks as part of my research, and visualization is a very important part of it. The problem with network visualizations is that most people...",
    "body": "I use networks as part of my research, and visualization is a very important part of it. The problem with network visualizations is that most people default to node-link diagrams that end up looking like hairballs. Thankfully, if we put some thought into visualizing networks rationally, we can get aesthetic, beautiful and simultaneously informative network visualizations that are much more appealing than hairballs. My time at SciPy 2016, hearing the keynote by Brian Granger about Altair, and talking with Aric Hagberg, has given me a ton of inspiration on how rational network visualizations can be done right. What does it mean to be drawn \"rationally\"? I think it means following the principles of good data visualization - in which size, shape, location, colour and transparency are all used to convey meaning. As applied to network visualizations, the key is in placing and styling the nodes in some \"rational\" or \"logical\" fashion: 1. Spatial location (i.e. coordinates) follows some regular organization (e.g. in a shape, or along lines), based on grouping or ordering. 2. Node colour follows some grouping, or ordering, or scaling. (Note - it is in that order - colour doesn\u2019t work well for scale.) 3. Highlights, such as borders, can be used to accentuate nodes of interest. Once the coordinates for each node are computed, drawing edges is a much easier matter. Meaning can be attached to edges by: 1. Filtering out edges that are unnecessary to display. 2. Adjusting line thickness to convey quantitative data. 3. Colouring lines to convey grouping of edges. 4. Highlights, such as using different line styles, thickness, or colour (as long as they are not used to convey other things), to accentuate edges of interest. Finally, it may be better to show panels of subsets of the graph, rather than the whole graph itself, rather than shove the entire graph into one visualization. This is because extracting subsets of edges (and even just the relevant nodes) and displaying them on separate plots reduces visual clutter. Hive Plots and Circos Plots are two examples of network visualizations done well, for they conform to these principles. They will both be available in the [](https://github.com/ericmjl/nxviz) package that I will be co-developing, and I hope to use to highlight these details. I think there are other creative layouts as well, such as Arc Plots and Parallel Line Plots that can help us gain a more rational view of the structure of our network data. From an API design perspective, the \"grammar\"-based API that specifies how things are coloured makes a ton of sense. or example, a HivePlot API may look as such: The user need not specify how individual nodes are coloured, as long as the is present in the metadata. Likewise for node colours and edge colours. Implementing this into , I think, is going to be a really challenging, fun and rewarding thing to do!",
    "tags": [
      "data visualization",
      "data science",
      "nxviz",
      "graph visualization",
      "network science"
    ],
    "pub_date": "2016-07-16",
    "type": "blog"
  },
  {
    "id": "blog-scipy-2016-hallway-track",
    "url": "/blog/2016/7/16/scipy-2016-hallway-track/",
    "title": "SciPy 2016 Hallway Track",
    "summary": "Like PyCon 2016, I ended up doing the Hallway Track at SciPy 2016, which really meant hanging out in the hallway, talking with people, fixing code...",
    "body": "Like PyCon 2016, I ended up doing the Hallway Track at SciPy 2016, which really meant hanging out in the hallway, talking with people, fixing code bugs, and writing. Oh, and not to mention, running/jogging/walking on the treadmill to burn off all the food they've fed us. Part of the rationale for doing the hallway track is because all of the talks will be online (they are already going up!) on YouTube, so I can selectively listen to the ones that I'm most interested in later. (We also have a grant deadline to meet today, so I've needed time to work on that.) I re-connected with Hugo Bowne-Anderson, who works at DataCamp as a curriculum engineer. He was previously in the field of quantitative physical biology, and now works to bring data science skills to the workforce. A really funny giant Ozzylander (Australian)! We are working together on bringing some of my network analysis material into DataCamp - exciting stuff! I got a chance to talk with Aric Hagberg, who is the lead developer for NetworkX. He shared with me some stories of how they got NetworkX started, and his thoughts on the future of NetworkX as a package. I definitely learned a lot from him on the software engineering side. I had a chance to meet Greg Wilson, finally, in person! He led our software and data carpentry instructor training. He always has interesting stories to tell. I re-connected with Carol Willing, and a few other MIT-related people (Noelle and Harriet). Carol is always telling me to stay out of trouble, just as a grandmother-like figure would :P. She also wrote a very encouraging comment for me at the end of the tutorial session that I led on Network Analysis. Such a grandma-type! I had a lot of fun time chatting with my roommate, Tom Brander, who has a ton of industry experience and was once an employee of Citibank. He is a master networker, and connected me with tons of people at the conference. I also met some of the financial aid recipients, such as Sartaj, Aman, and AMiT. The reason Amit spells his name that way is because Amit is a commonly-used names in India, so that\u2019s his way of differentiation. I also re-connected with Harsh Gupta, whom I met last year - he has leveraged his experiences with SymPy and GSoC to do some really cool other things! I also connected with Nelson Liu (UW), who showed me quite a bit of the ropes behind Pokemon Go on the first night of SciPy. (Yes, I was the one responsible for that channel.) Turns out it wasn\u2019t all games and play only; Chris Niemara put together - a Pokemon implementation that used good\u2019ol physics code and Bokeh plots! (SoftEng nerds out there - do not read it as and !) During the sprints, I had a chance to sprint with the Bokeh team! I learned much more about the internals of Bokeh than I could have just reading the developer documentation alone (side note: it\u2019s a complicated library of code, but very well organized). Bryan Van de Ven and Sarah Bird were also amazingly helpful during the sprint as well, just as Jens Nielsen, Thomas Caswell and Benjamin Root helped me during last year\u2019s sprint with the [](http://matplotlib.org) team. I recommended that Noelle, also a graduate student at MIT, sprint with the MPL team. In my opinion, they have a very mature and easy-to-follow workflow coupled with a dozen low-hanging fixes that a newcomer to open source can help with. In parallel, with SciPy, there was another scientific conference ongoing somewhere else, and I was seeing the \"new post-doc positions available!\" (re-)tweets flooding my feed. I personally dislike those tweets because I think post-docs ought to be paid at least double the current NIH pay rate. SciPy conferences don\u2019t have those tweets happening, and I think it is for the betterment of the community that way. All-in-all, SciPy 2016 was yet another really fun community conference! (Apologies if I forgot the names of other people; there's just too many to remember!) Looking forward to helping out with next year\u2019s conference!",
    "tags": [
      "scipy",
      "conferences",
      "python"
    ],
    "pub_date": "2016-07-16",
    "type": "blog"
  },
  {
    "id": "blog-scipy-2016-tutorials",
    "url": "/blog/2016/7/14/scipy-2016-tutorials/",
    "title": "SciPy 2016 Tutorials",
    "summary": "And we\u2019re done! Two days of fun, intense, and hopefully mind-expanding tutorials at SciPy 2016. I led two sessions of my network analysis workshop.",
    "body": "And we\u2019re done! Two days of fun, intense, and hopefully mind-expanding tutorials at SciPy 2016. I led two sessions of my network analysis workshop. Originally, I was only going to lead one session, and I thought I\u2019d get a niche crowd and a small class. However, it turned out that there was much more interest than expected, and it turned out there was a waitlist, so the tutorial committee chairs (Justin Vincent and Ben Root) opened up another slot for the tutorial. This was mostly unexpected because I tended to consider network analysis and network science to be a niche topic in the scientific world. On day 1, the crowd was approximately 60+ people, and on day 2, the class was only 20 people. On both days, it was a fun time sharing the content that I\u2019d prepared with everybody there. I think the nature of a small class made it logistically much easier to get through all 6 notebooks. The statistical inference notebook, however, went over the heads of many people today; I think statistical thinking is something that needs to be inculcated over time. I am thinking of leaving statistical thinking out for next year (which might be the final year that I do this tutorial, but I\u2019ll see). The feedback from the tutorial crowd was very encouraging, very thoughtful as well! I personally scanned them all and read each one. I spent a bit of time then uploading the positive ones to my personal site, and fiddling around to get a carousel working. They can be found here.",
    "tags": [
      "scipy",
      "conferences",
      "python"
    ],
    "pub_date": "2016-07-14",
    "type": "blog"
  },
  {
    "id": "blog-getting-the-most-out-of-mentoring",
    "url": "/blog/2016/7/12/getting-the-most-out-of-mentoring/",
    "title": "Getting the Most out of Mentoring",
    "summary": "On Thursday, 7 July 2016, I attended a workshop on effective mentoring by Bruce Birren at the Broad Institute.",
    "body": "On Thursday, 7 July 2016, I attended a workshop on effective mentoring by Bruce Birren at the Broad Institute. Bruce is an experienced scientist at the Broad, and he has had decades of experience mentoring many trainee scientists. To my (pleasant) surprise, he knew who I was (by name)! (I don't remember introducing myself to him before yesterday.) The biggest takeaway that I think Bruce was trying to convey was the idea that mentees can, and should, actively take a role in managing the mentor-mentee relationship. The reality for mentees is that mentors either care or don\u2019t care about them (the latter are rare); if they care, they likely don\u2019t have enough time to devote to each and every one of their mentees. Rare is the mentor who can consistently provide one-on-one mentorship to their mentee. Therefore, the reality for most mentees is that they have to actively take charge of the relationship in order to make the relationship mutually beneficial. (FWIW, I\u2019m not endorsing this scenario as most ideal, but merely describing it as \"the way things are\".) An interesting concept that Bruce conveyed was the definition of mentorship as being: \"A symbiotic relationship aimed at advancing careers and career satisfaction for both the mentor and the mentee.\" Yes, that is right, it is a mutually-beneficial relationship, not a one-way relationship. This actually empowers the mentee to figure out how to deliver value to the mentor. It does make sense, if we think about it - if a mentee is known to be successful and acknowledges help from his/her mentor, the mentor\u2019s reputation is enhanced as well. A rising tide floats all ships. Practically speaking, because of the mentor\u2019s time constraints, the mentee has the biggest onus of preparing for every meeting with the mentor. I\u2019m in this boat right now, as I prepare for meetings with my committee members and (potential) future collaborators. From experience, this means making a few adjustments to how we present our updates (which I think are reasonable ones to make). Firstly, I think we have to assume that our mentor does not remember what\u2019s happened since the last meeting. This means crafting the updates in such a way that helps them connect what you\u2019re saying to some \"bigger picture\" - whether that means scientific goals, professional goals, or personal goals. Secondly, we have to adjust the way we present the material such that our mentor is most receptive to it. The example Bruce gave was that of himself (which mirrors myself as well). He calls himself a \"whiteboard person\" - someone who thinks aloud and visually on the board; I use the whiteboard to anchor transient thoughts and get a feel for the big picture. If Bruce were my mentor, it would be ineffective for me to simply talk with him in a room without a whiteboard. The onus is on the mentee to learn how to communicate in a fashion that helps the mentor understand where the mentee is. (Of course, it helps that the mentor adjusts and takes notes accordingly too, but we have to approach the relationship without this expectation.) Thirdly, the onus is on us as mentees to help our mentors focus how they can help us the most effectively. This means determining the agenda and communicating to them expectations for each meeting, and asking them what they expect of us as well. Going up to them and asking for \"advice in general\" isn\u2019t effective. Going to them with a set of specific questions/goals that we think they can help us with is. For example, I may set the agenda for a meeting with my mentor, with the expected goals of determining date ranges for the next committee meeting and defence dates, expectations for what to prepare for the next committee meeting, and ask for letters of support for the next career move. Mentors don\u2019t have extra-sensory perception; we need to communicate with them how we think they can help us. Specifically for scientists, it is important that individuals learn how to progressively become independent researchers. By independent, I define it as \"being able to, out of one\u2019s own capacity, (1) identify, (2) articulate, (3) defend the importance of, and (4) generate and defend the principled interpretation of data that provides a path towards solving a problem.\" This has to be learned over time, and each person comes in with aspects of each part at different maturity levels. The challenge for the mentee, then, is to identify where s/he is weak in each of these, and ask for specific help from a variety of mentors towards working on one\u2019s weak spots. Okay, to reiterate, the biggest thing that Bruce conveyed to us was that mentees can and should actively manage the mentor-mentee relationship. To borrow a quote that Bruce shared from the Harvard Business Review, \"Some superiors spell out their expectations very explicitly. But most do not. Ultimately the burden falls on the subordinate to discover what the boss\u2019 expectations are.\" (Gabarro and Kotter, 1980, HBR) Likewise for mentor-mentee relationships.",
    "tags": [],
    "pub_date": "2016-07-12",
    "type": "blog"
  },
  {
    "id": "blog-leaders-provide-safe-zones-to-fail",
    "url": "/blog/2016/7/6/leaders-provide-safe-zones-to-fail/",
    "title": "Leaders Provide Safe Zones to Fail",
    "summary": "Unlike my other two posts on leadership (Chapter 1 and Chapter 2 of \"The Shaping of an Effective Leader\"), this one doesn't have a source from which...",
    "body": "Unlike my other two posts on leadership (Chapter 1 and Chapter 2 of \"The Shaping of an Effective Leader\"), this one doesn't have a source from which I'm quoting. It comes more from experience. There are many great qualities of great leadership. One of the ways that leaders can be great is that they inspire excellence in those who follow them. I think one thing that may get swept under the rug (as an assumption) is that great leaders enable others under them to be excellent by providing them with safe zones to fail. The safe zone encourages experimentation, which we know generally produces better outcomes than sticking and committing to a single path. They let their followers fail at in front of them first, with very few negative consequences, and use that time to coach them towards better performance. Having someone who does that for you can be really empowering. Leaders, let your followers fail (in meaningful ways, of course).",
    "tags": [],
    "pub_date": "2016-07-06",
    "type": "blog"
  },
  {
    "id": "blog-the-shaping-of-an-effective-leader-chapter-2",
    "url": "/blog/2016/7/4/the-shaping-of-an-effective-leader-chapter-2/",
    "title": "The Shaping of an Effective Leader: Chapter 2",
    "summary": "Some of my notes from Chapter 2 of the book.",
    "body": "Leadership Tasks These are the leadership tasks that lead to effectiveness: 1. Developing a plan of action. 1. Determining a method of analysis 1. Cultivating an understanding of the tools needed. 1. Making decisions. 1. Committing to a course of action, allocating resources so that the greatest opportunities can be realized. The purpose of an organization is to get uncommon results from common individuals. Effective Practices 1. Ask what needs to be done. 1. Focus not on what you like to do but on what is right for this organization. 1. Develop a bias for action and develop plans reflective of that bias. 1. Work to make effective decisions - and follow them up. 1. Communicate the appropriate amount of information at the appropriate level of transparency. 1. Maintain a focus on opportunities and innovations rather than problems. 1. Coordinate and run productive and effective team meetings. 1. Build an effective team. Beebe gave the example of U.S. President Harry Truman. He was widely regarded as being strong in domestic policy and weak in foreign policy. Truman didn't inherit a country at peace internationally; rather he came into the presidency in the middle of international turmoil. Truman surrounded himself with foreign policy experts, who in turn helped him \"level up\" his foreign policy skills. In striving for effectiveness, treating people with \"fairness\" (on paper) is actually ineffective. Beebe states that Peter Drucker argues that leaders should focus on motivating the small handful/percentage of people who deliver the largest amount of desired results, keeping them motivated in line with the mission of the organization. Challenges The challenge that a leader faces here is that of envy. There may be direct or indirect reports who are stronger in a particular skill, relative to myself. I have to avoid envy cropping up, as that will diminish my effectiveness in motivating, rewarding, and retaining the services of those individuals. People we enlist/hire to help us on our mission should be better than us in some respect, otherwise it's wasteful effort duplication cropping up again.",
    "tags": [
      "reading notes",
      "leadership"
    ],
    "pub_date": "2016-07-04",
    "type": "blog"
  },
  {
    "id": "blog-productivity-hack-get-rid-of-visual-clutter",
    "url": "/blog/2016/6/30/productivity-hack-get-rid-of-visual-clutter/",
    "title": "Productivity Hack: Get Rid of Visual Clutter",
    "summary": "How simple visual decluttering can helped me work more effectively.",
    "body": "For the past few days, I have moved into a default full-screen mode, in which I hid both my dock and my menu bar. (Hiding the menu bar is enabled by going to and checking . This I did without explicitly launching stuff in \"full-screen mode\", which most Mac apps have the capability for. Not sure why, but I think the little tweak of having the menu bar hidden (though still open) somehow has a large effect. My desktop ends up looking like the following image (by default): Additionally, I have started to see the importance of having a single browser tab open, or a single document open, or a single (and logically coherent) set of Python modules open to focus on a single task. By removing these distracting visual elements out of the way, I can deeply focus on the task at hand instead. No longer will there be a little lingering icon that I'm tempted to click. There's no longer the element of distraction that comes from other \"stuff\" that's running on my computer.",
    "tags": [
      "productivity"
    ],
    "pub_date": "2016-06-30",
    "type": "blog"
  },
  {
    "id": "blog-new-network-analysis-logo",
    "url": "/blog/2016/6/24/new-network-analysis-logo/",
    "title": "New Network Analysis Logo!",
    "summary": "I made a new logo for the Network Analysis Made Simple tutorial!",
    "body": "I just finished reworking the examples that I'm using for my Network Analysis tutorial, and I managed to create a representative logo for it! :) Inspired by Giles Hall, who hashed people's names to create uniquely procedurally-generated snowflakes (implying that everybody's a special snowflake, that is) (talk). In this case, I wrote a function that computes a numerical hash of a person's name, which is then parsed into a string to determine the order in which each of those network diagrams are added on. More details can be found in this notebook!",
    "tags": [
      "network analysis",
      "teaching",
      "education",
      "tutorials"
    ],
    "pub_date": "2016-06-24",
    "type": "blog"
  },
  {
    "id": "blog-tooling-up-for-plain-text-academic-writing-in-markdown",
    "url": "/blog/2016/6/22/tooling-up-for-plain-text-academic-writing-in-markdown/",
    "title": "Tooling up for Plain Text Academic Writing in Markdown",
    "summary": "My tooling I used for academic plain-text writing. Mostly done for my paper, also for my thesis.",
    "body": "I finally got down to doing it! Here's my brief notes on how I set it up; it may change a bit later on, as my needs evolve, but it should be enough instructions for others to get setup. Motivation I'm writing a white paper on genomic surveillance, so I thought I'd experiment with something new. By switching over to writing in plain text, I can ignore all distractions by other word processors. I'm looking at you, Word, Google Docs and others. Because I get to write in plain text, I also get to version control the text. By selectively committing logical blocks, I can also easily selectively revert unwanted commits; it's rare, but once in a while it's saved my work. Because it's plain text, I can export to anywhere with a bit of configuration. Because it's written publicly, it's great. Overview What I do here is basically use Sublime Text 3 (ST3) as a word processor. BibTex libraries are used for keeping track of papers, but I like Papers for organizing more than BibDesk - automatic downloading of PDFs is the best part. My compromise, then, is to export my Papers library to a master BibTex library prior to compiling everything into a single PDF. Papers can also add in Pandoc-style citations, which Pandoc can then parse to make an automatic bibliography. Everything is kept under version control, for reasons stated above. It basically took me one morning's worth of time invested in tooling this up, but I can already imagine the amount of time saved by minimizing the time spent on formatting references, figure numbers, versioning, and (most of all) being able to write without being distracted. Tools 1. Papers for Mac (paid) 1. Pandoc (free) 1. Sublime Text (free/paid) - you can use it for free, but I decided to fess up and pay for it, the text editor is that good, that powerful, that I would encourage you to do the same too. 1. Git and GitHub (free) Setup Central Bibliography 1. Set up a version controlled GitHub repository for the master bibliography file. 1. In Papers, export the library to that repository's directory. Take note of the . General Tooling 1. Install Pandoc, either using , or download a fresh copy of the installer binary. Note the . 1. Install the LaTeX distribution, make sure that is bundled. Note the . 1. Fork the Citation Styles repository, and clone it to disk. 1. Install the plugin, to enable automatic figure numbering. Again, take note of the . Sublime Text Tooling Install: 1. Package Control 1. Pandoc 1. CiteBibTex 1. AcademicMarkdown 1. Git 1. GitGutter 1. PackageSync 1. BracketHighlighter 1. WordCount Configure: Pandoc Apart from placing these under the section, you may want to do the same for the and sections. CiteBibTex Find the corresponding configuration fields, and change them to the following (making appropriate changes): User Interface Today I learned that ST3 has a \"Distraction Free Writing Mode\", under the menu. Earlier on, I also learned that it has a pane view mode, also under . Both have associated shortcut keys. My writing interface ended up looking something like what's in Figure {@fig:two-pane}. {#fig:two-pane} My outline is on the right, and the main text is on the left, and there's no distracting tabs, sliding preview, or directory structure (as is what I'm used to for coding). Writing Get started by adding the YAML headers in the document (Figure {@fig:yaml-header}). {#fig:yaml-header} Specifically, the format of what I have above is: More details on what metadata can be stored in the headers can be found on the Pandoc README. Citations Citations are done in Markdown by inserting: where the is automatically generated by Papers, usually in the form of . An example of what gets inserted is . I was reading through Papers' documentation on the generation of a \"universal\" citekey, and I quite like the idea. I think the specification is worth a read, and is an idea worth spreading (sorry, I like TED talks). I intentionally configured my ST3 Pandoc package settings to use a global master library, rather than a project-specific one. I think it stemmed more from laziness than anything else; one less thing to manage is a better thing. Generating Outputs Note that the way I had configured Pandoc (above) for PDF outputs was to use the master library for matching up references. An additional thing I did was to keep a copy of the citation style language (CSL) markup file in the same directory as the Markdown document. Within ST3, we can use the Command Palette to quickly generate the PDF output desired. Steps are: Select Pandoc Select PDF as the output Inspect that gorgeous PDF! Check your references! Just to show how the figures get numbered correctly (I don't have any in the draft whitepaper I'm writing), you can inspect the source code for this blog post, and the associated pdf file. Note how I've not numbered anything except the associated files. It's pretty cool. Alrighty - and that's it! Hope it helps others too.",
    "tags": [
      "academia",
      "writing",
      "grad school"
    ],
    "pub_date": "2016-06-22",
    "type": "blog"
  },
  {
    "id": "blog-in-defence-of-extreme-openness",
    "url": "/blog/2016/6/22/in-defence-of-extreme-openness/",
    "title": "In Defence of Extreme Openness",
    "summary": "I found an awesome slide deck by Jake Vanderplas. It's a great case for doing science in the open; highly recommended.",
    "body": "I found an awesome slide deck by Jake Vanderplas. It's a great case for doing science in the open; highly recommended.",
    "tags": [
      "data science",
      "open science"
    ],
    "pub_date": "2016-06-22",
    "type": "blog"
  },
  {
    "id": "blog-the-shaping-of-an-effective-leader-chapter-1",
    "url": "/blog/2016/6/21/the-shaping-of-an-effective-leader-chapter-1/",
    "title": "The Shaping of an Effective Leader: Chapter 1",
    "summary": "Author: Gayle D. Beebe. I\u2019ll be putting together a series of blog posts as my book reading notes. This is the first of many more to come.",
    "body": "Author: Gayle D. Beebe. I\u2019ll be putting together a series of blog posts as my book reading notes. This is the first of many more to come. Introduction 8 principles of leadership (pyramid). Ordered below from foundation to tip: 1. Character - moral integrity. 2. Competence - abilities, skills. 3. Chemistry - how well a team works together. 4. Culture - how the team works together. 5. Compatibility - between personal goals/values and organization goals/values. 6. Convictions - what should be done, and how? 7. Connections - to those who work under us. 8. Contribution - the thing that will outlive us. In chapter 1, Beebe discusses Character first. Character A leader cannot be without character - defined as moral integrity. Effective leadership ultimately is about consistency. That can only be founded on a strong character. The moral tone for an organization starts at the top. Also listed in this chapter are 8 vices and virtues of the leadership pyramid. Still digesting it, but here they are summarized: 1. Character: Gluttony vs. Temperance 4. Competence: Envy vs. Contentment 5. Chemistry: Greed vs. Generosity 6. Culture: Anger vs. Mildness 7. Compatibility: Pride vs. Humility 8. Convictions: Lust vs. Fidelity 9. Connections: Indifference vs. Perseverance 10. Contribution: Melancholy vs. Perspective Each leader has at least one of the vices manifest in his/her life; it will be, to paraphrase in my own words, our own cross to bear. Favourite quote: \"To trust a leader, it is not necessary to like him. Nor it is necessary to agree with him. Trust is the conviction that the leader means what he says. Effective leadership\u2026 is not based on being clever; is based primarily on being consistent. (Peter Drucker) 10 Qualities that reflect our character - a leader with well-formed character: 1. leads from a foundation of integrity. 2. displays wisdom and judgment. 3. has the ability to absorb and undo the evil of others. 4. works with understanding and respect. 5. works for the greater good. 6. is temperate in all matters. 7. balances a confidence in his or her own ability with humility. 8. is calm, loyal, prudent and discerning. 9. hires well, communicates clearly, and trusts. 10. balances a concern for the welfare of employees with the need to achieve positive results. My own thoughts on character: We leaders must create the environment that makes it easy, even encouraged or celebrated, to figure out our own mistakes, correct them, and share them with others. On personal flaws: Totally agree with notion that we each have our own flaws to deal with. Key idea from the book - magnify team\u2019s and individual\u2019s strengths such that they matter the most to the team; diminish importance of weaknesses to the point that they don\u2019t matter at all. Nobody can be all-round perfect, so no point trying to make that happen either.",
    "tags": [],
    "pub_date": "2016-06-21",
    "type": "blog"
  },
  {
    "id": "blog-yoga-for-python",
    "url": "/blog/2016/6/17/yoga-for-python/",
    "title": "Yoga for Python",
    "summary": "Prof. Allen Downey (Olin College) described my Network Analysis tutorial as being like yoga with Python.",
    "body": "Prof. Allen Downey (Olin College) described my Network Analysis tutorial as being like yoga with Python. :D I&#39;m following Eric Ma&#39;s tutorial on Network Analysis. It&#39;s very relaxing, like yoga with Python. @ericmjl https://t.co/WawPdDmmiG&mdash; Allen Downey (@AllenDowney) June 16, 2016",
    "tags": [
      "network analysis",
      "tutorials",
      "conferences",
      "pycon"
    ],
    "pub_date": "2016-06-17",
    "type": "blog"
  },
  {
    "id": "blog-boston-area-antibiotic-resistance-network-baarn-meeting-2016",
    "url": "/blog/2016/6/16/boston-area-antibiotic-resistance-network-baarn-meeting-2016/",
    "title": "Boston Area Antibiotic Resistance Network (BAARN) Meeting 2016",
    "summary": "Just attended the BAARN 2016 meeting and learned a lot! \ud83e\uddea\ud83d\udd2c Discussed genomic and phenotypic surveillance, the economic challenges of antibiotic development, and the need for rapid detection of bacterial and viral infections. Also, shared thoughts on my current projects tying viral genotype to phenotype. \ud83e\uddec\ud83e\udda0",
    "body": "Yesterday I attended a very fruitful [BAARN 2016 meeting][baarn]. This was the fourth annual meeting, and the range of topics spanned the scientific (new technologies for treating and diagnosing) to economic (incentives for development of new antibiotics and diagnostic technologies) to medical (outlining the real unmet needs that doctors are facing). I learned a ton; the $100 registration fee was definitely worthwhile. [baarn]: https://www.eventbrite.com/e/baarn-2016-boston-area-antibiotic-resistance-network-tickets-22500283955 There were three things I took note of that I think could be translated over from the bacterial surveillance world to the viral surveillance world. (1) Genomic and Phenotypic Surveillance This one was a big take-home for me, given the research direction I\u2019m hoping to pursue after grad school. Genomic surveillance is the use of genome sequence to determine the evolutionary history and potential risk of a pathogen. In the bacterial world, genomic surveillance efforts are mostly concentrated on finding known antibiotic resistance genes. Phenotypic surveillance involves actually experimentally testing a pathogen for some phenotype. In the bacterial world, this means testing a cultured isolate against a panel of drugs. In my mind, these two are inseparable. Genomic surveillance will always be cheaper and faster to carry out, given the advances in portable sequencers; phenotypic surveillance is going to be rate-limited by biochemistry. On the other hand, only systematic phenotypic measurements can give us the necessary data to link genome sequence to phenotype. In my view, the function that connects genome/protein sequence to phenotype is complex enough that for pure surveillance (not scientific) purposes, machine learning models are probably the best tools to use. With tools like [TPOT][tpot], it should be trivial to automate the selection of best predictive features and models. The tough part, then, is generating a gold-standard, epidemiologically relevant dataset. [tpot]: http://www.randalolson.com/2016/05/08/tpot-a-python-tool-for-automating-data-science/ (2) Incentives Another interesting point that came out is the notion that antibiotic drugs are the only drug whose value depreciates with its widespread usage. The main factor at play here is the emergence of drug resistant bacteria. This makes it financially disadvantageous for companies to come in and invest in making new drugs. One alternative incentive system that was brought up is best described with an analogy - fire stations. Taxpayers pay into a common pool of money, which funds fire stations and firemen. As opposed to paying firemen per fire that they put out, we pay them a fixed amount as a public good/insurance against fire disasters. Likewise, as opposed to paying drug companies per dosage of antibiotics that are sold, a better incentive may be to guarantee payment at (peer-/externally-reviewed) drug development milestones, in addition to a sum for maintaining an arsenal of drugs that could be stewarded when needed. (3) Rapid Detection [Dr. Angela Caliendo][angela] (Brown University) brought up some really, really useful points during her talk, which I took note of in my [bullet-point notes][notes]. Rapid detection was the point that stuck the most. [angela]: https://vivo.brown.edu/display/ac184 [notes]: /blog/2016/6/16/baarn-2016-bullet-point-notes/ \"Rapid\" is a nebulous concept, so Angela\u2019s talk helped bring some clarity here. For the physician (and by my own extension, front-line epidemiologist), the major unmet medical needs for countering infectious disease outbreaks are: 1. Distinguishing bacterial from viral infection. 2. Identifying bacterial pathogen. 3. Identifying viral pathogen 4. Measuring susceptibility of pathogen. In each case, \"rapid\" means different things. To distinguish bacterial from viral infection, and to be most useful in guiding physician decision-making, \"rapid\" means within minutes, at the bedside, doable by a non-laboratory trained person. For viral detection, it has to likewise be within 15-20 minutes to be \"rapid\". For bacterial detection, an hour or less is best, and for susceptibility testing, less than 6 hours is optimal. Knowing these points really helps set the engineering constraints properly. Basically, any useful assay has to work within minutes, be operable by a non-laboratory trained person, and accept blood/swabs as input. Solid phase (bio)chemistry is going to be really important. Synthesis/Thoughts My current projects are geared towards tying viral genotype to phenotype, with the goal of predicting a virus\u2019 pathogenic risk profile from its sequence. Because sequence data is the input, and not blood/swab samples, I won\u2019t be playing in the rapid detection space. However, I think the tools I hope to develop will play well in the susceptibility prediction/determination space. With the MinION sequencer, the sequence of a virus can be determined within one day of isolation. Now, the seq",
    "tags": [
      "conferences",
      "antibiotic resistance",
      "baarn 2016",
      "genomic surveillance",
      "phenotypic surveillance",
      "machine learning",
      "tpot",
      "drug development",
      "incentives",
      "rapid detection",
      "viral detection",
      "bacterial detection",
      "susceptibility testing",
      "minion sequencer",
      "viral genotype",
      "phenotype prediction"
    ],
    "pub_date": "2016-06-16",
    "type": "blog"
  },
  {
    "id": "blog-baarn-2016-bullet-point-notes",
    "url": "/blog/2016/6/16/baarn-2016-bullet-point-notes/",
    "title": "BAARN 2016 Bullet Point Notes",
    "summary": "I attended the 2016 Boston Area Antibiotic Resistance Network meeting, held at the American Association for the Advancement of Science (AAAS). Here\u2019s my bullet-point notes version of what went on.",
    "body": "Session 1: New antibiotic strategies and therapeutics: In this section, scientists and group leaders presented novel ways of thinking about anti-bacterial treatment. 1. Small molecules: focus on how to get molecules into the cell. (Anita Miller, Entasis) 2. Phage lysins: one component of phages, add externally to a bacteria. (Vincent Fischetti, Rockefeller University) 3. Ecobiotics: Consortia of commensal bacteria to restore general diversity, or impact specific pathways. (Matthew Henn, Seres Therapeutics) 4. Chemical discovery: Gene clusters can predict biochemical activity of natural products, great for discovering new chemicals. Cool point I took home - a gene cluster that encodes a molecule that destroys function of a common protein may also encode a resistant version of that version. (Emily Balskus, Harvard University) Session 2: Creating a sustainable ecosystem for antibiotic production. In this section, representatives from finance, manufacturing, regulatory, and public policy discussed how they thought a sustainable ecosystem for producing antibiotics could be fostered. Panel members: - Joseph Larsen, BARDA - Cara Cassino, Contrafect - Edward Cox, FDA - Alan Carr, Needham & Co. Are there enough ideas in the pipeline? - BARDA model: product accelerator; public-private partnerships. - Missing: a \"pull\" incentive - if I develop X successfully, I will get paid Y. Need lawmakers\u2019 help. Are there enough ideas in the pipeline? - BARDA model: product accelerator; public-private partnerships. - Missing: a \"pull\" incentive - if I develop X successfully, I will get paid Y. Need lawmakers\u2019 help. - There\u2019s a move towards targeted drug therapies. - New paradigms come with unknown risks. - Market around antibiotics: there\u2019s underlying interest, but some skepticism and fear towards biotech. - There are sub-sections of antibiotics space (e.g. gram positives) that can do well, because of unmet need. - Regulatory: yet, if there\u2019s an unmet need, then we\u2019re behind. Private vs. societal value are not aligned. - Antibiotics: a bit like fire department. We don\u2019t pay fire department by the number of times they put out fires, but as an insurance. Payments are de-linked from the amount of usage. - Private sector model isn\u2019t going to foster investment. - Incentives need to factor in not just pathogens, but also clinical manifestations of the pathogens. Diagnostics? - Rapid, within 1-2 hours, which can be good enough to form the basis for decision making, is an unmet need. - Susceptibility testing is also important. Why is there not reduced antibiotic usage in agriculture? - Companies that manufacture these products are told to voluntarily remove indications for non-therapeutic uses. - By end of year, antibiotics will need to be prescribed by a vet. - Antibiotics have growth promotion and prophylactic non-therapeutic uses. - Interestingly: high prices per course discourages lax usage. Interesting\u2026\u2026 But what if the economic & regulatory model was changed? New antibiotics are a bit like the different types of fire extinguishers (one for petrochemical, one for electrical, etc.). You only buy a new fire extinguisher if it is sufficiently different. New antibiotics have to likewise be sufficiently different. What\u2019s the one thing that has to be done? - From small antibiotic developer\u2019s perspective: prioritization is needed. Favourable reimbursement is needed. Also support the fire department model, but don\u2019t discriminate between large and small developers. - Push incentives are helping, but we need the \"pull\" incentives, i.e. the known return on investment. - Parallels between rare diseases and antibiotics. \"Fire department\"-style incentives might be a good way forward. - Rapid, high resolution diagnostics would be really transformative, for example, in their use in clinical trials for rare infections. - Clinical trial networks, infrastructure to be able to study new, narrow-spectrum agents. Session 3: Entrepreneurship Things to note: - Make sure the science is solid. Nature cannot be fooled. - Regulatory issues must be thought of right from the beginning. - Technology can come from anywhere. A good company is one that addresses an unmet need. - Talk with regulatory people early on. They sometimes have productive things to say as well. - Get a great team together. They should know the market, believe in the mission. - Focus on proof of concept and differentiation. If these are achieved, good things can happen. - Talk with people you trust, who are willing to tell you \"this idea is a bad idea\". Session 4: Diagnostics - From talking with others over lunch, I found that diagnostics have to be technically so simple (i.e. solid phase chemistry). - Clinical goals: - Reduce use of antibiotics - Rapid diagnosis and treatment - Data to guide the most appropriate antibiotic - Clinicians need to know when to withhold and when to deploy antibiotics. - Unmet clinical needs: - Rapid, simple, inexpensive to distinguish viral from bacterial infectio",
    "tags": [
      "conferences",
      "meeting notes"
    ],
    "pub_date": "2016-06-16",
    "type": "blog"
  },
  {
    "id": "blog-gaining-focus-going-analog",
    "url": "/blog/2016/6/15/gaining-focus-going-analog/",
    "title": "Gaining Focus: Going Analog",
    "summary": "I have found that I most successfully focus when I have my scribble book by my side. Jotting down what I'm going to do next on the notebook focuses...",
    "body": "I have found that I most successfully focus when I have my scribble book by my side. Jotting down what I'm going to do next on the notebook focuses my mind more than jotting it down in Evernote, which is done digitally. I think it's the combination of having it recorded on a companion \"screen\" to my MacBook Air.",
    "tags": [],
    "pub_date": "2016-06-15",
    "type": "blog"
  },
  {
    "id": "blog-how-to-give-a-great-data-science-workshop",
    "url": "/blog/2016/6/14/how-to-give-a-great-data-science-workshop/",
    "title": "How to give a great Data Science Workshop",
    "summary": "I recently was asked by Dr. Hugo Bowne-Anderson (a really chill guy, who probably would insist that I drop the \"Dr.\") to contribute some of my...",
    "body": "I recently was asked by Dr. Hugo Bowne-Anderson (a really chill guy, who probably would insist that I drop the \"Dr.\") to contribute some of my thoughts on what makes a great data science workshop. Having been on both sides as an instructor and a learner, here's some of my thoughts. What distinguishes a Data Science Workshop from a Data Science talk or lecture? I think the distinguishing mark of a data science workshop is that the instructor will make sure that there is hands-on coding involved. The only exception to this that I have experienced was in Bang Wong's workshop on data visualization, where we did group discussion instead. In both cases, participant involvement is very important. What are the 3 most important qualities for a great Data Science Workshop to have and the 3 most important for it to NOT have? Why (one sentence on each quality)? My top 3 important qualities are: 1. Interactive & hands-on: in line with the MIT motto Mens et Manus, the best workshops apply theory soon after learning it. 2. Targeted: The best workshops have their difficulty tailored to their audience\u2019s skill level. 3. Expandable knowledge: Ideally, the workshop isn\u2019t the end of learning; the best workshops point students to other resources to further the learning. It should be evident that the opposites of the above should be avoided, but I\u2019ll put out a few ideas of common good teaching practices that might (paradoxically) be best avoided in a workshop setting: 1. Repetition: I think there should be as little conceptual repetition as possible; while it\u2019s good for learning, but I think it\u2019s not suited for a workshop setting, where I think the goal is a \"sufficiently deep introductory survey\" (an oxymoron of terms). 2. Lecturing: This is admittedly tricky; there has to be enough lecturing, but not too much, and I think the best instructors intersperse them sparingly throughout the workshop. 3. Handling questions: Questions are great during the breaks and after, but I\u2019ve seen that the best instructors, and most cooperative students as well, defer their tougher and more detailed questions till a later time. How do you avoid losing your audience in the 1st 5 minutes? The most important thing I learned was to keep the energy high. My verbal and body language have to show that I'm excited to share my knowledge, thrilled to meet everybody present, and eager to continue the conversation afterwards. The best instructors that I\u2019ve seen, such as Allen Downey, do this. Following that, I think it\u2019s important to set expectations, to clarify what will and won\u2019t be covered. What tools/software/resources do you use when giving a Workshop? What tools/software do you ask your attendees to use? The topic I teach the most has been network/graph analysis fundamentals, using the NetworkX API to introduce these ideas. For this, I use Python (and the scientific Python stack, in particular), the Jupyter notebook, and on GitHub. I like using the RISE plugin for Jupyter, which instantly turns my code and markdown cells into slides that I can advance through. Everybody complains about font sizes being too small; RISE solves that instantly for the notebook, and I can spice up the theme by changing the CSS. Apart from that, where possible, I try to make use of sticky notes. It\u2019s something I picked up from my Software & Data Carpentry instructor training. Green stands for \"I'm good to go!\", while red stands for \"I'm in need of some help.\" What would you like to find in a \"How to teach a great Data Science Workshop\" article? I\u2019d like to see real feedback from students about their instructors. Nothing beats having direct feedback from the participants. I\u2019d also like to see a list of questions that instructors should ask themselves (or have a coach-like figure ask them) after each workshop, so that we can train ourselves to focus on the most important aspects of teaching.",
    "tags": [],
    "pub_date": "2016-06-14",
    "type": "blog"
  },
  {
    "id": "blog-form-and-function",
    "url": "/blog/2016/6/13/form-and-function/",
    "title": "Form and Function",
    "summary": "When writing code, it\u2019s often a great idea to write tests in parallel. Today I experienced how important it is to have a split window.",
    "body": "When writing code, it\u2019s often a great idea to write tests in parallel. Today I experienced how important it is to have a split window. I write code on my left pane, and I can immediately set up the corresponding test on the right pane. Just as such. I used to not write my tests immediately, and I\u2019m right now suffering that technical debt in parts of my code. That was before learning that Sublime Text 3 allows me to do split window panes (with the shortcut key ). Now that I know it, writing tests in parallel with my code is much easier. Form indeed informs function.",
    "tags": [],
    "pub_date": "2016-06-13",
    "type": "blog"
  },
  {
    "id": "blog-abstractions",
    "url": "/blog/2016/6/10/abstractions/",
    "title": "Abstractions",
    "summary": "Matt Johnson (of HIPS) had an interesting comment on Wednesday. LLVM's relationship to machine code is a bit like Deep Learning's relationship to...",
    "body": "Matt Johnson (of HIPS) had an interesting comment on Wednesday. LLVM's relationship to machine code is a bit like Deep Learning's relationship to Bayesian models. Deep learning abstracts out details into a reliable black box, unlike Bayesian models in which all details need to be known. Likewise, LLVM abstracts out machine code in a reliable fashion, so that higher level programming languages don't have to think about that stuff. Interesting.",
    "tags": [
      "programming",
      "bayesian",
      "data science",
      "deep learning"
    ],
    "pub_date": "2016-06-10",
    "type": "blog"
  },
  {
    "id": "blog-network-science-and-statistics-applications-and-fundamentals",
    "url": "/blog/2016/6/9/network-science-and-statistics-applications-and-fundamentals/",
    "title": "Network Science and Statistics: Applications and Fundamentals",
    "summary": "I will be at SciPy 2016 this year, to deliver a workshop titled \"Network Science and Statistics: Applications and Fundamentals\".",
    "body": "I will be at SciPy 2016 this year, to deliver a workshop titled \"Network Science and Statistics: Applications and Fundamentals\". A huge thanks to those who have registered! The materials are already available on GitHub, but will be updated within the coming weeks as I incorporate feedback from this year's PyCon 2016 and ODSC East 2016 participants. Looking forward to being in Austin and meeting with everybody!",
    "tags": [
      "scipy",
      "conferences",
      "python",
      "network science",
      "graph theory",
      "data science"
    ],
    "pub_date": "2016-06-09",
    "type": "blog"
  },
  {
    "id": "blog-how-to-speed-up-scientific-python-code",
    "url": "/blog/2016/6/7/how-to-speed-up-scientific-python-code/",
    "title": "How to speed up scientific Python code",
    "summary": "I highlight the use of PyPy, , , , and Cython as strategies for speeding up your Python code.",
    "body": "Introduction For scientists who are looking to take their Python code to the next level, options are abound. I\u2019ve decided to put down in writing a short summary of what I\u2019ve learned so far. Disclaimers: - the \"best use cases\" part comes from personal experience, not from referenced literature, and is by necessity going to be of limited scope. - your mileage\u2026 ahem, speed increases may vary. (yada yada yada) - your algorithm complexity matters more than these tools below. - learn how to profile your code, and then profile, profile, and profile before you optimize. Now that the disclaimers are out of the way, here\u2019s the shortlist, in no particular order: PyPy Great if you have only pure Python code, have to deal with things in Python objects, and import other modules that are only pure Python. My use cases: Agent-based simulations, re-implementing random draws from a variety of distributions. Upsides: Speedups can be massive; in my hands anywhere between 8-100X. PyPy is a really simple drop-in replacement for the regular CPython that we\u2019re all used to. Downsides: Have to download an alternate Python interpreter. I have yet to successfully use PyPy as my backend. Need to tweak environment variables to get to install packages/modules such that they\u2019re accessible by . and Great if you\u2019re writing things that are, by nature, arrays of stuff, with operations that can be vectorized across all rows (or columns). My use cases: Matrix math. Upsides: and are well-supported, community-favoured, thoroughly tested and all-round great standard scientific computing libraries to use. Easily installable via ! Downsides: Very few, except that Python overhead is incurred every time a function is called, so one has to get used to avoid calling same function over and over in a loop. Exciting stuff from Continuum Analytics developer team! Basically a JIT compiler (not unlike PyPy) but for numeric code instead. Can speed up code, according to their claims. My use cases: Matrix math and numeric code. Upsides: Uses the CPython interpreter that we\u2019re all used to. Speedups gained by simply adding a function decorator. Also easily installable by ! Downsides: Doesn\u2019t work with Python objects very well right now. Cython Only for the brave of heart! A simple way to continue writing Python-ish code that then gets compiled to C, which is blazing fast. My use cases: Not sure yet. It is good, don\u2019t get me wrong. Upsides: I\u2019ve done a head-to-head vs. Cython version of what I\u2019m trying to do. 25X speed-up. And again, easily installable by . (It\u2019s probably clear by now that I\u2019m a Continuum Analytics fanboi.) Downside: Have to learn a new-ish language with some intricacies in order to really get the massive speed-ups. I initially could only write Cython code that didn\u2019t break, but would end up with little speed-up (1-1.5X max). Going faster needed a lot more tweaking, probably lots more practice. Bonus! How did I pick up each of these? It was some mixture of following online tutorials, trying to modify examples myself, reading Stack Overflow, and coding with another advanced person who was willing to teach me stuff. I found the last one to be the most effective of them all, as the back-and-forth is great for learning. Hope this post helps you decide what to try next for speeding up your code!",
    "tags": [
      "python",
      "programming",
      "optimization"
    ],
    "pub_date": "2016-06-07",
    "type": "blog"
  },
  {
    "id": "blog-the-pycon-ers-guide-to-the-hallway-track",
    "url": "/blog/2016/6/3/the-pycon-ers-guide-to-the-hallway-track/",
    "title": "The PyCon attendee's Guide to the Hallway Track",
    "summary": "This year at PyCon 2016, totally inspired by DB, I did the \"hallway track\". What's the hallway track, you say? Basically, it means spending the...",
    "body": "This year at PyCon 2016, totally inspired by DB, I did the \"hallway track\". What's the hallway track, you say? Basically, it means spending the majority of my time in the hallways of the conference, and doing the following: 1. Taking time to do your own thing: I rebuilt my personal website on a cheaper host and with a static site generator. 2. Talking with other people and connecting with them. I learned a ton about what other people do. 3. Catching up with familiar faces and strengthening acquaintance connections. I caught up with Travis (Continuum Analytics CEO) and his team (Christine & Matt), Allen (whom I helped TA his tutorial), Giles (who helped me kickstart half my thesis) and en zyme (the professor who continues to connect me with interesting people). I did an extreme thing, which was that I missed every single talk, including the keynotes, and didn't go for anything. In retrospect, I wouldn't recommend doing that, for the following reasons: 1. As a 3rd year PyCon attendee, it still makes sense to have heard a few talks so that there's a common topic to talk about with newcomer attendees. A few times I was caught off guard not knowing what else to talk about. 2. For newcomers, I'd still recommend listening in on the talks. They're immensely educational! Like Ned Batchelder, I have come to believe that everybody has a talk inside of them as well; listening in on how other speakers did their thing can give you inspiration to talk about how you did your thing. Having attended a number of conferences over the years, PyCon remains my favourite community conference, with SciPy a very, very close second. The difference is just that I learn a broader range of useful things at PyCon than at SciPy; otherwise, the people are amazing in both conferences. Hope to be in PDX again at PyCon 2017!",
    "tags": [],
    "pub_date": "2016-06-03",
    "type": "blog"
  },
  {
    "id": "blog-new-website",
    "url": "/blog/2016/6/1/new-website/",
    "title": "New website!",
    "summary": "Having learned more about the web dev/software engineering world over the past few years, I have become more curious about static sites.",
    "body": "Having learned more about the web dev/software engineering world over the past few years, I have become more curious about static sites. Speed and security were my main concerns; WordPress has given me issues in the past. I heard quite a bit about the use of static site generators, such as Pelican, Jekyll and such, but they were relatively more effort to work with than I'm used to. This year, I heard about Lektor, which is a static site generator written in Python, written by the guys who write Flask as well, but I couldn't figure out how to use it properly. On day 2 here, I finally had a lesson from David Baumgold on how Lektor works, and I was sold. So I decided to migrate over! I fired up my own DigitalOcean droplet, bought a new test domain, and made this new site. Overall, it definitely runs faster than my old site. DO runs on SSDs, and the site is static which eliminates the need to query a database. Looking forward to continuing to use it! Slowly, I will be migrating older posts into here.",
    "tags": [],
    "pub_date": "2016-06-01",
    "type": "blog"
  },
  {
    "id": "blog-dividing-people-into-small-groups-with-python",
    "url": "/blog/2016/5/31/dividing-people-into-small-groups-with-python/",
    "title": "Dividing People into Small Groups with Python",
    "summary": "In our Bible Study small group, I have found through empirical observation that when the group size is large (\\>5 people) and homogeneous (all...",
    "body": "In our Bible Study small group, I have found through empirical observation that when the group size is large (\\>5 people) and homogeneous (all guys/girls, all believers, all Bible study leaders), Bible study tends to be either too flat or too chatty, too boring or too distracted, and all-round just not beneficial for learning. On the other hand, when the group is small (3-5 people) and diverse (guys & girls, baptized + seekers together, counsellors spread out), learning takes place. (Outside of Bible study groups, I find this to be true anyways, and doing it this way prevents the formation of exclusivist insider-groups.) It\u2019s challenging to do this division by hand though, as there can always be subtle biases that creep in. So I decided to use a bit of information theory and Python to do this division in an unbiased fashion. The result? My own hand-crafted small group web app that keeps track of group members in a larger group, and uses a simple genetic algorithm for shuffling them into optimally diverse groups of people. The data categories used are simple, and by no means do I use this to \"categorize\" people for privileges, they\u2019re only used for assigning responsibilities in the group. We use gender (M, F), faith status (baptized, believer, seeker, unknown), and role (facilitator, counsellor, none). The algorithm essentially works as such: 1. Determine the number of small groups to keep the group size within 3-5 people. 2. Randomly distribute individuals across the groups, by first distributing the facilitators, and then everybody else. 3. Until max number of tries has been reached: 1. Scoring Step: Compute Shannon entropy within each group, and sum up Shannon entropy scores across each category, across all groups. 2. Proposal Step: Propose to swap two random individuals. 3. Comparison Step: Compute new Shannon entropy score under the swap. If it does not decrease Shannon entropy and passes the \"exclusion criteria\", accept swap. Else, pass. 4. Return all the small groups. A note on the comparison step: In other algorithms I\u2019ve seen, acceptance is conditional if and only if the score (Shannon entropy) is increased, but in this case, not decreasing is \u2018good enough\u2019. I have my engineer hat on. I added a way to include \"exclusion criteria\", such as the scenario where it would be inappropriate to put two people in the same group, for example, where there is a simmering conflict in the midst, or where the relationship between the two could be distracting to learning. Right now, that functionality is baked into the back-end, but I am designing an implementation to make it accessible through the front-end. The web app is written in Python, and uses only two packages: (link) and (link). Front-end uses HTML with (link) templating and Bootstrap CSS. I wrote the GUI using because I didn\u2019t need fancy stuff that Django offered, and was simple enough for me to run locally. I opted for only because it was an even simpler, lightweight version of a database (as a JSON) file, and was sufficient for what I needed too. Of course, I\u2019m quite sure this can be re-implemented in Django/SQLite, and made infinitely more fancy :). The code for is available on GitHub, along with instructions for how to use it. Enjoy!",
    "tags": [],
    "pub_date": "2016-05-31",
    "type": "blog"
  },
  {
    "id": "blog-pycon-2016-tutorials-days-1-and-2",
    "url": "/blog/2016/5/29/pycon-2016-tutorials-days-1-and-2/",
    "title": "PyCon 2016 Tutorials (Days 1 & 2)",
    "summary": "In a flash, the PyCon 2016 tutorials are over! My session on network analysis was on the first day, in the morning.",
    "body": "In a flash, the PyCon 2016 tutorials are over! My session on network analysis was on the first day, in the morning. Overall, things went smoothly, and because of the competency level of the class, I was able to cover all of the material, including the ones that we usually don\u2019t have enough time to get to (computational statistical inference on graphs, and bipartite graphs). Most of the time, at the end of the workshop, I hear feedback on how to improve specific material, and details on what was new or useful for the participants. However, this time round, there was little of it. I was initially a bit disappointed, as I usually find ways to use the feedback to decide what to tweak for the next iteration. Later, over lunches and coffees (or in other tutorials), some participants did share their thoughts and feedback, and it was overall positive. Last night, I also shared some of these thoughts with David Baumgold (who led a Git tutorial) over a group trip to Powells, and that was a nice cheer-up as well. I learned a bit about Lektor from David. Seriously thinking about moving my personal site out of WordPress and into Lektor, and off from BlueHost and onto DigitalOcean. Speed, cost, and customizability what I\u2019m really thinking about right now. Speaking of Powells: that bookstore is big! I had to ask for a bit of help to find the \"science\" section: Me: \"Hi! I\u2019m looking for books in the sciences. Where should I go?\" Staff: \"Hmm, did you mean \u2018science fiction\u2019 or the \u2018hard sciences\u2019?\" Me: \"Ah yes, I meant the \u2018hard sciences\u2019. I\u2019m a \u2018hard scientist\u2019 myself.\" They had books from conference proceedings, \"open problems in computational sciences\", deep physics books\u2026 I was wowed, but didn\u2019t buy anything; I ended up getting two books on minecraft instead. :P (They\u2019re not for me, they\u2019re for a colleague\u2019s son.) I also saw Panic\u2019s sign - you can actually control the colour of their sign through a web app! Totally agree with David - that corner of Portland is one \u2018magical\u2019 corner. On the second day, I decided to help out Prof. Allen Downey with his tutorials. I know Allen through the Boston Python User Group and from being a PyCon tutorial instructor before. His tutorials are always fun, hands-on, entertaining, and most importantly, a chance to learn something new. I like his philosophy too - leveraging the very practical skill of computation to learn more abstract things like statistics. He led two tutorials, one on Bayesian statistics and one on Computational statistics. Highly recommend attending his tutorials at PyCon! It just so happened that my allergies flared up today as well. Two people, one an attendee and one an AV staff member (Jacob), offered ibuprofen to help deal with the general discomfort. Much kindness shown here. Looking forward to the next few days of talks. Keeping the learning going!",
    "tags": [
      "pycon",
      "conferences",
      "python"
    ],
    "pub_date": "2016-05-29",
    "type": "blog"
  },
  {
    "id": "blog-odsc-east",
    "url": "/blog/2016/5/22/odsc-east/",
    "title": "ODSC East",
    "summary": "I had a ton of fun delivering a workshop on network analysis fundamentals at ODSC East yesterday! This is my bullet-point journal version of my...",
    "body": "I had a ton of fun delivering a workshop on network analysis fundamentals at ODSC East yesterday! This is my bullet-point journal version of my thoughts over ODSC East. 1. Learned a ton from Bang Wong (of the Broad Institute) and Mark Schindler (of GroupVisual) about DataViz &amp; User Experience (DVUX). 1. Didn\u2019t expect that the workshop would be over-subscribed! I was expecting the topic to be a bit more niche. Lots of kind tweets and feedback. Material are all available on GitHub. 3. Invited to contribute content to DataCamp on network analysis. Timeline approximately Fall 2016 or Spring 2017. Strongly considering it. 4. Talked one-on-one with a manager in the Facebook infrastructure data science team. FB gets a lot of stick for privacy reasons; after this talk, I realize they have bigger, altruistic plans that rarely get talked about. The short story is that there always some degree of tradeoff, and it sometimes takes a company amassing resources in order to do things that require a big jump rather than incremental improvements. 5. I like Dask, great talk by Matthew Rocklin (slides). Time to try it out. 6. Great to see biological applications featured at ODSC, especially on Sunday. Neglected tropical diseases and big microscopy analysis. the tweets: The tweets are archived here. It'll serve as my \"feel good\" memory stack if I ever need to return to it. great workshop on #networkanalysis by @ericmjl &amp; Open Data Science Conference (ODSC) @odsc #odsc \u2014 dan addyson (@DataSawr) May 21, 2016 Awesome Network Analysis workshop by @ericmjl at #ODSC \u2022 https://t.co/pCvbT8OhTZ #DataScience \ud83d\udd78 pic.twitter.com/HTfulTIcFF \u2014 Anoush Najarian (@anoushnajarian) May 21, 2016 Network Analysis Made Simple: @ericmjl posted his Python tutorial online at https://t.co/6gqDGofLDx #ODSC #Python \u2014 Sharon Machlis (@sharon000) May 21, 2016 'Network-Analysis-Made-Simple' an informative &amp; simple talk from @ericmjl on @odsc day 2 https://t.co/VFxyHK3K4o #ODSCBoston \u2014 Nilufer Polat (@oksitosin) May 22, 2016",
    "tags": [
      "odsc",
      "conferences",
      "data science"
    ],
    "pub_date": "2016-05-22",
    "type": "blog"
  },
  {
    "id": "blog-new-funding-from-the-broad-next10",
    "url": "/blog/2016/5/20/new-funding-from-the-broad-next10/",
    "title": "New funding from the Broad Next10!",
    "summary": "Some good news: funding to try out a new idea :)",
    "body": "(As is now become somewhat habitual, I\u2019m reporting a week late to get some clarity in thought.) Really humbling and yet exciting week last week. With my colleagues Tony and Jared (Blainey lab) at the Broad, we won a $40,000 Broad Next10 (Bn10) grant to conduct exploratory and hopefully \"catalytic\" experiments to develop influenza polymerase phenotyping assays that can be done at scale and at low cost, with the stretch goal of making it plug-and-play for other viral polymerases. We also won another $40,000 Bn10 grant scale the phenotyping of influenza neuraminidase drug resistance to oseltamivir (a.k.a. tamiflu). It\u2019s humbling because finally there\u2019s a team of people who think these ideas are worth taking a risk on, and are willing to take a quantifiable $80,000 (total) gamble on it. It\u2019s also an exciting time, because I have been working on the (cheaper) computational side of things for a while, and I have become convinced that endless optimization of the computation cannot beat simply having better data measured, and this funding enables us to run some experiments towards scalably generating that data. We have one year to accomplish this goal, and we are planning to treat this money as \"accelerator\" money to get a minimum viable prototype out and ready.",
    "tags": [
      "academia",
      "feel good"
    ],
    "pub_date": "2016-05-20",
    "type": "blog"
  },
  {
    "id": "blog-pyflatten-a-package-for-flattening-nested-data-structures",
    "url": "/blog/2016/5/16/pyflatten-a-package-for-flattening-nested-data-structures/",
    "title": "PyFlatten: A package for flattening nested data structures",
    "summary": "Yesterday, I released [PyFlatten][pyflatten] to PyPI - it's a utility that can flatten nested data structures (e.g.",
    "body": "Yesterday, I released [PyFlatten][pyflatten] to PyPI - it's a utility that can flatten nested data structures (e.g. list of lists; dictionaries of lists of tuples) into a single 1-by-N vector, while also returning an 'unflattener' function that can restore the original data structure from the flattened version. [pyflatten]: https://pypi.python.org/pypi/pyflatten/ The [source code][pyflatten] are available on GitHub, where I make clear that I can't take credit for writing the code - I can only credit myself for factoring it out of [autograd][autograd] for others to use. The real heroes are [David Duvenaud][david], [Dougal Maclaurin][dougal], and [Matt Johnson][matt] of the [Harvard Intelligent & Probabilistic Systems][hips] group. With David's permission I am releasing it for public use. [autograd]: https://github.com/HIPS/autograd [david]: http://people.seas.harvard.edu/~dduvenaud/ [dougal]: http://users.physics.harvard.edu/~maclaurin/ [matt]: http://people.csail.mit.edu/mattjj/ [hips]: http://hips.seas.harvard.edu Hope it comes in handy for whatever your project is!",
    "tags": [],
    "pub_date": "2016-05-16",
    "type": "blog"
  },
  {
    "id": "blog-recent-papers",
    "url": "/blog/2016/5/11/recent-papers/",
    "title": "Recent Papers",
    "summary": "Finally, after ~2 years of learning, working, collaborating and writing, there's been a slew of papers from our research lab going out.",
    "body": "Finally, after ~2 years of learning, working, collaborating and writing, there's been a slew of papers from our research lab going out. Really happy to have finally contributed to our collective scientific knowledge, both through my own efforts and through working with others. Here's the list of papers, and I'm absolutely thrilled to have contributed to the scientific story in each of those: Ma, E. J., Hill, N. J., Zabilansky, J., Yuan, K. &amp; Runstadler, J. A. Reticulate evolution is favored in influenza niche switching. Proc. Natl. Acad. Sci. U.S.A. 201522921 (2016). doi:10.1073/pnas.1522921113 [link] Hill, N. J. et. al. Transmission of influenza reflects seasonality of wild birds across the annual cycle. Ecology Letters (2016). [just accepted! not yet available] Bahl, J. et al. Ecosystem Interactions Underlie the Spread of Avian Influenza A Viruses with Pandemic Potential. PLoS Pathogens 12, e1005620 (2016). [link] Hussein, I. T. M. et al. A point mutation in the polymerase protein PB2 allows a reassortant H9N2 influenza isolate of wild-bird origin to replicate in human cells. Infection, Genetics and Evolution 41, 279\u2013288 (2016). [link] Hussein, I. T. M. et al. New England harbor seal H3N8 influenza virus retains avian-like receptor specificity. Scientific Reports 6, 21428 (2016). [link] Bui, V. N. et al. Genetic characterization of a rare H12N3 avian influenza virus isolated from a green-winged teal in Japan. Virus Genes 50, 1\u20135 (2015). [link] Okay, so the next question is - are you graduating? To which I will respond: :-) Jokes aside, I'm initiating the discussions with my advisor &amp; committee now. The end is in sight!",
    "tags": [],
    "pub_date": "2016-05-11",
    "type": "blog"
  },
  {
    "id": "blog-scipy-2016-financial-aid-committee",
    "url": "/blog/2016/5/11/scipy-2016-financial-aid-committee/",
    "title": "SciPy 2016 Financial Aid Committee",
    "summary": "This year, I had the privilege of serving on the SciPy (Scientific Python Conference) 2016 financial aid committee, and I will be headed to Austin,...",
    "body": "This year, I had the privilege of serving on the SciPy (Scientific Python Conference) 2016 financial aid committee, and I will be headed to Austin, TX to present a tutorial on on fundamental and statistical network analysis. In some ways, put in its most basic terms, being on the FinAid committee was really about finding the best ways to spend somebody else's money - or else better known as \"stewardship\". We will be releasing a document, at first internally to this year's organizing committee chairs (Aric &amp; Prabhu) on how we did the selection, with the final goal of getting their approval for releasing it publicly for documentation, as well as a suggestion for next year's committee. I've already received some emails from scholarship recipients expressing their thanks for being selected. As one who was in the same spot last year, I'm really happy to pay it forward this way. Looking forward to meeting them in Austin!",
    "tags": [
      "scipy",
      "conferences",
      "python"
    ],
    "pub_date": "2016-05-11",
    "type": "blog"
  },
  {
    "id": "blog-lessons-learned-from-publishing-the-reassortment-paper",
    "url": "/blog/2016/4/7/lessons-learned-from-publishing-the-reassortment-paper/",
    "title": "Lessons learned from publishing the reassortment paper",
    "summary": "The paper can be found here (preprint, freely available; accepted at PNAS and in press). In no order of importance, here are the things I would tell...",
    "body": "The paper can be found here (preprint, freely available; accepted at PNAS and in press). In no order of importance, here are the things I would tell myself to do from the start. Lesson 1. Computational work requires simulated data. Creating simulated data is paramount. Just as writing an idea down for an audience forces out the details, creating simulated data for an algorithm forces out the assumptions. Lesson 2. Use pre-submission inquiries! Don\u2019t waste time formatting papers for journals. Write it up, write the abstract, and use pre-submission inquiries to rapidly iterate over the journals that are likely to accept or reject the paper. Lesson 3. Use pre-print servers. Scientists funded by public money should have their work disseminated back to the public in due time. Put written work on pre-print servers; Knowledge Without Barriers is worth it! In fact, it may even pay off to be radical, and write the whole paper in the open. Lesson 4. Dare to try for a wide audience. If not for Jon\u2019s encouragement, I might not have had the guts to try for the broad readership journals. In most cases, I know it won\u2019t pay off; in this case, I\u2019m thankful it did. Lesson 5. Writing with clarity is difficult. Crafting the scientific narrative around the data was a difficult iterative process. It was difficult taking that step of chopping off a ton of derivative data that did not contribute to the scientific insight being conveyed, but in the end, I think it was necessary. Lesson 6. The non-linear path in science is real. Israeli scientist Uri Alon described the non-linear path that a scientist takes. At the outset, I first thought of the scientific narrative to this project as being \"a better reassortment finding algorithm\", and that\u2019s where and how I focused my efforts (small datasets, simulations). It later changed to \"here\u2019s the state of reassortment in the IRD\", and that was reflected in expanded data scope and optimizations (hacks, really) to work with computing clusters and large datasets. But only later I realized the really exciting problem we were solving was \"quantifying reticulate evolution importance in context of ecological niche switching\", a broadly general problem with great basic scientific interest (if nonetheless lacking in public health importance). Therein lies the tension of all creative work. One needs to convince people of one\u2019s direction early on. Yet, later on, the direction may pivot, and one needs to be ready for that, and to convince stakeholders in it. Lesson 7. A first-draft template for scientific project management. I now think of it as a \"folder\" of stuff that should be kept together, version controlled, and done openly. This is just a first draft, still evolvable. 1. Data - everything collected and used, in its raw-est form available. 2. Code - for processing data, and for generating figures. 1. New software packages should be kept as a sub-folder, isolated from the rest of the work. 1. Software packages, for the scientist, are essentially code written that provide functions written to do the similar stuff over and over. 1. Protocols - for experimental work. 1. Manuscript - written openly as the work progresses. Lesson 8: The transferrable things a graduate student has to learn. By this, I mean stuff beyond the \"good\u2019ol do-your-experiments/write-your-code properly\". This is an incomplete list, hoping to expand it further. 1. Prioritizing time to get the most important stuff done, not merely being efficient in crossing off todo lists. 1. Saying \"no\" to good stuff, to leave time to say \"yes\" to the best stuff. (same as the above point, really.) 1. Learning how to craft a narrative that ties data together logically, and connects the data to a specifically important problem. 1. Leveraging others\u2019 strengths to achieve common goals, and expanding one\u2019s own strengths. 1. Learning how to learn and apply new things really quickly. (Something I\u2019d like to expand on later.) Lesson 9: Keep writing. Ultimately, the end product of our work is a written piece. Therefore, early on, start writing the narrative as an abstract. Get the narrative out there. And then rewrite and expand on the narrative until it is self-coherent, coherent with the data, and connects to an important problem. Lesson 10: The problem space is infinite. I don\u2019t believe that scientific competition is healthy. The problem space is infinite; the narrative space is as well. Getting scooped is not something to be worried about. Work openly, and rapidly advance your work on hand.",
    "tags": [],
    "pub_date": "2016-04-07",
    "type": "blog"
  },
  {
    "id": "blog-the-back-story-to-our-publication-on-influenza-reassortment",
    "url": "/blog/2016/4/7/the-back-story-to-our-publication-on-influenza-reassortment/",
    "title": "The back-story to our publication on influenza reassortment.",
    "summary": "For the reader of a newly-published article, all that we see is precisely that - the article itself. We rarely get to hear about the back-story of...",
    "body": "For the reader of a newly-published article, all that we see is precisely that - the article itself. We rarely get to hear about the back-story of that paper, or the choices that were made, the struggles involved, and the emotional ride taken. I thought I'd take the time to document what the back-story to our manuscript on influenza reassortment was like. Hopefully, it'll let other junior trainees know that nobody's alone in the struggle. Now, where shall I start...? I think the best part to start is when we first conceived the idea. My advisor, Jon Runstadler, had just taken me on. Because I joined only at the end of my 2nd year, I had some catching up to do. The problem of influenza reassortment had started to catch my eye. From my reading of the literature, reassortment was implicated in all known pandemics. The problem of predicting reassortment sounded like something impactful (in the sense of being practical, not prestigious) that I could dedicate my time to. I also happened to learn about the Influenza Research Database, which housed a large dataset of sequenced influenza genomes. I thought, \"How great might it be if we could identify every reassortant virus in the database? We could possibly predict the next pandemic!\" In retrospect, it's a naive idea to think that we can identify every single one of them accurately and use that data alone to predict pandemics. But my lack of knowledge on phylogenetics, reassortment, and influenza biology gave me enough ignorance to knuckle on. (I happen to believe that sufficient ignorance is necessary for creative breakthroughs.) Fast-forward two years later, the following things happen: Committee Meeting 1: I get a lecture from my committee about the importance of simulated data for a computational study. I was a complete newcomer to computational research at that point, so it was a sorely needed reminder. October 2013: Dr. Justin Bahl, an expert in the use of Bayesian phylogenetic methods, gives us a masterclass on how to use BEAST, and I bounce some of my ideas off him. Huge learning opportunity! December 2013: After hundreds of hours of debugging, I hack some d3.js code to visualize reassortment! We get some nice bouncy reassortment + clonal transmission trees. And then I vow to avoid Javascript as much as possible... June 2014: After hundreds of hours of debugging, I manage to reproduce, using our reassortment detection method, known reassortant viruses. Hooray! September 2014: After hundreds of hours of debugging, I finally manage to get my code running on a download of the IRD. And then I figure out that I need better software engineering skills... December 2014: With little debugging needed, we manage to show that the method we're using essentially is a very, very good approximation to a phylogenetic reconstruction. Ooh yeah! February 2015: Jon suggests that I start writing up a manuscript. I re-run the code on a fresh download of the database, with some refactoring done, and it still works! At this point, I still don't know what the story is... June 2015: After some exploratory analysis, I realize that the dataset on hand and the identification of reassortant viruses is an awesome grounding for an ecological study of influenza evolution. That becomes the story that we write up. Let the Submissions Begin As I was writing, I had in mind a number of target journals for the paper. Because of the nature of the story, and balancing the desire to go open access, I had a vague shortlist that included a group of microbiology, ecology, and genetics journals. But when I discussed it with Jon, the first place we were going to submit the article to was revealed to me. Nature. Alrighty... The stakes are raised! I go about formatting the paper for Nature. It takes about 1 month to check that everything is in order, including the cover letter, the figures, the Jupyter notebooks. One afternoon, in Jon's office, we submit the paper together. 11 days later, Nature editorially declines the manuscript. Looking back on Jon's forwarded email to me, it's the best words I could have heard given that this was the first rejection manuscript rejection letter I had ever received. \"Well, it's their loss.\" In the declination letter, the Nature editors inform us that a new journal, Nature Microbiology, is in the process of being setup. Since it's a convenient option, and the editors suggested it, we go ahead with the option. We have a quick discussion on a backup journal. I suggest eLife, because they're a new journal, is open-access, has broad readership, supports new researchers (e.g. with reference letters), doesn't have nit-picky formatting requirements, and all-in-all had a great up-and-coming reputation. We agree to go with that if Nature Microbiology rejects. 8 days later, Nature Microbiology editorially declines the manuscript. I spend another week or two reformatting for eLife, and getting everything in order. 4 days later, eLife contacts us! Their editors actually read through",
    "tags": [
      "science",
      "graduate school",
      "influenza",
      "peer review"
    ],
    "pub_date": "2016-04-07",
    "type": "blog"
  },
  {
    "id": "blog-life-after-science-one-a-journey-in-computation-and-creativity",
    "url": "/blog/2016/3/29/life-after-science-one-a-journey-in-computation-and-creativity/",
    "title": "Life after Science One: A Journey in Computation and Creativity",
    "summary": "Some reflections on my journey since finishing the Science One first-year program at UBC. It's been a winding journey to where I am now.",
    "body": "Writer's note: This blog post was written for the class of Science One 2015/2016, 9 years on after my own experience in Science One 2006/2007. My classmate, Jacob Bayless, is giving a talk to them titled \"Life After Science One\", and reached out to me for some perspectives on learning computation. Here's my piece, for him, and for this year's class and beyond. 10 years ago, I joined UBC as a student in the Science One program. That year was a fun year, and one of the best educational experiences I've had. During Science One, we learned to think integratively across disciplines. For example, we saw how order of magnitude estimation, a common tool in physics, could be applied to ecology, biochemistry, and thermodynamics. As another example, we learned about the application of ordinary differential equations to ecology and immunology in predator-prey systems. That way of thinking - by bridging disciplines and meshing ideas - is something I've re-discovered, re-encountered, and re-applied over and over in my research career to date. It doesn\u2019t stop, even after Science One. There was something I wish was emphasized back in my year, which you all now have the privilege of learning: computation and [computational thinking][compthink]. [compthink]: https://en.wikipedia.org/wiki/Computational_thinking While at UBC, I did some quantitative classes, including multivariable calculus, introductory programming (Java was all the rage back then), and statistics, but nothing more than that. By the end of my undergraduate training, I was thoroughly trained in molecular biology, but utterly helpless with programming ecosystems. I only knew how to transcribe and translate DNA sequences in Java. Later, I transitioned into doing computational work during my PhD. I was ending my 2nd year in the MIT Biological Engineering department, and in search of a good topic to work on for my thesis. I was a 2-month old Pythonista (this is what Python programmers call ourselves, so you all are Pythonistas now!) at that point, and I was teaching myself Python to improve cloning workflows by automating PCR primer design. On the recommendation of a friend, I checked out the Boston Python meetup group. There, I met Giles, a software developer with a gene synthesis company, and a former [Broadie][broadie]. I told him this idea I had to classify all of the internal genes of the influenza virus. Looking back, it\u2019s a bad idea to try attempting this for a thesis project, but Giles and I were both naive enough about the problem that we set about talking through it. I drew him a matrix, he came back with an idea, we Googled stuff up, I came up with an idea, and drew another thing. [broadie]: http://www.broadinstitute.org Rinse, wash, repeat. It was such an energizing and exciting time! Giles looks at our ideas, and says, \"I think you need a clustering algorithm. Try\u2026 [affinity propagation][ap]. It\u2019s a relatively new one, but nonetheless has a mature implementation available in Python. Search [][sklearn] for it.\" In effect, he was asking a 2-month old Pythonista to do machine learning in Python. Well played, Giles; fast forward a few months, next thing I knew, he had effectively kickstarted the groundwork for my thesis, which would eventually evolve into a [computational study][biorxiv] of influenza\u2019s capacity for reticulate evolution (through reassortment) and its importance in switching hosts (or ecological niches). My advisor Jon, though not a computationally trained person himself, trusted me with the freedom to learn, fail, and create under his mentorship, and I am hoping that in due time, we can reap the fruit of this trust. [ap]: http://science.sciencemag.org/content/315/5814/972.full [sklearn]: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.AffinityPropagation.html [biorxiv]: http://biorxiv.org/content/early/2016/03/22/033514 We ended up never using machine learning in that paper above, but a few things happened that got me deeper into computation. My experience working with the scikit-learn library piqued my interest in machine learning tools as applied to biology. I learned about network analysis through Allen Downey\u2019s book, \"Think Complexity\", and incorporated it as my main modelling tool as well. I went to PyCon 2014 and 2015 (in Montreal), giving tutorials on data analysis and network analysis. While on conference, I also learned a ton as well about how to use the scikit-learn libraries, and good practices in the software development world that could be used in the research world. This year, I will be at PyCon 2016 giving a [tutorial][pycon] on statistical network analysis as well, while also continuing the learning journey in the Python and data science worlds. The learning journey continues; I soon discovered for myself that scikit-learn alone wasn\u2019t enough, and I\u2019ve started learning the internals of deep learning from a [research group][hips] up at Harvard, with the goal of applying it to developing highly interpretable ",
    "tags": [
      "reflections",
      "science one",
      "academia"
    ],
    "pub_date": "2016-03-29",
    "type": "blog"
  },
  {
    "id": "blog-webinar",
    "url": "/blog/2016/3/22/webinar/",
    "title": "Webinar",
    "summary": "Today, I gave a webinar to the IRD and ViPR technical and advisory board meeting. There were a number of challenges to giving a webinar that I want...",
    "body": "Today, I gave a webinar to the IRD and ViPR technical and advisory board meeting. There were a number of challenges to giving a webinar that I want to reflect on here, as a note to my future self and others who may read this entry. In contrast to my previous entry, I'm choosing to write this down fairly soon after doing the presentation, so that I can remember the details clearly. In the lead-up to the webinar, I felt pretty well-prepared. I had done at least 3-4 rounds of narration on my own, so I was very familiar with the content, transitions, and flow. I also had a set of slides prepared for the Q&A session at the end. What I did not expect, but now have come to learn about, was the challenge of interacting with an audience that I can only hear but not see. Visual communication is important. During the webinar, because I could not see my audience, I made the mistake of perceiving audience questions as being communicated in an aggressive fashion, rather than in a neutral or friendly fashion. I went into a defensive posture, and tended to respond to questions with a lot of rambling detail rather than answering the question directly. I think this was a weakness in my presentation this time round. Jon's feedback at the end also also corroborated this. I also made the mistake of not being thoroughly prepared for the Q&A. The questions that were asked were questions that I had dealt with about 1.5 years ago, when I was in an earlier stage of the work I was presenting. However, because my focus now has shifted to other work, I was not prepared to tackle those questions again. I think at a bigger level, I didn't do the necessary \"mental model\" preparation for the talk, in which I should have rehearsed multiple \"what-if\" scenarios. (I've been reading Charles Duhigg's [Smarter Faster Better: The Secrets of Being Productive in Life and Business][amazon], and the 'construction of mental models' is a core concept illustrated in there.) [amazon]: http://www.amazon.com/Smarter-Faster-Better-Productive-Business-ebook/dp/B00Z3FRYB0 To get around this, I think the next time I do a webinar, I should make the following preparations: - Construct a mental model of the presentation as being a \"dialogue\", rather than a \"defence\". (I think it applies to all research presentations, really.) - A pen & paper to jot down the question as it's being asked. It's a tool to slow my mind down, to prevent myself from making snap-judgment assumptions about what is being asked. - Be prepared to paraphrase the question back to the questioner prior to responding. - Prepare for a broad spectrum of questions to be answered, not just those that were recently asked in my context. - Recognize that it's totally okay to have problems picked out during a presentation, even one that is more of a \"sales-pitch\" type. There's always a first, and this first webinar was definitely a big educational experience for me. Though I think I didn't perform top-notch in the Q&A portion, I hope I still left a positive impression on the IRD/ViPR team.",
    "tags": [],
    "pub_date": "2016-03-22",
    "type": "blog"
  },
  {
    "id": "blog-paper-revisions",
    "url": "/blog/2016/3/21/paper-revisions/",
    "title": "Paper revisions",
    "summary": "We've submitted back our [reassortment paper][paper] post-review. I've refrained from writing about this for about a week, mainly so that I can give...",
    "body": "We've submitted back our [reassortment paper][paper] post-review. I've refrained from writing about this for about a week, mainly so that I can give myself enough emotional distance to reflect on it. On the whole, I'm feeling very thankful to the reviewers for their highly constructive reviews. [paper]: http://www.ericmajinglong.com/2015/12/03/reticulate-evolution-and-microbial-ecology-2/ During the month-long review period, I was bracing myself emotionally for a thrashing review (esp. from a 3rd reviewer); after all I had seen such a fate befall two of my colleagues and a collaborator, all of whom wrote manuscripts on work that I was a part of. While there were constructive and legitimate criticisms in those reviewer reports, the number of comments that reflected more emotional ranting than reasoned argumentation left me baffled at the peer review process. (Aside: they mostly came from the fabled 3rd\u2026 and sometimes a 4th reviewer.) When I first went over the reviews I was stunned reading the reviews; in my meeting with my advisor Jon about the reviews, he quipped that his \"faith in the review process has been restored.\" Firstly, we didn't have to deal with the oft-fabled \"3rd reviewer\"; in the life sciences, the 3rd reviewer is often joked to be the one who shoots down a manuscript unreasonably. Secondly, the two reviewers gave us very encouraging and constructive comments. In retrospect, the reviewers comments were direct, thorough, and really helped us refine the scope of the claims we were making. (We may have overstepped the reasonable bounds of our claims in some places in the text.) Reviewer 1 was also very helpful in finding places where, because of familiarity with this manuscript, I had forgotten one chunk of text that should have been present. Reviewer 2 was overall very encouraging and supportive of the manuscript, and only raised points of clarification. So, to the reviewers, thank you for the reviews. If I'm given the chance to do so, I will pass on to the next paper the same qualities of the reviews you gave me (directness, thoroughness, being reasoned). At this point, after all the editorial rejections by other journals, I'm just happy to have the paper reviewed, have its flaws pointed out. My hope is that our response was satisfactory, and I'm hoping we have a fair chance at having the reassortment paper published!",
    "tags": [],
    "pub_date": "2016-03-21",
    "type": "blog"
  },
  {
    "id": "blog-flask-jinja2-bootstrapcss-and-thoughts-on-being-a-maker",
    "url": "/blog/2016/3/18/flask-jinja2-bootstrapcss-and-thoughts-on-being-a-maker/",
    "title": "Flask, Jinja2, bootstrap.css - and thoughts on being a maker",
    "summary": "I recently built a [front-end GUI][gui] for one of my projects, which is a [primer calculator][primer] for doing Gibson assembly with influenza...",
    "body": "I recently built a [front-end GUI][gui] for one of my projects, which is a [primer calculator][primer] for doing Gibson assembly with influenza segments. [gui]: http://github.com/ericmjl/flu-gibson-webui/ [primer]: http://github.com/ericmjl/flu-gibson/ (small detour) The back-drop to this is that 7 years after the invention of the Gibson assembly method in the synthetic biology world (where I used to be), influenza researchers are still stuck with restriction cloning. If they switched to the Gibson (and other seamless) assembly methods, their time wasted on troubleshooting restriction cloning could be drastically reduced. I wrote this simple utility to help bridge that gap. I had learned HTML many years back. I think I was still a teenager back then. So coming back to it now, I was both delighted and surprised to see the many new developments in the markup language. Anyways, I built the FluGibson web interface with two intents. Firstly, for it to be a quick-and-dirty, simple utility that an experimental influenza researcher could use in their day-to-day. The user provides a nucleotide sequence and a name for that DNA part (something that\u2019s convenient for the user to remember), and selects a standard plasmid backbone that the flu community uses. It returns a series of cloning primers and sequencing primers, and a PCR protocol for amplifying the DNA parts using the Phusion polymerase. Secondly, for it to be a prototype of a tool that I hope gets implemented in the [Influenza Research Database][ird], where a researcher can do a one-click computation (and maybe even ordering) of the primers needed to clone an influenza segment from cDNA. [ird]: www.fludb.org/ The FluGibson front-end is a Flask app, and as such runs in any modern browser. The backend is the FluGibson Python package that I wrote; it\u2019s still a bit incomplete in terms of the examples, but given time, I\u2019ll get those fixed up. Being a maker, rather than a consumer, is a very empowering thing. Being a maker enables me to build what I need to have built to do what I need to get done. It takes time to learn the skill, but in the end I think it pays off. I\u2019d like to encourage whoever\u2019s reading this blog, go be a maker. Go make stuff that you think has tangible value for the world. It\u2019s fun, it\u2019s emotionally rewarding, and may bring a financial return. :-)",
    "tags": [],
    "pub_date": "2016-03-18",
    "type": "blog"
  },
  {
    "id": "blog-science-and-applied-science-the-philosophies",
    "url": "/blog/2016/3/17/science-and-applied-science-the-philosophies/",
    "title": "Science and Applied Science - The Philosophies",
    "summary": "Random thought came to my mind today. The process of science is the careful observation, measurement, and falsification of hypotheses of the world.",
    "body": "Random thought came to my mind today. The process of science is the careful observation, measurement, and falsification of hypotheses of the world. It is immensely useful. IMO, a worldview that gives rise to genuine curiosity, meticulousness, and integrity is the only worldview that can give rise to science that is conducted at its best. When that fundamental worldview is absent, basic, curiosity-driven, exploratory science cannot be conducted. Applied science is the application of science to problems. Think engineering. Biological, chemical, mechanical, electrical engineering. Medicine. IMO, a worldview that seeks the holistic betterment of the human race is the only worldview that can give rise to applied sciences done in the pursuit of a better world. When that fundamental worldview is absent, applied science can only bring about the destruction of the planet, of all creation, of the human race. A person's worldview matters. Make sure that the fundamental lenses through which we view the world are correct, that they satisfyingly answer the question of origins, morality, purpose and destiny at the level of the emotions, the intellect, and the physical being. Absent this, take the worldview to its logical conclusions, and there can only be despair if it does not satisfy us in these ways.",
    "tags": [
      "philosophy",
      "science"
    ],
    "pub_date": "2016-03-17",
    "type": "blog"
  },
  {
    "id": "blog-tests-enabled-science",
    "url": "/blog/2016/3/15/tests-enabled-science/",
    "title": "Tests-Enabled Science",
    "summary": "In the software development world, I learned about the importance of writing tests for one\u2019s software.",
    "body": "In the software development world, I learned about the importance of writing tests for one\u2019s software. Since then, I have incorporated this habit in my own work, where as part of my more recent work, I write tests for the software I write to conduct scientific research. This has got me thinking about why tests are such an effective tool. I think there\u2019s got to be at least a few reasons. 1. Tests effectively are a contract between current and future selves. My current self is writing a contract that has to be enforced by changes made by my future self. If future self makes changes that break the contract, tests will catch them. Of course, this all assumes that the conditions under which the contract was written still hold true. If they change, then the contract can be broken. 1. Tests force my current self to be explicit about what exactly I am writing. This is much better than slapping together a script in a quick-and-dirty fashion. (Granted, there is a time and place for quick-and-dirty scripts.) 1. When done with (semi-)automatic testing frameworks, such as , the test suite is automatically run from start to finish. This reduces the cognitive load of running the tests one-by-one, and reinforces the cohesiveness of the code logic. Okay, so what exactly do I mean by tests? Here\u2019s a few thoughts. 1. Data integrity tests. By this, I mean tests that are situated in the directory where the data reside, that encode known properties of the data. For example: 1. Number of rows. 1. Number of columns. 1. The column names. 1. The hash of each row of data. 1. The hash of each column of data. 1. Unit tests. By this, I mean tests that are written that ensure that a function does exactly what it\u2019s expected to do. This is the most common concept of a test. This is applicable to: 1. Code that are written to manipulate data. 1. Code written that are part of a software package or software utility developed. While it takes time, I think computational scientists should write tests for their code as a matter of routine practice. Not sure if good test writing is enforceable, but it should definitely be done more.",
    "tags": [],
    "pub_date": "2016-03-15",
    "type": "blog"
  },
  {
    "id": "blog-r-for-statistics-python-for-data-processing",
    "url": "/blog/2016/3/13/r-for-statistics-python-for-data-processing/",
    "title": "R for Statistics, Python for Data Processing?",
    "summary": "I\u2019ve heard this refrain many times. However, the distinction never really made sense to me. R and Python are merely programming languages.",
    "body": "I\u2019ve heard this refrain many times. However, the distinction never really made sense to me. R and Python are merely programming languages. You don\u2019t have to do stats in R and data processing in Python. You can do data processing in R, and statistics in Python. What is R? It\u2019s a programming language designed by statisticians, and so there\u2019s tons of one-liner functions to do stats easily. What is Python? It\u2019s a programming language that\u2019s really well-designed for general purpose computing, so it\u2019s really expressive, and others can build tools on top of it. What is data processing? I don\u2019t think I can do justice to its definition here, but I\u2019ll offer my own simple take: making data usable for other programming functions. What is statistics? I think statistics, at its core, is really about describing/summarizing data, and figuring out how probable our data came from some model of randomness. That\u2019s all it is, and it\u2019s all about playing with numbers, really. There\u2019s nothing more than that. Technically, you can do statistics in any programming language, because technically, all programming languages deal with numbers... Which brings me to the point I want to make - as long as you have data, and you\u2019re doing data science, you technically can use any language for it; the differences are not in the language itself, but in the ecosystem, ease-of-use, and other aspects. Other bloggers have written about the benefits of using a single language, which include: No cognitive costs associated with syntax switching (the biggest reason for me). No need to worry about interfacing between languages. No need to pepper file I/O calls throughout the code for each portion of the project that uses its own language. So how do you choose? Here\u2019s my criteria (beyond simply Python and R): What is the larger context of your project? Do you have colleagues you are interfacing with? What languages are they using? Are there packages that allow you to do stuff related to your project in a quick-and-dirty fashion? Is the language able to use other language\u2019s packages? How mature is the package ecosystem for your language? Do you foresee using other packages as your project expands? So I hope the point is clear: it isn\u2019t so much that \"R is for statistics and Python is for data processing\". I think commoner programmers have to get that dogma out of their heads. It\u2019s simply that R was designed by statisticians for doing statistics, while an ecosystem of Python tools sprung up to do data processing. Nowadays, as the R-blogger mentioned, Python can also do a ton of stuff that R can do, because packages are being written that replicate R package functionality and more, such as how Continuum Analytics (awesome company!) wrote the bokeh package that allows us Pythonistas to deploy web-based data visualizations. Likewise, R can do a ton of stuff that used to be the domain of Python, and it's getting a lot of corporate support from Microsoft. So choose according to your needs. After all, as Wes McKinney wrote, the real problem isn't R vs. Python. It's the ability to move data seamlessly.",
    "tags": [
      "python",
      "data science",
      "statistics",
      "R"
    ],
    "pub_date": "2016-03-13",
    "type": "blog"
  },
  {
    "id": "blog-chalk-talk",
    "url": "/blog/2016/2/13/chalk-talk/",
    "title": "Chalk Talk",
    "summary": "Yesterday, I shook things up at the Broad Institute's Infectious Disease Program seminar series by doing a chalk talk instead of a slide deck presentation. It was less stressful to prepare for, the audience was engaged, and I got some valuable feedback. \ud83c\udfa4\ud83d\udd2c\ud83d\udc65 I've learned a lot from this experience and I think chalk talks should become the norm again for research presentations. \ud83d\udcdd\ud83d\udca1",
    "body": "Yesterday, I presented my work on influenza ecology and evolution at the [Infectious Disease Program][idp] seminar series at the [Broad Institute][broad]. Most of the talks I've seen there were Keynote or PowerPoint slide deck talks, and so this time round, I thought I might shake things up and do a chalk talk (or whiteboard talk) instead. The whirlwind that went through my mind after the talk meant that it's only today that, with a clear mind, I could find the time and space to write about it. [idp]: https://www.broadinstitute.org/scientific-community/science/programs/infectious-disease-program/infectious-disease-program [broad]: http://broadinstitute.org I think it was quite a fun experience. In terms of preparation, I found it to be much less stressful than doing a slide deck, because it meant I was not fiddling around with software trying to get proportions and alignments perfect. I gave myself two rounds of talking through the material on a blackboard as preparation as well. Finally, I put a set of sketches in my notebook as a reference during the talk (particularly because there was one computation toy example which I found a bit difficult to commit to memory). During the talk, the audience was quite encouraging, and I remember seeing a good amount of nodding of understanding (no droopy eyes), and there was a good amount of positive interruption from the audience for questions and such. After the talk, I also had the opportunity to interact with the audience, and get some feedback from my host, [Danny Park][dpark], on how it went. [dpark]: https://www.broadinstitute.org/bios/daniel-park-phd On reflection, there\u2019s some things I\u2019ve learned from doing my first public chalk talk: 1. It\u2019s much less stressful to prepare for. I\u2019m beginning to feel that slide decks are way too restrictive, both for preparation and delivery. 2. Danny commented that because of the nature of the chalk talk, where the speaker has to write down important points, the pace of the talk slows down to a much more comfortable pace for the audience. My colleague, Nichola, also mentioned the same point after the talk. 3. I think I could have done better with polishing the delivery. One example would be being less repetitive, which is something I notice happens quite a bit. I think chalk talks should become the norm again, especially for research talks, where the ideas are complex and the presenter is forced to give a simplified version of the logic behind an experiment or an analysis. [Imagine how big of an impact we could have if we didn\u2019t waste people\u2019s time with poorly made PowerPoint slides and poorly delivered talks][dance]! [dance]: https://www.ted.com/talks/johnbohannondancevspowerpointamodest_proposal?language=en",
    "tags": [
      "chalk talk",
      "public speaking",
      "research presentation",
      "influenza research",
      "broad institute",
      "seminar series",
      "audience interaction",
      "talk preparation",
      "slide decks",
      "whiteboard talk",
      "presentation skills",
      "feedback",
      "science communication",
      "infectious disease",
      "evolution"
    ],
    "pub_date": "2016-02-13",
    "type": "blog"
  },
  {
    "id": "blog-scikit-learn-tutorial",
    "url": "/blog/2016/2/9/scikit-learn-tutorial/",
    "title": "scikit-learn tutorial",
    "summary": "This past Monday, I led a hands-on session at the Broad Institute, showing how to use the scikit-learn API, as well as common coding patterns for...",
    "body": "This past Monday, I led a hands-on session at the Broad Institute, showing how to use the scikit-learn API, as well as common coding patterns for running machine learning algorithms on the data. First off, I was totally surprised at how many people signed up for the event - it \"sold out\" (tickets were free) within 3 hours of opening registration. I was quite floored - and I know it's not because I'm some famous dude who knows ML algorithms. Rather, it told me and my co-organizers that there is most certainly great demand here for this topic, and it should be run again. I found it to be a great opportunity to put some of my Software Carpentry/Data Carpentry tools to use. For example, we used sticky notes to indicate class progress (we used blue for \"all done\", and red for \"need help\"). I also tried to ensure that participants could walk away from the tutorial knowing how to do something that they could use immediately in their research. The examples I used (hosted openly on Github) were based on transforming sequence information into a sequence feature matrix, which is then fed into a selected machine learning algorithm. I think this seemed to suit the crowd, but I would love to use different examples as well, for example microbiome data or transcriptomics data. Feedback given by the participants was overall quite positive. To some, I could have explained things a bit more clearly, which I could recognize as an area I will need to improve on for delivering a second round. Others loved the hands-on instruction; in their feedback it was just the right delivery format for the workshop. One participant asked me, \"Why do you host these workshops?\" (I had done one on statistics with some fellow BE colleagues.) I hadn't thought much about that question, so my instinctive response was, \"For fun - I'm finding ways to share knowledge, for the benefit of the community.\" I stand by that thought, as I think it's important for knowledge-bearers to make copies of their knowledge, for the sake of giving back to the community. Yet, stemming from knowledge of myself, I also think that sharing programming knowledge around is an insurance policy against personal stagnation. Put in simple terms, if others know how to wield the tools that I know how to do, then I had better continue levelling up my skill to stand out. It also gets boring and sometimes frustrating person after a while being the \"go-to\" person for things ML-related or computing-related; it means I can't devote time to exploring new things. So sharing is fun, and is an insurance policy against boredom and frustration - why not share then? :) All in all, looking forward to reporting on the workshop at the next Broad NextGen meeting, and hosting another iteration at a later time!",
    "tags": [
      "data science",
      "software carpentry",
      "data carpentry"
    ],
    "pub_date": "2016-02-09",
    "type": "blog"
  },
  {
    "id": "blog-reticulate-evolution-and-microbial-ecology",
    "url": "/blog/2015/12/3/reticulate-evolution-and-microbial-ecology/",
    "title": "Reticulate Evolution and Microbial Ecology",
    "summary": "I am happy to announce that, with my advisor's (Jon Runstadler, MIT) approval, I've uploaded my first 1st-author paper to BioRXiv.",
    "body": "I am happy to announce that, with my advisor's (Jon Runstadler, MIT) approval, I've uploaded my first 1st-author paper to BioRXiv. This may sound surprising, to document and write about the paper pre-peer review, rather than after being formally accepted after peer-preview in a journal. This choice is borne out of two desires. Firstly, I am fed up with having the editorial process decide whether my work can be sent for peer evaluation, based on some perceived notion of broad impact or editorial alignment. This manuscript has gone through four editorial rejections and (hopefully not) counting\u2026 I acknowledge that \"4\" is a small number, but my patience and tolerance level for bureaucratic delays is very low, and I think the editorial process is a big reason delaying the work from review. After internal review and public presentation in talks and posters, peer review and evaluation is the final step in getting my work evaluated in the public domain. Really, it\u2019s about getting feedback on whether what I\u2019ve worked on constitutes sufficiently rigorous scientific work or not. I\u2019d like to know that earlier rather than later, and I do not believe that editors should be the gatekeepers of this process. Pre-print servers let me release the work to the public domain without tough barriers to break down, which help me achieve this goal. Secondly, I\u2019m really, like really, excited about the results in this paper, regardless of what others think about its impact. It\u2019s hard to contain this PhD candidate\u2019s enthusiasm for his work. I\u2019m particularly proud of the work done here, especially the code written from scratch to answer this question, with everything written and designed to be reproducible. Thus, much like an artist has a natural drive towards showcasing work s/he is proud of, I have this desire to get it out there. I hope you enjoy it. Manuscript in Brief Title: Reticulate evolution is favoured in microbial niche switching. Question being answered: Can we measure how important reticulate evolutionary processes that result in gene exchange are in enabling microbes to switch between ecological niches? tl;dr answer: Yes we can, and when switching ecological niches, the ability to exchange genes is more important the greater the difference between the niches. Definitions: I think some definitions are required before I proceed. - Reticulate evolution: A non-tree-like evolutionary trajectory. It involves processes like recombination, reassortment, horizontal gene transfer, sexual reproduction etc. In this paper, we detect reassortment amongst influenza A viruses. - Ecological niche: Ecology is the study of organisms and their interactions with one another and their environment. Organisms occupy certain niches - such as particular habitats, producing and consuming particular combinations of nutrients. In this paper, an ecological niche is defined as a particular host species of the influenza A virus. - Switching: When a microbe switches ecological niches, it has either entered a new ecological niche - different environment, a new set of host species it interacts with etc. In this paper, we define niche switching as switching between different hosts, as the host is the environment for the influenza A virus. Brief Background: In the field of ecology, there\u2019s this idea that gene exchange between two microbes can lead to genetic diversity, and lead to fitness advantages in new environments. To the best of my knowledge, this idea has not been tested explicitly (and I am happy to be corrected on this). In our paper, we sought to test whether, in switching between different host species, gene shuffling between influenza A viruses was more prevalent than clonal transmission. If so, it would allude to a broader principle that reticulate evolutionary events are important when microbes change ecological niches. Why should we care? Why should we care about this problem that, seemingly, only microbes have to deal with? Well, we know that microbes and human health intersect, and this is borne out in the scientific literature. Specifically with the influenza virus, every new pandemic virus that has emerged in human populations has been shown to be a reassortant virus. It looks quite clear to me that reticulate evolution intersects with host and microbe ecology, impacting human health. How did we answer the scientific question at hand? To do this, we used data from the Influenza Research Database, which houses influenza sequence data augmented with collection date and host species metadata. This makes it a really suitable dataset to answer the broader question, \"Is reticulate evolution is favoured in ecological niche switches?\". This is because of a few reasons. Firstly, it is a densely-sampled dataset with a global scope, which allows us to detect rare reassortment events. Secondly, we can trace source-sink paths through different host species, by using time and genetic data. Finally, influenza can undergo reassortment, which is its reticul",
    "tags": [
      "influenza",
      "graduate school",
      "peer review",
      "thesis",
      "data science"
    ],
    "pub_date": "2015-12-03",
    "type": "blog"
  },
  {
    "id": "blog-profiling-pypy-vs-python-for-agent-based-simulation",
    "url": "/blog/2015/11/28/profiling-pypy-vs-python-for-agent-based-simulation/",
    "title": "Profiling PyPy vs. Python for Agent-Based Simulation",
    "summary": "Outline 1. Introduction: 1. Motivation 1. Model description 1. Link to code 1. Environment Setup 1. Performance 1. Python vs.",
    "body": "Outline 1. Introduction: 1. Motivation 1. Model description 1. Link to code 1. Environment Setup 1. Performance 1. Python vs. PyPy on one parameter set. 1. Vary number of hosts, record time. Introduction As part of my PhD dissertation, I wanted to investigate the role of host ecology on the generation of reassortant viruses. Knowing myself to be a fairly algebra-blind person, I decided that an agent-based model (ABM) was going to be much more manageable than writing ODEs. (Actually, the real reason is that I\"m modelling discrete states, rather than continuous states, but yes, I will admit that I do take longer than your average programmer with algebra.) Model Description Starting with our intuition of host-pathogen interactions, I implemented a custom ABM using Python classes - \"Hosts\" and \"Viruses\". Viruses \"Viruses\" had two segments, representing a segmented virus (like the Influenza or Lassa virus), each with a color (red or blue), and can infect Hosts (which are likewise red or blue). Viruses that are of a particular color prefer to infect hosts of the same color, but can still infect hosts of of a different colour, just at a lower probability. If two viruses are present in the same host, then there can be, at some small probability, the opportunity for gene sharing to occur. One of the virus' segments determines host immunity; if the virus encounters a host which has immunity against its color, then the probability of infection drastically decreases, and it is likely that the virus will eventually be cleared. Hosts \"Hosts\" are where viruses replicate. Hosts gain immunity to one of the segment's colors, after a set number of days of infection. When a host gains immunity to a particular virus color, it can much more successfully fend off a new infection with that same color. Hosts also interact with one another. They may have a strong preference for a host of the same color, a.k.a. homophily. Code My code for the simulations can be found on this Github repository. The details of the simulation are still a work in progress, as these ideas are still early stage. My point on this blog post here will be to try to compare PyPy against CPython on performance. However, I do welcome further comments on the modelling, if you've taken the time to read through my code. Code for the statistical draws can be found on this other Github repository. Environment Setup My CPython environment is managed by conda. (Highly recommended! Download here. Make sure to get Python 3!) I installed pypy and pypy3 under my home directory on Ubuntu Linux, and ensured that my bash shell $PATH variable also pointed to ~/pypy[3]/bin. Performance Let's take a look at the performance of the CPython vs. PyPy using pure-Python code. Default parameters I first started with 1000 agents in the simulation, with the simulation running for 150 time steps. Under these circumstances, on an old Asus U30J with 8GB RAM and an SSD hard disk, Core i3 2.27GHz, executing the simulation with PyPy required only 13.4 seconds, while executing with CPython required 110.5 seconds. 10x speedup. Varying number of hosts in the model I wanted to measure the time complexity of the simulation as a function of the number of hosts. Therefore, I varied the number of hosts from 100 to 1600, in steps of 300. Partial (mostly because of laziness) results are tabulated below. (Yes, this degree of laziness would never fly in grad school.) Agents PyPy Trial 1 PyPy Trial 2 PyPy Trial 3 CPython Trial 1 CPython Trial 2 CPython Trial 3 1000 13.4 12.8 12.9 110.5 700 8.63 9.02 8.65 53.7 400 4.35 4.33 4.66 18.2 18.2 100 1.03 1.00 1.17 1.47 1.48 1.45 As we can see, PyPy wins when the number of iterations is large. Statistical Draws I use statistical Bernoulli trials (biased coin flips) extensively in the simulation. Yet, one thing that is conspicuously unavialable to PyPy users (in an easily installable format) is the scientific Python stack. Most of that boils down to numpy. Rather than fiddle with trying to get numpy, scipy and other packages installed, I re-implemented my own bernoulli function. python from stats.bernoulli import bernoulli from time import time start = time() berndraws = bernoulli(0.5).rvs(10000) mean = sum(berndraws) / len(berndraws) end = time() print(end - start) And for the CPython/scipy version: from scipy.stats import bernoulli from time import time start = time() berndraws = bernoulli(0/5).rvs(10000) mean = sum(berndraws) / len(bern_draws) end = time() print(end - start) Bernoulli Draws PyPy + Custom (1) PyPy + Custom (2) PyPy + Custom (3) CPython + SciPy (1) CPython + SciPy (2) CPython + SciPy (3) 1000000 0.271 0.241 0.206 0.486 0.513 0.481 100000 0.0437 0.0421 0.0473 0.0534 0.0794 0.0493 10000 0.0311 0.0331 0.0345 0.00393 0.00410 0.00387 As we can see, scipy is quite optimized, and outperforms at lower number of statistical draws. Things only become better for PyPy as the number of draws increases. Summary Some things that I've learned from this exercise: 1.",
    "tags": [],
    "pub_date": "2015-11-28",
    "type": "blog"
  },
  {
    "id": "blog-predicting-hiv-drug-resistance-phenotype-from-genotype",
    "url": "/blog/2015/9/28/predicting-hiv-drug-resistance-phenotype-from-genotype/",
    "title": "Predicting HIV Drug Resistance Phenotype from Genotype",
    "summary": "A blog post detailing how I built a model to predict drug resistance from HIV protease sequence.",
    "body": "Note to Reader: I\u2019d highly suggest reading this blog post on the left half of your screen, and have the Jupyter notebook on the right half of your screen. Makes things a bit easier to follow. I recently have been writing a proposal to conduct some experiments to predict viral RNA polymerase activity, in some standardized unit, from protein genotype. The main application of this would be to be able to conduct quantitative surveillance in a precise fashion. For example, with HIV, as treatment progresses, the virus accumulates mutations that confer resistance. For a physician treating a patient infected with HIV, would it be possible to determine, from sequence data alone, what would be the degree of predicted viral resistance for that patient\u2019s virus? Knowing fully that this problem has been tackled many times in the past from multiple angles, I wanted to know how easily I could set up an ML workflow to go from sequence to predicted drug resistance. The goal of this blog post is to document how easy it was for me to get up and running, using Python packages really well-written Python packages. Raw Code All of my code can be found on my Github repository. You can also jump directly to the Jupyter notebook that I reference code from in this article. Data Source and Preprocessing I sourced the data from the Stanford HIV Drug Resistance Database. Specifically, I downloaded the high quality, filtered set of Genotype-to-Phenotype mappings, for protease inhibitors, nucleoside reverse transcriptase inhibitors, and non-nucleoside reverse transcriptase inhibitors. I wrote a few custom functions to preprocess the data, including the following steps: 1. Replacing all \"\" characters with the consensus sequences. I am guessing that they use the \"\" character in place to help highlight where the mutations are; much more human readable. 2. Removing sequences that had more than one mutation present. Mostly a function of being lazy than anything else. 2. Removing sequences with ambiguous amino acids. These are a bit harder to deal with down the road. From biological background knowledge, it\u2019s unlikely that excluding them would be detrmimental. 3. Dropping all conserved amino acid positions. They add nothing to the analysis. 4. Binarizing the columns. This transforms the letters of the amino acid into a first-pass feature set, in which the binarized columns indicate whether or not an amino acid is present at a given position or not. These are found in my customfuncs.py module, which I imported into the Jupyter notebooks. Having futzed around for about a day copying/pasting blocks of code, I refactored the code into separate functions for readability, so that only the \"business logic\" is shown in the notebook. Train/Test Split It is a standard practice to split the dataset into a training and test set. K-fold cross-validation is quite easy to do using . Given an and a matrix, to split it into , , , and , simply do the function call: Model Training To train models, I used the scikit-learn package to help. It\u2019s useful to note that has a consistent API - every regressor model has a and a function. This \u2018modular\u2019 style allowed me to wrap the series of function calls into single-line functions, and thus quickly try out a variety of models to see what out-of-box predictive power would be. Using the Random Forest Regressor as an example, I wrapped up the training and plotting phases: Here, simply refers to the module I wrote to refactor out the repetitive boilerplate code needed. The ensemble learners also include feature importances, i.e. an identification of the columns in the data that best predict the outcome of interest. I wrapped the feature importances code to make it easy to plot: In particular, I used the ensemble learners, which are known to be pretty powerful for learning tasks. And for comparison, I pitted them against a number of linear models as well. As you can see in the notebooks, the ensemble learners outperformed the linear models, at least for a binarized amino acid feature set. This makes intuitive sense - protein sequence to function is non-linear, and highly contextual. Neural Networks! One of the things I wanted to highlight here was how Daniel Nouri\u2019s nolearn package made it easy for me to start experimenting with neural networks. By no means am I a deep learning expert - I consider myself too algebra-blind (but by no means code-blind!) to learn the math behind it all. However, I know that my learning style of diving into the deep end and doing a lot of hands-on trials would help me get a fairly good intuitive grasp of how to do it. So after futzing around on a GPU cluster for a few days trying to get it configured right, I got , and up and running. (Note: A GPU makes light(er) work of training artificial neural nets. CPUs take around 5-10x more time. Highly recommended to use a GPU with neural nets. Shout-out to my PhD thesis committee member, Mark Bathe, for giving me access to his lab's GPU machine!) \u2019s API is",
    "tags": [
      "data science",
      "drug resistance",
      "academia",
      "grad school"
    ],
    "pub_date": "2015-09-28",
    "type": "blog"
  },
  {
    "id": "blog-in-which-i-trained-a-neural-network",
    "url": "/blog/2015/9/3/in-which-i-trained-a-neural-network/",
    "title": "In Which I Trained A Neural Network :)",
    "summary": "I have decided to link to my Jupyter notebook &amp; github repository instead of re-writing the whole post here. I hope you enjoy it! :)",
    "body": "I have decided to link to my Jupyter notebook &amp; github repository instead of re-writing the whole post here. I hope you enjoy it! :)",
    "tags": [
      "data science",
      "deep learning",
      "neural networks"
    ],
    "pub_date": "2015-09-03",
    "type": "blog"
  },
  {
    "id": "blog-software-engineering-skills-for-data-analytics",
    "url": "/blog/2015/8/18/software-engineering-skills-for-data-analytics/",
    "title": "Software Engineering Skills for Data Analytics",
    "summary": "When you think about software engineering skills, you probably don't think about the analytics types, or data scientist (DS) teams.",
    "body": "When you think about software engineering skills, you probably don't think about the analytics types, or data scientist (DS) teams. This is a reasonable thought. Data scientists aren't in the business of building software, they're in the business of using software to analyze data. That said, I think it's still important for a data scientist (or analytics person, for that matter), to know some basic software engineering skills. Here's the why, followed by the what. Why should an analytics person who uses code to perform analysis care about good software engineering practices? 1. It will help you write better, reusable analysis code. 1. It will help you when you come back to your analysis code later on. 1. It will help you integrate with software teams that you may have to work with. 1. It will help you share the tools that you end up developing. What basic skills should one at least be knowledgeable about, if not able to implement? 1. Defining specifications: being able to create specific and precise descriptions of what's needed, using standardized language. 1. Refactoring: being able to pull out chunks of code that are repeated, to turn them into function calls. 1. Writing unit/data tests: being able to write tests that ensure the integrity of a function or a block of data. 1. Packaging and distribution: being able to share the code, with other people. 1. Version control: being able to keep track of every meaningful change made to the code or data. 1. Documentation: being able to explain to someone other than yourself what the software tool or analysis code is all about. 1. Continual automated testing &amp; integration: having a continuous integration system automatically run the necessary software and data tests, and report code testing coverage. Here's where I see them being implemented. Building your own tools Sometimes, the tools that you need to get your work done aren't already written, or they're scattered about. You'll need to write your own tools to get your job done. The history of some Python packages, like , or were born out of this necessity. Here, the ability to use good software engineering practices as described above will make a huge difference when trying to orient newcomers to the tool and help new contributors jump in easily. And even if you don't end up sharing the code with others, if you still end up reusing your code, all of the above practices will help with codebase maintenance. Developing your analysis pipeline Your analysis pipeline is the sequence of steps that are needed to get from raw data to interpretable data to insights. Along the way, you may encounter blocks of code that get copy/pasted elsewhere. Or, you might find yourself writing a series of functions that always take the same inputs - necessitating your own OOP-based tool. Knowing good software engineering practices will help you keep your code clean, readable, and reusable across your analysis pipeline. It'll also help others interpret and verify your data analysis steps. Documentation, in this realm, is particularly important. Data tests, are something software engineers don't do, but you might wish to. Data tests that run on continuous integration platforms serve as an automatically-enforced contract between you, your data provider, and your data delivery. If anything changes, continuous integration should catch it early on, and not later. An example of a data test is ensuring that the number of columns/rows stays the same, or that a hash of the original data file hasn't changed (i.e. data integrity). Integrating with Data Products Data scientists in industry don't work in a vacuum: there is usually a software product waiting at the end. Those software products are (hopefully) maintained with good software engineering practices. Getting good at the practice of good software engineering will help lubricate communication and workflows with the software engineers who may be responsible for integrating the new algorithm or analysis pipeline. Even in the academic or government world, where the end product may not be a monolithic system but a series of, perhaps, web dashboards, knowing good software engineering practices will ease the transition from analytics to product. Of course, any good thing done too much will become a bad thing. Don't over-specify straight from the start, start with something clear enough. Don't over-refactor if it isn't necessary. Don't worry about finding every test case, just test enough so the most common bugs are caught. Or use something like Hypothesis to do property-based testing. (The maintainer threw in the towel recently, citing financial reasons, I think, but as it stands it's already pretty darn good.) Don't package every last tool you develop, only share what's necessary. And over-indulging on documentation can take away from good coding. Write enough for yourself to review your code, and let others help you refine it if it's actually needed. Tracking every little minor change is al",
    "tags": [],
    "pub_date": "2015-08-18",
    "type": "blog"
  },
  {
    "id": "blog-scipy-2015-done",
    "url": "/blog/2015/7/13/scipy-2015-done/",
    "title": "SciPy 2015 - Done!",
    "summary": "The conference is over! I get to go home now, but I also will miss being a part of the community. Hope to go back next year! Thoughts 1.",
    "body": "The conference is over! I get to go home now, but I also will miss being a part of the community. Hope to go back next year! Thoughts 1. It was fun to set so many new people from a variety of disciplines. This conference was right in the sweet spot of nerdy-ness and social interaction. I think I lost track of the number of business cards I handed out to people, but it's definitely dented my collection! 2. I learned a lot, particularly about the variety of packages that other people have developed to solve a variety of problems. 3. Swag! This time it was in the form of a free book by Kurt Smith, on using Cython to speed up Python code. Also got a bunch of stickers. At PyCon, I didn't know where to stick them, and I was hesitant to stick them to my laptop (I like a clean laptop), so I stuck them to my poster tube instead. 4. I gave a lightning talk on Testing Data, for data integrity purposes. Later, I was contacted by Greg, who leads the Software Carpentry (SWC)initiative, on providing some material on doing data tests. Looks like it could be fun! And I cannot wait to get my own SWC instructor training - c'mon Greg! 5. My roommate, Chuan, was a physician from China, who was in the Houston area doing a year of research. I had a great time conversing about code and culture with him, and I learned a lot about contemporary Chinese medicine from him. 6. Finally, I participated in my first ever coding sprint! It was with the matplotlib team. It was a great learning experience, participating in the actual modern git workflows of software development. I helped with making changes to the documented examples, a task suitable to first timer sprinters (as it doesn't risk messing up the established code logic). Seeing my first merged PR to a major software project gave me an absolute thrill :). I also got to observe how to do Continuous Integration with auto testing. Next conference I will most certainly make time for at least part of a coding sprint. 7. I missed a bunch of talks on the second and third day, because I needed some headspace to finish up thinking about this paper that I am writing. However, because of the great efforts by the AV team that Enthought hired, it's possible to view them online after the conference. This also have those who couldn't attend the conference a chance to access the conference materials. Kudos to them! This year's conference was a really great experience. I learned lots, learned about the many people doing cool stuff with the scientific Python stack, and made new connections with them too. I would highly recommend joining SciPy 2016, and I hope to make it an annual thing with PyCon!",
    "tags": [
      "scipy",
      "conferences",
      "python"
    ],
    "pub_date": "2015-07-13",
    "type": "blog"
  },
  {
    "id": "blog-on-the-humbleness-of-conference-attendees",
    "url": "/blog/2015/7/13/on-the-humbleness-of-conference-attendees/",
    "title": "On the 'humbleness' of conference attendees",
    "summary": "Conferences are made up of people, just like any other group of human beings grouping together. What makes one differ from another really boils down...",
    "body": "Conferences are made up of people, just like any other group of human beings grouping together. What makes one differ from another really boils down to the people. I read a tweet recently that described the SciPy 2015 conference as having really 'humble' attendees. This was exactly my feeling! It was great to see such a community of developers and scientists who, knowing that while they may be domain experts there is still much to learn, choose to carry themselves in a really humble way. I think this was one of the reasons why I really enjoyed SciPy. It was devoid of the ego that plagues other field-specific conferences. Yet another reason to go again!",
    "tags": [
      "conferences",
      "scipy",
      "python"
    ],
    "pub_date": "2015-07-13",
    "type": "blog"
  },
  {
    "id": "blog-scipy-2015-talks-day-1",
    "url": "/blog/2015/7/8/scipy-2015-talks-day-1/",
    "title": "SciPy 2015: Talks Day 1",
    "summary": "Morning Session 1. Will millenials ever get married?: Survival analysis made simple. 2. Distarray: A tool to do distributed computing on arrays. 3.",
    "body": "Morning Session 1. Will millenials ever get married?: Survival analysis made simple. 2. Distarray: A tool to do distributed computing on arrays. 3. Teaching with Jupyter: for autograding of notebooks, and to provide a uniform environment for teaching and learning. 4. Open Source data archive for melanoma screening: How to build a system that helps diagnose melanoma. Mid-Day Session 1. Story time with Bokeh: State of Bokeh\u2019s plotting package. 2. VisPy: Harnessing The GPU For Fast, High-Level Visualization: A very impressive package for doing real-time visualization! 3. HoloViews: Interactive data visualizations made really easy. Afternoon Session 1. Deep learning crash course: A real crash course! 2. PyStruct (structured prediction): a package for doing structured prediction. I then left to get a breather, and prepare a bit for my lighting talk!",
    "tags": [
      "scipy",
      "conferences",
      "python"
    ],
    "pub_date": "2015-07-08",
    "type": "blog"
  },
  {
    "id": "blog-scipy-2015-geospatial-data-tutorial",
    "url": "/blog/2015/7/7/scipy-2015-geospatial-data-tutorial/",
    "title": "SciPy 2015: Geospatial Data Tutorial",
    "summary": "This morning, I attended the Geospatial Data tutorial. It wasn\u2019t a filled lecture hall, but that was likely because the topic is a bit more...",
    "body": "This morning, I attended the Geospatial Data tutorial. It wasn\u2019t a filled lecture hall, but that was likely because the topic is a bit more specialized. That said, I think it was a tutorial with great content. Part of my own research work may eventually incorporate working with geospatial data - to predict influenza viral reassortment, in particular. Therefore, I was looking forward to learning more about the packages that are used to read and manipulate geospatial data. This tutorial provided the overview I was looking for. Tutorial Content In this tutorial, we were taught how to inspect and manipulate geospatial data in a Pythonic fashion. The first thing we learned was how to inspect geospatial data using the package. Most important point I learned was that geospatial data is usually stored as a GeoJSON format. The second thing we learned was the package, which allowed us to draw arbitrary shapes and perform set operations on them. This one went smoothly, and I found the trivial examples provided to actually be quite instructive and informative. The third thing we learned was , where we learned how to load raster images of geographic regions, and combine them with their geographic metadata. The final thing that I picked up was . Arguably the easiest portion to follow, I was also pleasantly surprised as to how many common/intuitive operations that I could think of were also represented in the API. Tutorial Pace I hit a snag using the and packages, so I eventually settled on following the tutorial on the projector screen instead. Apart from that, it was the section that was the easiest to follow, as the API was very similar to the API. Other Thoughts Overall, I think this tutorial was a great overview of the packages that can be used to read and manipulate geospatial data. I can tell that our tutorial leader, Kelsey, placed quite a bit of effort in preparing the variety of data sets and examples. Perhaps a bit more environment testing prior to the tutorial may have helped us; I was getting tripped up quite a bit on and installation. On a separate note, I\u2019ve noticed that most of the installation or usage issues came because of libraries not being found/linked properly. That may be a burden for the package authors to address, rather than the tutorial leaders.",
    "tags": [
      "scipy",
      "conferences",
      "python"
    ],
    "pub_date": "2015-07-07",
    "type": "blog"
  },
  {
    "id": "blog-scipy-2015-computational-statistics-ii-tutorial",
    "url": "/blog/2015/7/7/scipy-2015-computational-statistics-ii-tutorial/",
    "title": "SciPy 2015: Computational Statistics II Tutorial",
    "summary": "The final tutorial that I sat in today was the intermediate computational statistics tutorial. This was led by Chris Fonnesbeck, prof at Vanderbilt...",
    "body": "The final tutorial that I sat in today was the intermediate computational statistics tutorial. This was led by Chris Fonnesbeck, prof at Vanderbilt University, fellow Vancouverite, also one of the maintainers of the PyMC3 package. Tutorial Content In this tutorial, Chris covered: 1. Data cleaning/preparation - using pandas. 1. Density estimation - using the numpy and scipy packages; mechanics: method of moments and maximum likelihood estimators. 1. Fitting regression models. Tutorial Pace The initial part of the tutorial was heavily pandas oriented. I think it was useful for the fairly large fraction of the class that was not well-versed with pandas. In my own case, however, I skipped forward to the second notebook in order to explore a bit. The time spent on pandas was about 1 hr 45 minutes; we only got to the second topic at 2:45 pm. The latter parts were quite useful. I think the mechanics of thinking through statistical modelling problems isn\u2019t commonly emphasized in stats classes. As such, just like I had mentioned in my review of the first tutorial, the mechanics on \"how to do stuff\" proved to be really helpful. Overall Thoughts This was the one that I was particularly anticipating, as I was hoping to learn the mechanics of doing Bayesian statistical analysis in PyMC3. However, the tutorial content was not that, possibly because this material was already covered last year and recorded (for YouTube posterity). Instead, I was pleasantly surprised by the content covered here instead. Definitely was an expansion of my thinking. Two full days of learning has been quite an intellectual adventure! Many thanks to all of the tutorial leaders for their preparation and hard work; count me as one more person who\u2019s learned lots!",
    "tags": [
      "scipy",
      "conferences",
      "python"
    ],
    "pub_date": "2015-07-07",
    "type": "blog"
  },
  {
    "id": "blog-scipy-2015-cython-tutorial",
    "url": "/blog/2015/7/6/scipy-2015-cython-tutorial/",
    "title": "SciPy 2015: Cython Tutorial",
    "summary": "This morning, I sat in the Cython tutorial by Kurt Smith, of Enthought, at SciPy 2015. We\u2019re only halfway in at the time of starting the writing,...",
    "body": "This morning, I sat in the Cython tutorial by Kurt Smith, of Enthought, at SciPy 2015. We\u2019re only halfway in at the time of starting the writing, but it has been a really useful tutorial! Tutorial Content Kurt has designed this tutorial to be an intermediate tutorial, where participants are expected to have had some experience with Python before. During this tutorial, we covered two practical ways to speed up Python code. One is to declare Python variables, functions or objects as C types using the statement, and the other is to simply wrap existing C code. Tutorial Pace It was a very well-paced tutorial, and Kurt did a great job of structuring the content such that each exercise followed roughly the same pattern. We spent approximately 20-30 minutes on each section, partly because of the really advanced questions (from my point of view) that other participants asked. Coding Practice We got four activities to get used to doing things. Each practice activity was small but not trivial - basically adding the declarations or writing the wrapper code, not needing to write complex algorithms. Reflections Already in the first half of the tutorial, I think I have gained two superpowers. The first is that I can speed up my own code without needing to write C, if I need to, simply by adding and statements where needed. The second is that I can use pre-existing C code that others have written if I need to. Both ways, I can easily speed up my computation with minimal effort. Having Python under my belt to do scientific computing was already a superpower; now, faster Python is also in the repertoire! From an education standpoint, I think it is also true that knowing the \"howto\" steps is really important in learning programming. For the majority of scientists, why certain things work on the computer might not be as important as knowing how to make certain things work - as computational scientists, we\u2019re most interested in getting answers, and preferably getting them fast. The simple and structured examples, helped a lot with this.",
    "tags": [
      "scipy",
      "conferences",
      "python"
    ],
    "pub_date": "2015-07-06",
    "type": "blog"
  },
  {
    "id": "blog-scipy-2015-bokeh-and-blaze-tutorial",
    "url": "/blog/2015/7/6/scipy-2015-bokeh-and-blaze-tutorial/",
    "title": "SciPy 2015: Bokeh & Blaze Tutorial",
    "summary": "I sat in the Bokeh/Blaze tutorial this afternoon. It was a challenging tutorial, but a good one nonetheless.",
    "body": "I sat in the Bokeh/Blaze tutorial this afternoon. It was a challenging tutorial, but a good one nonetheless. I could tell that our tutorial leader, Christine, put in quite a bit of effort in preparing the tutorial content, so kudos to her on that! And the rest of the bokeh team were on hand to support her, and act as TAs for us, so this was also a plus point for Continuum Analytics. Tutorial Pace Initially it was challenging for most of the crowd. I had futzed around a bit with before, so the first three exercises were straightforward. But the rest of the package was a bit cryptic, especially for someone like me coming from the world. I actually wonder if the way the objects are organized mirrors somehow? For example, in , we have the object, the objects, and then all of the other objects that the object contains as the plot is being drawn. It seemed like the objects are organized in a similar fashion, giving coders quite a bit of flexibility in creating a plot. Anyways, I digress. The tutorial pace was fast initially, but Christine toned it back a bit. We got quite a bit of opportunity to try our hand at coding nonetheless. I think API familiarity was the biggest issue for us in the crowd; I am guessing that if we were more familiar with the API, then we would be able to more easily figure out the coding patterns that are used, and thus have more success in our own attempts at the tutorial content. Towards the end of the portion of the tutorial, I found it helpful that Christine pushed her content to Github, so that we could follow along without worrying too much about falling behind on the coding content. I think in the case of plots, I\u2019m of two minds on whether it\u2019s better to modify existing code or to try writing it from scratch - I guess they each have their own pedagogical place. As for the portion, because the API is so similar to the API, I found it very easy to follow along as well. Tutorial Content My biggest takeaways were three-fold. The first takeaway was how the API exposed allows for a high-level, medium-level and low-level construction of web figures. The second takeaway was how to do dynamically-updating plots, at least at a basic level. That said, it still isn\u2019t intuitive just yet, maybe the API design has to be a bit more matured for it to be so. That said, I\u2019m stoked to learn more about how to do it! The final takeaway was how to convert between different data sources. The API is pretty cool - I hope they continue to mirror the API! Other Thoughts There were a lot of questions raised during the tutorial, particularly on package features. For example, convenience on doing particular operations was a big one. Can it be made easier to do callbacks without the server? Can it be made easier to make different types plots? Can it be made more intuitive to modify a particular part of the figure (i.e. styling)? I hope the team is listening! On our part in the community, I think it\u2019s important also to keep identifying these issues, and posting them on their Github repository. I will begin to make this a habit myself :). One thing that I think might help is to have some illustration of what objects are being drawn to the screen, and their attributes that are modifiable. That would really help with getting familiar with the API. I remember struggling massively through the API, simply because I couldn\u2019t map object name to figure element and vice-versa. I\u2019ve put in a Github issue; for my coming projects, I\u2019m thinking of developing a web-based \"live figure\" to break the mould of academic publishing. Static figures have their place, but live \"dynamic\" figures for time-aware studies also have their place too, and those figures\u2019 most important, time-sensitive insights cannot be adequately summarized by a static version - e.g. spread of Ebola as the outbreak progresses. Overall, I\u2019m really glad I took this tutorial, and I\u2019m looking forward to building my next set of datavizs\u2019 in Bokeh!",
    "tags": [
      "scipy",
      "conferences",
      "python"
    ],
    "pub_date": "2015-07-06",
    "type": "blog"
  },
  {
    "id": "blog-evaluate-on-a-spider-plot",
    "url": "/blog/2015/6/27/evaluate-on-a-spider-plot/",
    "title": "Evaluate on a Spider Plot",
    "summary": "Observation: 1. Humans like to measure things. 1. Humans also like to evaluate things. 1. Humans usually like to have only one metric for...",
    "body": "Observation: 1. Humans like to measure things. 1. Humans also like to evaluate things. 1. Humans usually like to have only one metric for evaluations. Example: Academic performance. 1. Performance is measured using grades: letter, or percentage. 1. Performance is evaluated on how good or bad the letter/percentage is. 1. All of that gets condensed to one number: GPA/Average. It's easy to evaluate stuff on a single number. But it also creates some problems. Nuances are lost. Subspecialty strengths are not shown. Could we do better? I think so. Proposal: Rather than evaluate stuff on a single line, I think we should start evaluating people on a multi-dimensional spider plot. Keep the metrics, but start evaluating people on more metrics than one. Example: Job performance 1. Peer-reviewed perception of performance. 1. Company metrics attained. 1. Cross-departmental engagements. Basically, measure more things that are valued than just the single thing that seemingly captures everything... but doesn't. Thoughts?",
    "tags": [],
    "pub_date": "2015-06-27",
    "type": "blog"
  },
  {
    "id": "blog-test-all-your-data",
    "url": "/blog/2015/6/24/test-all-your-data/",
    "title": "Test All Your Data!",
    "summary": "On Monday, I gave my first ever lighting talk at the Boston Python meetup. Though I was kinda nervous right before the talk, overall, I found it to...",
    "body": "On Monday, I gave my first ever lighting talk at the Boston Python meetup. Though I was kinda nervous right before the talk, overall, I found it to be really fun! My main message was that we should write tests for our data when doing data analysis. What\u2019s the benefits? 1. Things that should be true about the data at this point, and in the future, will be formally coded in an automatically runnable script. 2. If the data ever needs to change, this serves as an automated way of having a sanity check in place prior to running data analysis. 3. If the data ever changes inadvertently, we have tests to ensure data integrity. My slides are attached here, hopefully they\u2019ll come in handy to someone, someday. Or maybe to you, my reader. :-) Q&A From memory, these were some of the questions that I fielded right after the talk. Q: What happens if your data cannot fit in memory? A: Try reading in just a random subset of the data, just the head, just the tail, or streaming it row/line by row/line. Q: When do you know you have enough tests written before you can begin doing other things, such as modelling? A: You don\u2019t. Just keep writing them. You\u2019ll never have enough tests. You will discover more assumptions about your data as you go along, which you can code up in your test script. Q: What\u2019s your general workflow for writing tests? A: Data analysis occupies a big chunk of my time. If I am thinking about the data, I will realize I have assumptions I\u2019m making about the data. At those moments of realization is the best time to jot down the assumption to be encoded. Sometimes the epiphanies come in the shower or while sitting on the toilet bowl or eating a burrito. Have a notebook handy :). Prior to embarking on the data analysis journey, I will run . Following data analysis, I will run those tests one more time. At the end of adding in a test, I will also run . It\u2019s now become a reflex.",
    "tags": [],
    "pub_date": "2015-06-24",
    "type": "blog"
  },
  {
    "id": "blog-pushing-past-myself",
    "url": "/blog/2015/6/4/pushing-past-myself/",
    "title": "Pushing Past Myself",
    "summary": "Yesterday, I decided to go for a 5K run on my own. This was a break from my usual routine (which is a routine, now that I\u2019ve finished it at 4...",
    "body": "Yesterday, I decided to go for a 5K run on my own. This was a break from my usual routine (which is a routine, now that I\u2019ve finished it at 4 regular intervals) - I solo\u2019d it, rather than running with my running buddy Lin (I didn\u2019t ask him ahead of time for today\u2019s run). I\u2019m no runner - I only got back into it at the age of 27, which meant a whole 9 years (since junior college) when I last did running for martial arts training. But after struggling for over a year to get past jogging 2 km, this year I\u2019ve attempted four 5 km runs, each one better and better, but none breaking the 40 minutes mark. Yesterday, I did it. Call me out for waxing philosophical, I think there\u2019s stuff I experienced during this run that reflect some of what I\u2019ve learned over the past 4 years of graduate school. Importance of Starting My initial goal was just to finish the 5K run on my own, which considering I had started it on my own, was already an accomplishment. I\u2019m usually a self-starter when it comes to brainiac things, but physical exercise is one where it\u2019s a bit tougher to get me started. The real barrier is in the initial stages - just getting out. Sometimes it\u2019s too cold. Sometimes I\u2019m too full. Sometimes I\u2019m making too much progress on my code. Whatever it is, I usually can find a reason, or sometimes just an excuse, for not going out for a run. This time round, I just went, \"Screw the reasons. Yeah, I might be a bit trippy from a lack of sleep last night (finished The Martian in one sitting), and I\u2019m mentally fatigued from running my colleague\u2019s stats and (re)writing a Python utility to automatically design Gibson assembly primers from FASTA files. Screw it. I\u2019m just going to run.\" With that initial commitment, and the execution, soon enough I found myself way too far in to back out. Importance of Intermediate Goals Okay, so it\u2019s not a marathon, but 5K ain\u2019t a 100 m sprint either. It\u2019s easy to just give up running and switch to walking, if there\u2019s no intermediate goals for the running. I found myself jogging at a comfortable pace when I was doing 0.7 km every 5 minutes. So I gave myself goals at 0.7 km intervals. Since RunKeeper gives me reports every 5 minutes, I paid the most attention at 15 min., when I was starting to feel the fatigue in the legs the most. When I learned that I was on pace (2.1 km), that gave me a boost to continue going. When I learned that at 30 minutes in, I was at 3.8 km, though I was about 400 m off from where I should be at 0.7 km/5 minutes, I realized I might just be able to get a chance at breaking the 40 minute mark if I just sped up a little bit. Setting these intermediate goals kept me paced and going. In graduate school, it\u2019s at least 5 years on average. Intermediate goals are hugely important. Intermediate goals are those experimental or computational wins that tell you whether your project is worth pursuing or not. They\u2019re also the measurable progress goals once you\u2019ve determined the feasibility of your project. And once you have those goals and start hitting them, the endorphin rush keeps you going. Importance of Your Internal Monologue It\u2019s easy to give up the running and just switch to walking. But some chantable mantras really helped. When I got the stitches, I started muttering, \"Ok, walk it off, walk it off, walk it off\u2026\" When my legs began cramping, and I had to walk, I chanted, \"Just a bit faster, just a bit faster\u2026\" In my case, it was an externalized internal monologue; others might not resort to chanting it. It was borne out of a belief that I could do it. My internal monologue was that, I can do it, and this time might be the time, and (not but) if I fail at it, I still have another chance. Graduate school really consists of the many small wins that accumulate up into a thesis. Those small wins only come through struggling the many small losses that come by. (I\u2019ve failed spectacularly once while in graduate school, but got a second chance; I\u2019ve had my submitted manuscript rejected twice editorially, and I\u2019ve doubted my abilities to do good research that is also recognized by others; I\u2019ve nearly wiped out valuable old data because of command-line incompetence as well. So there, we\u2019ve all been through our own version of failures.) My own internal monologue has changed from \"success-seeking\" to \"resilience building\". I think resilience, in the long-run, is much more valuable than success. And in graduate school, over the 5 years that we\u2019ll spend here, it\u2019s probably the most valuable thing we\u2019ll take away. Importance of Support These past three runs, I\u2019ve chosen to post it to Facebook, knowing fully that I\u2019ve got a few friends whose running records would entitle them to laugh off my run as \"peanuts\". But no, that\u2019s not who they are. A few more have run at least one 5K. Some have done Iron Man and Tough Mudder races (I think that\u2019s what it\u2019s called). I\u2019m not in this running business to run these races. Damn, I just trying to get fit, having subjected my body to unfit habits for the ",
    "tags": [],
    "pub_date": "2015-06-04",
    "type": "blog"
  },
  {
    "id": "blog-thoughts-on-open-data-science-conference",
    "url": "/blog/2015/6/2/thoughts-on-open-data-science-conference/",
    "title": "Thoughts on Open Data Science Conference",
    "summary": "1. Great turnout! People from everywhere - Conneticut, Michigan, Boston, Ukraine etc. Many companies as well. 2. Hiccups with the workshops.",
    "body": "1. Great turnout! People from everywhere - Conneticut, Michigan, Boston, Ukraine etc. Many companies as well. 2. Hiccups with the workshops. On Talks All-round mostly high quality talks. Favorites: Booz Allen Hamilton\u2019s opener (4th of the opening talks), in which they encouraged Data Scientists to go beyond correlations and move into finding out causations. Best point I\u2019ve seen about the data science world. On Job Booths Darn! It looks like the Broad is more interested in post-docs than staff computational biologists. :( I hope my impressions are wrong. On Workshops I would run the workshops in the style of PyCon Tutorials. Concentrate them, require participants to pre-register for them, and perhaps pay a refundable deposit for their spot. Refundable deposits via EventBrite are not impossible. Other Suggestions Try booking a venue much closer to the entrance, perhaps? Food - okay, maybe at a low registration fee, not possible to provide. But catering for 1000+ people might be more cost effective than having us buy our own lunches?",
    "tags": [
      "conferences",
      "odsc",
      "data science"
    ],
    "pub_date": "2015-06-02",
    "type": "blog"
  },
  {
    "id": "blog-how-to-do-testing-as-a-practice-in-data-analysis",
    "url": "/blog/2015/5/30/how-to-do-testing-as-a-practice-in-data-analysis/",
    "title": "How to do Testing as a Practice in Data Analysis",
    "summary": "How can we do tests for our data as a data scientist? Here's my first blog post on how I think data tests can be written. Basically, if you depend on it, write a test for it!",
    "body": "Recently, I\u2019ve just wrapped up the data analysis tasks for one of my projects, and have finally finished the writing up to the stage of being ready for submission. I\u2019m still going to choose to keep it under wraps until it finally gets published somewhere - don\u2019t want to count chickens before they\u2019re hatched! But the paper isn\u2019t the main point of this post; the main point is on the importance of doing tests as part of the data analysis workflow, especially for large data analysis. Why \"data testing\"? Data comes to us analytics-minded people - but rarely is it structured, cleaned, and ready for analysis & modelling. Even if it were generated by machines, there may be bugs in the data - and by bugs, I mean things we didn\u2019t expect to see about the data. Therefore, I think it\u2019s important for us to check our assumptions about the data, prior to and during analysis, to catch anything that is out of whack. I would argue that the best way to do this is by employing an \"automated testing\" mindset and framework. Doing this builds a list of data integrity checks that can speed up the catching of bugs during the analysis process, can provide feedback to the data providers/generators. In the event that the data gets updated, we can automatically check the integrity of the datasets that we work with prior to proceeding. How to begin data testing? As I eventually found out through experience, doing data tests isn\u2019t hard at all. The core requirements are: 1. An automated testing package (e.g. ) 2. A Python script called , with functions that express the assumptions one is making about the data. To get started, ensure that you have Python installed. As usual, I recommend the Anaconda distribution of Python. Once that is installed, install the package by typing in your Terminal: . Once that is done, in your folder, created a blank script called . In this script, you will write functions that express the data integrity tests for your data. To illustrate some of the basic logic behind testing, I have provided a Github repository of some test data and an example script. The material is available on https://github.com/ericmjl/data-testing-tutorial. To use this \u2018tutorial\u2019 material, you can clone the repo to disk, and install the dependencies mentioned on the Github site. The example data set provided is a Divvy data set, which is a simple data set on which we can do some tests. There is a clean data set, and a corrupt data set in which one cell of the CSV file has a \"hello\", and another cell has a \"world\" present in the and columns, defying what one would expect about two of the data columns. If you inspect the data, you will see that there are different columns present. To begin testing the data, we can write in the following lines of code in the : import pandas as pd data = pd.readcsv('data/DivvyStations2013.csv', indexcol=0) def testcolumnlatitudedtype(): \"\"\" Checks that the dtype of the 'Latitude' column is a float. \"\"\" assert data[\u2019atitude\u2019].dtype == float If you fire up a Terminal window, into the directory, and execute . You should see the following terminal output: ****. If you now change the function to check the DataFrame, such that the assert statement is: assert datacorrupt['altitude'].dtype == float The output should include the error message: > assert df['latitude'].dtype == float E assert dtype('O') == float At this point, because the assertion statement failed, you would thus know that the data suffered a corruption. In this case, there is only one data set that is of interest. However, if you find that it\u2019s important to test more than one file of a similar data set, you can encapsulate the test code in a function call embedded in the test function as such: def testcolumnlatitudedtype(): \"\"\" Checks that the dtype of the 'Latitude' column is a float. \"\"\" def columnlatitudedtype(df): assert df['latitude'].dtype == float columnlatitudedtype(data) columnlatitudedtype(datacorrupt) In this way, you would be testing all of the data files together. You can also opt to do similar encapsulation, abstraction etc. if it helps with automating the test cases. As one speaker once said (as far as my memory can recall), if you use the same block of code twice, encapsulate it in a function. How to continue testing? You can extend the script by adding more test functions. is able to do automated test discovery, by looking for the prefix to a function. Therefore, simply make sure all of your test functions have the prefix before them. Need some ideas on when to add more tests? 1. Any time you come up against a bug in your data, add a test. 2. Any time you think of a new assumption you\u2019re making about the data, add a test. Happy Testing! :D",
    "tags": [
      "testing",
      "data analysis",
      "data science"
    ],
    "pub_date": "2015-05-30",
    "type": "blog"
  },
  {
    "id": "blog-pycon-2015-a-long-overdue-report",
    "url": "/blog/2015/5/29/pycon-2015-a-long-overdue-report/",
    "title": "PyCon 2015 - A Long-Overdue Report",
    "summary": "My thoughts on another wonderful year at PyCon!",
    "body": "This year, I had the privilege of attending PyCon 2015 as a tutorial instructor again. Unlike last year, which was an eye-opener for me, this year, I decided to take a more educational approach, and set a goal to learn as much new things about programming in Python that I didn't previously know. Old/New things I re-/learned In no order of importance: 1. Inspired by JakeVDP\u2019s talk: Use functions whenever possible! And try to cast your data/analysis problems in terms of matrices for massive speedups. Re-implementing one problem that I had tried, I experienced a million-fold speedup using compared to pure Python loops. A million fold! 2. Testing, a practice primarily found in the software development world, is really important for data analytics! I will have a blog post on this coming. 3. The top data scientists\u2019 workflows sometimes involve weird things like \"averaging\" multiple models. I think this is simultaneously a blessing and a curse. A blessing because it gets us answers that we need; a curse because it tells us nothing about how the world works. Maybe I\u2019m a bit too much of a scientist. (Update: Booz Allen Hamilton speaker @ today's ODSC said we have to know how the world works, and not just stop at describe what the world looks like. The best comment I've heard to date!) 4. Network analysis is a trending topic, given the tutorials and talks that I saw. I will update my network analysis tutorial and re-apply for next year as a tutorial instructor. I do wish I could have gone for Sarah Guido\u2019s tutorial. 5. Continuum Analytics is a company with vision and doing exciting stuff. Props to Travis and his team! I love what they\u2019re doing, building tools for developers and analytics people. I wish them many good years ahead as a company! :) 6. , built by Continuum Analytics, is pretty awesome. Incidents and Infections I had a norovirus infection, and was suffering the symptoms (diarrhea, fever, and fatigue) on the bus to MTL. The symptoms persisted right up to the point my tutorial was due to start. Miraculously, thank God for that, I was able to hold on throughout the tutorial without needing to go to the bathroom once - and I don\u2019t think I passed it to anybody. I collapsed on a couch outside the lunch area in the afternoon, and was spotted by security\u2026 \"Monsieur? Monsieur?\" I hastily grabbed my badge and uttered, \"Oui, oui, Pycon!\", which readily stress-tested the extent of my French language knowledge. At thtat point, I went back to the hotel and collapsed again, for 16 hours. Cool People Again, in no particular order: 1. Travis Oliphant and Peter Wang, Continuum Analytics 2. Jake van der Plas, eScience Institute @ UW 3. Stuart Williams and Ruben Orduz, Tutorial Coordinators Feelings I\u2019d do PyCon 2016! Hopefully I will have new things that I can share with the community next year. I\u2019m also hoping to be able to do a network analysis &amp; statistics tutorial at SciPy 2016, wherever that may be held.",
    "tags": [
      "pycon",
      "conferences",
      "python"
    ],
    "pub_date": "2015-05-29",
    "type": "blog"
  },
  {
    "id": "blog-the-roost-stand",
    "url": "/blog/2015/5/1/the-roost-stand/",
    "title": "The Roost Stand",
    "summary": "In praise of the Roost stand, which I bought from Kickstarter!",
    "body": "A while ago, I backed the Roost Stand on Kickstarter. Of all the things I\u2019ve backed so far, I think the Roost Stand is the only one that I\u2019d consider an investment. It\u2019s saved my neck from countless hours of pain, especially when working from home. It's a daily feature of my workstation. It\u2019s turned coffee shop tables into workstations. Taking it around is really portable as well! Can\u2019t wait to see what the Roost makers have up their sleeves next!",
    "tags": [
      "gadgets",
      "gear"
    ],
    "pub_date": "2015-05-01",
    "type": "blog"
  },
  {
    "id": "blog-semantic-versioning-for-papers-a-manifesto",
    "url": "/blog/2015/4/3/semantic-versioning-for-papers-a-manifesto/",
    "title": "Semantic Versioning for Papers: A Manifesto",
    "summary": "On how to do semantic versioning of papers.",
    "body": "In writing the current paper I\u2019m working on, I have decided to adopt a semantic versioning scheme for each draft of the paper. There\u2019s probably a ton out there, and I think I got a bit fed up with other versioning schemes where people tag on \\_INITIALS to the end of the file. Moreover, I found value in tracking the evolution of the paper - in other words, how does the final product look compared to the original? Therefore, I thought, why not adopt a numbering system that semantically makes sense, the same way that it works for code? Shamelessly copying http://semver.org, here\u2019s my proposed scheme. Given a version number MAJOR.MINOR.PATCH, for a prose body of text, increment the: 1. MAJOR version after each submission, to keep track of the number of times a paper has been submitted. 2. MINOR version after each: 3. re-arrangement of logic, 4. large word-smithing or rephrasing of things, and 5. addition of new insights compared to the previous version 3. PATCH version after making: 4. grammatical or spelling changes 5. substitute individual words for other words I will note that formatting is intentionally not dealt with here, but is assumed to be part of the MAJOR version increment when formatting a manuscript for submission. This is because a writer ought not to be concerned with formatting in the writing stages. A writer ought to be most concerned with getting his/her thoughts into prose form. Now, for the figures, which I believe should be developed in parallel but separately from the text. Given a version number MAJOR.MINOR.PATCH, for a document that lays out the organization of figures, increment the: 1. MAJOR version after each submission. 2. MINOR version after each: 3. addition, removal or rearrangement of figures, 4. changing of figure representations (i.e. scatterplot changed to 2D histogram), 5. major changes to the figure caption/legend 6. PATCH version after each: 7. grammatical or spelling changes, in the figure caption/legend, 8. minor word substitutions or additions/deletions in the figure caption/legend, 9. resizing of figures for aesthetic purposes. So far, I have tried to keep the figure versions in sync with the text versions to keep things really simple. This system has worked well, as I usually do an export of both the text and the figures at the same time, incrementing whichever needs to be incremented accordingly. When this first manuscript is done, next steps would be to run a \u2018diff\u2019 to see how the final version differs from version 0.1.0. Can\u2019t wait for that to happen!",
    "tags": [
      "data science",
      "versioning",
      "software development",
      "academia",
      "grad school",
      "paper writing"
    ],
    "pub_date": "2015-04-03",
    "type": "blog"
  },
  {
    "id": "blog",
    "url": "/blog/",
    "title": "Blog",
    "summary": "",
    "body": "",
    "tags": [],
    "pub_date": "",
    "type": "blog"
  },
  {
    "id": "talks-careful-walk-through-probability-distributions-using-python",
    "url": "/talks/careful-walk-through-probability-distributions-using-python/",
    "title": "A Careful Walk Through Probability Distributions | PyCon US 2020",
    "summary": "A gentle journey through probability distributions that transforms your data analysis from guesswork to confidence.",
    "body": "youtube #### url: https://www.youtube.com/watch?v=G7SIcvWrAKs description #### text: Probability distributions can be intimidating, but they don't have to be. In this talk, I take you on a gentle journey through the world of probability distributions using Python. No heavy math\u2014just practical insights and code that you can actually use. You'll discover how understanding distributions can transform your data analysis from guesswork to confidence.",
    "tags": [],
    "pub_date": "2020-05-13",
    "type": "talks"
  },
  {
    "id": "projects-a-pedagogical-introduction-to-score-models",
    "url": "/projects/a-pedagogical-introduction-to-score-models/",
    "title": "A Pedagogical Introduction to Score Models",
    "summary": "In 2022, I became fascinated with score models. To help myself learn the core ideas better, I put together a pedagogical introduction on the topic.",
    "body": "github #### url: https://github.com/ericmjl/score-models resource #### label: Website url: https://ericmjl.github.io/score-models/",
    "tags": [],
    "pub_date": "",
    "type": "projects"
  },
  {
    "id": "talks-an-attempt-at-demystifying-bayesian-deep-learning",
    "url": "/talks/an-attempt-at-demystifying-bayesian-deep-learning/",
    "title": "An Attempt at Demystifying Bayesian Deep Learning | PyData NYC 2017",
    "summary": "In which I tell you why Bayesian deep learning is nothing really special, delivered at PyData NYC 2017.",
    "body": "description #### text: At PyData NYC 2017, I presented a talk on the intuition behind Deep Learning (DL) and Bayesian DL, using mostly pictures and code, and with as little math as possible. youtube #### url: https://www.youtube.com/watch?v=s0S6HFdPtlA slides #### url: http://ericmjl.github.io/bayesian-deep-learning-demystified",
    "tags": [],
    "pub_date": "2017-12-21",
    "type": "talks"
  },
  {
    "id": "talks-an-attempt-at-demystifying-graph-deep-learning",
    "url": "/talks/an-attempt-at-demystifying-graph-deep-learning/",
    "title": "An Attempt at Demystifying Graph Deep Learning | PyData Global 2021",
    "summary": "In this talk, inspired by my 2017 attempt at demystifying Bayesian deep learning, I now attempt to demystify graph deep learning.",
    "body": "youtube #### url: https://youtu.be/cSN-DqPJzY0 slides #### url: https://ericmjl.github.io/graph-deep-learning-demystified/#/title-slide github #### url: https://github.com/ericmjl/graph-deep-learning-demystified/",
    "tags": [],
    "pub_date": "2022-01-08",
    "type": "talks"
  },
  {
    "id": "projects-bayesian-analysis-recipes",
    "url": "/projects/bayesian-analysis-recipes/",
    "title": "Bayesian Analysis Recipes",
    "summary": "A collection of Bayesian statistical models for general usage, implemented in PyMC3",
    "body": "description #### text: I once saw a probability estimate with mean 0.8 and variance 0.3. From that point onwards, I knew frequentist estimates could be horribly wrong, and decided to go Bayesian. As part of my learning journey, I decided to make publicly available a GitHub repository of Bayesian statistical analysis recipes in PyMC3 featuring models and data that I've seen elsewhere. Most of them I implemented from scratch, to get familiar with PyMC3 syntax and to get familiar with the logic of Bayesian statistical modelling. Some models that are implemented here include: - Binary and multinomial classification. - Neural networks - Linear regression - Hierarchical modelling github #### url: https://github.com/ericmjl/bayesian-analysis-recipes",
    "tags": [],
    "pub_date": "",
    "type": "projects"
  },
  {
    "id": "teaching-bayesian-data-science-simulation-tutorial-scipy-2020",
    "url": "/teaching/bayesian-data-science-simulation-tutorial-scipy-2020/",
    "title": "Bayesian Data Science by Simulation | SciPy 2020",
    "summary": "Build intuition by coding your way through probability distributions\u2014where simulation reveals insights that analytical approaches miss.",
    "body": "youtube #### url: https://www.youtube.com/watch?v=8eh5A72hIWM description #### text: Sometimes the best way to understand statistics is to simulate it yourself. Hugo and I take you on a hands-on journey through Bayesian thinking, where you'll build intuition by coding your way through probability distributions, KL divergence, and the subtle art of conditioning versus marginalizing. This isn't just theory\u2014you'll see how simulation can reveal insights that analytical approaches miss entirely.",
    "tags": [],
    "pub_date": "2020-07-17",
    "type": "teaching"
  },
  {
    "id": "teaching-bayesian-statistical-modelling",
    "url": "/teaching/bayesian-statistical-modelling/",
    "title": "Bayesian Data Science Two Ways: Simulation and Probabilistic Programming | SciPy 2018",
    "summary": "Bayesian statistical analysis concepts, taught using PyMC3, with Hugo Bowne-Anderson.",
    "body": "description #### text: I have taught Bayesian statistical modelling with Hugo Bowne-Anderson at SciPy 2018. Our tutorial takes on two parts. Firstly, we build our participants' intuition for Bayes' rule using computational simulation methods. Secondly, we then segue into the abstractions of probabilistic programming to solve problems, including estimation, comparison, and regression. youtube #### url: https://www.youtube.com/watch?v=89ye2hfsAsk notebooks #### url: https://github.com/ericmjl/bayesian-stats-modelling-tutorial",
    "tags": [],
    "pub_date": "2018-07-17",
    "type": "teaching"
  },
  {
    "id": "teaching-bayesian-data-science-probabilistic-programming-scipy-2019",
    "url": "/teaching/bayesian-data-science-probabilistic-programming-scipy-2019/",
    "title": "Bayesian Data Science: Probabilistic Programming | SciPy 2019",
    "summary": "From uncertainty to understanding\u2014embrace probabilistic thinking and build models that reflect what you know (and what you don't).",
    "body": "youtube #### url: https://www.youtube.com/watch?v=2wvt6GPZl1U description #### text: Ready to think like a Bayesian? This deep dive takes you from uncertainty to understanding in just over three hours. I'll show you how to embrace uncertainty instead of running from it, using Python tools that make probabilistic thinking feel natural. You'll build models that actually reflect what you know (and what you don't), transforming your approach to data science from point estimates to probability distributions.",
    "tags": [],
    "pub_date": "2019-07-12",
    "type": "teaching"
  },
  {
    "id": "talks-bayesian-statistical-analysis-with-pymc3",
    "url": "/talks/bayesian-statistical-analysis-with-pymc3/",
    "title": "Bayesian Statistical Analysis with Python | PyCon 2017",
    "summary": "How to do parameter estimation and case/control comparison in PyMC3, delivered at PyCon 2017.",
    "body": "description #### text: At PyCon 2017, I illustrate, using four statistical analysis problems, how to do parameter estimation and case/control comparison (A/B testing) with PyMC3 code. youtube #### url: https://www.youtube.com/watch?v=p1IB4zWq9C8 notebooks #### url: https://github.com/ericmjl/bayesian-stats-talk",
    "tags": [],
    "pub_date": "2017-05-21",
    "type": "talks"
  },
  {
    "id": "talks-beyond-two-groups-generalized-abcde-testing",
    "url": "/talks/beyond-two-groups-generalized-abcde-testing/",
    "title": "Beyond Two Groups: Generalized A/B[/C/D/E...] Testing | PyCon 2019",
    "summary": "My rant against canned statistical procedures, delivered at PyCon 2019.",
    "body": "description #### text: At PyCon 2019, I delivered my rant against canned statistical procedures (and more generally, canned modelling), showing cases where one might use the t-test but run into wrong inferences as a result. youtube #### url: https://www.youtube.com/watch?v=Pt37qA351yk slides #### url: https://ericmjl.github.io/bayesian-generalized-abcde-testing",
    "tags": [],
    "pub_date": "2019-05-06",
    "type": "talks"
  },
  {
    "id": "bio",
    "url": "/bio/",
    "title": "Bio",
    "summary": "Professional Bio As Senior Principal Data Scientist at Moderna Eric leads the Data Science and Artificial Intelligence (Research) team to accelerate...",
    "body": "Professional Bio As Senior Principal Data Scientist at Moderna Eric leads the Data Science and Artificial Intelligence (Research) team to accelerate science to the speed of thought. Prior to Moderna, he was at the [Novartis Institutes for Biomedical Research][nibr] conducting biomedical data science research with a focus on using Bayesian statistical methods in the service of discovering medicines for patients. Prior to Novartis, he was an Insight Health Data Fellow in the summer of 2017 and defended his doctoral [thesis] in the [Department of Biological Engineering][be] at [MIT][mit] in the spring of 2017. [moderna]: https://www.modernatx.com [nibr]: https://www.novartis.com/research-and-development [thesis]: https://ericmjl.github.io/thesis [be]: https://be.mit.edu/ [mit]: https://web.mit.edu/ Eric is also an open-source software developer and has led the development of [], a clean API for cleaning data in Python, and [], a visualization package for NetworkX. He is also on the core developer team of NetworkX and PyMC. In addition, he gives back to the community through code contributions, blogging, teaching, and writing. []: https://github.com/ericmjl/pyjanitor []: https://github.com/ericmjl/nxviz His personal life motto is found in the Gospel of Luke 12:48.",
    "tags": [],
    "pub_date": "",
    "type": "bio"
  },
  {
    "id": "books",
    "url": "/books/",
    "title": "Books",
    "summary": "",
    "body": "",
    "tags": [],
    "pub_date": "",
    "type": "books"
  },
  {
    "id": "teaching-building-llm-agents-made-simple-pydata-boston-2025",
    "url": "/teaching/building-llm-agents-made-simple-pydata-boston-2025/",
    "title": "Building LLM Agents Made Simple | PyData Boston 2025",
    "summary": "Building LLM Agents Made Simple | PyData Boston 2025",
    "body": "youtube #### url: https://www.youtube.com/watch?v=WXSXOSHb0UY youtube #### url: https://www.youtube.com/watch?v=rTKt5Wv_HTI description #### text: Learn to build practical LLM agents using LlamaBot and Marimo notebooks. This hands-on tutorial teaches the most important lesson in agent development: start with workflows, not technology. We'll build a complete back-office automation system through three agents: a receipt processor that extracts data from PDFs, an invoice writer that generates documents, and a coordinator that orchestrates both. This demonstrates the fundamental pattern for agent systems\u2014map your boring workflows first, build focused agents for specific tasks, then compose them so agents can use other agents as tools. By the end, you'll understand how to identify workflows worth automating, build agents with decision-making loops, compose agents into larger systems, and integrate them into your own work. You'll leave with working code and confidence to automate repetitive tasks. Prerequisites: Intermediate Python, familiarity with APIs, basic LLM understanding. Participants should have Ollama and models installed beforehand (setup instructions provided). Materials: GitHub repository with Marimo notebooks. Setup uses Pixi for dependency management.",
    "tags": [],
    "pub_date": "2025-12-15",
    "type": "teaching"
  },
  {
    "id": "teaching-building-with-llms-made-simple-scipy-2025",
    "url": "/teaching/building-with-llms-made-simple-scipy-2025/",
    "title": "Building with LLMs Made Simple | SciPy 2025",
    "summary": "Create robust, production-ready LLM applications that actually work\u2014no hype, just practical code you can use tomorrow.",
    "body": "youtube #### url: https://www.youtube.com/watch?v=lF8fXWp89J0&t=11188s description #### text: LLMs are everywhere, but building with them shouldn't feel like magic. In this hands-on session, I'll show you how to create robust, production-ready applications that actually work. We'll cover everything from prompt engineering that doesn't break to building systems that handle the messy reality of real-world AI applications. No hype, just practical code you can use tomorrow.",
    "tags": [],
    "pub_date": "2025-08-02",
    "type": "teaching"
  },
  {
    "id": "talks-computational-biology-in-bioengineering",
    "url": "/talks/computational-biology-in-bioengineering/",
    "title": "Computational Biology in Bioengineering | UBC",
    "summary": "Virtual fireside chat about how computational biology shows up in Bioengineering.",
    "body": "youtube #### url: https://youtu.be/jcqYY8ymL8M description #### text: A virtual fireside chat in which I describe how computational biology, in its multiple forms, shows up in the discipline we call biological engineering.",
    "tags": [],
    "pub_date": "2022-04-05",
    "type": "talks"
  },
  {
    "id": "open-source-cupy",
    "url": "/open-source/cupy/",
    "title": "CuPy",
    "summary": "Contributed matrix power API to CuPy.",
    "body": "description #### text: In my experiments with new array libraries, I contributed a matrix power implementation to CuPy. My goal was to enable fast network science operations on the GPU. resource #### label: Pull requests url: https://github.com/cupy/cupy/pull/1374#event-1687619071",
    "tags": [],
    "pub_date": "",
    "type": "open-source"
  },
  {
    "id": "books-data-science-bootstrap",
    "url": "/books/data-science-bootstrap/",
    "title": "Data Science Bootstrap",
    "summary": "Learn how to get your computer organized for your best data science work!",
    "body": "book #### url: https://leanpub.com/dsbootstrap resource #### label: Website url: https://ericmjl.github.io/data-science-bootstrap-notes/",
    "tags": [],
    "pub_date": "",
    "type": "books"
  },
  {
    "id": "teaching-deep-learning-fundamentals",
    "url": "/teaching/deep-learning-fundamentals/",
    "title": "Deep Learning Fundamentals | SciPy 2019",
    "summary": "Model, loss, and optimizer: the core components of deep learning. Come learn more; at the end, we even build the beginnings of a deep learning framework!",
    "body": "description #### text: This tutorial has its public debut at SciPy 2019. In this tutorial, I showed the class the fundamentals of deep learning in the form of \"model, loss, optimizer\". youtube #### url: https://www.youtube.com/watch?v=JPBz7-UCqRo notebooks #### url: https://github.com/ericmjl/dl-workshop",
    "tags": [],
    "pub_date": "2019-07-12",
    "type": "teaching"
  },
  {
    "id": "projects-deep-phenotype-learning",
    "url": "/projects/deep-phenotype-learning/",
    "title": "Deep Phenotype Learning",
    "summary": "Using deep learning to predict viral phenotype from genotype.",
    "body": "description #### text: In order to achieve real-time surveillance, we need machine learning models with high learning capacity that are also highly interpretable. I am currently working on extending neural fingerprints to protein structures using convolutions on graph-structured data. In the process, we are writing a graph convolution implementation as a Python package, as well as a software package for converting protein 3-D structures into its corresponding \"protein interaction graph\" representation. While these tools are developed with the goal of deep learning in mind, we also anticipate their general use as well. Software: 1. Graph Fingerprint on GitHub 1. Protein Interaction Network on GitHub 1. Protein Convolutional Networks on GitHub Senior Collaborators: 1. Prof. Jonathan A. Runstadler 1. Prof. David K. Duvenaud",
    "tags": [],
    "pub_date": "",
    "type": "projects"
  },
  {
    "id": "talks-ensuring-reproducibility-with-pixi-a-live-demo-and-discussion",
    "url": "/talks/ensuring-reproducibility-with-pixi-a-live-demo-and-discussion/",
    "title": "Ensuring Reproducibility with Pixi | prefix",
    "summary": "In this video, Hugo Bowne-Anderson and I chat about the revolutionary package manager Pixi. We discuss myexperiences with Pixi, emphasizing its advantages in package management, reproducibility, and collaboration within data science teams. Key topics include the benefits of lock files, simplifying Docker and GPU environments, and the impact of Pixi on reproducibility and developer efficiency.",
    "body": "youtube #### url: https://www.youtube.com/watch?v=EQgklFqumGI",
    "tags": [],
    "pub_date": "2024-10-01",
    "type": "talks"
  },
  {
    "id": "projects-essays-on-data-science",
    "url": "/projects/essays-on-data-science/",
    "title": "Essays on Data Science",
    "summary": "A curated collection of of essays that I believe are be useful to the broader Python data science community.",
    "body": "description #### text: I have curated a collection of blog posts and non-blog content into a collection of essays that I believe will be beneficial to the broader Python data science community at large. resource #### label: Website url: https://ericmjl.github.io/essays-on-data-science/",
    "tags": [],
    "pub_date": "",
    "type": "projects"
  },
  {
    "id": "projects-flu-forecaster",
    "url": "/projects/flu-forecaster/",
    "title": "Flu Forecaster",
    "summary": "Using variational autoencoders and gaussian processes to forecast the protein sequence of influenza.",
    "body": "description #### text: As part of my Insight project, I built Flu Forecaster, a project that aims to forecast influenza sequence evolution using deep learning. In it, I used a combination of variational autoencoders (VAEs) to translate time-stamped influenza protein sequences into a continuous coordinate space, and then used gaussian process regression to forecast future continuous coordinates that could be translated back to sequence space. resource #### label: Interactive Blog Post url: https://flu-forecaster.ericmjl.com/",
    "tags": [],
    "pub_date": "",
    "type": "projects"
  },
  {
    "id": "research-high-throughput-viral-phenotyping",
    "url": "/research/high-throughput-viral-phenotyping/",
    "title": "High-Throughput Viral Phenotyping",
    "summary": "Data science cannot be done without good data. In order to apply machine learning to the prediction of infectious disease risk, we need to generate large amounts of molecular characterization data in a low-cost and scalable fashion. I am working with colleagues to extend the Massively Parallel Reporter Assays, developed at the Broad Institute, as a way to parallelize the characterization of influenza A virus RNA polymerase variants. I am also devising assays to measure influenza neuraminidase activity in a massively parallel fashion, as well as a Bayesian framework for analyzing high throughput measurement data. Senior Collaborators: 1. Prof. Jonathan A. Runstadler 1. Prof. Paul Blainey Resources: 1. Bayesian Framework | github 1. Systematic Measurement | lab notebook Funded Research Proposals 1. Broad Next10: MPRA and Influenza Polymerase | pdf 1. Co-written with Tony Kulesa and Jared Kehe, Blainey Lab (MIT & Broad Institute) 1. Broad Next10: Neuraminidase Characterization | pdf",
    "body": "",
    "tags": [],
    "pub_date": "",
    "type": "research"
  },
  {
    "id": "talks-how-software-skillsets-will-accelerate-your-data-science-work",
    "url": "/talks/how-software-skillsets-will-accelerate-your-data-science-work/",
    "title": "How Software Skillsets Will Accelerate Your Data Science Work | UMBC",
    "summary": "Me explaining exactly what I state in the title!\ud83d\udc46",
    "body": "youtube #### url: https://youtu.be/7NEQApSLT1U description #### text: In this talk at the University of Maryland Baltimore County's Masters of Data Science program, I share with students and faculty how software development skillsets accelerate a data scientist's ability to deliver on projects. slides #### url: https://hackmd.io/@ericmjl/software-ds",
    "tags": [],
    "pub_date": "2022-05-02",
    "type": "talks"
  },
  {
    "id": "projects-influenza-ecology-and-evolution",
    "url": "/projects/influenza-ecology-and-evolution/",
    "title": "Influenza Ecology & Evolution",
    "summary": "Figuring out if genome shuffling is important for influenza host switching.",
    "body": "description #### text: With its segmented genome, influenza viruses can reassort with other influenza viruses to produce hybrid progeny. Think of it as being like shuffling a red and a blue deck of cards in a box, and picking out each member of the suite at random. As part of my thesis work, I developed an phylogenetic heuristic algorithm to identify reassortant influenza viruses. Using this method, my colleagues and I were able to show that reassortment is over-represented (relative to a null model) when crossing between viral hosts; additionally, the more evolutionarily distant two viral hosts were, the more over-represented reassortment was. This may generalize across domains of life, where reticulate evolution enables organisms to more easily switch between ecological niches. resource #### label: Thesis url: http://ericmjl.github.io/thesis",
    "tags": [],
    "pub_date": "",
    "type": "projects"
  },
  {
    "id": "research-influenza-ecology-and-reticulate-evolution",
    "url": "/research/influenza-ecology-and-reticulate-evolution/",
    "title": "Influenza Ecology and Reticulate Evolution",
    "summary": "With its segmented genome, influenza viruses can reassort with other influenza viruses to produce hybrid progeny. Think of it as being like shuffling a red and a blue deck of cards in a box, and picking out each member of the suite at random. We have developed an phylogenetic heuristic algorithm to identify reassortant influenza viruses. Using this method, my colleagues and I were able to show that reassortment is over-represented (relative to a null model) when crossing between viral hosts; additionally, the more evolutionarily distant two viral hosts were, the more over-represented reassortment was. This may generalize across domains of life, where reticulate evolution enables organisms to more easily switch between ecological niches. References: 1. Ma, E. J. et. al. Reticulate evolution is favored in influenza niche switching. PNAS (2016) | link | preprint | pdf 1. Hill, N. J. et. al. Transmission of influenza reflects seasonality of wild birds across the annual cycle. Ecology Letters (2016) | link | pdf 1. Bui, V. N. et. al. Genetic characterization of a rare H12N3 avian influenza virus isolated from a green-winged teal in Japan. Virus Genes (2015) | link | pdf 1. Bahl, J et. al. Ecosystem interactions and reassortment dynamics underlie the emergence of influenza A viruses with pandemic potential. PLoS Pathogens (2016) | link | pdf Software: 1. Influenza Reassortment Detector [](https://doi.org/10.5281/zenodo.33421) 1. Influenza Global Reassortment Analysis [](https://doi.org/10.5281/zenodo.33422) Senior Collaborators: 1. Prof. Jonathan A. Runstadler 1. Prof. Justin Bahl 1. Dr. Nichola J. Hill",
    "body": "",
    "tags": [],
    "pub_date": "",
    "type": "research"
  },
  {
    "id": "open-source-jax-unirep",
    "url": "/open-source/jax-unirep/",
    "title": "jax-unirep",
    "summary": "A performant reimplementation of the UniRep model.",
    "body": "description #### text: My intern, Arkadij Kummer, and I found that the original implementation of UniRep was slow and non-performant, so we reimplemented it in pure NumPy/JAX. github #### url: https://github.com/ElArkk/jax-unirep",
    "tags": [],
    "pub_date": "",
    "type": "open-source"
  },
  {
    "id": "open-source-llamabot",
    "url": "/open-source/llamabot/",
    "title": "llamabot",
    "summary": "A Pythonic interface to LLMs, with sane defaults.",
    "body": "github #### url: https://github.com/ericmjl/llamabot description #### text: I built LlamaBot to teach myself stuff about the world of LLMs. The product of that work is something I'm freely offering back to the world. Meanwhile, it's got some pretty cool functionality in there that I've implemented as demonstrations of how to use it. For example, one can chat with papers within your Zotero library. Also, you can use it to write commit messages for code diffs. Do check it out!",
    "tags": [],
    "pub_date": "",
    "type": "open-source"
  },
  {
    "id": "talks-llamabot-pythonic-interface-to-large-language-models-scipy-2024",
    "url": "/talks/llamabot-pythonic-interface-to-large-language-models-scipy-2024/",
    "title": "LlamaBot: a Pythonic interface to Large Language Models | SciPy 2024",
    "summary": "Making LLMs feel like first-class citizens in your Python workflow\u2014no more wrestling with API calls or prompt engineering headaches.",
    "body": "youtube #### url: https://www.youtube.com/watch?v=oALhUYhSCHU description #### text: Ever felt frustrated trying to wrangle LLMs into your Python code? I built LlamaBot to solve exactly that problem. In this talk, I show you how to make LLMs feel like first-class citizens in your Python workflow\u2014no more wrestling with API calls or prompt engineering headaches. You'll see how a simple, clean interface can transform the way you think about AI in your applications.",
    "tags": [],
    "pub_date": "2024-08-27",
    "type": "talks"
  },
  {
    "id": "teaching-magical-numpy-with-jax-tutorial",
    "url": "/teaching/magical-numpy-with-jax-tutorial/",
    "title": "Magical NumPy with JAX | PyCon US 2021",
    "summary": "NumPy with superpowers\u2014automatic differentiation, JIT compilation, and seamless GPU acceleration for lightning-fast scientific computing.",
    "body": "youtube #### url: https://www.youtube.com/watch?v=ztthQJQFe20 description #### text: NumPy is great, but what if it could be magical? JAX takes everything you love about NumPy and adds automatic differentiation, JIT compilation, and seamless GPU acceleration. In this tutorial, I'll show you how to write code that's both readable and lightning-fast, transforming your scientific computing from slow and sequential to fast and parallel. It's like NumPy, but with superpowers.",
    "tags": [],
    "pub_date": "2021-06-04",
    "type": "teaching"
  },
  {
    "id": "open-source-matplotlib",
    "url": "/open-source/matplotlib/",
    "title": "matplotlib",
    "summary": "Converted examples in gallery to use API instead of deprecated API.",
    "body": "description #### text: I contributed to the enhancement proposal 12, which proposed reorganizing the examples gallery to make it easier for users to find relevant examples. Part of the problem was that there were old examples, but the new API was now preferred over the old API (which was really present mostly to convert MATLAB users). I helped fix all the examples by changing statements to explicit This was my very first open source contribution, and was basically 70+ pull requests to the library. That mechanical repetition helped me get very familiar with and continuous integration, and showed me the importance of software skills in data work. resource #### label: Pull Request url: https://github.com/matplotlib/matplotlib/pulls?q=is%3Apr+author%3Aericmjl+is%3Aclosed",
    "tags": [],
    "pub_date": "",
    "type": "open-source"
  },
  {
    "id": "talks-modern-principled-data-science-workflow",
    "url": "/talks/modern-principled-data-science-workflow/",
    "title": "Modern, Principled Data Science Workflow | PyData Boston 2020",
    "summary": "In this talk, I share five key principles, learned from the software development world, that help accelerate data science project development and amplify impact.",
    "body": "description #### text: In this talk, I share five key principles, learned from the software development world, that help accelerate data science project development and amplify impact. slides #### url: https://ericmjl.github.io/principled-ds-workflow/ github #### url: https://github.com/ericmjl/principled-ds-workflow youtube #### url: https://www.youtube.com/watch?v=Dx2vG6qmtPs",
    "tags": [],
    "pub_date": "2020-07-23",
    "type": "talks"
  },
  {
    "id": "research-molecular-characterization-of-influenza-a-viruses",
    "url": "/research/molecular-characterization-of-influenza-a-viruses/",
    "title": "Molecular Characterization of Influenza A Viruses",
    "summary": "By doing experiments on isolated viruses, we can tell whether they are dangerous or not. I have contributed to the design and analysis of experiments that characterize the molecular biology of newly isolated and unusual viral isolates. My contributions include image quantification, handling and analysis of large amounts of sequence data, and statistical analyses. References: 1. Hussein, I. T. M. et al. New England harbor seal H3N8 influenza virus retains avian-like receptor specificity and replicates in human lung cells. Scientific Reports (2016) | link | pdf 1. Hussein, I. T. M. et. al. A point mutation in the polymerase protein PB2 alleviates the restriction on a wild reassortant H9N2 influenza isolate, enabling replication in human cells. Infection, Genetics & Evolution (2016) | link | pdf Software: 1. H9N2 global sequence analysis on Zenodo Senior Collaborators: 1. Prof. Jonathan A. Runstadler 1. Dr. Islam T. M. Hussein",
    "body": "",
    "tags": [],
    "pub_date": "",
    "type": "research"
  },
  {
    "id": "teaching-network-analysis-fundamentals-pydata-2015",
    "url": "/teaching/network-analysis-fundamentals-pydata-2015/",
    "title": "Network Analysis Fundamentals | PyData 2015",
    "summary": "Think about data as connections rather than just points\u2014reveal patterns that traditional analysis misses.",
    "body": "youtube #### url: https://www.youtube.com/watch?v=wcrwASR5DCQ description #### text: Networks are everywhere\u2014from social media to protein interactions to your morning commute. In this tutorial, I'll show you how to think about data as connections rather than just points, revealing patterns that traditional analysis misses. You'll learn to see the hidden structure in your data and discover insights that only emerge when you start thinking in terms of relationships.",
    "tags": [],
    "pub_date": "2015-12-04",
    "type": "teaching"
  },
  {
    "id": "books-network-analysis-made-simple",
    "url": "/books/network-analysis-made-simple/",
    "title": "Network Analysis Made Simple",
    "summary": "The eBook companion to my tutorial series Network Analysis Made Simple.",
    "body": "book #### url: https://leanpub.com/nams notebooks #### url: https://ericmjl.github.io/Network-Analysis-Made-Simple",
    "tags": [],
    "pub_date": "",
    "type": "books"
  },
  {
    "id": "teaching-network-analysis-made-simple",
    "url": "/teaching/network-analysis-made-simple/",
    "title": "Network Analysis Made Simple",
    "summary": "My take on teaching network analysis and graph theory concepts, using NetworkX. Taught at many conferences since 2015.",
    "body": "description #### text: I taught myself graph theory in graduate school, as a tool for analyzing influenza evolutionary trajectories. Borrowing the theme of Allen Downey's \"X Made Simple\" series, I have started my own Network Analysis Made Simple series of Jupyter notebooks, to share this knowledge freely with everybody. notebooks #### url: https://ericmjl.github.io/Network-Analysis-Made-Simple youtube #### url: https://www.youtube.com/watch?v=ED4NZ-4EWRw book #### url: https://leanpub.com/nams",
    "tags": [],
    "pub_date": "2019-07-16",
    "type": "teaching"
  },
  {
    "id": "teaching-network-analysis-made-simple-eric-ma-mridul-seth-scipy-2022",
    "url": "/teaching/network-analysis-made-simple-eric-ma-mridul-seth-scipy-2022/",
    "title": "Network Analysis Made Simple | SciPy 2022",
    "summary": "Extract meaningful insights from connected data with practical tools and a new way of thinking about your data.",
    "body": "youtube #### url: https://www.youtube.com/watch?v=LTnsaD-rXW4 description #### text: Mridul and I team up to demystify network analysis, showing you how to extract meaningful insights from connected data. We'll cover everything from basic graph theory to advanced community detection, with plenty of real-world examples. Whether you're analyzing social networks, biological systems, or any kind of relational data, you'll leave with practical tools and a new way of thinking about your data.",
    "tags": [],
    "pub_date": "2022-08-01",
    "type": "teaching"
  },
  {
    "id": "teaching-network-analysis-made-simple-scipy-2025",
    "url": "/teaching/network-analysis-made-simple-scipy-2025/",
    "title": "Network Analysis Made Simple | SciPy 2025",
    "summary": "Think about data as connections rather than just points\u2014reveal patterns that traditional analysis misses.",
    "body": "youtube #### url: https://www.youtube.com/watch?v=r0k_cMxqva0 description #### text: Networks are everywhere\u2014from social media to protein interactions to your morning commute. In this tutorial, I'll show you how to think about data as connections rather than just points, revealing patterns that traditional analysis misses. You'll learn to see the hidden structure in your data and discover insights that only emerge when you start thinking in terms of relationships.",
    "tags": [],
    "pub_date": "2025-08-01",
    "type": "teaching"
  },
  {
    "id": "teaching-network-science-and-statistics-fundamentals-and-applications-scipy-2017",
    "url": "/teaching/network-science-and-statistics-fundamentals-and-applications-scipy-2017/",
    "title": "Network Science and Statistics - Fundamentals and Applications | SciPy 2017",
    "summary": "Where it all started\u2014thinking about data as networks to reveal hidden patterns in social systems and biological networks.",
    "body": "youtube #### url: https://www.youtube.com/watch?v=KilcBIgaQ-8 description #### text: This is where it all started\u2014my first deep dive into network science at SciPy 2017. I'll show you how to think about data as networks, from the basics of graph theory to advanced statistical methods. You'll discover how network thinking can reveal hidden patterns in everything from social systems to biological networks, with plenty of hands-on examples to make it stick.",
    "tags": [],
    "pub_date": "2017-08-05",
    "type": "teaching"
  },
  {
    "id": "talks-networks-networks-everywhere",
    "url": "/talks/networks-networks-everywhere/",
    "title": "Networks, Networks Everywhere | Big Data Boston 2016",
    "summary": "The many places that graphs can be found, delivered at Big Data Boston 2016 and hosted by DataCamp.",
    "body": "description #### text: In this talk, I describe how networks and their applications are ubiquitous, and can be used to solve problems that are otherwise difficult to reason about. youtube #### url: https://www.youtube.com/watch?v=TuaAlpfTeZs slides #### url: https://ericmjl.github.io/big-data-boston-2016/",
    "tags": [],
    "pub_date": "2017-03-10",
    "type": "talks"
  },
  {
    "id": "open-source-nxviz",
    "url": "/open-source/nxviz/",
    "title": "nxviz",
    "summary": "Creator and core developer.",
    "body": "description #### text: I am consolidating my Hive Plot and Circos Plot implementations into a single visualization package. Design goals include: - A rational naming system (where not already existent) for each plot. - Hive Plots - Circos Plots - Panel Plots (with Horizontal/Vertical orientations) - Arc Plots - ...and more! - A declarative API for each type of plot, the user to delcare: - Node positioning - Node size (area, radius) - Node colour - Edge colour - Edge linewidth Contributors include [Jon][1], [Leo][2], and [Nelson][3] - very happy to be working with them! [1]: http://jonchar.net/ [2]: https://github.com/leotrs [3]: http://nelsonliu.me/ github #### url: https://github.com/ericmjl/nxviz/",
    "tags": [],
    "pub_date": "",
    "type": "open-source"
  },
  {
    "id": "projects-open-hardware-for-wildlife-monitoring",
    "url": "/projects/open-hardware-for-wildlife-monitoring/",
    "title": "Open Hardware for Wildlife Monitoring",
    "summary": "I helped make cheap cameras that continuously monitor wildlife.",
    "body": "description #### text: Zoonotic infections in humans originate in wild animals. To better understand their contact structure, I have been developing an open source hardware and software kit for monitoring wild animal behaviour using the Raspberry Pi, Python, and 3D printing. The time-lapse cameras, which we call TikiCams (they look like Hawaiian lamps when mounted), are based on off-the-shelf hardware available at computer and hardware stores. Videos: - Video 1 - Video 2 - Video 3 Images: - Hanging cameras - Laptop + Pi in shed - Tikicams in the wild - image 1 - Tikicams in the wild - image 2 - Selfie with the Tikis github #### url: https://github.com/ericmjl/tikicam-config",
    "tags": [],
    "pub_date": "",
    "type": "projects"
  },
  {
    "id": "open-source",
    "url": "/open-source/",
    "title": "Open Source",
    "summary": "",
    "body": "",
    "tags": [],
    "pub_date": "",
    "type": "open-source"
  },
  {
    "id": "talks-productive-patterns-for-agent-assisted-programming-pydata-boston-2025",
    "url": "/talks/productive-patterns-for-agent-assisted-programming-pydata-boston-2025/",
    "title": "Productive Patterns for Agent-Assisted Programming | PyData Boston 2025",
    "summary": "Productive Patterns for Agent-Assisted Programming",
    "body": "youtube #### url: https://www.youtube.com/watch?v=XXR2pYGNgs description #### text: Live stream for my PyData Boston 2025 talk on Productive Patterns for Agent-Assisted Programming. youtube #### url: https://www.youtube.com/watch?v=YuA9x1aSZ4 description #### text: Official PyData channel recording.",
    "tags": [],
    "pub_date": "2025-12-10",
    "type": "talks"
  },
  {
    "id": "projects",
    "url": "/projects/",
    "title": "Projects",
    "summary": "",
    "body": "",
    "tags": [],
    "pub_date": "",
    "type": "projects"
  },
  {
    "id": "open-source-pyjanitor",
    "url": "/open-source/pyjanitor/",
    "title": "pyjanitor",
    "summary": "Creator and core developer. Convenient APIs for data cleaning.",
    "body": "description #### text: I developed , an open source port of the R package janitor, to bring friendly APIs to data cleaning tasks for data scientists. It features an API that helps data scientists express their data cleaning/preprocessing pipeline in a verb-based, fluent fashion. github #### url: https://github.com/ericmjl/pyjanitor",
    "tags": [],
    "pub_date": "",
    "type": "open-source"
  },
  {
    "id": "talks-pyjanitor-clean-apis-for-cleaning-data-scipy-2019",
    "url": "/talks/pyjanitor-clean-apis-for-cleaning-data-scipy-2019/",
    "title": "pyjanitor: Clean APIs for Cleaning Data | SciPy 2019",
    "summary": "Transforming data preprocessing from a chore into poetry with method chaining that makes your code readable and maintainable.",
    "body": "youtube #### url: https://www.youtube.com/watch?v=sSIT0rJh2OM description #### text: Data cleaning doesn't have to be a nightmare. I created pyjanitor because I was tired of writing ugly, unreadable pandas code. This talk shows you how method chaining can make your data preprocessing feel like poetry instead of a chore. You'll see how a simple API change can transform your entire data science workflow.",
    "tags": [],
    "pub_date": "2019-07-13",
    "type": "talks"
  },
  {
    "id": "open-source-pymc3",
    "url": "/open-source/pymc3/",
    "title": "PyMC",
    "summary": "Contributed bug fixes pertaining to the GPU, documentation, and provided a definition of the \"mode\" for the Weibull distribution.",
    "body": "description #### text: I have made multiple PRs to PyMC, which were bug fixes, documentation and small feature additions. Bug Fix I found a bug in PyMC's multinomial random variate sampler, related to floating point precision issues while moving numbers from the GPU to the CPU, when working on my Bayesian analysis recipes repository. Specifically, we get probability values that sum to infinitesimally larger than one. I thus submitted a patch that fixed that converts the probabilities to precision and re-normalizes the probabilities to 1, before using them for random sampling. Documentation I contributed small changes to the docs, to make it super clear that precision and standard deviation parameterizations were alternate parameterizations, and only one was needed. Feature Addition I contributed the definition of a \"mode\" for the Weibull distribution, so that I could use it in a Mixture Weibull. resource #### label: Pull requests url: https://github.com/pymc-devs/pymc3/pulls?utf8=%E2%9C%93&q=is%3Apr+author%3Aericmjl+",
    "tags": [],
    "pub_date": "",
    "type": "open-source"
  },
  {
    "id": "research",
    "url": "/research/",
    "title": "Research",
    "summary": "",
    "body": "",
    "tags": [],
    "pub_date": "",
    "type": "research"
  },
  {
    "id": "talks-software-testing-in-open-source-and-data-science",
    "url": "/talks/software-testing-in-open-source-and-data-science/",
    "title": "Software Testing in Open Source and Data Science | Data Umbrella",
    "summary": "In which I talk about how software testing is awesome for open source and data science.",
    "body": "youtube #### url: https://www.youtube.com/watch?v=bJGgVoV4GTc",
    "tags": [],
    "pub_date": "2022-08-30",
    "type": "talks"
  },
  {
    "id": "talks",
    "url": "/talks/",
    "title": "Talks",
    "summary": "",
    "body": "",
    "tags": [],
    "pub_date": "",
    "type": "talks"
  },
  {
    "id": "teaching",
    "url": "/teaching/",
    "title": "Teaching",
    "summary": "",
    "body": "",
    "tags": [],
    "pub_date": "",
    "type": "teaching"
  },
  {
    "id": "talks-testing-for-data-scientists",
    "url": "/talks/testing-for-data-scientists/",
    "title": "Testing for Data Scientists | PyData Ann Arbor 2020",
    "summary": "How data scientists can incorporate testing into their development, delivered at PyData Ann Arbor in Jan 2020.",
    "body": "description #### text: In this talk, I use two examples to illustrate how testing can be better incorporated into a data scientist's workflow. At the same time, I also introduce two libraries, Hypothesis and Great Expectations, which can help with testing code and data. slides #### url: https://ericmjl.github.io/testing-for-data-scientists/ youtube #### url: https://www.youtube.com/v=5RKuHvZERLY",
    "tags": [],
    "pub_date": "2020-01-06",
    "type": "talks"
  },
  {
    "id": "user-manual",
    "url": "/user-manual/",
    "title": "User Manual",
    "summary": "A guide to working well with me. Inspired by Jason Liu. Read me to work well with me This is a short guide to how I operate and how we can work well...",
    "body": "A guide to working well with me. Inspired by Jason Liu. Read me to work well with me This is a short guide to how I operate and how we can work well together. Consider it a set of default assumptions to help you navigate working with me. If anything feels off or doesn\u2019t work for you, let\u2019s talk. What you can expect from me I don\u2019t like exercising power. I\u2019ll rarely invoke authority. I prefer to convince through reason. When I push strongly, it\u2019s usually because I believe the cost of inaction is high. I care about clarity and momentum. I try to help maintain clear priorities, reduce friction, and ship valuable work regularly. I believe in autonomy and learning. You\u2019ll have room to explore your own ideas. Sometimes I\u2019ll let you repeat paths I\u2019ve taken if I think there\u2019s value in learning them firsthand. If there\u2019s a rabbit hole, I\u2019ll warn you about it. I won\u2019t stop you from exploring it, but I\u2019ll value it even more if you take the warning seriously and learn from my experience before going down that path. New missteps are encouraged. Repeated ones are annoying \u2014 I\u2019ll call them out. I value thoughtfulness. I appreciate intentionality \u2014 in code, in communication, and in how we spend time. Fast is good, but deliberate is better. What I expect from you Own your craft. Whether you\u2019re junior or senior, I expect you to take pride in your work and keep learning. Stay sharp. Competency and curiosity are baseline. Show the code when we're on technical matters. I expect to see code, not vague descriptions of what the code does. Code speaks loud and can surface assumptions. If we're working on something technical, bring the actual code into the conversation. Ship things that matter. Shipping things matters. You don\u2019t need to wait for perfection. Shipping regularly builds trust, generates feedback, and accelerates learning. I care that we ship stuff out to our customers and collaborators. And I care that what we ship matters. Master micro-efficiencies. Keyboard shortcuts. Smart tooling. Command-line magic. These things compound. They make you faster and earn you respect. For my complete collection of shell aliases, see my data science bootstrap notes. Cultivate your network. Find a few people inside or outside your team whose feedback you trust and whose knowledge you can draw from. Learning doesn\u2019t have to be top-down. Interview externally once a year. It\u2019s a healthy habit. For everyone, it helps keep your interviewing muscle strong and gives you a clearer view of the industry. If you\u2019re working directly with me and I\u2019m your line manager, feel free to share \u2014 it gives me a sense of your ambitions. I treat it as data, not disloyalty. My quirks I think in outlines. Structure helps me process faster. Clearly articulated thoughts matter most. If you\u2019re prepping for a meeting with me, consider sending your thoughts written down ahead-of-time. That helps me help you. I like efficiency, but not at the cost of care. I love fast loops and tight cycles. I also value stepping back to think through edge cases, consequences, and tradeoffs. I admire useful craft. If you show me a better way to do something (especially something repeatable), I'll probably adopt it \u2014 and remember that you taught me. If you're junior and seeking advice Come prepared. If you're looking for guidance or mentorship, I deeply appreciate when you've thought through your questions in advance. Preparation signals intentionality and helps us make the most of our time together. Aim for value. I want our conversation to be worth your while. Bring your real challenges, uncertainties, or opportunities. I\u2019ll do my best to offer insight \u2014 or help you discover it. Take what inspires. Not everything I say will apply directly to your situation. That\u2019s okay. I often speak from experience, and sometimes it\u2019s just meant to spark ideas. Inspiration matters. The more sources you draw from, the more creative your thinking can become. My beliefs about tools and AI Use AI where it helps you move faster or think better. You don't need permission. AI can be an extension of your brain \u2014 a fast, tireless assistant. Use it well. AI-generated meeting prep is great. If AI helps you organize your thoughts and clearly articulate what you're thinking \u2014 especially for meetings or check-ins \u2014 go for it. I welcome written-down thoughts ahead of time, even if they're transcribed or structured with AI. If that makes you more effective, I'm all for it. I expect you to be using AI to write code. If so, you've spent less time labouring over the code, and there's less reason for ego to be tied to it. I want us to be able to talk about broader design choices that AI can then implement for you. When we discuss, I want us to explore plausible technical paths, not defend a single implementation. Postface Let\u2019s build something great together :)",
    "tags": [],
    "pub_date": "",
    "type": "user-manual"
  },
  {
    "id": "talks-using-bayesian-methods-in-bio-sciences-with-eric-ma",
    "url": "/talks/using-bayesian-methods-in-bio-sciences-with-eric-ma/",
    "title": "Using Bayesian Methods in Bio-Sciences | PyMC Labs",
    "summary": "How Bayesian thinking can handle the complexity and uncertainty inherent in biological systems\u2014better statistics for better science.",
    "body": "youtube #### url: https://www.youtube.com/watch?v=29KUakl8ZvY description #### text: Biology is messy, and traditional statistics often fall short. In this deep dive, I explore how Bayesian methods can handle the complexity and uncertainty inherent in biological systems. You'll see real examples from my work where Bayesian thinking led to insights that frequentist approaches missed entirely. It's not just about better statistics\u2014it's about better science.",
    "tags": [],
    "pub_date": "2023-06-13",
    "type": "talks"
  },
  {
    "id": "home",
    "url": "/",
    "title": "Welcome!",
    "summary": "Hi! I'm Eric, and I lead Research Data Science in the Data Science and AI group at Moderna Therapeutics (July 2021).",
    "body": "Hi! I'm Eric, and I lead Research Data Science in the Data Science and AI group at Moderna Therapeutics (July 2021). Prior to that, I was part of a special ops data team at the Novartis Institutes for Biomedical Research's Informatics department (Sep 2017). Prior to that, I was an Insight Health Data Sciences Fellow, and defended my doctoral thesis in the Department of Biological Engineering at MIT earlier in 2017. My life's motto is quoted from the Gospel of Luke: From everyone who has been given much, much will be demanded; and from the one who has been entrusted with much, much more will be asked. Luke 12:48 I am an open source contributor and developer, a speaker and educator at Python conferences. I also have authored two pedagogical Data Science books. My professional mission is to make science run at the speed of thought so that we can discover new medicines for patients faster. Professionally, I am on the Editorial Review Board of the Journal of Machine Learning Research, and have served on the organizing committees for PyCon and SciPy conferences as part of the conferences' respective Financial Aid teams. If you'd like to connect, please feel free to send me a Shortmail message. When I update my blog, I usually send out an accompanying Substack post. To stay up-to-date on what I'm writing about, please consider subscribing there. And if you'd like to sponsor the coffee that helps me make the content, please consider using GitHub Sponsors. For the curious, this website was hand-crafted with \u2764\ufe0f using Terminal.css and Lektor. Font is Berkeley Mono by the US Graphics Company.",
    "tags": [],
    "pub_date": "",
    "type": "home"
  }
]